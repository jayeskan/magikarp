{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# load data to pandas\n",
    "# train\n",
    "# tune parameters on holdout set\n",
    "# test\n",
    "# kfolds cross validation\n",
    "#######\n",
    "# comparisons:\n",
    "# linear regression\n",
    "# decision trees (random forest via scikit learn)\n",
    "# ff neural network (lin, relu, lin) higher order polyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "from core_insure.assessor.home_assessor import HomeAssessor\n",
    "from ruamel.yaml import YAML\n",
    "import numpy as np\n",
    "import math\n",
    "from torch import nn\n",
    "\n",
    "LABEL_KEY = 'claimAmount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing and training methods\n",
    "def value_of_interest(value):\n",
    "    if pd.isna(value):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_row_xy(row):\n",
    "    y_label = row[LABEL_KEY] if value_of_interest(row[LABEL_KEY]) else 0\n",
    "    row = row.drop([LABEL_KEY])\n",
    "    rowiter = row.iteritems()\n",
    "    x_array = [item[1] if value_of_interest(item[1]) else 0 for item in rowiter]\n",
    "    return x_array, y_label\n",
    "\n",
    "def get_all_xy(df):\n",
    "    all_x = []\n",
    "    all_y = []\n",
    "    for row in df.iterrows():\n",
    "        row_contents = row[1]\n",
    "        x, y = get_row_xy(row_contents)\n",
    "        all_x.append(x)\n",
    "        all_y.append([y])\n",
    "    return all_x, all_y\n",
    "\n",
    "def train_calculate_test_loss(curr_assessor, tr_x, tr_y, te_x, te_y, v_x, v_y):\n",
    "    curr_assessor.train(tr_x, tr_y, v_x, v_y)\n",
    "    model = curr_assessor.model\n",
    "    \n",
    "    # Calculate test loss\n",
    "    te_y_pred = model.eval(te_x)\n",
    "    test_loss = model.loss(model._torch_var(te_y_pred), model._torch_var(te_y))\n",
    "    print(f'Test Loss: {test_loss}')\n",
    "    \n",
    "    return test_loss\n",
    "\n",
    "def train_test_val_split(curr_dataframe, train_percent, test_percent, val_percent):\n",
    "    total_rows = curr_dataframe.shape[0]\n",
    "    print(f'Total test set: {total_rows}')\n",
    "    train_size = math.floor(total_rows*(train_percent/100))\n",
    "    test_size = math.floor(total_rows*(test_percent/100))\n",
    "    val_size = math.floor(total_rows*(val_percent/100))\n",
    "    print(f'Training set: {train_size}, Testing set: {test_size}, Validation set: {val_size}')\n",
    "    train_x, train_y = get_all_xy(curr_dataframe.iloc[0:train_size])\n",
    "    print(f'Train set created, size: {len(train_x)}')\n",
    "    last_test_index = train_size + test_size\n",
    "    test_x, test_y = get_all_xy(curr_dataframe.iloc[train_size:last_test_index])\n",
    "    print(f'Test set created, size: {len(test_x)}')\n",
    "    last_val_index = last_test_index + val_size\n",
    "    val_x, val_y = get_all_xy(curr_dataframe.iloc[last_test_index:last_val_index])\n",
    "    print(f'Val set created, size: {len(val_x)}')\n",
    "    return {\n",
    "        'train_x': train_x,\n",
    "        'train_y': train_y,\n",
    "        'test_x': test_x,\n",
    "        'test_y': test_y,\n",
    "        'val_x': val_x,\n",
    "        'val_y': val_y\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisajiang/magikarp/core_insure/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (31,32,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Load data to pandas\n",
    "# This dataset was downloaded from: \n",
    "# https://www.fema.gov/openfema-dataset-individual-assistance-housing-registrants-large-disasters-v1\n",
    "file = open('core_insure/data/IndividualAssistanceHousingRegistrantsLargeDisasters.csv', 'r')\n",
    "pd_data = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ordereddict([('base', ordereddict([('filepath', '.')])), ('data', ordereddict([('dbname', 'postgres'), ('host', 'localhost'), ('port', 5432)])), ('celery', ordereddict([('broker_url', 'amqp://localhost')])), ('assessor', ordereddict([('model', 'linear_regression'), ('linear_regression', ordereddict([('output_size', 1), ('lr', 1e-05), ('momentum', 0), ('epochs', 500)])), ('simple_nn', ordereddict([('output_size', 1), ('lr', 1e-05), ('epochs', 500), ('hidden_size', 50)])), ('filepath', '.')]))])\n"
     ]
    }
   ],
   "source": [
    "# Get config\n",
    "default_config_file = open('core_insure/config.yaml', 'r')\n",
    "yaml = YAML()\n",
    "config = yaml.load(default_config_file)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['censusBlockId', 'censusYear', 'claimAmount', 'damagedZipCode', 'destroyed', 'disasterNumber', 'floodDamage', 'floodInsurance', 'foundationDamage', 'foundationDamageAmount', 'grossIncome', 'habitabilityRepairsRequired', 'homeOwnersInsurance', 'householdComposition', 'inspected', 'personalPropertyEligible', 'ppfvl', 'primaryResidence', 'rentalAssistanceAmount', 'rentalAssistanceEligible', 'rentalResourceZipCode', 'repairAmount', 'repairAssistanceEligible', 'replacementAmount', 'replacementAssistanceEligible', 'roofDamage', 'roofDamageAmount', 'rpfvl', 'sbaEligible', 'specialNeeds', 'tsaCheckedIn', 'tsaEligible', 'waterLevel']\n"
     ]
    }
   ],
   "source": [
    "# Data processing\n",
    "# Columns available\n",
    "all_columns = list(pd_data.columns.values)\n",
    "\n",
    "# calculate label: claimAmount based on repairs and replacements\n",
    "pd_data = pd_data.fillna(0)\n",
    "pd_data[LABEL_KEY] = pd_data['repairAmount'] + pd_data['replacementAmount']\n",
    "\n",
    "# For now, filter on just numerical data\n",
    "# Later, can convert string columns (embedding, bins, num categories)\n",
    "only_numerical_data = pd_data._get_numeric_data()\n",
    "sorted_numerical_df = pd_data.reindex(sorted(only_numerical_data.columns), axis=1)\n",
    "numeric_columns = sorted(sorted_numerical_df.columns.values)\n",
    "print(list(numeric_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277242"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all data where repairs were not made\n",
    "nonzero_numerical_df = sorted_numerical_df.replace(0, np.nan)\n",
    "nonzero_numerical_df = nonzero_numerical_df[nonzero_numerical_df[LABEL_KEY].notnull()] \n",
    "len(nonzero_numerical_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "columns = [\n",
    "    'roofDamageAmount',\n",
    "    'foundationDamageAmount',\n",
    "    'floodDamageAmount',\n",
    "    'waterLevel',\n",
    "    'rentalAssistanceAmount',\n",
    "    'ppfvl',\n",
    "    'rpfvl',\n",
    "    LABEL_KEY\n",
    "]\n",
    "final_sorted_df_nz = pd.DataFrame(nonzero_numerical_df, columns=columns)\n",
    "final_sorted_df = pd.DataFrame(sorted_numerical_df, columns=columns)\n",
    "\n",
    "final_sorted_df_nz = final_sorted_df_nz.replace(np.nan, 0)\n",
    "final_sorted_df = final_sorted_df.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test set: 100000\n",
      "Training set: 70000, Testing set: 20000, Validation set: 10000\n",
      "Train set created\n",
      "Test set created\n",
      "Val set created\n",
      "Total test set: 100000\n",
      "Training set: 70000, Testing set: 20000, Validation set: 10000\n",
      "Train set created\n",
      "Test set created\n",
      "Val set created\n"
     ]
    }
   ],
   "source": [
    "# 80-20 Split\n",
    "# Full dataset of all claims\n",
    "full_dataset = final_sorted_df.head(100000)\n",
    "train_test_val_dataset = train_test_val_split(full_dataset, 70, 20, 10)\n",
    "\n",
    "# Subset of only approved claims\n",
    "full_dataset_nz = final_sorted_df_nz.head(100000)\n",
    "train_test_val_dataset_nz = train_test_val_split(full_dataset_nz, 70, 20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear regression - ALL CLAIMS\n",
    "assessor_config =  config['assessor']\n",
    "assessor_config['linear_regression']['epochs'] = 1000\n",
    "assessor = HomeAssessor(assessor_config)\n",
    "\n",
    "train_calculate_test_loss(assessor, \n",
    "                          train_test_val_dataset['train_x'], \n",
    "                          train_test_val_dataset['train_y'],\n",
    "                          train_test_val_dataset['test_x'],\n",
    "                          train_test_val_dataset['test_y'],\n",
    "                          train_test_val_dataset['val_x'],\n",
    "                          train_test_val_dataset['val_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train_Loss: 6272.978515625, Val_Loss: 6690.60498046875\n",
      "Epoch 1, Train_Loss: 6253.2080078125, Val_Loss: 6669.0888671875\n",
      "Epoch 2, Train_Loss: 6233.55712890625, Val_Loss: 6647.59912109375\n",
      "Epoch 3, Train_Loss: 6213.60986328125, Val_Loss: 6626.13916015625\n",
      "Epoch 4, Train_Loss: 6193.828125, Val_Loss: 6604.6171875\n",
      "Epoch 5, Train_Loss: 6174.05224609375, Val_Loss: 6583.150390625\n",
      "Epoch 6, Train_Loss: 6154.3896484375, Val_Loss: 6561.6767578125\n",
      "Epoch 7, Train_Loss: 6134.6845703125, Val_Loss: 6540.20068359375\n",
      "Epoch 8, Train_Loss: 6114.9873046875, Val_Loss: 6518.74755859375\n",
      "Epoch 9, Train_Loss: 6095.2724609375, Val_Loss: 6497.275390625\n",
      "Epoch 10, Train_Loss: 6075.57080078125, Val_Loss: 6475.81982421875\n",
      "Epoch 11, Train_Loss: 6055.857421875, Val_Loss: 6454.37744140625\n",
      "Epoch 12, Train_Loss: 6036.18212890625, Val_Loss: 6432.93017578125\n",
      "Epoch 13, Train_Loss: 6016.34814453125, Val_Loss: 6411.44580078125\n",
      "Epoch 14, Train_Loss: 5996.703125, Val_Loss: 6390.01416015625\n",
      "Epoch 15, Train_Loss: 5976.87890625, Val_Loss: 6368.591796875\n",
      "Epoch 16, Train_Loss: 5957.15673828125, Val_Loss: 6347.1396484375\n",
      "Epoch 17, Train_Loss: 5937.5341796875, Val_Loss: 6325.73974609375\n",
      "Epoch 18, Train_Loss: 5917.93310546875, Val_Loss: 6304.3642578125\n",
      "Epoch 19, Train_Loss: 5898.44580078125, Val_Loss: 6283.0595703125\n",
      "Epoch 20, Train_Loss: 5879.00439453125, Val_Loss: 6261.91259765625\n",
      "Epoch 21, Train_Loss: 5859.853515625, Val_Loss: 6240.9931640625\n",
      "Epoch 22, Train_Loss: 5840.59814453125, Val_Loss: 6220.12353515625\n",
      "Epoch 23, Train_Loss: 5821.7333984375, Val_Loss: 6199.3505859375\n",
      "Epoch 24, Train_Loss: 5802.91455078125, Val_Loss: 6178.658203125\n",
      "Epoch 25, Train_Loss: 5783.98193359375, Val_Loss: 6158.02587890625\n",
      "Epoch 26, Train_Loss: 5765.30517578125, Val_Loss: 6137.50830078125\n",
      "Epoch 27, Train_Loss: 5746.693359375, Val_Loss: 6117.05322265625\n",
      "Epoch 28, Train_Loss: 5728.150390625, Val_Loss: 6096.693359375\n",
      "Epoch 29, Train_Loss: 5709.7275390625, Val_Loss: 6076.4833984375\n",
      "Epoch 30, Train_Loss: 5691.43896484375, Val_Loss: 6056.357421875\n",
      "Epoch 31, Train_Loss: 5673.29150390625, Val_Loss: 6036.30859375\n",
      "Epoch 32, Train_Loss: 5654.96240234375, Val_Loss: 6016.3564453125\n",
      "Epoch 33, Train_Loss: 5636.87060546875, Val_Loss: 5996.47216796875\n",
      "Epoch 34, Train_Loss: 5618.87646484375, Val_Loss: 5976.6904296875\n",
      "Epoch 35, Train_Loss: 5600.91162109375, Val_Loss: 5956.98388671875\n",
      "Epoch 36, Train_Loss: 5583.24609375, Val_Loss: 5937.466796875\n",
      "Epoch 37, Train_Loss: 5565.60400390625, Val_Loss: 5918.09814453125\n",
      "Epoch 38, Train_Loss: 5548.052734375, Val_Loss: 5898.83447265625\n",
      "Epoch 39, Train_Loss: 5530.591796875, Val_Loss: 5879.69970703125\n",
      "Epoch 40, Train_Loss: 5513.3017578125, Val_Loss: 5860.66943359375\n",
      "Epoch 41, Train_Loss: 5496.21435546875, Val_Loss: 5841.72216796875\n",
      "Epoch 42, Train_Loss: 5479.02001953125, Val_Loss: 5822.87451171875\n",
      "Epoch 43, Train_Loss: 5462.0390625, Val_Loss: 5804.19140625\n",
      "Epoch 44, Train_Loss: 5445.19970703125, Val_Loss: 5785.556640625\n",
      "Epoch 45, Train_Loss: 5428.52685546875, Val_Loss: 5767.09521484375\n",
      "Epoch 46, Train_Loss: 5412.001953125, Val_Loss: 5748.72021484375\n",
      "Epoch 47, Train_Loss: 5395.6015625, Val_Loss: 5730.5166015625\n",
      "Epoch 48, Train_Loss: 5379.23828125, Val_Loss: 5712.5078125\n",
      "Epoch 49, Train_Loss: 5362.97607421875, Val_Loss: 5694.5849609375\n",
      "Epoch 50, Train_Loss: 5346.99658203125, Val_Loss: 5676.85302734375\n",
      "Epoch 51, Train_Loss: 5330.921875, Val_Loss: 5659.2724609375\n",
      "Epoch 52, Train_Loss: 5315.14794921875, Val_Loss: 5641.80419921875\n",
      "Epoch 53, Train_Loss: 5299.50732421875, Val_Loss: 5624.4404296875\n",
      "Epoch 54, Train_Loss: 5283.775390625, Val_Loss: 5607.18359375\n",
      "Epoch 55, Train_Loss: 5268.2451171875, Val_Loss: 5590.0625\n",
      "Epoch 56, Train_Loss: 5252.87646484375, Val_Loss: 5573.08984375\n",
      "Epoch 57, Train_Loss: 5237.64892578125, Val_Loss: 5556.27587890625\n",
      "Epoch 58, Train_Loss: 5222.43359375, Val_Loss: 5539.58056640625\n",
      "Epoch 59, Train_Loss: 5207.3564453125, Val_Loss: 5522.982421875\n",
      "Epoch 60, Train_Loss: 5192.44921875, Val_Loss: 5506.52099609375\n",
      "Epoch 61, Train_Loss: 5177.48828125, Val_Loss: 5490.1962890625\n",
      "Epoch 62, Train_Loss: 5162.794921875, Val_Loss: 5473.97412109375\n",
      "Epoch 63, Train_Loss: 5148.26806640625, Val_Loss: 5457.80322265625\n",
      "Epoch 64, Train_Loss: 5133.63330078125, Val_Loss: 5441.79345703125\n",
      "Epoch 65, Train_Loss: 5119.158203125, Val_Loss: 5425.857421875\n",
      "Epoch 66, Train_Loss: 5104.84619140625, Val_Loss: 5410.0947265625\n",
      "Epoch 67, Train_Loss: 5090.52197265625, Val_Loss: 5394.37109375\n",
      "Epoch 68, Train_Loss: 5076.35791015625, Val_Loss: 5378.6884765625\n",
      "Epoch 69, Train_Loss: 5062.326171875, Val_Loss: 5363.17822265625\n",
      "Epoch 70, Train_Loss: 5048.5830078125, Val_Loss: 5347.88134765625\n",
      "Epoch 71, Train_Loss: 5034.7216796875, Val_Loss: 5332.6904296875\n",
      "Epoch 72, Train_Loss: 5021.14453125, Val_Loss: 5317.5400390625\n",
      "Epoch 73, Train_Loss: 5007.716796875, Val_Loss: 5302.4892578125\n",
      "Epoch 74, Train_Loss: 4994.2724609375, Val_Loss: 5287.61572265625\n",
      "Epoch 75, Train_Loss: 4980.86962890625, Val_Loss: 5272.82275390625\n",
      "Epoch 76, Train_Loss: 4967.693359375, Val_Loss: 5258.11962890625\n",
      "Epoch 77, Train_Loss: 4954.49853515625, Val_Loss: 5243.404296875\n",
      "Epoch 78, Train_Loss: 4941.36767578125, Val_Loss: 5228.70458984375\n",
      "Epoch 79, Train_Loss: 4928.35693359375, Val_Loss: 5214.12255859375\n",
      "Epoch 80, Train_Loss: 4915.3837890625, Val_Loss: 5199.60009765625\n",
      "Epoch 81, Train_Loss: 4902.38818359375, Val_Loss: 5185.07080078125\n",
      "Epoch 82, Train_Loss: 4889.48046875, Val_Loss: 5170.69970703125\n",
      "Epoch 83, Train_Loss: 4876.923828125, Val_Loss: 5156.32373046875\n",
      "Epoch 84, Train_Loss: 4864.16552734375, Val_Loss: 5142.0234375\n",
      "Epoch 85, Train_Loss: 4851.541015625, Val_Loss: 5127.7421875\n",
      "Epoch 86, Train_Loss: 4839.173828125, Val_Loss: 5113.54541015625\n",
      "Epoch 87, Train_Loss: 4826.71044921875, Val_Loss: 5099.474609375\n",
      "Epoch 88, Train_Loss: 4814.33837890625, Val_Loss: 5085.4921875\n",
      "Epoch 89, Train_Loss: 4801.98388671875, Val_Loss: 5071.37353515625\n",
      "Epoch 90, Train_Loss: 4789.64208984375, Val_Loss: 5057.32958984375\n",
      "Epoch 91, Train_Loss: 4777.23681640625, Val_Loss: 5043.3115234375\n",
      "Epoch 92, Train_Loss: 4764.8662109375, Val_Loss: 5029.26904296875\n",
      "Epoch 93, Train_Loss: 4752.51953125, Val_Loss: 5015.29296875\n",
      "Epoch 94, Train_Loss: 4740.2421875, Val_Loss: 5001.3232421875\n",
      "Epoch 95, Train_Loss: 4728.09619140625, Val_Loss: 4987.3974609375\n",
      "Epoch 96, Train_Loss: 4716.02490234375, Val_Loss: 4973.63330078125\n",
      "Epoch 97, Train_Loss: 4703.884765625, Val_Loss: 4959.9169921875\n",
      "Epoch 98, Train_Loss: 4691.8349609375, Val_Loss: 4946.2919921875\n",
      "Epoch 99, Train_Loss: 4679.875, Val_Loss: 4932.71337890625\n",
      "Epoch 100, Train_Loss: 4667.9462890625, Val_Loss: 4919.23779296875\n",
      "Epoch 101, Train_Loss: 4656.14697265625, Val_Loss: 4905.78076171875\n",
      "Epoch 102, Train_Loss: 4644.47265625, Val_Loss: 4892.38818359375\n",
      "Epoch 103, Train_Loss: 4632.77197265625, Val_Loss: 4879.08447265625\n",
      "Epoch 104, Train_Loss: 4621.1220703125, Val_Loss: 4865.841796875\n",
      "Epoch 105, Train_Loss: 4609.58984375, Val_Loss: 4852.66064453125\n",
      "Epoch 106, Train_Loss: 4598.16650390625, Val_Loss: 4839.5537109375\n",
      "Epoch 107, Train_Loss: 4586.7138671875, Val_Loss: 4826.49609375\n",
      "Epoch 108, Train_Loss: 4575.46484375, Val_Loss: 4813.59130859375\n",
      "Epoch 109, Train_Loss: 4564.23193359375, Val_Loss: 4800.71337890625\n",
      "Epoch 110, Train_Loss: 4553.12109375, Val_Loss: 4787.92578125\n",
      "Epoch 111, Train_Loss: 4542.11962890625, Val_Loss: 4775.15966796875\n",
      "Epoch 112, Train_Loss: 4531.0771484375, Val_Loss: 4762.49951171875\n",
      "Epoch 113, Train_Loss: 4520.20703125, Val_Loss: 4749.87939453125\n",
      "Epoch 114, Train_Loss: 4509.5107421875, Val_Loss: 4737.3017578125\n",
      "Epoch 115, Train_Loss: 4498.73095703125, Val_Loss: 4724.81640625\n",
      "Epoch 116, Train_Loss: 4488.109375, Val_Loss: 4712.38037109375\n",
      "Epoch 117, Train_Loss: 4477.72265625, Val_Loss: 4700.0654296875\n",
      "Epoch 118, Train_Loss: 4467.30322265625, Val_Loss: 4687.8017578125\n",
      "Epoch 119, Train_Loss: 4457.0439453125, Val_Loss: 4675.6376953125\n",
      "Epoch 120, Train_Loss: 4446.92724609375, Val_Loss: 4663.55029296875\n",
      "Epoch 121, Train_Loss: 4436.77197265625, Val_Loss: 4651.58203125\n",
      "Epoch 122, Train_Loss: 4426.80078125, Val_Loss: 4639.79052734375\n",
      "Epoch 123, Train_Loss: 4416.90087890625, Val_Loss: 4628.10302734375\n",
      "Epoch 124, Train_Loss: 4407.056640625, Val_Loss: 4616.4833984375\n",
      "Epoch 125, Train_Loss: 4397.31884765625, Val_Loss: 4604.9736328125\n",
      "Epoch 126, Train_Loss: 4387.76708984375, Val_Loss: 4593.65185546875\n",
      "Epoch 127, Train_Loss: 4378.26904296875, Val_Loss: 4582.4169921875\n",
      "Epoch 128, Train_Loss: 4368.86279296875, Val_Loss: 4571.3408203125\n",
      "Epoch 129, Train_Loss: 4359.73681640625, Val_Loss: 4560.48779296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130, Train_Loss: 4350.6474609375, Val_Loss: 4549.7392578125\n",
      "Epoch 131, Train_Loss: 4341.63818359375, Val_Loss: 4539.11181640625\n",
      "Epoch 132, Train_Loss: 4332.82177734375, Val_Loss: 4528.60888671875\n",
      "Epoch 133, Train_Loss: 4324.08154296875, Val_Loss: 4518.2158203125\n",
      "Epoch 134, Train_Loss: 4315.4990234375, Val_Loss: 4507.99853515625\n",
      "Epoch 135, Train_Loss: 4307.0302734375, Val_Loss: 4497.87939453125\n",
      "Epoch 136, Train_Loss: 4298.49853515625, Val_Loss: 4487.76953125\n",
      "Epoch 137, Train_Loss: 4290.06298828125, Val_Loss: 4477.68603515625\n",
      "Epoch 138, Train_Loss: 4281.66845703125, Val_Loss: 4467.65966796875\n",
      "Epoch 139, Train_Loss: 4273.5029296875, Val_Loss: 4457.84912109375\n",
      "Epoch 140, Train_Loss: 4265.4072265625, Val_Loss: 4448.15576171875\n",
      "Epoch 141, Train_Loss: 4257.4521484375, Val_Loss: 4438.63623046875\n",
      "Epoch 142, Train_Loss: 4249.44580078125, Val_Loss: 4429.15771484375\n",
      "Epoch 143, Train_Loss: 4241.6171875, Val_Loss: 4419.82177734375\n",
      "Epoch 144, Train_Loss: 4233.9990234375, Val_Loss: 4410.63037109375\n",
      "Epoch 145, Train_Loss: 4226.45703125, Val_Loss: 4401.57177734375\n",
      "Epoch 146, Train_Loss: 4218.97314453125, Val_Loss: 4392.65380859375\n",
      "Epoch 147, Train_Loss: 4211.62939453125, Val_Loss: 4383.892578125\n",
      "Epoch 148, Train_Loss: 4204.45556640625, Val_Loss: 4375.1591796875\n",
      "Epoch 149, Train_Loss: 4197.52197265625, Val_Loss: 4366.54736328125\n",
      "Epoch 150, Train_Loss: 4190.5888671875, Val_Loss: 4358.060546875\n",
      "Epoch 151, Train_Loss: 4183.7412109375, Val_Loss: 4349.75439453125\n",
      "Epoch 152, Train_Loss: 4177.00146484375, Val_Loss: 4341.56591796875\n",
      "Epoch 153, Train_Loss: 4170.4345703125, Val_Loss: 4333.54931640625\n",
      "Epoch 154, Train_Loss: 4164.00048828125, Val_Loss: 4325.6494140625\n",
      "Epoch 155, Train_Loss: 4157.6826171875, Val_Loss: 4317.890625\n",
      "Epoch 156, Train_Loss: 4151.40625, Val_Loss: 4310.2861328125\n",
      "Epoch 157, Train_Loss: 4145.50732421875, Val_Loss: 4302.80224609375\n",
      "Epoch 158, Train_Loss: 4139.59619140625, Val_Loss: 4295.4716796875\n",
      "Epoch 159, Train_Loss: 4133.8681640625, Val_Loss: 4288.2958984375\n",
      "Epoch 160, Train_Loss: 4128.2548828125, Val_Loss: 4281.2353515625\n",
      "Epoch 161, Train_Loss: 4122.6396484375, Val_Loss: 4274.3623046875\n",
      "Epoch 162, Train_Loss: 4117.3955078125, Val_Loss: 4267.58837890625\n",
      "Epoch 163, Train_Loss: 4112.0830078125, Val_Loss: 4261.0849609375\n",
      "Epoch 164, Train_Loss: 4106.98583984375, Val_Loss: 4254.73388671875\n",
      "Epoch 165, Train_Loss: 4102.15380859375, Val_Loss: 4248.62060546875\n",
      "Epoch 166, Train_Loss: 4097.25244140625, Val_Loss: 4242.69482421875\n",
      "Epoch 167, Train_Loss: 4092.628662109375, Val_Loss: 4236.927734375\n",
      "Epoch 168, Train_Loss: 4088.069580078125, Val_Loss: 4231.279296875\n",
      "Epoch 169, Train_Loss: 4083.630126953125, Val_Loss: 4225.8291015625\n",
      "Epoch 170, Train_Loss: 4079.438720703125, Val_Loss: 4220.53759765625\n",
      "Epoch 171, Train_Loss: 4075.127685546875, Val_Loss: 4215.40576171875\n",
      "Epoch 172, Train_Loss: 4071.08837890625, Val_Loss: 4210.38330078125\n",
      "Epoch 173, Train_Loss: 4067.0380859375, Val_Loss: 4205.4521484375\n",
      "Epoch 174, Train_Loss: 4063.279052734375, Val_Loss: 4200.68017578125\n",
      "Epoch 175, Train_Loss: 4059.58349609375, Val_Loss: 4196.12451171875\n",
      "Epoch 176, Train_Loss: 4055.911865234375, Val_Loss: 4191.7265625\n",
      "Epoch 177, Train_Loss: 4052.3427734375, Val_Loss: 4187.51220703125\n",
      "Epoch 178, Train_Loss: 4048.98974609375, Val_Loss: 4183.4150390625\n",
      "Epoch 179, Train_Loss: 4045.748046875, Val_Loss: 4179.474609375\n",
      "Epoch 180, Train_Loss: 4042.61767578125, Val_Loss: 4175.63720703125\n",
      "Epoch 181, Train_Loss: 4039.430908203125, Val_Loss: 4171.91552734375\n",
      "Epoch 182, Train_Loss: 4036.48583984375, Val_Loss: 4168.265625\n",
      "Epoch 183, Train_Loss: 4033.5419921875, Val_Loss: 4164.73291015625\n",
      "Epoch 184, Train_Loss: 4030.70068359375, Val_Loss: 4161.2998046875\n",
      "Epoch 185, Train_Loss: 4027.978515625, Val_Loss: 4157.94384765625\n",
      "Epoch 186, Train_Loss: 4025.465576171875, Val_Loss: 4154.67138671875\n",
      "Epoch 187, Train_Loss: 4022.843017578125, Val_Loss: 4151.5078125\n",
      "Epoch 188, Train_Loss: 4020.498291015625, Val_Loss: 4148.50732421875\n",
      "Epoch 189, Train_Loss: 4018.10791015625, Val_Loss: 4145.6044921875\n",
      "Epoch 190, Train_Loss: 4015.910888671875, Val_Loss: 4142.806640625\n",
      "Epoch 191, Train_Loss: 4013.774169921875, Val_Loss: 4140.1240234375\n",
      "Epoch 192, Train_Loss: 4011.728515625, Val_Loss: 4137.62451171875\n",
      "Epoch 193, Train_Loss: 4009.688232421875, Val_Loss: 4135.232421875\n",
      "Epoch 194, Train_Loss: 4007.809814453125, Val_Loss: 4132.93603515625\n",
      "Epoch 195, Train_Loss: 4005.94921875, Val_Loss: 4130.65966796875\n",
      "Epoch 196, Train_Loss: 4004.191162109375, Val_Loss: 4128.50048828125\n",
      "Epoch 197, Train_Loss: 4002.49365234375, Val_Loss: 4126.390625\n",
      "Epoch 198, Train_Loss: 4000.876708984375, Val_Loss: 4124.35205078125\n",
      "Epoch 199, Train_Loss: 3999.44140625, Val_Loss: 4122.36279296875\n",
      "Epoch 200, Train_Loss: 3997.76416015625, Val_Loss: 4120.470703125\n",
      "Epoch 201, Train_Loss: 3996.3232421875, Val_Loss: 4118.6376953125\n",
      "Epoch 202, Train_Loss: 3994.976806640625, Val_Loss: 4116.86376953125\n",
      "Epoch 203, Train_Loss: 3993.7646484375, Val_Loss: 4115.1728515625\n",
      "Epoch 204, Train_Loss: 3992.453369140625, Val_Loss: 4113.5595703125\n",
      "Epoch 205, Train_Loss: 3991.309814453125, Val_Loss: 4112.0234375\n",
      "Epoch 206, Train_Loss: 3990.210693359375, Val_Loss: 4110.5537109375\n",
      "Epoch 207, Train_Loss: 3989.164794921875, Val_Loss: 4109.17529296875\n",
      "Epoch 208, Train_Loss: 3988.115234375, Val_Loss: 4107.841796875\n",
      "Epoch 209, Train_Loss: 3987.158447265625, Val_Loss: 4106.60302734375\n",
      "Epoch 210, Train_Loss: 3986.2080078125, Val_Loss: 4105.4150390625\n",
      "Epoch 211, Train_Loss: 3985.356689453125, Val_Loss: 4104.28515625\n",
      "Epoch 212, Train_Loss: 3984.49462890625, Val_Loss: 4103.19775390625\n",
      "Epoch 213, Train_Loss: 3983.675537109375, Val_Loss: 4102.162109375\n",
      "Epoch 214, Train_Loss: 3982.965576171875, Val_Loss: 4101.18701171875\n",
      "Epoch 215, Train_Loss: 3982.281982421875, Val_Loss: 4100.26318359375\n",
      "Epoch 216, Train_Loss: 3981.59912109375, Val_Loss: 4099.38818359375\n",
      "Epoch 217, Train_Loss: 3980.86572265625, Val_Loss: 4098.51171875\n",
      "Epoch 218, Train_Loss: 3980.2734375, Val_Loss: 4097.7294921875\n",
      "Epoch 219, Train_Loss: 3979.77099609375, Val_Loss: 4096.96240234375\n",
      "Epoch 220, Train_Loss: 3979.16162109375, Val_Loss: 4096.3017578125\n",
      "Epoch 221, Train_Loss: 3978.6875, Val_Loss: 4095.609619140625\n",
      "Epoch 222, Train_Loss: 3978.17822265625, Val_Loss: 4094.94873046875\n",
      "Epoch 223, Train_Loss: 3977.711181640625, Val_Loss: 4094.30908203125\n",
      "Epoch 224, Train_Loss: 3977.25537109375, Val_Loss: 4093.75634765625\n",
      "Epoch 225, Train_Loss: 3976.893798828125, Val_Loss: 4093.1767578125\n",
      "Epoch 226, Train_Loss: 3976.478271484375, Val_Loss: 4092.6552734375\n",
      "Epoch 227, Train_Loss: 3976.10693359375, Val_Loss: 4092.140869140625\n",
      "Epoch 228, Train_Loss: 3975.823974609375, Val_Loss: 4091.6083984375\n",
      "Epoch 229, Train_Loss: 3975.250244140625, Val_Loss: 4091.1572265625\n",
      "Epoch 230, Train_Loss: 3974.943603515625, Val_Loss: 4090.71923828125\n",
      "Epoch 231, Train_Loss: 3974.661865234375, Val_Loss: 4090.30029296875\n",
      "Epoch 232, Train_Loss: 3974.361572265625, Val_Loss: 4089.897216796875\n",
      "Epoch 233, Train_Loss: 3974.1005859375, Val_Loss: 4089.50390625\n",
      "Epoch 234, Train_Loss: 3973.81201171875, Val_Loss: 4089.1923828125\n",
      "Epoch 235, Train_Loss: 3973.688232421875, Val_Loss: 4088.839111328125\n",
      "Epoch 236, Train_Loss: 3973.373779296875, Val_Loss: 4088.551513671875\n",
      "Epoch 237, Train_Loss: 3973.18896484375, Val_Loss: 4088.248046875\n",
      "Epoch 238, Train_Loss: 3973.072021484375, Val_Loss: 4088.018310546875\n",
      "Epoch 239, Train_Loss: 3972.901123046875, Val_Loss: 4087.77392578125\n",
      "Epoch 240, Train_Loss: 3972.7021484375, Val_Loss: 4087.5380859375\n",
      "Epoch 241, Train_Loss: 3972.606201171875, Val_Loss: 4087.3271484375\n",
      "Epoch 242, Train_Loss: 3972.477294921875, Val_Loss: 4087.087890625\n",
      "Epoch 243, Train_Loss: 3972.3291015625, Val_Loss: 4086.910400390625\n",
      "Epoch 244, Train_Loss: 3972.269287109375, Val_Loss: 4086.76318359375\n",
      "Epoch 245, Train_Loss: 3972.177734375, Val_Loss: 4086.5859375\n",
      "Epoch 246, Train_Loss: 3971.909423828125, Val_Loss: 4086.450439453125\n",
      "Epoch 247, Train_Loss: 3971.83642578125, Val_Loss: 4086.293212890625\n",
      "Epoch 248, Train_Loss: 3971.803955078125, Val_Loss: 4086.17431640625\n",
      "Epoch 249, Train_Loss: 3971.72900390625, Val_Loss: 4086.050048828125\n",
      "Epoch 250, Train_Loss: 3971.654296875, Val_Loss: 4085.923583984375\n",
      "Epoch 251, Train_Loss: 3971.60546875, Val_Loss: 4085.8671875\n",
      "Epoch 252, Train_Loss: 3971.53564453125, Val_Loss: 4085.767578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 253, Train_Loss: 3971.524658203125, Val_Loss: 4085.703125\n",
      "Epoch 254, Train_Loss: 3971.494873046875, Val_Loss: 4085.59033203125\n",
      "Epoch 255, Train_Loss: 3971.42041015625, Val_Loss: 4085.552490234375\n",
      "Epoch 256, Train_Loss: 3971.424072265625, Val_Loss: 4085.4755859375\n",
      "Epoch 257, Train_Loss: 3971.36767578125, Val_Loss: 4085.415283203125\n",
      "Epoch 258, Train_Loss: 3971.361328125, Val_Loss: 4085.3408203125\n",
      "Epoch 259, Train_Loss: 3971.33935546875, Val_Loss: 4085.2548828125\n",
      "Epoch 260, Train_Loss: 3971.31982421875, Val_Loss: 4085.222412109375\n",
      "Epoch 261, Train_Loss: 3971.2685546875, Val_Loss: 4085.184814453125\n",
      "Epoch 262, Train_Loss: 3971.259765625, Val_Loss: 4085.1484375\n",
      "Epoch 263, Train_Loss: 3971.237548828125, Val_Loss: 4085.113525390625\n",
      "Epoch 264, Train_Loss: 3971.2060546875, Val_Loss: 4085.05029296875\n",
      "Epoch 265, Train_Loss: 3971.180419921875, Val_Loss: 4085.031982421875\n",
      "Epoch 266, Train_Loss: 3971.1884765625, Val_Loss: 4085.0263671875\n",
      "Epoch 267, Train_Loss: 3971.177978515625, Val_Loss: 4084.998046875\n",
      "Epoch 268, Train_Loss: 3971.177978515625, Val_Loss: 4084.963623046875\n",
      "Epoch 269, Train_Loss: 3971.179931640625, Val_Loss: 4084.94873046875\n",
      "Epoch 270, Train_Loss: 3971.19140625, Val_Loss: 4084.91162109375\n",
      "Epoch 271, Train_Loss: 3971.21142578125, Val_Loss: 4084.89990234375\n",
      "Epoch 272, Train_Loss: 3971.18115234375, Val_Loss: 4084.859619140625\n",
      "Epoch 273, Train_Loss: 3971.19873046875, Val_Loss: 4084.830322265625\n",
      "Epoch 274, Train_Loss: 3971.128662109375, Val_Loss: 4084.8427734375\n",
      "Epoch 275, Train_Loss: 3971.140625, Val_Loss: 4084.80908203125\n",
      "Epoch 276, Train_Loss: 3971.135498046875, Val_Loss: 4084.79443359375\n",
      "Epoch 277, Train_Loss: 3971.15478515625, Val_Loss: 4084.78759765625\n",
      "Epoch 278, Train_Loss: 3971.099853515625, Val_Loss: 4084.768310546875\n",
      "Epoch 279, Train_Loss: 3971.0400390625, Val_Loss: 4084.769287109375\n",
      "Epoch 280, Train_Loss: 3971.0615234375, Val_Loss: 4084.749267578125\n",
      "Epoch 281, Train_Loss: 3971.1103515625, Val_Loss: 4084.736328125\n",
      "Epoch 282, Train_Loss: 3971.125, Val_Loss: 4084.730712890625\n",
      "Epoch 283, Train_Loss: 3971.13232421875, Val_Loss: 4084.720458984375\n",
      "Epoch 284, Train_Loss: 3971.146484375, Val_Loss: 4084.706298828125\n",
      "Epoch 285, Train_Loss: 3971.140625, Val_Loss: 4084.6962890625\n",
      "Epoch 286, Train_Loss: 3971.114013671875, Val_Loss: 4084.702880859375\n",
      "Epoch 287, Train_Loss: 3971.0986328125, Val_Loss: 4084.6552734375\n",
      "Epoch 288, Train_Loss: 3971.106201171875, Val_Loss: 4084.65234375\n",
      "Epoch 289, Train_Loss: 3971.1044921875, Val_Loss: 4084.61962890625\n",
      "Epoch 290, Train_Loss: 3971.11279296875, Val_Loss: 4084.6220703125\n",
      "Epoch 291, Train_Loss: 3971.085693359375, Val_Loss: 4084.539306640625\n",
      "Epoch 292, Train_Loss: 3971.07470703125, Val_Loss: 4084.538818359375\n",
      "Epoch 293, Train_Loss: 3971.0556640625, Val_Loss: 4084.545166015625\n",
      "Epoch 294, Train_Loss: 3971.064208984375, Val_Loss: 4084.534912109375\n",
      "Epoch 295, Train_Loss: 3971.06201171875, Val_Loss: 4084.533935546875\n",
      "Epoch 296, Train_Loss: 3971.06201171875, Val_Loss: 4084.530029296875\n",
      "Epoch 297, Train_Loss: 3971.056396484375, Val_Loss: 4084.512451171875\n",
      "Epoch 298, Train_Loss: 3971.052734375, Val_Loss: 4084.50634765625\n",
      "Epoch 299, Train_Loss: 3971.052734375, Val_Loss: 4084.498046875\n",
      "Epoch 300, Train_Loss: 3971.052734375, Val_Loss: 4084.49951171875\n",
      "Epoch 301, Train_Loss: 3971.052001953125, Val_Loss: 4084.498291015625\n",
      "Epoch 302, Train_Loss: 3971.043212890625, Val_Loss: 4084.492431640625\n",
      "Epoch 303, Train_Loss: 3971.041015625, Val_Loss: 4084.4931640625\n",
      "Epoch 304, Train_Loss: 3971.02587890625, Val_Loss: 4084.489990234375\n",
      "Epoch 305, Train_Loss: 3971.023193359375, Val_Loss: 4084.4892578125\n",
      "Epoch 306, Train_Loss: 3971.00390625, Val_Loss: 4084.4931640625\n",
      "Epoch 307, Train_Loss: 3971.001953125, Val_Loss: 4084.486328125\n",
      "Epoch 308, Train_Loss: 3970.998046875, Val_Loss: 4084.48291015625\n",
      "Epoch 309, Train_Loss: 3970.994384765625, Val_Loss: 4084.48193359375\n",
      "Epoch 310, Train_Loss: 3970.993896484375, Val_Loss: 4084.479248046875\n",
      "Epoch 311, Train_Loss: 3970.99609375, Val_Loss: 4084.476806640625\n",
      "Epoch 312, Train_Loss: 3970.9951171875, Val_Loss: 4084.470458984375\n",
      "Epoch 313, Train_Loss: 3970.99072265625, Val_Loss: 4084.465576171875\n",
      "Epoch 314, Train_Loss: 3970.990234375, Val_Loss: 4084.458740234375\n",
      "Epoch 315, Train_Loss: 3970.9892578125, Val_Loss: 4084.456298828125\n",
      "Epoch 316, Train_Loss: 3970.98095703125, Val_Loss: 4084.455078125\n",
      "Epoch 317, Train_Loss: 3970.978759765625, Val_Loss: 4084.451904296875\n",
      "Epoch 318, Train_Loss: 3970.974609375, Val_Loss: 4084.447265625\n",
      "Epoch 319, Train_Loss: 3970.972900390625, Val_Loss: 4084.44482421875\n",
      "Epoch 320, Train_Loss: 3970.96728515625, Val_Loss: 4084.438720703125\n",
      "Epoch 321, Train_Loss: 3970.92333984375, Val_Loss: 4084.410888671875\n",
      "Epoch 322, Train_Loss: 3970.966796875, Val_Loss: 4084.381591796875\n",
      "Epoch 323, Train_Loss: 3970.98291015625, Val_Loss: 4084.376708984375\n",
      "Epoch 324, Train_Loss: 3970.99609375, Val_Loss: 4084.3447265625\n",
      "Epoch 325, Train_Loss: 3970.962646484375, Val_Loss: 4084.348876953125\n",
      "Epoch 326, Train_Loss: 3970.95263671875, Val_Loss: 4084.330078125\n",
      "Epoch 327, Train_Loss: 3970.95361328125, Val_Loss: 4084.327880859375\n",
      "Epoch 328, Train_Loss: 3970.947998046875, Val_Loss: 4084.31884765625\n",
      "Epoch 329, Train_Loss: 3970.95458984375, Val_Loss: 4084.322021484375\n",
      "Epoch 330, Train_Loss: 3970.93994140625, Val_Loss: 4084.327880859375\n",
      "Epoch 331, Train_Loss: 3970.969970703125, Val_Loss: 4084.32275390625\n",
      "Epoch 332, Train_Loss: 3970.96630859375, Val_Loss: 4084.337890625\n",
      "Epoch 333, Train_Loss: 3970.937255859375, Val_Loss: 4084.3369140625\n",
      "Epoch 334, Train_Loss: 3970.93798828125, Val_Loss: 4084.341552734375\n",
      "Epoch 335, Train_Loss: 3970.8681640625, Val_Loss: 4084.36279296875\n",
      "Epoch 336, Train_Loss: 3970.8876953125, Val_Loss: 4084.346435546875\n",
      "Epoch 337, Train_Loss: 3970.8955078125, Val_Loss: 4084.376708984375\n",
      "Epoch 338, Train_Loss: 3970.88916015625, Val_Loss: 4084.35009765625\n",
      "Epoch 339, Train_Loss: 3970.863037109375, Val_Loss: 4084.348388671875\n",
      "Epoch 340, Train_Loss: 3970.9111328125, Val_Loss: 4084.326416015625\n",
      "Epoch 341, Train_Loss: 3970.91064453125, Val_Loss: 4084.308837890625\n",
      "Epoch 342, Train_Loss: 3970.915771484375, Val_Loss: 4084.311279296875\n",
      "Epoch 343, Train_Loss: 3970.93212890625, Val_Loss: 4084.28515625\n",
      "Epoch 344, Train_Loss: 3970.93017578125, Val_Loss: 4084.277099609375\n",
      "Epoch 345, Train_Loss: 3970.921630859375, Val_Loss: 4084.270751953125\n",
      "Epoch 346, Train_Loss: 3970.91845703125, Val_Loss: 4084.268310546875\n",
      "Epoch 347, Train_Loss: 3970.91796875, Val_Loss: 4084.263671875\n",
      "Epoch 348, Train_Loss: 3970.91064453125, Val_Loss: 4084.27001953125\n",
      "Epoch 349, Train_Loss: 3970.871826171875, Val_Loss: 4084.249267578125\n",
      "Epoch 350, Train_Loss: 3970.849365234375, Val_Loss: 4084.255126953125\n",
      "Epoch 351, Train_Loss: 3970.84521484375, Val_Loss: 4084.249267578125\n",
      "Epoch 352, Train_Loss: 3970.84619140625, Val_Loss: 4084.250732421875\n",
      "Epoch 353, Train_Loss: 3970.794921875, Val_Loss: 4084.255126953125\n",
      "Epoch 354, Train_Loss: 3970.768798828125, Val_Loss: 4084.2724609375\n",
      "Epoch 355, Train_Loss: 3970.806396484375, Val_Loss: 4084.253173828125\n",
      "Epoch 356, Train_Loss: 3970.791748046875, Val_Loss: 4084.247314453125\n",
      "Epoch 357, Train_Loss: 3970.778564453125, Val_Loss: 4084.267578125\n",
      "Epoch 358, Train_Loss: 3970.78662109375, Val_Loss: 4084.26171875\n",
      "Epoch 359, Train_Loss: 3970.799072265625, Val_Loss: 4084.277099609375\n",
      "Epoch 360, Train_Loss: 3970.78271484375, Val_Loss: 4084.266845703125\n",
      "Epoch 361, Train_Loss: 3970.76708984375, Val_Loss: 4084.263671875\n",
      "Epoch 362, Train_Loss: 3970.7685546875, Val_Loss: 4084.263916015625\n",
      "Epoch 363, Train_Loss: 3970.759765625, Val_Loss: 4084.2568359375\n",
      "Epoch 364, Train_Loss: 3970.762939453125, Val_Loss: 4084.2548828125\n",
      "Epoch 365, Train_Loss: 3970.762451171875, Val_Loss: 4084.249267578125\n",
      "Epoch 366, Train_Loss: 3970.757080078125, Val_Loss: 4084.249267578125\n",
      "Epoch 367, Train_Loss: 3970.757568359375, Val_Loss: 4084.246337890625\n",
      "Epoch 368, Train_Loss: 3970.751953125, Val_Loss: 4084.23876953125\n",
      "Epoch 369, Train_Loss: 3970.742431640625, Val_Loss: 4084.23193359375\n",
      "Epoch 370, Train_Loss: 3970.741943359375, Val_Loss: 4084.229248046875\n",
      "Epoch 371, Train_Loss: 3970.736572265625, Val_Loss: 4084.226806640625\n",
      "Epoch 372, Train_Loss: 3970.740234375, Val_Loss: 4084.217529296875\n",
      "Epoch 373, Train_Loss: 3970.7451171875, Val_Loss: 4084.21728515625\n",
      "Epoch 374, Train_Loss: 3970.733642578125, Val_Loss: 4084.217529296875\n",
      "Epoch 375, Train_Loss: 3970.73291015625, Val_Loss: 4084.1640625\n",
      "Epoch 376, Train_Loss: 3970.73193359375, Val_Loss: 4084.15771484375\n",
      "Epoch 377, Train_Loss: 3970.71728515625, Val_Loss: 4084.139892578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 378, Train_Loss: 3970.697998046875, Val_Loss: 4084.138916015625\n",
      "Epoch 379, Train_Loss: 3970.6953125, Val_Loss: 4084.13720703125\n",
      "Epoch 380, Train_Loss: 3970.69482421875, Val_Loss: 4084.114501953125\n",
      "Epoch 381, Train_Loss: 3970.692626953125, Val_Loss: 4084.110107421875\n",
      "Epoch 382, Train_Loss: 3970.689453125, Val_Loss: 4084.113525390625\n",
      "Epoch 383, Train_Loss: 3970.6875, Val_Loss: 4084.102294921875\n",
      "Epoch 384, Train_Loss: 3970.6865234375, Val_Loss: 4084.09765625\n",
      "Epoch 385, Train_Loss: 3970.678955078125, Val_Loss: 4084.0908203125\n",
      "Epoch 386, Train_Loss: 3970.67431640625, Val_Loss: 4084.089599609375\n",
      "Epoch 387, Train_Loss: 3970.671142578125, Val_Loss: 4084.080322265625\n",
      "Epoch 388, Train_Loss: 3970.6669921875, Val_Loss: 4084.078369140625\n",
      "Epoch 389, Train_Loss: 3970.664794921875, Val_Loss: 4084.07470703125\n",
      "Epoch 390, Train_Loss: 3970.66064453125, Val_Loss: 4084.072021484375\n",
      "Epoch 391, Train_Loss: 3970.6591796875, Val_Loss: 4084.073974609375\n",
      "Epoch 392, Train_Loss: 3970.65087890625, Val_Loss: 4084.071533203125\n",
      "Epoch 393, Train_Loss: 3970.618408203125, Val_Loss: 4084.07275390625\n",
      "Epoch 394, Train_Loss: 3970.611572265625, Val_Loss: 4084.06640625\n",
      "Epoch 395, Train_Loss: 3970.609375, Val_Loss: 4084.05712890625\n",
      "Epoch 396, Train_Loss: 3970.603515625, Val_Loss: 4084.06005859375\n",
      "Epoch 397, Train_Loss: 3970.599365234375, Val_Loss: 4084.063720703125\n",
      "Epoch 398, Train_Loss: 3970.591552734375, Val_Loss: 4084.056396484375\n",
      "Epoch 399, Train_Loss: 3970.5654296875, Val_Loss: 4083.991943359375\n",
      "Epoch 400, Train_Loss: 3970.56494140625, Val_Loss: 4083.990478515625\n",
      "Epoch 401, Train_Loss: 3970.57421875, Val_Loss: 4083.985595703125\n",
      "Epoch 402, Train_Loss: 3970.569091796875, Val_Loss: 4083.976806640625\n",
      "Epoch 403, Train_Loss: 3970.568603515625, Val_Loss: 4083.97412109375\n",
      "Epoch 404, Train_Loss: 3970.5654296875, Val_Loss: 4083.969482421875\n",
      "Epoch 405, Train_Loss: 3970.56005859375, Val_Loss: 4083.96484375\n",
      "Epoch 406, Train_Loss: 3970.556396484375, Val_Loss: 4083.961669921875\n",
      "Epoch 407, Train_Loss: 3970.552734375, Val_Loss: 4083.95361328125\n",
      "Epoch 408, Train_Loss: 3970.54296875, Val_Loss: 4083.945556640625\n",
      "Epoch 409, Train_Loss: 3970.535400390625, Val_Loss: 4083.93310546875\n",
      "Epoch 410, Train_Loss: 3970.517578125, Val_Loss: 4083.916748046875\n",
      "Epoch 411, Train_Loss: 3970.50830078125, Val_Loss: 4083.846435546875\n",
      "Epoch 412, Train_Loss: 3970.525634765625, Val_Loss: 4083.84033203125\n",
      "Epoch 413, Train_Loss: 3970.53125, Val_Loss: 4083.824462890625\n",
      "Epoch 414, Train_Loss: 3970.53857421875, Val_Loss: 4083.819580078125\n",
      "Epoch 415, Train_Loss: 3970.53662109375, Val_Loss: 4083.813232421875\n",
      "Epoch 416, Train_Loss: 3970.48046875, Val_Loss: 4083.81884765625\n",
      "Epoch 417, Train_Loss: 3970.37890625, Val_Loss: 4083.79443359375\n",
      "Epoch 418, Train_Loss: 3970.368896484375, Val_Loss: 4083.779296875\n",
      "Epoch 419, Train_Loss: 3970.369384765625, Val_Loss: 4083.772705078125\n",
      "Epoch 420, Train_Loss: 3970.368408203125, Val_Loss: 4083.78125\n",
      "Epoch 421, Train_Loss: 3970.360595703125, Val_Loss: 4083.771240234375\n",
      "Epoch 422, Train_Loss: 3970.403564453125, Val_Loss: 4083.779296875\n",
      "Epoch 423, Train_Loss: 3970.42919921875, Val_Loss: 4083.773681640625\n",
      "Epoch 424, Train_Loss: 3970.427001953125, Val_Loss: 4083.7783203125\n",
      "Epoch 425, Train_Loss: 3970.35888671875, Val_Loss: 4083.80712890625\n",
      "Epoch 426, Train_Loss: 3970.390869140625, Val_Loss: 4083.84326171875\n",
      "Epoch 427, Train_Loss: 3970.35986328125, Val_Loss: 4083.853515625\n",
      "Epoch 428, Train_Loss: 3970.353759765625, Val_Loss: 4083.85791015625\n",
      "Epoch 429, Train_Loss: 3970.345947265625, Val_Loss: 4083.859130859375\n",
      "Epoch 430, Train_Loss: 3970.349365234375, Val_Loss: 4083.8466796875\n",
      "Epoch 431, Train_Loss: 3970.349609375, Val_Loss: 4083.840087890625\n",
      "Epoch 432, Train_Loss: 3970.35107421875, Val_Loss: 4083.77197265625\n",
      "Epoch 433, Train_Loss: 3970.3662109375, Val_Loss: 4083.746826171875\n",
      "Epoch 434, Train_Loss: 3970.377197265625, Val_Loss: 4083.750732421875\n",
      "Epoch 435, Train_Loss: 3970.376220703125, Val_Loss: 4083.73388671875\n",
      "Epoch 436, Train_Loss: 3970.374755859375, Val_Loss: 4083.718505859375\n",
      "Epoch 437, Train_Loss: 3970.374755859375, Val_Loss: 4083.71435546875\n",
      "Epoch 438, Train_Loss: 3970.36669921875, Val_Loss: 4083.70751953125\n",
      "Epoch 439, Train_Loss: 3970.355224609375, Val_Loss: 4083.717529296875\n",
      "Epoch 440, Train_Loss: 3970.353271484375, Val_Loss: 4083.720703125\n",
      "Epoch 441, Train_Loss: 3970.35009765625, Val_Loss: 4083.709228515625\n",
      "Epoch 442, Train_Loss: 3970.291748046875, Val_Loss: 4083.724365234375\n",
      "Epoch 443, Train_Loss: 3970.248779296875, Val_Loss: 4083.719970703125\n",
      "Epoch 444, Train_Loss: 3970.198974609375, Val_Loss: 4083.69921875\n",
      "Epoch 445, Train_Loss: 3970.22314453125, Val_Loss: 4083.697509765625\n",
      "Epoch 446, Train_Loss: 3970.188720703125, Val_Loss: 4083.686767578125\n",
      "Epoch 447, Train_Loss: 3970.175048828125, Val_Loss: 4083.681640625\n",
      "Epoch 448, Train_Loss: 3970.174072265625, Val_Loss: 4083.675537109375\n",
      "Epoch 449, Train_Loss: 3970.2060546875, Val_Loss: 4083.651611328125\n",
      "Epoch 450, Train_Loss: 3970.2021484375, Val_Loss: 4083.647216796875\n",
      "Epoch 451, Train_Loss: 3970.205322265625, Val_Loss: 4083.64990234375\n",
      "Epoch 452, Train_Loss: 3970.19970703125, Val_Loss: 4083.64208984375\n",
      "Epoch 453, Train_Loss: 3970.195556640625, Val_Loss: 4083.642333984375\n",
      "Epoch 454, Train_Loss: 3970.18798828125, Val_Loss: 4083.6298828125\n",
      "Epoch 455, Train_Loss: 3970.183837890625, Val_Loss: 4083.629150390625\n",
      "Epoch 456, Train_Loss: 3970.146240234375, Val_Loss: 4083.64404296875\n",
      "Epoch 457, Train_Loss: 3970.139892578125, Val_Loss: 4083.638427734375\n",
      "Epoch 458, Train_Loss: 3970.17919921875, Val_Loss: 4083.62451171875\n",
      "Epoch 459, Train_Loss: 3970.162353515625, Val_Loss: 4083.61474609375\n",
      "Epoch 460, Train_Loss: 3970.15087890625, Val_Loss: 4083.600830078125\n",
      "Epoch 461, Train_Loss: 3970.145751953125, Val_Loss: 4083.593505859375\n",
      "Epoch 462, Train_Loss: 3970.148681640625, Val_Loss: 4083.589599609375\n",
      "Epoch 463, Train_Loss: 3970.1435546875, Val_Loss: 4083.5927734375\n",
      "Epoch 464, Train_Loss: 3970.13671875, Val_Loss: 4083.54833984375\n",
      "Epoch 465, Train_Loss: 3970.080810546875, Val_Loss: 4083.564697265625\n",
      "Epoch 466, Train_Loss: 3970.076416015625, Val_Loss: 4083.555908203125\n",
      "Epoch 467, Train_Loss: 3970.0703125, Val_Loss: 4083.547607421875\n",
      "Epoch 468, Train_Loss: 3970.074951171875, Val_Loss: 4083.53271484375\n",
      "Epoch 469, Train_Loss: 3970.0498046875, Val_Loss: 4083.5400390625\n",
      "Epoch 470, Train_Loss: 3970.040283203125, Val_Loss: 4083.53515625\n",
      "Epoch 471, Train_Loss: 3970.074462890625, Val_Loss: 4083.515625\n",
      "Epoch 472, Train_Loss: 3970.08447265625, Val_Loss: 4083.5\n",
      "Epoch 473, Train_Loss: 3970.07861328125, Val_Loss: 4083.505615234375\n",
      "Epoch 474, Train_Loss: 3970.0703125, Val_Loss: 4083.493896484375\n",
      "Epoch 475, Train_Loss: 3970.056640625, Val_Loss: 4083.489501953125\n",
      "Epoch 476, Train_Loss: 3970.05859375, Val_Loss: 4083.485595703125\n",
      "Epoch 477, Train_Loss: 3970.056640625, Val_Loss: 4083.486328125\n",
      "Epoch 478, Train_Loss: 3970.05712890625, Val_Loss: 4083.474365234375\n",
      "Epoch 479, Train_Loss: 3970.032470703125, Val_Loss: 4083.468017578125\n",
      "Epoch 480, Train_Loss: 3970.03515625, Val_Loss: 4083.4736328125\n",
      "Epoch 481, Train_Loss: 3969.9775390625, Val_Loss: 4083.47998046875\n",
      "Epoch 482, Train_Loss: 3969.9638671875, Val_Loss: 4083.492431640625\n",
      "Epoch 483, Train_Loss: 3969.9912109375, Val_Loss: 4083.483154296875\n",
      "Epoch 484, Train_Loss: 3969.882080078125, Val_Loss: 4083.40869140625\n",
      "Epoch 485, Train_Loss: 3969.87841796875, Val_Loss: 4083.390869140625\n",
      "Epoch 486, Train_Loss: 3969.911865234375, Val_Loss: 4083.37158203125\n",
      "Epoch 487, Train_Loss: 3969.9013671875, Val_Loss: 4083.359130859375\n",
      "Epoch 488, Train_Loss: 3969.91455078125, Val_Loss: 4083.352783203125\n",
      "Epoch 489, Train_Loss: 3969.911376953125, Val_Loss: 4083.3466796875\n",
      "Epoch 490, Train_Loss: 3969.90673828125, Val_Loss: 4083.3447265625\n",
      "Epoch 491, Train_Loss: 3969.8994140625, Val_Loss: 4083.3388671875\n",
      "Epoch 492, Train_Loss: 3969.882080078125, Val_Loss: 4083.33251953125\n",
      "Epoch 493, Train_Loss: 3969.87841796875, Val_Loss: 4083.335205078125\n",
      "Epoch 494, Train_Loss: 3969.873779296875, Val_Loss: 4083.327880859375\n",
      "Epoch 495, Train_Loss: 3969.830810546875, Val_Loss: 4083.341552734375\n",
      "Epoch 496, Train_Loss: 3969.825927734375, Val_Loss: 4083.339599609375\n",
      "Epoch 497, Train_Loss: 3969.808349609375, Val_Loss: 4083.34228515625\n",
      "Epoch 498, Train_Loss: 3969.852294921875, Val_Loss: 4083.337158203125\n",
      "Epoch 499, Train_Loss: 3969.840087890625, Val_Loss: 4083.32763671875\n",
      "Epoch 500, Train_Loss: 3969.826171875, Val_Loss: 4083.324462890625\n",
      "Epoch 501, Train_Loss: 3969.818115234375, Val_Loss: 4083.33447265625\n",
      "Epoch 502, Train_Loss: 3969.814453125, Val_Loss: 4083.33447265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 503, Train_Loss: 3969.822998046875, Val_Loss: 4083.3095703125\n",
      "Epoch 504, Train_Loss: 3969.76513671875, Val_Loss: 4083.2900390625\n",
      "Epoch 505, Train_Loss: 3969.802490234375, Val_Loss: 4083.264892578125\n",
      "Epoch 506, Train_Loss: 3969.667236328125, Val_Loss: 4083.246826171875\n",
      "Epoch 507, Train_Loss: 3969.67822265625, Val_Loss: 4083.251953125\n",
      "Epoch 508, Train_Loss: 3969.67138671875, Val_Loss: 4083.239501953125\n",
      "Epoch 509, Train_Loss: 3969.66259765625, Val_Loss: 4083.23486328125\n",
      "Epoch 510, Train_Loss: 3969.658447265625, Val_Loss: 4083.2294921875\n",
      "Epoch 511, Train_Loss: 3969.66259765625, Val_Loss: 4083.23193359375\n",
      "Epoch 512, Train_Loss: 3969.651611328125, Val_Loss: 4083.219482421875\n",
      "Epoch 513, Train_Loss: 3969.6357421875, Val_Loss: 4083.211181640625\n",
      "Epoch 514, Train_Loss: 3969.62548828125, Val_Loss: 4083.216064453125\n",
      "Epoch 515, Train_Loss: 3969.583984375, Val_Loss: 4083.21728515625\n",
      "Epoch 516, Train_Loss: 3969.703369140625, Val_Loss: 4083.226806640625\n",
      "Epoch 517, Train_Loss: 3969.731689453125, Val_Loss: 4083.22802734375\n",
      "Epoch 518, Train_Loss: 3969.715576171875, Val_Loss: 4083.215087890625\n",
      "Epoch 519, Train_Loss: 3969.70556640625, Val_Loss: 4083.22119140625\n",
      "Epoch 520, Train_Loss: 3969.7001953125, Val_Loss: 4083.230712890625\n",
      "Epoch 521, Train_Loss: 3969.687255859375, Val_Loss: 4083.224853515625\n",
      "Epoch 522, Train_Loss: 3969.645751953125, Val_Loss: 4083.20068359375\n",
      "Epoch 523, Train_Loss: 3969.64697265625, Val_Loss: 4083.197509765625\n",
      "Epoch 524, Train_Loss: 3969.639892578125, Val_Loss: 4083.187255859375\n",
      "Epoch 525, Train_Loss: 3969.638916015625, Val_Loss: 4083.173583984375\n",
      "Epoch 526, Train_Loss: 3969.643310546875, Val_Loss: 4083.166748046875\n",
      "Epoch 527, Train_Loss: 3969.64013671875, Val_Loss: 4083.152099609375\n",
      "Epoch 528, Train_Loss: 3969.636962890625, Val_Loss: 4083.1376953125\n",
      "Epoch 529, Train_Loss: 3969.639404296875, Val_Loss: 4083.132080078125\n",
      "Epoch 530, Train_Loss: 3969.645751953125, Val_Loss: 4083.126708984375\n",
      "Epoch 531, Train_Loss: 3969.465087890625, Val_Loss: 4083.1279296875\n",
      "Epoch 532, Train_Loss: 3969.465576171875, Val_Loss: 4083.115234375\n",
      "Epoch 533, Train_Loss: 3969.46337890625, Val_Loss: 4083.10009765625\n",
      "Epoch 534, Train_Loss: 3969.45556640625, Val_Loss: 4083.098388671875\n",
      "Epoch 535, Train_Loss: 3969.451904296875, Val_Loss: 4083.090087890625\n",
      "Epoch 536, Train_Loss: 3969.447265625, Val_Loss: 4083.085693359375\n",
      "Epoch 537, Train_Loss: 3969.435546875, Val_Loss: 4083.073974609375\n",
      "Epoch 538, Train_Loss: 3969.43212890625, Val_Loss: 4083.07958984375\n",
      "Epoch 539, Train_Loss: 3969.415771484375, Val_Loss: 4083.074462890625\n",
      "Epoch 540, Train_Loss: 3969.416748046875, Val_Loss: 4083.0791015625\n",
      "Epoch 541, Train_Loss: 3969.4423828125, Val_Loss: 4083.01318359375\n",
      "Epoch 542, Train_Loss: 3969.4326171875, Val_Loss: 4083.005126953125\n",
      "Epoch 543, Train_Loss: 3969.42578125, Val_Loss: 4083.001953125\n",
      "Epoch 544, Train_Loss: 3969.41943359375, Val_Loss: 4082.9931640625\n",
      "Epoch 545, Train_Loss: 3969.414306640625, Val_Loss: 4082.987548828125\n",
      "Epoch 546, Train_Loss: 3969.408447265625, Val_Loss: 4082.986083984375\n",
      "Epoch 547, Train_Loss: 3969.403076171875, Val_Loss: 4082.979248046875\n",
      "Epoch 548, Train_Loss: 3969.38525390625, Val_Loss: 4082.986083984375\n",
      "Epoch 549, Train_Loss: 3969.380615234375, Val_Loss: 4082.98291015625\n",
      "Epoch 550, Train_Loss: 3969.35302734375, Val_Loss: 4082.97509765625\n",
      "Epoch 551, Train_Loss: 3969.345458984375, Val_Loss: 4082.96875\n",
      "Epoch 552, Train_Loss: 3969.338134765625, Val_Loss: 4082.95751953125\n",
      "Epoch 553, Train_Loss: 3969.33203125, Val_Loss: 4082.949951171875\n",
      "Epoch 554, Train_Loss: 3969.322509765625, Val_Loss: 4082.948486328125\n",
      "Epoch 555, Train_Loss: 3969.31787109375, Val_Loss: 4082.944091796875\n",
      "Epoch 556, Train_Loss: 3969.3193359375, Val_Loss: 4082.947509765625\n",
      "Epoch 557, Train_Loss: 3969.323486328125, Val_Loss: 4082.92919921875\n",
      "Epoch 558, Train_Loss: 3969.29931640625, Val_Loss: 4082.8564453125\n",
      "Epoch 559, Train_Loss: 3969.298828125, Val_Loss: 4082.855712890625\n",
      "Epoch 560, Train_Loss: 3969.293212890625, Val_Loss: 4082.852294921875\n",
      "Epoch 561, Train_Loss: 3969.286865234375, Val_Loss: 4082.841552734375\n",
      "Epoch 562, Train_Loss: 3969.278076171875, Val_Loss: 4082.830078125\n",
      "Epoch 563, Train_Loss: 3969.267578125, Val_Loss: 4082.827880859375\n",
      "Epoch 564, Train_Loss: 3969.259033203125, Val_Loss: 4082.8369140625\n",
      "Epoch 565, Train_Loss: 3969.271240234375, Val_Loss: 4082.827880859375\n",
      "Epoch 566, Train_Loss: 3969.26171875, Val_Loss: 4082.821533203125\n",
      "Epoch 567, Train_Loss: 3969.274169921875, Val_Loss: 4082.8076171875\n",
      "Epoch 568, Train_Loss: 3969.26708984375, Val_Loss: 4082.8037109375\n",
      "Epoch 569, Train_Loss: 3969.26220703125, Val_Loss: 4082.796875\n",
      "Epoch 570, Train_Loss: 3969.256591796875, Val_Loss: 4082.79150390625\n",
      "Epoch 571, Train_Loss: 3969.252197265625, Val_Loss: 4082.7880859375\n",
      "Epoch 572, Train_Loss: 3969.24755859375, Val_Loss: 4082.777099609375\n",
      "Epoch 573, Train_Loss: 3969.223876953125, Val_Loss: 4082.781982421875\n",
      "Epoch 574, Train_Loss: 3969.221435546875, Val_Loss: 4082.7744140625\n",
      "Epoch 575, Train_Loss: 3969.224609375, Val_Loss: 4082.7744140625\n",
      "Epoch 576, Train_Loss: 3968.98193359375, Val_Loss: 4082.75634765625\n",
      "Epoch 577, Train_Loss: 3968.979248046875, Val_Loss: 4082.740478515625\n",
      "Epoch 578, Train_Loss: 3968.971923828125, Val_Loss: 4082.738037109375\n",
      "Epoch 579, Train_Loss: 3968.9677734375, Val_Loss: 4082.731201171875\n",
      "Epoch 580, Train_Loss: 3968.963134765625, Val_Loss: 4082.727294921875\n",
      "Epoch 581, Train_Loss: 3968.957763671875, Val_Loss: 4082.724853515625\n",
      "Epoch 582, Train_Loss: 3968.948486328125, Val_Loss: 4082.72509765625\n",
      "Epoch 583, Train_Loss: 3968.944091796875, Val_Loss: 4082.722412109375\n",
      "Epoch 584, Train_Loss: 3968.96044921875, Val_Loss: 4082.712890625\n",
      "Epoch 585, Train_Loss: 3968.953125, Val_Loss: 4082.707275390625\n",
      "Epoch 586, Train_Loss: 3968.9462890625, Val_Loss: 4082.698486328125\n",
      "Epoch 587, Train_Loss: 3968.9404296875, Val_Loss: 4082.695556640625\n",
      "Epoch 588, Train_Loss: 3968.9384765625, Val_Loss: 4082.6884765625\n",
      "Epoch 589, Train_Loss: 3968.9306640625, Val_Loss: 4082.684326171875\n",
      "Epoch 590, Train_Loss: 3968.90869140625, Val_Loss: 4082.681640625\n",
      "Epoch 591, Train_Loss: 3968.9013671875, Val_Loss: 4082.6728515625\n",
      "Epoch 592, Train_Loss: 3968.90185546875, Val_Loss: 4082.6640625\n",
      "Epoch 593, Train_Loss: 3968.78125, Val_Loss: 4082.634521484375\n",
      "Epoch 594, Train_Loss: 3968.779052734375, Val_Loss: 4082.62890625\n",
      "Epoch 595, Train_Loss: 3968.76806640625, Val_Loss: 4082.62353515625\n",
      "Epoch 596, Train_Loss: 3968.764404296875, Val_Loss: 4082.619140625\n",
      "Epoch 597, Train_Loss: 3968.76171875, Val_Loss: 4082.61669921875\n",
      "Epoch 598, Train_Loss: 3968.75830078125, Val_Loss: 4082.606689453125\n",
      "Epoch 599, Train_Loss: 3968.76611328125, Val_Loss: 4082.602294921875\n",
      "Epoch 600, Train_Loss: 3968.76171875, Val_Loss: 4082.598388671875\n",
      "Epoch 601, Train_Loss: 3968.75830078125, Val_Loss: 4082.589599609375\n",
      "Epoch 602, Train_Loss: 3968.733642578125, Val_Loss: 4082.58251953125\n",
      "Epoch 603, Train_Loss: 3968.726318359375, Val_Loss: 4082.57568359375\n",
      "Epoch 604, Train_Loss: 3968.720458984375, Val_Loss: 4082.57568359375\n",
      "Epoch 605, Train_Loss: 3968.714599609375, Val_Loss: 4082.567138671875\n",
      "Epoch 606, Train_Loss: 3968.71044921875, Val_Loss: 4082.562744140625\n",
      "Epoch 607, Train_Loss: 3968.704833984375, Val_Loss: 4082.5556640625\n",
      "Epoch 608, Train_Loss: 3968.7021484375, Val_Loss: 4082.552001953125\n",
      "Epoch 609, Train_Loss: 3968.66552734375, Val_Loss: 4082.499267578125\n",
      "Epoch 610, Train_Loss: 3968.656982421875, Val_Loss: 4082.4951171875\n",
      "Epoch 611, Train_Loss: 3968.65234375, Val_Loss: 4082.48486328125\n",
      "Epoch 612, Train_Loss: 3968.647705078125, Val_Loss: 4082.479248046875\n",
      "Epoch 613, Train_Loss: 3968.635986328125, Val_Loss: 4082.47119140625\n",
      "Epoch 614, Train_Loss: 3968.63232421875, Val_Loss: 4082.46728515625\n",
      "Epoch 615, Train_Loss: 3968.62255859375, Val_Loss: 4082.4599609375\n",
      "Epoch 616, Train_Loss: 3968.6162109375, Val_Loss: 4082.455078125\n",
      "Epoch 617, Train_Loss: 3968.604248046875, Val_Loss: 4082.44677734375\n",
      "Epoch 618, Train_Loss: 3968.590087890625, Val_Loss: 4082.432861328125\n",
      "Epoch 619, Train_Loss: 3968.53759765625, Val_Loss: 4082.37109375\n",
      "Epoch 620, Train_Loss: 3968.55126953125, Val_Loss: 4082.3076171875\n",
      "Epoch 621, Train_Loss: 3968.53125, Val_Loss: 4082.24072265625\n",
      "Epoch 622, Train_Loss: 3968.53125, Val_Loss: 4082.20849609375\n",
      "Epoch 623, Train_Loss: 3968.483154296875, Val_Loss: 4082.181640625\n",
      "Epoch 624, Train_Loss: 3968.5078125, Val_Loss: 4082.160888671875\n",
      "Epoch 625, Train_Loss: 3968.496337890625, Val_Loss: 4082.138427734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 626, Train_Loss: 3968.42822265625, Val_Loss: 4082.08447265625\n",
      "Epoch 627, Train_Loss: 3968.4580078125, Val_Loss: 4082.107177734375\n",
      "Epoch 628, Train_Loss: 3968.42919921875, Val_Loss: 4082.202392578125\n",
      "Epoch 629, Train_Loss: 3968.4599609375, Val_Loss: 4082.18115234375\n",
      "Epoch 630, Train_Loss: 3968.475830078125, Val_Loss: 4082.17041015625\n",
      "Epoch 631, Train_Loss: 3968.41552734375, Val_Loss: 4082.181884765625\n",
      "Epoch 632, Train_Loss: 3968.435546875, Val_Loss: 4082.103515625\n",
      "Epoch 633, Train_Loss: 3968.41552734375, Val_Loss: 4082.083251953125\n",
      "Epoch 634, Train_Loss: 3968.4296875, Val_Loss: 4082.060791015625\n",
      "Epoch 635, Train_Loss: 3968.421875, Val_Loss: 4082.04931640625\n",
      "Epoch 636, Train_Loss: 3968.40087890625, Val_Loss: 4082.064697265625\n",
      "Epoch 637, Train_Loss: 3968.404052734375, Val_Loss: 4082.039306640625\n",
      "Epoch 638, Train_Loss: 3968.392333984375, Val_Loss: 4082.05712890625\n",
      "Epoch 639, Train_Loss: 3968.421875, Val_Loss: 4082.111083984375\n",
      "Epoch 640, Train_Loss: 3968.3955078125, Val_Loss: 4082.12548828125\n",
      "Epoch 641, Train_Loss: 3968.348876953125, Val_Loss: 4082.13916015625\n",
      "Epoch 642, Train_Loss: 3968.348388671875, Val_Loss: 4082.168701171875\n",
      "Epoch 643, Train_Loss: 3968.341552734375, Val_Loss: 4082.151611328125\n",
      "Epoch 644, Train_Loss: 3968.326904296875, Val_Loss: 4082.11962890625\n",
      "Epoch 645, Train_Loss: 3968.29248046875, Val_Loss: 4082.1015625\n",
      "Epoch 646, Train_Loss: 3968.334716796875, Val_Loss: 4082.08447265625\n",
      "Epoch 647, Train_Loss: 3968.355712890625, Val_Loss: 4082.052734375\n",
      "Epoch 648, Train_Loss: 3968.31494140625, Val_Loss: 4081.994873046875\n",
      "Epoch 649, Train_Loss: 3968.32958984375, Val_Loss: 4081.968505859375\n",
      "Epoch 650, Train_Loss: 3968.326904296875, Val_Loss: 4081.9599609375\n",
      "Epoch 651, Train_Loss: 3968.310791015625, Val_Loss: 4081.964111328125\n",
      "Epoch 652, Train_Loss: 3968.287109375, Val_Loss: 4081.963134765625\n",
      "Epoch 653, Train_Loss: 3968.26513671875, Val_Loss: 4082.030029296875\n",
      "Epoch 654, Train_Loss: 3968.310302734375, Val_Loss: 4082.014892578125\n",
      "Epoch 655, Train_Loss: 3968.303466796875, Val_Loss: 4082.001708984375\n",
      "Epoch 656, Train_Loss: 3968.267822265625, Val_Loss: 4081.9296875\n",
      "Epoch 657, Train_Loss: 3968.256103515625, Val_Loss: 4081.9287109375\n",
      "Epoch 658, Train_Loss: 3968.24951171875, Val_Loss: 4081.96044921875\n",
      "Epoch 659, Train_Loss: 3968.23486328125, Val_Loss: 4081.9736328125\n",
      "Epoch 660, Train_Loss: 3968.23779296875, Val_Loss: 4081.948486328125\n",
      "Epoch 661, Train_Loss: 3968.185546875, Val_Loss: 4081.89990234375\n",
      "Epoch 662, Train_Loss: 3968.1630859375, Val_Loss: 4081.88916015625\n",
      "Epoch 663, Train_Loss: 3968.15869140625, Val_Loss: 4081.884521484375\n",
      "Epoch 664, Train_Loss: 3968.1572265625, Val_Loss: 4081.88525390625\n",
      "Epoch 665, Train_Loss: 3968.158203125, Val_Loss: 4081.870849609375\n",
      "Epoch 666, Train_Loss: 3968.1572265625, Val_Loss: 4081.837646484375\n",
      "Epoch 667, Train_Loss: 3968.166015625, Val_Loss: 4081.84033203125\n",
      "Epoch 668, Train_Loss: 3968.171875, Val_Loss: 4081.82080078125\n",
      "Epoch 669, Train_Loss: 3968.153076171875, Val_Loss: 4081.824462890625\n",
      "Epoch 670, Train_Loss: 3968.16455078125, Val_Loss: 4081.808349609375\n",
      "Epoch 671, Train_Loss: 3968.145263671875, Val_Loss: 4081.752685546875\n",
      "Epoch 672, Train_Loss: 3968.117431640625, Val_Loss: 4081.772705078125\n",
      "Epoch 673, Train_Loss: 3968.09130859375, Val_Loss: 4081.775146484375\n",
      "Epoch 674, Train_Loss: 3968.12841796875, Val_Loss: 4081.78759765625\n",
      "Epoch 675, Train_Loss: 3968.11328125, Val_Loss: 4081.823974609375\n",
      "Epoch 676, Train_Loss: 3968.08056640625, Val_Loss: 4081.83837890625\n",
      "Epoch 677, Train_Loss: 3968.0517578125, Val_Loss: 4081.853515625\n",
      "Epoch 678, Train_Loss: 3968.0439453125, Val_Loss: 4081.849609375\n",
      "Epoch 679, Train_Loss: 3968.0283203125, Val_Loss: 4081.843994140625\n",
      "Epoch 680, Train_Loss: 3968.031494140625, Val_Loss: 4081.8359375\n",
      "Epoch 681, Train_Loss: 3968.05078125, Val_Loss: 4081.819580078125\n",
      "Epoch 682, Train_Loss: 3968.052490234375, Val_Loss: 4081.7958984375\n",
      "Epoch 683, Train_Loss: 3968.07275390625, Val_Loss: 4081.78125\n",
      "Epoch 684, Train_Loss: 3968.052001953125, Val_Loss: 4081.742919921875\n",
      "Epoch 685, Train_Loss: 3968.069091796875, Val_Loss: 4081.703125\n",
      "Epoch 686, Train_Loss: 3968.03662109375, Val_Loss: 4081.620849609375\n",
      "Epoch 687, Train_Loss: 3967.989990234375, Val_Loss: 4081.620849609375\n",
      "Epoch 688, Train_Loss: 3967.986328125, Val_Loss: 4081.62109375\n",
      "Epoch 689, Train_Loss: 3967.975341796875, Val_Loss: 4081.61962890625\n",
      "Epoch 690, Train_Loss: 3968.019287109375, Val_Loss: 4081.5966796875\n",
      "Epoch 691, Train_Loss: 3968.017333984375, Val_Loss: 4081.590087890625\n",
      "Epoch 692, Train_Loss: 3967.998291015625, Val_Loss: 4081.609130859375\n",
      "Epoch 693, Train_Loss: 3967.990966796875, Val_Loss: 4081.5966796875\n",
      "Epoch 694, Train_Loss: 3967.989013671875, Val_Loss: 4081.59765625\n",
      "Epoch 695, Train_Loss: 3967.989501953125, Val_Loss: 4081.560791015625\n",
      "Epoch 696, Train_Loss: 3967.9775390625, Val_Loss: 4081.547119140625\n",
      "Epoch 697, Train_Loss: 3967.968505859375, Val_Loss: 4081.5556640625\n",
      "Epoch 698, Train_Loss: 3967.962890625, Val_Loss: 4081.544921875\n",
      "Epoch 699, Train_Loss: 3967.96533203125, Val_Loss: 4081.5439453125\n",
      "Epoch 700, Train_Loss: 3967.95703125, Val_Loss: 4081.5458984375\n",
      "Epoch 701, Train_Loss: 3967.87939453125, Val_Loss: 4081.540771484375\n",
      "Epoch 702, Train_Loss: 3967.884765625, Val_Loss: 4081.516357421875\n",
      "Epoch 703, Train_Loss: 3967.874267578125, Val_Loss: 4081.523193359375\n",
      "Epoch 704, Train_Loss: 3967.866943359375, Val_Loss: 4081.5283203125\n",
      "Epoch 705, Train_Loss: 3967.811279296875, Val_Loss: 4081.52392578125\n",
      "Epoch 706, Train_Loss: 3967.809814453125, Val_Loss: 4081.51806640625\n",
      "Epoch 707, Train_Loss: 3967.849609375, Val_Loss: 4081.505615234375\n",
      "Epoch 708, Train_Loss: 3967.80810546875, Val_Loss: 4081.46728515625\n",
      "Epoch 709, Train_Loss: 3967.7490234375, Val_Loss: 4081.432373046875\n",
      "Epoch 710, Train_Loss: 3967.7431640625, Val_Loss: 4081.425537109375\n",
      "Epoch 711, Train_Loss: 3967.744384765625, Val_Loss: 4081.419189453125\n",
      "Epoch 712, Train_Loss: 3967.7490234375, Val_Loss: 4081.392822265625\n",
      "Epoch 713, Train_Loss: 3967.7119140625, Val_Loss: 4081.393310546875\n",
      "Epoch 714, Train_Loss: 3967.724365234375, Val_Loss: 4081.384765625\n",
      "Epoch 715, Train_Loss: 3967.419921875, Val_Loss: 4081.367919921875\n",
      "Epoch 716, Train_Loss: 3967.531982421875, Val_Loss: 4081.397705078125\n",
      "Epoch 717, Train_Loss: 3967.50439453125, Val_Loss: 4081.419677734375\n",
      "Epoch 718, Train_Loss: 3967.445556640625, Val_Loss: 4081.419921875\n",
      "Epoch 719, Train_Loss: 3967.450439453125, Val_Loss: 4081.4287109375\n",
      "Epoch 720, Train_Loss: 3967.424560546875, Val_Loss: 4081.474365234375\n",
      "Epoch 721, Train_Loss: 3967.421142578125, Val_Loss: 4081.462890625\n",
      "Epoch 722, Train_Loss: 3967.436767578125, Val_Loss: 4081.397705078125\n",
      "Epoch 723, Train_Loss: 3967.46826171875, Val_Loss: 4081.3740234375\n",
      "Epoch 724, Train_Loss: 3967.45361328125, Val_Loss: 4081.3583984375\n",
      "Epoch 725, Train_Loss: 3967.33984375, Val_Loss: 4081.326416015625\n",
      "Epoch 726, Train_Loss: 3967.3349609375, Val_Loss: 4081.294921875\n",
      "Epoch 727, Train_Loss: 3967.344482421875, Val_Loss: 4081.2958984375\n",
      "Epoch 728, Train_Loss: 3967.345458984375, Val_Loss: 4081.294921875\n",
      "Epoch 729, Train_Loss: 3967.291015625, Val_Loss: 4081.2294921875\n",
      "Epoch 730, Train_Loss: 3967.308837890625, Val_Loss: 4081.238037109375\n",
      "Epoch 731, Train_Loss: 3967.341796875, Val_Loss: 4081.231689453125\n",
      "Epoch 732, Train_Loss: 3967.3779296875, Val_Loss: 4081.2568359375\n",
      "Epoch 733, Train_Loss: 3967.38525390625, Val_Loss: 4081.26318359375\n",
      "Epoch 734, Train_Loss: 3967.33349609375, Val_Loss: 4081.271484375\n",
      "Epoch 735, Train_Loss: 3967.32568359375, Val_Loss: 4081.2626953125\n",
      "Epoch 736, Train_Loss: 3967.328857421875, Val_Loss: 4081.24951171875\n",
      "Epoch 737, Train_Loss: 3967.32080078125, Val_Loss: 4081.24072265625\n",
      "Epoch 738, Train_Loss: 3967.29833984375, Val_Loss: 4081.244384765625\n",
      "Epoch 739, Train_Loss: 3967.344970703125, Val_Loss: 4081.218505859375\n",
      "Epoch 740, Train_Loss: 3967.259765625, Val_Loss: 4081.20068359375\n",
      "Epoch 741, Train_Loss: 3967.26953125, Val_Loss: 4081.167236328125\n",
      "Epoch 742, Train_Loss: 3967.214111328125, Val_Loss: 4081.17236328125\n",
      "Epoch 743, Train_Loss: 3967.13818359375, Val_Loss: 4081.085205078125\n",
      "Epoch 744, Train_Loss: 3967.130126953125, Val_Loss: 4081.076416015625\n",
      "Epoch 745, Train_Loss: 3967.130126953125, Val_Loss: 4081.083984375\n",
      "Epoch 746, Train_Loss: 3967.185302734375, Val_Loss: 4081.0615234375\n",
      "Epoch 747, Train_Loss: 3967.1689453125, Val_Loss: 4081.060302734375\n",
      "Epoch 748, Train_Loss: 3967.164306640625, Val_Loss: 4081.0712890625\n",
      "Epoch 749, Train_Loss: 3967.1455078125, Val_Loss: 4081.07080078125\n",
      "Epoch 750, Train_Loss: 3967.154296875, Val_Loss: 4081.030517578125\n",
      "Epoch 751, Train_Loss: 3967.080322265625, Val_Loss: 4081.02197265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 752, Train_Loss: 3967.101318359375, Val_Loss: 4081.00048828125\n",
      "Epoch 753, Train_Loss: 3967.086181640625, Val_Loss: 4080.989990234375\n",
      "Epoch 754, Train_Loss: 3967.079345703125, Val_Loss: 4080.982421875\n",
      "Epoch 755, Train_Loss: 3967.058349609375, Val_Loss: 4080.984375\n",
      "Epoch 756, Train_Loss: 3967.04638671875, Val_Loss: 4080.993896484375\n",
      "Epoch 757, Train_Loss: 3967.07373046875, Val_Loss: 4080.98486328125\n",
      "Epoch 758, Train_Loss: 3967.00439453125, Val_Loss: 4080.963134765625\n",
      "Epoch 759, Train_Loss: 3967.012939453125, Val_Loss: 4080.943115234375\n",
      "Epoch 760, Train_Loss: 3966.99755859375, Val_Loss: 4080.93603515625\n",
      "Epoch 761, Train_Loss: 3966.990234375, Val_Loss: 4080.9287109375\n",
      "Epoch 762, Train_Loss: 3966.98779296875, Val_Loss: 4080.92041015625\n",
      "Epoch 763, Train_Loss: 3966.978271484375, Val_Loss: 4080.92724609375\n",
      "Epoch 764, Train_Loss: 3966.9619140625, Val_Loss: 4080.9267578125\n",
      "Epoch 765, Train_Loss: 3966.96630859375, Val_Loss: 4080.905517578125\n",
      "Epoch 766, Train_Loss: 3966.9619140625, Val_Loss: 4080.894287109375\n",
      "Epoch 767, Train_Loss: 3966.936767578125, Val_Loss: 4080.890380859375\n",
      "Epoch 768, Train_Loss: 3966.928466796875, Val_Loss: 4080.8955078125\n",
      "Epoch 769, Train_Loss: 3966.9765625, Val_Loss: 4080.887939453125\n",
      "Epoch 770, Train_Loss: 3966.94580078125, Val_Loss: 4080.879150390625\n",
      "Epoch 771, Train_Loss: 3966.93798828125, Val_Loss: 4080.87841796875\n",
      "Epoch 772, Train_Loss: 3966.927490234375, Val_Loss: 4080.866455078125\n",
      "Epoch 773, Train_Loss: 3966.920654296875, Val_Loss: 4080.861083984375\n",
      "Epoch 774, Train_Loss: 3966.86572265625, Val_Loss: 4080.865966796875\n",
      "Epoch 775, Train_Loss: 3966.90234375, Val_Loss: 4080.8408203125\n",
      "Epoch 776, Train_Loss: 3966.84521484375, Val_Loss: 4080.814697265625\n",
      "Epoch 777, Train_Loss: 3966.855224609375, Val_Loss: 4080.74365234375\n",
      "Epoch 778, Train_Loss: 3966.834228515625, Val_Loss: 4080.744873046875\n",
      "Epoch 779, Train_Loss: 3966.840576171875, Val_Loss: 4080.727294921875\n",
      "Epoch 780, Train_Loss: 3966.81005859375, Val_Loss: 4080.717529296875\n",
      "Epoch 781, Train_Loss: 3966.826904296875, Val_Loss: 4080.72412109375\n",
      "Epoch 782, Train_Loss: 3966.800537109375, Val_Loss: 4080.767578125\n",
      "Epoch 783, Train_Loss: 3966.8232421875, Val_Loss: 4080.726318359375\n",
      "Epoch 784, Train_Loss: 3966.82421875, Val_Loss: 4080.73876953125\n",
      "Epoch 785, Train_Loss: 3966.820556640625, Val_Loss: 4080.727294921875\n",
      "Epoch 786, Train_Loss: 3966.7744140625, Val_Loss: 4080.739501953125\n",
      "Epoch 787, Train_Loss: 3966.805419921875, Val_Loss: 4080.711181640625\n",
      "Epoch 788, Train_Loss: 3966.799072265625, Val_Loss: 4080.67724609375\n",
      "Epoch 789, Train_Loss: 3966.72314453125, Val_Loss: 4080.63671875\n",
      "Epoch 790, Train_Loss: 3966.750244140625, Val_Loss: 4080.596435546875\n",
      "Epoch 791, Train_Loss: 3966.725830078125, Val_Loss: 4080.59912109375\n",
      "Epoch 792, Train_Loss: 3966.72265625, Val_Loss: 4080.587158203125\n",
      "Epoch 793, Train_Loss: 3966.706787109375, Val_Loss: 4080.58203125\n",
      "Epoch 794, Train_Loss: 3966.697021484375, Val_Loss: 4080.58349609375\n",
      "Epoch 795, Train_Loss: 3966.71630859375, Val_Loss: 4080.5712890625\n",
      "Epoch 796, Train_Loss: 3966.676025390625, Val_Loss: 4080.580078125\n",
      "Epoch 797, Train_Loss: 3966.70068359375, Val_Loss: 4080.512451171875\n",
      "Epoch 798, Train_Loss: 3966.70947265625, Val_Loss: 4080.55908203125\n",
      "Epoch 799, Train_Loss: 3966.70166015625, Val_Loss: 4080.556396484375\n",
      "Epoch 800, Train_Loss: 3966.70361328125, Val_Loss: 4080.53125\n",
      "Epoch 801, Train_Loss: 3966.6796875, Val_Loss: 4080.48876953125\n",
      "Epoch 802, Train_Loss: 3966.671142578125, Val_Loss: 4080.480712890625\n",
      "Epoch 803, Train_Loss: 3966.663330078125, Val_Loss: 4080.47119140625\n",
      "Epoch 804, Train_Loss: 3966.654541015625, Val_Loss: 4080.4619140625\n",
      "Epoch 805, Train_Loss: 3966.6435546875, Val_Loss: 4080.463134765625\n",
      "Epoch 806, Train_Loss: 3966.633544921875, Val_Loss: 4080.444091796875\n",
      "Epoch 807, Train_Loss: 3966.572265625, Val_Loss: 4080.435302734375\n",
      "Epoch 808, Train_Loss: 3966.57421875, Val_Loss: 4080.4228515625\n",
      "Epoch 809, Train_Loss: 3966.56640625, Val_Loss: 4080.417236328125\n",
      "Epoch 810, Train_Loss: 3966.549560546875, Val_Loss: 4080.399658203125\n",
      "Epoch 811, Train_Loss: 3966.59326171875, Val_Loss: 4080.3935546875\n",
      "Epoch 812, Train_Loss: 3966.578857421875, Val_Loss: 4080.3916015625\n",
      "Epoch 813, Train_Loss: 3966.572265625, Val_Loss: 4080.392822265625\n",
      "Epoch 814, Train_Loss: 3966.57177734375, Val_Loss: 4080.401123046875\n",
      "Epoch 815, Train_Loss: 3966.55859375, Val_Loss: 4080.390380859375\n",
      "Epoch 816, Train_Loss: 3966.548095703125, Val_Loss: 4080.38232421875\n",
      "Epoch 817, Train_Loss: 3966.5361328125, Val_Loss: 4080.3740234375\n",
      "Epoch 818, Train_Loss: 3966.53125, Val_Loss: 4080.36669921875\n",
      "Epoch 819, Train_Loss: 3966.53076171875, Val_Loss: 4080.356689453125\n",
      "Epoch 820, Train_Loss: 3966.534423828125, Val_Loss: 4080.355224609375\n",
      "Epoch 821, Train_Loss: 3966.512939453125, Val_Loss: 4080.332763671875\n",
      "Epoch 822, Train_Loss: 3966.36669921875, Val_Loss: 4080.2958984375\n",
      "Epoch 823, Train_Loss: 3966.361572265625, Val_Loss: 4080.278076171875\n",
      "Epoch 824, Train_Loss: 3966.354248046875, Val_Loss: 4080.271240234375\n",
      "Epoch 825, Train_Loss: 3966.347412109375, Val_Loss: 4080.262451171875\n",
      "Epoch 826, Train_Loss: 3966.337890625, Val_Loss: 4080.2587890625\n",
      "Epoch 827, Train_Loss: 3966.322265625, Val_Loss: 4080.26171875\n",
      "Epoch 828, Train_Loss: 3966.314453125, Val_Loss: 4080.253173828125\n",
      "Epoch 829, Train_Loss: 3966.31591796875, Val_Loss: 4080.26513671875\n",
      "Epoch 830, Train_Loss: 3966.30859375, Val_Loss: 4080.255615234375\n",
      "Epoch 831, Train_Loss: 3966.30078125, Val_Loss: 4080.248046875\n",
      "Epoch 832, Train_Loss: 3966.28564453125, Val_Loss: 4080.22412109375\n",
      "Epoch 833, Train_Loss: 3966.279296875, Val_Loss: 4080.219970703125\n",
      "Epoch 834, Train_Loss: 3966.281982421875, Val_Loss: 4080.20361328125\n",
      "Epoch 835, Train_Loss: 3966.251953125, Val_Loss: 4080.144775390625\n",
      "Epoch 836, Train_Loss: 3966.247314453125, Val_Loss: 4080.13916015625\n",
      "Epoch 837, Train_Loss: 3966.234619140625, Val_Loss: 4080.13330078125\n",
      "Epoch 838, Train_Loss: 3966.2421875, Val_Loss: 4080.127685546875\n",
      "Epoch 839, Train_Loss: 3966.23046875, Val_Loss: 4080.127685546875\n",
      "Epoch 840, Train_Loss: 3966.2216796875, Val_Loss: 4080.126708984375\n",
      "Epoch 841, Train_Loss: 3966.218017578125, Val_Loss: 4080.13525390625\n",
      "Epoch 842, Train_Loss: 3966.213134765625, Val_Loss: 4080.126708984375\n",
      "Epoch 843, Train_Loss: 3966.203857421875, Val_Loss: 4080.107177734375\n",
      "Epoch 844, Train_Loss: 3966.1943359375, Val_Loss: 4080.092041015625\n",
      "Epoch 845, Train_Loss: 3966.182861328125, Val_Loss: 4080.07568359375\n",
      "Epoch 846, Train_Loss: 3966.18017578125, Val_Loss: 4080.06884765625\n",
      "Epoch 847, Train_Loss: 3966.171875, Val_Loss: 4080.061279296875\n",
      "Epoch 848, Train_Loss: 3966.118896484375, Val_Loss: 4079.983154296875\n",
      "Epoch 849, Train_Loss: 3966.112548828125, Val_Loss: 4079.97802734375\n",
      "Epoch 850, Train_Loss: 3966.107421875, Val_Loss: 4079.972900390625\n",
      "Epoch 851, Train_Loss: 3966.103271484375, Val_Loss: 4079.97412109375\n",
      "Epoch 852, Train_Loss: 3966.095947265625, Val_Loss: 4079.96630859375\n",
      "Epoch 853, Train_Loss: 3966.087890625, Val_Loss: 4079.956787109375\n",
      "Epoch 854, Train_Loss: 3966.081787109375, Val_Loss: 4079.949951171875\n",
      "Epoch 855, Train_Loss: 3966.07275390625, Val_Loss: 4079.932861328125\n",
      "Epoch 856, Train_Loss: 3966.060302734375, Val_Loss: 4079.918701171875\n",
      "Epoch 857, Train_Loss: 3966.0556640625, Val_Loss: 4079.909912109375\n",
      "Epoch 858, Train_Loss: 3966.02978515625, Val_Loss: 4079.889892578125\n",
      "Epoch 859, Train_Loss: 3966.024169921875, Val_Loss: 4079.884521484375\n",
      "Epoch 860, Train_Loss: 3966.003662109375, Val_Loss: 4079.869140625\n",
      "Epoch 861, Train_Loss: 3965.989013671875, Val_Loss: 4079.866455078125\n",
      "Epoch 862, Train_Loss: 3965.988525390625, Val_Loss: 4079.864501953125\n",
      "Epoch 863, Train_Loss: 3965.976318359375, Val_Loss: 4079.845947265625\n",
      "Epoch 864, Train_Loss: 3965.9697265625, Val_Loss: 4079.834716796875\n",
      "Epoch 865, Train_Loss: 3965.961181640625, Val_Loss: 4079.82958984375\n",
      "Epoch 866, Train_Loss: 3965.946044921875, Val_Loss: 4079.81884765625\n",
      "Epoch 867, Train_Loss: 3965.94091796875, Val_Loss: 4079.814453125\n",
      "Epoch 868, Train_Loss: 3965.931396484375, Val_Loss: 4079.80908203125\n",
      "Epoch 869, Train_Loss: 3965.927734375, Val_Loss: 4079.802490234375\n",
      "Epoch 870, Train_Loss: 3965.921875, Val_Loss: 4079.797607421875\n",
      "Epoch 871, Train_Loss: 3965.91455078125, Val_Loss: 4079.789306640625\n",
      "Epoch 872, Train_Loss: 3965.869140625, Val_Loss: 4079.751220703125\n",
      "Epoch 873, Train_Loss: 3965.859619140625, Val_Loss: 4079.734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 874, Train_Loss: 3965.85498046875, Val_Loss: 4079.722900390625\n",
      "Epoch 875, Train_Loss: 3965.843994140625, Val_Loss: 4079.714111328125\n",
      "Epoch 876, Train_Loss: 3965.8369140625, Val_Loss: 4079.70751953125\n",
      "Epoch 877, Train_Loss: 3965.82861328125, Val_Loss: 4079.69970703125\n",
      "Epoch 878, Train_Loss: 3965.82080078125, Val_Loss: 4079.69482421875\n",
      "Epoch 879, Train_Loss: 3965.816650390625, Val_Loss: 4079.691650390625\n",
      "Epoch 880, Train_Loss: 3965.80810546875, Val_Loss: 4079.682861328125\n",
      "Epoch 881, Train_Loss: 3965.79931640625, Val_Loss: 4079.671630859375\n",
      "Epoch 882, Train_Loss: 3965.7939453125, Val_Loss: 4079.66357421875\n",
      "Epoch 883, Train_Loss: 3965.790283203125, Val_Loss: 4079.6552734375\n",
      "Epoch 884, Train_Loss: 3965.773681640625, Val_Loss: 4079.600830078125\n",
      "Epoch 885, Train_Loss: 3965.760009765625, Val_Loss: 4079.593994140625\n",
      "Epoch 886, Train_Loss: 3965.753662109375, Val_Loss: 4079.584716796875\n",
      "Epoch 887, Train_Loss: 3965.7421875, Val_Loss: 4079.57080078125\n",
      "Epoch 888, Train_Loss: 3965.725341796875, Val_Loss: 4079.567138671875\n",
      "Epoch 889, Train_Loss: 3965.716552734375, Val_Loss: 4079.558837890625\n",
      "Epoch 890, Train_Loss: 3965.71337890625, Val_Loss: 4079.54931640625\n",
      "Epoch 891, Train_Loss: 3965.651123046875, Val_Loss: 4079.54833984375\n",
      "Epoch 892, Train_Loss: 3965.643310546875, Val_Loss: 4079.54150390625\n",
      "Epoch 893, Train_Loss: 3965.61474609375, Val_Loss: 4079.53515625\n",
      "Epoch 894, Train_Loss: 3965.60693359375, Val_Loss: 4079.529296875\n",
      "Epoch 895, Train_Loss: 3965.59326171875, Val_Loss: 4079.522705078125\n",
      "Epoch 896, Train_Loss: 3965.624755859375, Val_Loss: 4079.438720703125\n",
      "Epoch 897, Train_Loss: 3965.565673828125, Val_Loss: 4079.416015625\n",
      "Epoch 898, Train_Loss: 3965.576171875, Val_Loss: 4079.400390625\n",
      "Epoch 899, Train_Loss: 3965.552490234375, Val_Loss: 4079.346435546875\n",
      "Epoch 900, Train_Loss: 3965.538818359375, Val_Loss: 4079.33642578125\n",
      "Epoch 901, Train_Loss: 3965.403076171875, Val_Loss: 4079.280517578125\n",
      "Epoch 902, Train_Loss: 3965.342529296875, Val_Loss: 4079.16650390625\n",
      "Epoch 903, Train_Loss: 3965.379150390625, Val_Loss: 4079.097900390625\n",
      "Epoch 904, Train_Loss: 3965.376953125, Val_Loss: 4079.09912109375\n",
      "Epoch 905, Train_Loss: 3965.350341796875, Val_Loss: 4079.0859375\n",
      "Epoch 906, Train_Loss: 3965.400634765625, Val_Loss: 4079.05810546875\n",
      "Epoch 907, Train_Loss: 3965.35400390625, Val_Loss: 4079.072021484375\n",
      "Epoch 908, Train_Loss: 3965.22607421875, Val_Loss: 4079.08251953125\n",
      "Epoch 909, Train_Loss: 3965.21337890625, Val_Loss: 4079.188720703125\n",
      "Epoch 910, Train_Loss: 3965.20361328125, Val_Loss: 4079.24365234375\n",
      "Epoch 911, Train_Loss: 3965.24560546875, Val_Loss: 4079.19287109375\n",
      "Epoch 912, Train_Loss: 3965.164794921875, Val_Loss: 4079.098388671875\n",
      "Epoch 913, Train_Loss: 3965.223876953125, Val_Loss: 4079.005615234375\n",
      "Epoch 914, Train_Loss: 3965.273681640625, Val_Loss: 4078.9453125\n",
      "Epoch 915, Train_Loss: 3965.27490234375, Val_Loss: 4078.925537109375\n",
      "Epoch 916, Train_Loss: 3965.179443359375, Val_Loss: 4079.00439453125\n",
      "Epoch 917, Train_Loss: 3965.142333984375, Val_Loss: 4079.0869140625\n",
      "Epoch 918, Train_Loss: 3965.185302734375, Val_Loss: 4079.156005859375\n",
      "Epoch 919, Train_Loss: 3965.18359375, Val_Loss: 4079.157958984375\n",
      "Epoch 920, Train_Loss: 3964.942138671875, Val_Loss: 4079.033935546875\n",
      "Epoch 921, Train_Loss: 3964.935302734375, Val_Loss: 4078.9775390625\n",
      "Epoch 922, Train_Loss: 3964.9541015625, Val_Loss: 4078.94970703125\n",
      "Epoch 923, Train_Loss: 3964.943603515625, Val_Loss: 4078.939208984375\n",
      "Epoch 924, Train_Loss: 3964.906982421875, Val_Loss: 4078.944091796875\n",
      "Epoch 925, Train_Loss: 3964.885009765625, Val_Loss: 4078.978759765625\n",
      "Epoch 926, Train_Loss: 3964.96826171875, Val_Loss: 4079.0283203125\n",
      "Epoch 927, Train_Loss: 3964.949951171875, Val_Loss: 4079.0380859375\n",
      "Epoch 928, Train_Loss: 3964.845703125, Val_Loss: 4078.974365234375\n",
      "Epoch 929, Train_Loss: 3964.8603515625, Val_Loss: 4078.9404296875\n",
      "Epoch 930, Train_Loss: 3964.8369140625, Val_Loss: 4078.922119140625\n",
      "Epoch 931, Train_Loss: 3964.826171875, Val_Loss: 4078.843994140625\n",
      "Epoch 932, Train_Loss: 3964.80224609375, Val_Loss: 4078.825927734375\n",
      "Epoch 933, Train_Loss: 3964.733154296875, Val_Loss: 4078.802001953125\n",
      "Epoch 934, Train_Loss: 3964.6669921875, Val_Loss: 4078.729248046875\n",
      "Epoch 935, Train_Loss: 3964.620361328125, Val_Loss: 4078.646484375\n",
      "Epoch 936, Train_Loss: 3964.70068359375, Val_Loss: 4078.575927734375\n",
      "Epoch 937, Train_Loss: 3964.74609375, Val_Loss: 4078.55029296875\n",
      "Epoch 938, Train_Loss: 3964.765625, Val_Loss: 4078.552001953125\n",
      "Epoch 939, Train_Loss: 3964.7392578125, Val_Loss: 4078.541259765625\n",
      "Epoch 940, Train_Loss: 3964.648193359375, Val_Loss: 4078.54638671875\n",
      "Epoch 941, Train_Loss: 3964.669677734375, Val_Loss: 4078.59228515625\n",
      "Epoch 942, Train_Loss: 3964.539794921875, Val_Loss: 4078.531982421875\n",
      "Epoch 943, Train_Loss: 3964.447998046875, Val_Loss: 4078.54248046875\n",
      "Epoch 944, Train_Loss: 3964.417724609375, Val_Loss: 4078.5615234375\n",
      "Epoch 945, Train_Loss: 3964.456787109375, Val_Loss: 4078.552490234375\n",
      "Epoch 946, Train_Loss: 3964.37451171875, Val_Loss: 4078.521484375\n",
      "Epoch 947, Train_Loss: 3964.468994140625, Val_Loss: 4078.481201171875\n",
      "Epoch 948, Train_Loss: 3964.468505859375, Val_Loss: 4078.45849609375\n",
      "Epoch 949, Train_Loss: 3964.4677734375, Val_Loss: 4078.44970703125\n",
      "Epoch 950, Train_Loss: 3964.45751953125, Val_Loss: 4078.44873046875\n",
      "Epoch 951, Train_Loss: 3964.453369140625, Val_Loss: 4078.437255859375\n",
      "Epoch 952, Train_Loss: 3964.426025390625, Val_Loss: 4078.453125\n",
      "Epoch 953, Train_Loss: 3964.404052734375, Val_Loss: 4078.434326171875\n",
      "Epoch 954, Train_Loss: 3964.29296875, Val_Loss: 4078.421630859375\n",
      "Epoch 955, Train_Loss: 3964.294921875, Val_Loss: 4078.435302734375\n",
      "Epoch 956, Train_Loss: 3964.3017578125, Val_Loss: 4078.463623046875\n",
      "Epoch 957, Train_Loss: 3964.281982421875, Val_Loss: 4078.46435546875\n",
      "Epoch 958, Train_Loss: 3964.247802734375, Val_Loss: 4078.435546875\n",
      "Epoch 959, Train_Loss: 3964.339111328125, Val_Loss: 4078.317626953125\n",
      "Epoch 960, Train_Loss: 3964.377197265625, Val_Loss: 4078.2724609375\n",
      "Epoch 961, Train_Loss: 3964.38037109375, Val_Loss: 4078.24072265625\n",
      "Epoch 962, Train_Loss: 3964.340576171875, Val_Loss: 4078.249267578125\n",
      "Epoch 963, Train_Loss: 3964.304443359375, Val_Loss: 4078.27490234375\n",
      "Epoch 964, Train_Loss: 3964.15625, Val_Loss: 4078.3466796875\n",
      "Epoch 965, Train_Loss: 3964.236328125, Val_Loss: 4078.381591796875\n",
      "Epoch 966, Train_Loss: 3964.184326171875, Val_Loss: 4078.453125\n",
      "Epoch 967, Train_Loss: 3964.22412109375, Val_Loss: 4078.3623046875\n",
      "Epoch 968, Train_Loss: 3964.210693359375, Val_Loss: 4078.27587890625\n",
      "Epoch 969, Train_Loss: 3964.231689453125, Val_Loss: 4078.194091796875\n",
      "Epoch 970, Train_Loss: 3964.220458984375, Val_Loss: 4078.15283203125\n",
      "Epoch 971, Train_Loss: 3964.20703125, Val_Loss: 4078.143310546875\n",
      "Epoch 972, Train_Loss: 3964.210205078125, Val_Loss: 4078.156005859375\n",
      "Epoch 973, Train_Loss: 3964.16650390625, Val_Loss: 4078.17529296875\n",
      "Epoch 974, Train_Loss: 3964.1591796875, Val_Loss: 4078.22314453125\n",
      "Epoch 975, Train_Loss: 3964.032958984375, Val_Loss: 4078.230712890625\n",
      "Epoch 976, Train_Loss: 3964.125244140625, Val_Loss: 4078.24609375\n",
      "Epoch 977, Train_Loss: 3964.0126953125, Val_Loss: 4078.20751953125\n",
      "Epoch 978, Train_Loss: 3964.099609375, Val_Loss: 4078.154296875\n",
      "Epoch 979, Train_Loss: 3964.07080078125, Val_Loss: 4078.07470703125\n",
      "Epoch 980, Train_Loss: 3964.089599609375, Val_Loss: 4078.06591796875\n",
      "Epoch 981, Train_Loss: 3964.089111328125, Val_Loss: 4078.043701171875\n",
      "Epoch 982, Train_Loss: 3964.053466796875, Val_Loss: 4078.043212890625\n",
      "Epoch 983, Train_Loss: 3964.02294921875, Val_Loss: 4078.0537109375\n",
      "Epoch 984, Train_Loss: 3964.04150390625, Val_Loss: 4078.09716796875\n",
      "Epoch 985, Train_Loss: 3964.05078125, Val_Loss: 4078.073974609375\n",
      "Epoch 986, Train_Loss: 3964.004638671875, Val_Loss: 4077.95556640625\n",
      "Epoch 987, Train_Loss: 3963.96484375, Val_Loss: 4077.959228515625\n",
      "Epoch 988, Train_Loss: 3964.00634765625, Val_Loss: 4077.89794921875\n",
      "Epoch 989, Train_Loss: 3963.995361328125, Val_Loss: 4077.8623046875\n",
      "Epoch 990, Train_Loss: 3963.989990234375, Val_Loss: 4077.8515625\n",
      "Epoch 991, Train_Loss: 3963.968994140625, Val_Loss: 4077.872314453125\n",
      "Epoch 992, Train_Loss: 3963.916748046875, Val_Loss: 4077.90673828125\n",
      "Epoch 993, Train_Loss: 3963.9345703125, Val_Loss: 4077.9140625\n",
      "Epoch 994, Train_Loss: 3963.9228515625, Val_Loss: 4077.94677734375\n",
      "Epoch 995, Train_Loss: 3963.9072265625, Val_Loss: 4077.88232421875\n",
      "Epoch 996, Train_Loss: 3963.91259765625, Val_Loss: 4077.8701171875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 997, Train_Loss: 3963.665771484375, Val_Loss: 4077.826904296875\n",
      "Epoch 998, Train_Loss: 3963.6787109375, Val_Loss: 4077.79443359375\n",
      "Epoch 999, Train_Loss: 3963.666259765625, Val_Loss: 4077.773681640625\n",
      "Epoch 1000, Train_Loss: 3963.644287109375, Val_Loss: 4077.81005859375\n",
      "Epoch 1001, Train_Loss: 3963.607666015625, Val_Loss: 4077.827880859375\n",
      "Epoch 1002, Train_Loss: 3963.593994140625, Val_Loss: 4077.808349609375\n",
      "Epoch 1003, Train_Loss: 3963.5830078125, Val_Loss: 4077.799560546875\n",
      "Epoch 1004, Train_Loss: 3963.566162109375, Val_Loss: 4077.791259765625\n",
      "Epoch 1005, Train_Loss: 3963.582275390625, Val_Loss: 4077.7763671875\n",
      "Epoch 1006, Train_Loss: 3963.5830078125, Val_Loss: 4077.771240234375\n",
      "Epoch 1007, Train_Loss: 3963.4755859375, Val_Loss: 4077.738037109375\n",
      "Epoch 1008, Train_Loss: 3963.443115234375, Val_Loss: 4077.720703125\n",
      "Epoch 1009, Train_Loss: 3963.451904296875, Val_Loss: 4077.71923828125\n",
      "Epoch 1010, Train_Loss: 3963.4345703125, Val_Loss: 4077.681640625\n",
      "Epoch 1011, Train_Loss: 3963.435791015625, Val_Loss: 4077.6748046875\n",
      "Epoch 1012, Train_Loss: 3963.4267578125, Val_Loss: 4077.6923828125\n",
      "Epoch 1013, Train_Loss: 3963.3818359375, Val_Loss: 4077.6787109375\n",
      "Epoch 1014, Train_Loss: 3963.41162109375, Val_Loss: 4077.6630859375\n",
      "Epoch 1015, Train_Loss: 3963.388916015625, Val_Loss: 4077.6640625\n",
      "Epoch 1016, Train_Loss: 3963.380615234375, Val_Loss: 4077.6640625\n",
      "Epoch 1017, Train_Loss: 3963.373779296875, Val_Loss: 4077.656494140625\n",
      "Epoch 1018, Train_Loss: 3963.33349609375, Val_Loss: 4077.5712890625\n",
      "Epoch 1019, Train_Loss: 3963.330810546875, Val_Loss: 4077.536865234375\n",
      "Epoch 1020, Train_Loss: 3963.317138671875, Val_Loss: 4077.517578125\n",
      "Epoch 1021, Train_Loss: 3963.302490234375, Val_Loss: 4077.5244140625\n",
      "Epoch 1022, Train_Loss: 3963.302001953125, Val_Loss: 4077.523193359375\n",
      "Epoch 1023, Train_Loss: 3963.273681640625, Val_Loss: 4077.529296875\n",
      "Epoch 1024, Train_Loss: 3963.271728515625, Val_Loss: 4077.55029296875\n",
      "Epoch 1025, Train_Loss: 3963.2626953125, Val_Loss: 4077.536376953125\n",
      "Epoch 1026, Train_Loss: 3963.25390625, Val_Loss: 4077.533203125\n",
      "Epoch 1027, Train_Loss: 3963.223388671875, Val_Loss: 4077.503662109375\n",
      "Epoch 1028, Train_Loss: 3963.1669921875, Val_Loss: 4077.414306640625\n",
      "Epoch 1029, Train_Loss: 3963.1962890625, Val_Loss: 4077.377197265625\n",
      "Epoch 1030, Train_Loss: 3963.172607421875, Val_Loss: 4077.37158203125\n",
      "Epoch 1031, Train_Loss: 3963.16064453125, Val_Loss: 4077.3671875\n",
      "Epoch 1032, Train_Loss: 3963.170166015625, Val_Loss: 4077.3515625\n",
      "Epoch 1033, Train_Loss: 3963.1201171875, Val_Loss: 4077.36328125\n",
      "Epoch 1034, Train_Loss: 3963.1201171875, Val_Loss: 4077.35009765625\n",
      "Epoch 1035, Train_Loss: 3963.106689453125, Val_Loss: 4077.35205078125\n",
      "Epoch 1036, Train_Loss: 3963.101806640625, Val_Loss: 4077.34228515625\n",
      "Epoch 1037, Train_Loss: 3963.09619140625, Val_Loss: 4077.3251953125\n",
      "Epoch 1038, Train_Loss: 3963.076171875, Val_Loss: 4077.302734375\n",
      "Epoch 1039, Train_Loss: 3963.04541015625, Val_Loss: 4077.285888671875\n",
      "Epoch 1040, Train_Loss: 3963.0751953125, Val_Loss: 4077.263916015625\n",
      "Epoch 1041, Train_Loss: 3963.072509765625, Val_Loss: 4077.255615234375\n",
      "Epoch 1042, Train_Loss: 3963.062744140625, Val_Loss: 4077.246337890625\n",
      "Epoch 1043, Train_Loss: 3963.035888671875, Val_Loss: 4077.24072265625\n",
      "Epoch 1044, Train_Loss: 3962.991943359375, Val_Loss: 4077.239990234375\n",
      "Epoch 1045, Train_Loss: 3962.96875, Val_Loss: 4077.241943359375\n",
      "Epoch 1046, Train_Loss: 3962.991943359375, Val_Loss: 4077.21435546875\n",
      "Epoch 1047, Train_Loss: 3962.985107421875, Val_Loss: 4077.215576171875\n",
      "Epoch 1048, Train_Loss: 3962.94775390625, Val_Loss: 4077.17236328125\n",
      "Epoch 1049, Train_Loss: 3962.905029296875, Val_Loss: 4077.17236328125\n",
      "Epoch 1050, Train_Loss: 3962.912353515625, Val_Loss: 4077.14990234375\n",
      "Epoch 1051, Train_Loss: 3962.905517578125, Val_Loss: 4077.10400390625\n",
      "Epoch 1052, Train_Loss: 3962.914306640625, Val_Loss: 4077.087890625\n",
      "Epoch 1053, Train_Loss: 3962.9111328125, Val_Loss: 4077.078369140625\n",
      "Epoch 1054, Train_Loss: 3962.89697265625, Val_Loss: 4077.070068359375\n",
      "Epoch 1055, Train_Loss: 3962.86572265625, Val_Loss: 4077.068115234375\n",
      "Epoch 1056, Train_Loss: 3962.848388671875, Val_Loss: 4077.05810546875\n",
      "Epoch 1057, Train_Loss: 3962.830078125, Val_Loss: 4077.0576171875\n",
      "Epoch 1058, Train_Loss: 3962.810546875, Val_Loss: 4077.0419921875\n",
      "Epoch 1059, Train_Loss: 3962.7958984375, Val_Loss: 4076.992431640625\n",
      "Epoch 1060, Train_Loss: 3962.78759765625, Val_Loss: 4076.98046875\n",
      "Epoch 1061, Train_Loss: 3962.7822265625, Val_Loss: 4076.9716796875\n",
      "Epoch 1062, Train_Loss: 3962.77978515625, Val_Loss: 4076.9560546875\n",
      "Epoch 1063, Train_Loss: 3962.772216796875, Val_Loss: 4076.94189453125\n",
      "Epoch 1064, Train_Loss: 3962.763427734375, Val_Loss: 4076.93115234375\n",
      "Epoch 1065, Train_Loss: 3962.75341796875, Val_Loss: 4076.92236328125\n",
      "Epoch 1066, Train_Loss: 3962.736572265625, Val_Loss: 4076.926025390625\n",
      "Epoch 1067, Train_Loss: 3962.727294921875, Val_Loss: 4076.917236328125\n",
      "Epoch 1068, Train_Loss: 3962.72216796875, Val_Loss: 4076.9072265625\n",
      "Epoch 1069, Train_Loss: 3962.693603515625, Val_Loss: 4076.824462890625\n",
      "Epoch 1070, Train_Loss: 3962.683837890625, Val_Loss: 4076.81640625\n",
      "Epoch 1071, Train_Loss: 3962.676025390625, Val_Loss: 4076.80517578125\n",
      "Epoch 1072, Train_Loss: 3962.671142578125, Val_Loss: 4076.793701171875\n",
      "Epoch 1073, Train_Loss: 3962.65771484375, Val_Loss: 4076.788330078125\n",
      "Epoch 1074, Train_Loss: 3962.6455078125, Val_Loss: 4076.775146484375\n",
      "Epoch 1075, Train_Loss: 3962.634521484375, Val_Loss: 4076.763671875\n",
      "Epoch 1076, Train_Loss: 3962.615234375, Val_Loss: 4076.75048828125\n",
      "Epoch 1077, Train_Loss: 3962.61083984375, Val_Loss: 4076.739501953125\n",
      "Epoch 1078, Train_Loss: 3962.60400390625, Val_Loss: 4076.733642578125\n",
      "Epoch 1079, Train_Loss: 3962.53076171875, Val_Loss: 4076.709228515625\n",
      "Epoch 1080, Train_Loss: 3962.526123046875, Val_Loss: 4076.70166015625\n",
      "Epoch 1081, Train_Loss: 3962.5234375, Val_Loss: 4076.6962890625\n",
      "Epoch 1082, Train_Loss: 3962.51708984375, Val_Loss: 4076.687255859375\n",
      "Epoch 1083, Train_Loss: 3962.365234375, Val_Loss: 4076.672119140625\n",
      "Epoch 1084, Train_Loss: 3962.349365234375, Val_Loss: 4076.65673828125\n",
      "Epoch 1085, Train_Loss: 3962.335205078125, Val_Loss: 4076.6435546875\n",
      "Epoch 1086, Train_Loss: 3962.3310546875, Val_Loss: 4076.63330078125\n",
      "Epoch 1087, Train_Loss: 3962.3232421875, Val_Loss: 4076.62646484375\n",
      "Epoch 1088, Train_Loss: 3962.313232421875, Val_Loss: 4076.615234375\n",
      "Epoch 1089, Train_Loss: 3962.010986328125, Val_Loss: 4076.58154296875\n",
      "Epoch 1090, Train_Loss: 3961.9990234375, Val_Loss: 4076.57275390625\n",
      "Epoch 1091, Train_Loss: 3961.99169921875, Val_Loss: 4076.56005859375\n",
      "Epoch 1092, Train_Loss: 3961.995361328125, Val_Loss: 4076.543701171875\n",
      "Epoch 1093, Train_Loss: 3961.981201171875, Val_Loss: 4076.53271484375\n",
      "Epoch 1094, Train_Loss: 3961.955078125, Val_Loss: 4076.533203125\n",
      "Epoch 1095, Train_Loss: 3961.946044921875, Val_Loss: 4076.51611328125\n",
      "Epoch 1096, Train_Loss: 3961.952392578125, Val_Loss: 4076.49951171875\n",
      "Epoch 1097, Train_Loss: 3961.945068359375, Val_Loss: 4076.487548828125\n",
      "Epoch 1098, Train_Loss: 3961.923095703125, Val_Loss: 4076.430419921875\n",
      "Epoch 1099, Train_Loss: 3961.9208984375, Val_Loss: 4076.422119140625\n",
      "Epoch 1100, Train_Loss: 3961.911376953125, Val_Loss: 4076.40771484375\n",
      "Epoch 1101, Train_Loss: 3961.903564453125, Val_Loss: 4076.396728515625\n",
      "Epoch 1102, Train_Loss: 3961.89404296875, Val_Loss: 4076.388427734375\n",
      "Epoch 1103, Train_Loss: 3961.88525390625, Val_Loss: 4076.3779296875\n",
      "Epoch 1104, Train_Loss: 3961.875732421875, Val_Loss: 4076.3740234375\n",
      "Epoch 1105, Train_Loss: 3961.8642578125, Val_Loss: 4076.363525390625\n",
      "Epoch 1106, Train_Loss: 3961.85595703125, Val_Loss: 4076.351318359375\n",
      "Epoch 1107, Train_Loss: 3961.849609375, Val_Loss: 4076.3447265625\n",
      "Epoch 1108, Train_Loss: 3961.8134765625, Val_Loss: 4076.26123046875\n",
      "Epoch 1109, Train_Loss: 3961.7998046875, Val_Loss: 4076.24951171875\n",
      "Epoch 1110, Train_Loss: 3961.79052734375, Val_Loss: 4076.236328125\n",
      "Epoch 1111, Train_Loss: 3961.779296875, Val_Loss: 4076.2255859375\n",
      "Epoch 1112, Train_Loss: 3961.765380859375, Val_Loss: 4076.2119140625\n",
      "Epoch 1113, Train_Loss: 3961.755859375, Val_Loss: 4076.200439453125\n",
      "Epoch 1114, Train_Loss: 3961.7294921875, Val_Loss: 4076.152099609375\n",
      "Epoch 1115, Train_Loss: 3961.72216796875, Val_Loss: 4076.138916015625\n",
      "Epoch 1116, Train_Loss: 3961.70654296875, Val_Loss: 4076.127685546875\n",
      "Epoch 1117, Train_Loss: 3961.696533203125, Val_Loss: 4076.11669921875\n",
      "Epoch 1118, Train_Loss: 3961.669921875, Val_Loss: 4076.1064453125\n",
      "Epoch 1119, Train_Loss: 3961.65771484375, Val_Loss: 4076.09033203125\n",
      "Epoch 1120, Train_Loss: 3961.644775390625, Val_Loss: 4076.080078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1121, Train_Loss: 3961.63623046875, Val_Loss: 4076.069091796875\n",
      "Epoch 1122, Train_Loss: 3961.61962890625, Val_Loss: 4076.060791015625\n",
      "Epoch 1123, Train_Loss: 3961.615478515625, Val_Loss: 4076.04931640625\n",
      "Epoch 1124, Train_Loss: 3961.60400390625, Val_Loss: 4076.03564453125\n",
      "Epoch 1125, Train_Loss: 3961.589599609375, Val_Loss: 4076.023193359375\n",
      "Epoch 1126, Train_Loss: 3961.58349609375, Val_Loss: 4076.01513671875\n",
      "Epoch 1127, Train_Loss: 3961.572021484375, Val_Loss: 4076.00390625\n",
      "Epoch 1128, Train_Loss: 3961.5322265625, Val_Loss: 4075.962890625\n",
      "Epoch 1129, Train_Loss: 3961.523681640625, Val_Loss: 4075.9580078125\n",
      "Epoch 1130, Train_Loss: 3961.514892578125, Val_Loss: 4075.9462890625\n",
      "Epoch 1131, Train_Loss: 3961.50390625, Val_Loss: 4075.932861328125\n",
      "Epoch 1132, Train_Loss: 3961.492919921875, Val_Loss: 4075.929931640625\n",
      "Epoch 1133, Train_Loss: 3961.481689453125, Val_Loss: 4075.919677734375\n",
      "Epoch 1134, Train_Loss: 3961.4775390625, Val_Loss: 4075.903564453125\n",
      "Epoch 1135, Train_Loss: 3961.46826171875, Val_Loss: 4075.892333984375\n",
      "Epoch 1136, Train_Loss: 3961.4541015625, Val_Loss: 4075.8955078125\n",
      "Epoch 1137, Train_Loss: 3961.438720703125, Val_Loss: 4075.8271484375\n",
      "Epoch 1138, Train_Loss: 3961.424560546875, Val_Loss: 4075.81396484375\n",
      "Epoch 1139, Train_Loss: 3961.408447265625, Val_Loss: 4075.801513671875\n",
      "Epoch 1140, Train_Loss: 3961.396484375, Val_Loss: 4075.79150390625\n",
      "Epoch 1141, Train_Loss: 3961.393310546875, Val_Loss: 4075.78271484375\n",
      "Epoch 1142, Train_Loss: 3961.385498046875, Val_Loss: 4075.78271484375\n",
      "Epoch 1143, Train_Loss: 3961.3564453125, Val_Loss: 4075.767578125\n",
      "Epoch 1144, Train_Loss: 3961.351806640625, Val_Loss: 4075.756103515625\n",
      "Epoch 1145, Train_Loss: 3961.338134765625, Val_Loss: 4075.746337890625\n",
      "Epoch 1146, Train_Loss: 3961.330322265625, Val_Loss: 4075.74072265625\n",
      "Epoch 1147, Train_Loss: 3961.2958984375, Val_Loss: 4075.662841796875\n",
      "Epoch 1148, Train_Loss: 3961.28857421875, Val_Loss: 4075.648681640625\n",
      "Epoch 1149, Train_Loss: 3961.27587890625, Val_Loss: 4075.636474609375\n",
      "Epoch 1150, Train_Loss: 3961.27001953125, Val_Loss: 4075.6240234375\n",
      "Epoch 1151, Train_Loss: 3961.252685546875, Val_Loss: 4075.616455078125\n",
      "Epoch 1152, Train_Loss: 3961.23193359375, Val_Loss: 4075.609130859375\n",
      "Epoch 1153, Train_Loss: 3961.23388671875, Val_Loss: 4075.59228515625\n",
      "Epoch 1154, Train_Loss: 3961.21875, Val_Loss: 4075.582763671875\n",
      "Epoch 1155, Train_Loss: 3961.212890625, Val_Loss: 4075.5703125\n",
      "Epoch 1156, Train_Loss: 3961.195068359375, Val_Loss: 4075.565673828125\n",
      "Epoch 1157, Train_Loss: 3961.100830078125, Val_Loss: 4075.544921875\n",
      "Epoch 1158, Train_Loss: 3961.099365234375, Val_Loss: 4075.5283203125\n",
      "Epoch 1159, Train_Loss: 3961.095703125, Val_Loss: 4075.509521484375\n",
      "Epoch 1160, Train_Loss: 3961.0810546875, Val_Loss: 4075.49951171875\n",
      "Epoch 1161, Train_Loss: 3961.06201171875, Val_Loss: 4075.48681640625\n",
      "Epoch 1162, Train_Loss: 3961.044677734375, Val_Loss: 4075.47607421875\n",
      "Epoch 1163, Train_Loss: 3961.025390625, Val_Loss: 4075.454833984375\n",
      "Epoch 1164, Train_Loss: 3961.00927734375, Val_Loss: 4075.43408203125\n",
      "Epoch 1165, Train_Loss: 3960.96826171875, Val_Loss: 4075.3916015625\n",
      "Epoch 1166, Train_Loss: 3960.6728515625, Val_Loss: 4075.255126953125\n",
      "Epoch 1167, Train_Loss: 3960.612548828125, Val_Loss: 4075.1279296875\n",
      "Epoch 1168, Train_Loss: 3960.66845703125, Val_Loss: 4075.0205078125\n",
      "Epoch 1169, Train_Loss: 3960.63037109375, Val_Loss: 4074.98681640625\n",
      "Epoch 1170, Train_Loss: 3960.635986328125, Val_Loss: 4074.921630859375\n",
      "Epoch 1171, Train_Loss: 3960.63720703125, Val_Loss: 4074.977294921875\n",
      "Epoch 1172, Train_Loss: 3960.664794921875, Val_Loss: 4075.02490234375\n",
      "Epoch 1173, Train_Loss: 3960.584228515625, Val_Loss: 4075.0947265625\n",
      "Epoch 1174, Train_Loss: 3960.605712890625, Val_Loss: 4074.980712890625\n",
      "Epoch 1175, Train_Loss: 3960.51611328125, Val_Loss: 4074.94873046875\n",
      "Epoch 1176, Train_Loss: 3960.598388671875, Val_Loss: 4074.900390625\n",
      "Epoch 1177, Train_Loss: 3960.54443359375, Val_Loss: 4074.899658203125\n",
      "Epoch 1178, Train_Loss: 3960.547607421875, Val_Loss: 4074.90478515625\n",
      "Epoch 1179, Train_Loss: 3960.484130859375, Val_Loss: 4074.897705078125\n",
      "Epoch 1180, Train_Loss: 3960.55078125, Val_Loss: 4074.90673828125\n",
      "Epoch 1181, Train_Loss: 3960.48583984375, Val_Loss: 4074.873291015625\n",
      "Epoch 1182, Train_Loss: 3960.468994140625, Val_Loss: 4074.817626953125\n",
      "Epoch 1183, Train_Loss: 3960.412841796875, Val_Loss: 4074.785888671875\n",
      "Epoch 1184, Train_Loss: 3960.455322265625, Val_Loss: 4074.87158203125\n",
      "Epoch 1185, Train_Loss: 3960.4453125, Val_Loss: 4074.764892578125\n",
      "Epoch 1186, Train_Loss: 3960.391357421875, Val_Loss: 4074.76318359375\n",
      "Epoch 1187, Train_Loss: 3960.42919921875, Val_Loss: 4074.7626953125\n",
      "Epoch 1188, Train_Loss: 3960.376708984375, Val_Loss: 4074.76611328125\n",
      "Epoch 1189, Train_Loss: 3960.363037109375, Val_Loss: 4074.721923828125\n",
      "Epoch 1190, Train_Loss: 3960.36376953125, Val_Loss: 4074.733642578125\n",
      "Epoch 1191, Train_Loss: 3960.376220703125, Val_Loss: 4074.72607421875\n",
      "Epoch 1192, Train_Loss: 3960.37158203125, Val_Loss: 4074.709228515625\n",
      "Epoch 1193, Train_Loss: 3960.25927734375, Val_Loss: 4074.67919921875\n",
      "Epoch 1194, Train_Loss: 3960.248291015625, Val_Loss: 4074.662109375\n",
      "Epoch 1195, Train_Loss: 3960.255126953125, Val_Loss: 4074.573974609375\n",
      "Epoch 1196, Train_Loss: 3960.27294921875, Val_Loss: 4074.609619140625\n",
      "Epoch 1197, Train_Loss: 3960.239990234375, Val_Loss: 4074.601318359375\n",
      "Epoch 1198, Train_Loss: 3960.225830078125, Val_Loss: 4074.6103515625\n",
      "Epoch 1199, Train_Loss: 3960.220703125, Val_Loss: 4074.618408203125\n",
      "Epoch 1200, Train_Loss: 3960.203857421875, Val_Loss: 4074.5888671875\n",
      "Epoch 1201, Train_Loss: 3960.19287109375, Val_Loss: 4074.611083984375\n",
      "Epoch 1202, Train_Loss: 3960.17138671875, Val_Loss: 4074.6240234375\n",
      "Epoch 1203, Train_Loss: 3960.13037109375, Val_Loss: 4074.638916015625\n",
      "Epoch 1204, Train_Loss: 3960.082275390625, Val_Loss: 4074.580322265625\n",
      "Epoch 1205, Train_Loss: 3960.0771484375, Val_Loss: 4074.57763671875\n",
      "Epoch 1206, Train_Loss: 3960.05126953125, Val_Loss: 4074.532470703125\n",
      "Epoch 1207, Train_Loss: 3960.102783203125, Val_Loss: 4074.454833984375\n",
      "Epoch 1208, Train_Loss: 3960.084228515625, Val_Loss: 4074.447265625\n",
      "Epoch 1209, Train_Loss: 3960.052001953125, Val_Loss: 4074.484375\n",
      "Epoch 1210, Train_Loss: 3960.05810546875, Val_Loss: 4074.514404296875\n",
      "Epoch 1211, Train_Loss: 3960.013671875, Val_Loss: 4074.509521484375\n",
      "Epoch 1212, Train_Loss: 3959.99462890625, Val_Loss: 4074.485595703125\n",
      "Epoch 1213, Train_Loss: 3959.97900390625, Val_Loss: 4074.419921875\n",
      "Epoch 1214, Train_Loss: 3959.96484375, Val_Loss: 4074.42431640625\n",
      "Epoch 1215, Train_Loss: 3959.904541015625, Val_Loss: 4074.392333984375\n",
      "Epoch 1216, Train_Loss: 3959.918701171875, Val_Loss: 4074.359619140625\n",
      "Epoch 1217, Train_Loss: 3959.908935546875, Val_Loss: 4074.345947265625\n",
      "Epoch 1218, Train_Loss: 3959.8779296875, Val_Loss: 4074.367919921875\n",
      "Epoch 1219, Train_Loss: 3959.896728515625, Val_Loss: 4074.36962890625\n",
      "Epoch 1220, Train_Loss: 3959.851318359375, Val_Loss: 4074.37109375\n",
      "Epoch 1221, Train_Loss: 3959.845458984375, Val_Loss: 4074.36279296875\n",
      "Epoch 1222, Train_Loss: 3959.7919921875, Val_Loss: 4074.247314453125\n",
      "Epoch 1223, Train_Loss: 3959.77001953125, Val_Loss: 4074.232421875\n",
      "Epoch 1224, Train_Loss: 3959.782958984375, Val_Loss: 4074.214111328125\n",
      "Epoch 1225, Train_Loss: 3959.7578125, Val_Loss: 4074.214111328125\n",
      "Epoch 1226, Train_Loss: 3959.775146484375, Val_Loss: 4074.188720703125\n",
      "Epoch 1227, Train_Loss: 3959.723876953125, Val_Loss: 4074.188720703125\n",
      "Epoch 1228, Train_Loss: 3959.705078125, Val_Loss: 4074.16845703125\n",
      "Epoch 1229, Train_Loss: 3959.68994140625, Val_Loss: 4074.168701171875\n",
      "Epoch 1230, Train_Loss: 3959.6787109375, Val_Loss: 4074.157958984375\n",
      "Epoch 1231, Train_Loss: 3959.61279296875, Val_Loss: 4074.136474609375\n",
      "Epoch 1232, Train_Loss: 3959.639404296875, Val_Loss: 4074.09228515625\n",
      "Epoch 1233, Train_Loss: 3959.581298828125, Val_Loss: 4074.0458984375\n",
      "Epoch 1234, Train_Loss: 3959.615966796875, Val_Loss: 4074.071533203125\n",
      "Epoch 1235, Train_Loss: 3959.600341796875, Val_Loss: 4074.095947265625\n",
      "Epoch 1236, Train_Loss: 3959.589111328125, Val_Loss: 4074.0546875\n",
      "Epoch 1237, Train_Loss: 3959.5615234375, Val_Loss: 4074.001708984375\n",
      "Epoch 1238, Train_Loss: 3959.559814453125, Val_Loss: 4074.045166015625\n",
      "Epoch 1239, Train_Loss: 3959.546142578125, Val_Loss: 4074.03369140625\n",
      "Epoch 1240, Train_Loss: 3959.430419921875, Val_Loss: 4074.025146484375\n",
      "Epoch 1241, Train_Loss: 3959.415771484375, Val_Loss: 4074.013671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1242, Train_Loss: 3959.422607421875, Val_Loss: 4073.97802734375\n",
      "Epoch 1243, Train_Loss: 3959.40673828125, Val_Loss: 4073.910400390625\n",
      "Epoch 1244, Train_Loss: 3959.36376953125, Val_Loss: 4073.900390625\n",
      "Epoch 1245, Train_Loss: 3959.36083984375, Val_Loss: 4073.8916015625\n",
      "Epoch 1246, Train_Loss: 3959.358642578125, Val_Loss: 4073.9052734375\n",
      "Epoch 1247, Train_Loss: 3959.3623046875, Val_Loss: 4073.869140625\n",
      "Epoch 1248, Train_Loss: 3959.32080078125, Val_Loss: 4073.866455078125\n",
      "Epoch 1249, Train_Loss: 3959.31787109375, Val_Loss: 4073.80322265625\n",
      "Epoch 1250, Train_Loss: 3959.3037109375, Val_Loss: 4073.8203125\n",
      "Epoch 1251, Train_Loss: 3959.306396484375, Val_Loss: 4073.728759765625\n",
      "Epoch 1252, Train_Loss: 3959.28466796875, Val_Loss: 4073.70361328125\n",
      "Epoch 1253, Train_Loss: 3959.232421875, Val_Loss: 4073.719482421875\n",
      "Epoch 1254, Train_Loss: 3959.285400390625, Val_Loss: 4073.7587890625\n",
      "Epoch 1255, Train_Loss: 3959.231201171875, Val_Loss: 4073.760009765625\n",
      "Epoch 1256, Train_Loss: 3959.2265625, Val_Loss: 4073.730712890625\n",
      "Epoch 1257, Train_Loss: 3959.20458984375, Val_Loss: 4073.688720703125\n",
      "Epoch 1258, Train_Loss: 3959.120849609375, Val_Loss: 4073.603271484375\n",
      "Epoch 1259, Train_Loss: 3959.08935546875, Val_Loss: 4073.63671875\n",
      "Epoch 1260, Train_Loss: 3959.07568359375, Val_Loss: 4073.614013671875\n",
      "Epoch 1261, Train_Loss: 3959.07568359375, Val_Loss: 4073.609130859375\n",
      "Epoch 1262, Train_Loss: 3959.046875, Val_Loss: 4073.615234375\n",
      "Epoch 1263, Train_Loss: 3959.044921875, Val_Loss: 4073.57958984375\n",
      "Epoch 1264, Train_Loss: 3959.001220703125, Val_Loss: 4073.560791015625\n",
      "Epoch 1265, Train_Loss: 3959.012939453125, Val_Loss: 4073.546875\n",
      "Epoch 1266, Train_Loss: 3959.001220703125, Val_Loss: 4073.525146484375\n",
      "Epoch 1267, Train_Loss: 3958.972412109375, Val_Loss: 4073.518798828125\n",
      "Epoch 1268, Train_Loss: 3958.956298828125, Val_Loss: 4073.531982421875\n",
      "Epoch 1269, Train_Loss: 3958.9462890625, Val_Loss: 4073.525146484375\n",
      "Epoch 1270, Train_Loss: 3958.958251953125, Val_Loss: 4073.451904296875\n",
      "Epoch 1271, Train_Loss: 3958.966064453125, Val_Loss: 4073.41650390625\n",
      "Epoch 1272, Train_Loss: 3958.95947265625, Val_Loss: 4073.40771484375\n",
      "Epoch 1273, Train_Loss: 3958.898193359375, Val_Loss: 4073.361083984375\n",
      "Epoch 1274, Train_Loss: 3958.90185546875, Val_Loss: 4073.33251953125\n",
      "Epoch 1275, Train_Loss: 3958.87451171875, Val_Loss: 4073.3369140625\n",
      "Epoch 1276, Train_Loss: 3958.872802734375, Val_Loss: 4073.29931640625\n",
      "Epoch 1277, Train_Loss: 3958.826904296875, Val_Loss: 4073.384033203125\n",
      "Epoch 1278, Train_Loss: 3958.801025390625, Val_Loss: 4073.37890625\n",
      "Epoch 1279, Train_Loss: 3958.78857421875, Val_Loss: 4073.375244140625\n",
      "Epoch 1280, Train_Loss: 3958.776611328125, Val_Loss: 4073.359130859375\n",
      "Epoch 1281, Train_Loss: 3958.771728515625, Val_Loss: 4073.32080078125\n",
      "Epoch 1282, Train_Loss: 3958.78173828125, Val_Loss: 4073.29248046875\n",
      "Epoch 1283, Train_Loss: 3958.73095703125, Val_Loss: 4073.297119140625\n",
      "Epoch 1284, Train_Loss: 3958.755126953125, Val_Loss: 4073.27587890625\n",
      "Epoch 1285, Train_Loss: 3958.71044921875, Val_Loss: 4073.22998046875\n",
      "Epoch 1286, Train_Loss: 3958.701171875, Val_Loss: 4073.227294921875\n",
      "Epoch 1287, Train_Loss: 3958.67333984375, Val_Loss: 4073.204345703125\n",
      "Epoch 1288, Train_Loss: 3958.66845703125, Val_Loss: 4073.1533203125\n",
      "Epoch 1289, Train_Loss: 3958.6318359375, Val_Loss: 4073.1240234375\n",
      "Epoch 1290, Train_Loss: 3958.63623046875, Val_Loss: 4073.0966796875\n",
      "Epoch 1291, Train_Loss: 3958.64501953125, Val_Loss: 4073.1064453125\n",
      "Epoch 1292, Train_Loss: 3958.6181640625, Val_Loss: 4073.110107421875\n",
      "Epoch 1293, Train_Loss: 3958.556884765625, Val_Loss: 4073.120361328125\n",
      "Epoch 1294, Train_Loss: 3958.57177734375, Val_Loss: 4073.080078125\n",
      "Epoch 1295, Train_Loss: 3958.554443359375, Val_Loss: 4073.00439453125\n",
      "Epoch 1296, Train_Loss: 3958.576904296875, Val_Loss: 4072.977294921875\n",
      "Epoch 1297, Train_Loss: 3958.56689453125, Val_Loss: 4072.9755859375\n",
      "Epoch 1298, Train_Loss: 3958.540283203125, Val_Loss: 4072.9599609375\n",
      "Epoch 1299, Train_Loss: 3958.53857421875, Val_Loss: 4072.9462890625\n",
      "Epoch 1300, Train_Loss: 3958.486328125, Val_Loss: 4072.951904296875\n",
      "Epoch 1301, Train_Loss: 3958.4951171875, Val_Loss: 4072.92529296875\n",
      "Epoch 1302, Train_Loss: 3958.260009765625, Val_Loss: 4072.919677734375\n",
      "Epoch 1303, Train_Loss: 3958.2705078125, Val_Loss: 4072.896484375\n",
      "Epoch 1304, Train_Loss: 3958.160888671875, Val_Loss: 4072.861083984375\n",
      "Epoch 1305, Train_Loss: 3958.159912109375, Val_Loss: 4072.84228515625\n",
      "Epoch 1306, Train_Loss: 3958.18017578125, Val_Loss: 4072.852783203125\n",
      "Epoch 1307, Train_Loss: 3958.175048828125, Val_Loss: 4072.89111328125\n",
      "Epoch 1308, Train_Loss: 3958.1650390625, Val_Loss: 4072.8955078125\n",
      "Epoch 1309, Train_Loss: 3958.141357421875, Val_Loss: 4072.9287109375\n",
      "Epoch 1310, Train_Loss: 3958.139404296875, Val_Loss: 4072.845947265625\n",
      "Epoch 1311, Train_Loss: 3958.004638671875, Val_Loss: 4072.798095703125\n",
      "Epoch 1312, Train_Loss: 3958.005859375, Val_Loss: 4072.755126953125\n",
      "Epoch 1313, Train_Loss: 3958.04296875, Val_Loss: 4072.722900390625\n",
      "Epoch 1314, Train_Loss: 3957.974853515625, Val_Loss: 4072.73486328125\n",
      "Epoch 1315, Train_Loss: 3957.990478515625, Val_Loss: 4072.75390625\n",
      "Epoch 1316, Train_Loss: 3957.9580078125, Val_Loss: 4072.708740234375\n",
      "Epoch 1317, Train_Loss: 3957.94873046875, Val_Loss: 4072.68798828125\n",
      "Epoch 1318, Train_Loss: 3957.986328125, Val_Loss: 4072.681640625\n",
      "Epoch 1319, Train_Loss: 3957.93603515625, Val_Loss: 4072.66650390625\n",
      "Epoch 1320, Train_Loss: 3957.918212890625, Val_Loss: 4072.59521484375\n",
      "Epoch 1321, Train_Loss: 3957.82177734375, Val_Loss: 4072.62841796875\n",
      "Epoch 1322, Train_Loss: 3957.852294921875, Val_Loss: 4072.660888671875\n",
      "Epoch 1323, Train_Loss: 3957.781982421875, Val_Loss: 4072.627685546875\n",
      "Epoch 1324, Train_Loss: 3957.787841796875, Val_Loss: 4072.589599609375\n",
      "Epoch 1325, Train_Loss: 3957.84375, Val_Loss: 4072.583984375\n",
      "Epoch 1326, Train_Loss: 3957.822509765625, Val_Loss: 4072.476318359375\n",
      "Epoch 1327, Train_Loss: 3957.841796875, Val_Loss: 4072.46240234375\n",
      "Epoch 1328, Train_Loss: 3957.766845703125, Val_Loss: 4072.382080078125\n",
      "Epoch 1329, Train_Loss: 3957.797119140625, Val_Loss: 4072.447265625\n",
      "Epoch 1330, Train_Loss: 3957.722412109375, Val_Loss: 4072.451904296875\n",
      "Epoch 1331, Train_Loss: 3957.74365234375, Val_Loss: 4072.3759765625\n",
      "Epoch 1332, Train_Loss: 3957.7490234375, Val_Loss: 4072.34521484375\n",
      "Epoch 1333, Train_Loss: 3957.7294921875, Val_Loss: 4072.3623046875\n",
      "Epoch 1334, Train_Loss: 3957.689208984375, Val_Loss: 4072.38720703125\n",
      "Epoch 1335, Train_Loss: 3957.609619140625, Val_Loss: 4072.419921875\n",
      "Epoch 1336, Train_Loss: 3957.607666015625, Val_Loss: 4072.404052734375\n",
      "Epoch 1337, Train_Loss: 3957.560302734375, Val_Loss: 4072.37353515625\n",
      "Epoch 1338, Train_Loss: 3957.622314453125, Val_Loss: 4072.35009765625\n",
      "Epoch 1339, Train_Loss: 3957.632568359375, Val_Loss: 4072.277099609375\n",
      "Epoch 1340, Train_Loss: 3957.62646484375, Val_Loss: 4072.251708984375\n",
      "Epoch 1341, Train_Loss: 3957.610107421875, Val_Loss: 4072.2373046875\n",
      "Epoch 1342, Train_Loss: 3957.60693359375, Val_Loss: 4072.226806640625\n",
      "Epoch 1343, Train_Loss: 3957.538818359375, Val_Loss: 4072.2451171875\n",
      "Epoch 1344, Train_Loss: 3957.593505859375, Val_Loss: 4072.20751953125\n",
      "Epoch 1345, Train_Loss: 3957.507080078125, Val_Loss: 4072.160400390625\n",
      "Epoch 1346, Train_Loss: 3957.548828125, Val_Loss: 4072.1591796875\n",
      "Epoch 1347, Train_Loss: 3957.521728515625, Val_Loss: 4072.131103515625\n",
      "Epoch 1348, Train_Loss: 3957.516845703125, Val_Loss: 4072.147705078125\n",
      "Epoch 1349, Train_Loss: 3957.424560546875, Val_Loss: 4072.142333984375\n",
      "Epoch 1350, Train_Loss: 3957.445068359375, Val_Loss: 4072.15478515625\n",
      "Epoch 1351, Train_Loss: 3957.46923828125, Val_Loss: 4072.10205078125\n",
      "Epoch 1352, Train_Loss: 3957.451416015625, Val_Loss: 4072.0615234375\n",
      "Epoch 1353, Train_Loss: 3957.395751953125, Val_Loss: 4072.064697265625\n",
      "Epoch 1354, Train_Loss: 3957.36279296875, Val_Loss: 4071.992431640625\n",
      "Epoch 1355, Train_Loss: 3957.392578125, Val_Loss: 4072.003662109375\n",
      "Epoch 1356, Train_Loss: 3957.400634765625, Val_Loss: 4072.01953125\n",
      "Epoch 1357, Train_Loss: 3957.386474609375, Val_Loss: 4071.988037109375\n",
      "Epoch 1358, Train_Loss: 3957.378662109375, Val_Loss: 4071.98388671875\n",
      "Epoch 1359, Train_Loss: 3957.33984375, Val_Loss: 4071.910888671875\n",
      "Epoch 1360, Train_Loss: 3957.333984375, Val_Loss: 4071.897705078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1361, Train_Loss: 3957.336181640625, Val_Loss: 4071.92236328125\n",
      "Epoch 1362, Train_Loss: 3957.26025390625, Val_Loss: 4071.877197265625\n",
      "Epoch 1363, Train_Loss: 3957.2705078125, Val_Loss: 4071.900390625\n",
      "Epoch 1364, Train_Loss: 3957.237548828125, Val_Loss: 4071.85205078125\n",
      "Epoch 1365, Train_Loss: 3957.27783203125, Val_Loss: 4071.822509765625\n",
      "Epoch 1366, Train_Loss: 3957.21875, Val_Loss: 4071.786376953125\n",
      "Epoch 1367, Train_Loss: 3957.2529296875, Val_Loss: 4071.762451171875\n",
      "Epoch 1368, Train_Loss: 3957.19873046875, Val_Loss: 4071.76171875\n",
      "Epoch 1369, Train_Loss: 3957.23193359375, Val_Loss: 4071.75927734375\n",
      "Epoch 1370, Train_Loss: 3957.151611328125, Val_Loss: 4071.778076171875\n",
      "Epoch 1371, Train_Loss: 3957.101806640625, Val_Loss: 4071.779296875\n",
      "Epoch 1372, Train_Loss: 3957.11279296875, Val_Loss: 4071.7763671875\n",
      "Epoch 1373, Train_Loss: 3957.0546875, Val_Loss: 4071.737548828125\n",
      "Epoch 1374, Train_Loss: 3957.1259765625, Val_Loss: 4071.680908203125\n",
      "Epoch 1375, Train_Loss: 3957.058837890625, Val_Loss: 4071.63330078125\n",
      "Epoch 1376, Train_Loss: 3957.078857421875, Val_Loss: 4071.60595703125\n",
      "Epoch 1377, Train_Loss: 3957.056884765625, Val_Loss: 4071.57958984375\n",
      "Epoch 1378, Train_Loss: 3957.079345703125, Val_Loss: 4071.644775390625\n",
      "Epoch 1379, Train_Loss: 3956.7294921875, Val_Loss: 4071.6376953125\n",
      "Epoch 1380, Train_Loss: 3956.720458984375, Val_Loss: 4071.6767578125\n",
      "Epoch 1381, Train_Loss: 3956.7041015625, Val_Loss: 4071.639892578125\n",
      "Epoch 1382, Train_Loss: 3956.670166015625, Val_Loss: 4071.598876953125\n",
      "Epoch 1383, Train_Loss: 3956.72412109375, Val_Loss: 4071.505126953125\n",
      "Epoch 1384, Train_Loss: 3956.6669921875, Val_Loss: 4071.486083984375\n",
      "Epoch 1385, Train_Loss: 3956.697509765625, Val_Loss: 4071.476318359375\n",
      "Epoch 1386, Train_Loss: 3956.681640625, Val_Loss: 4071.49951171875\n",
      "Epoch 1387, Train_Loss: 3956.647216796875, Val_Loss: 4071.43408203125\n",
      "Epoch 1388, Train_Loss: 3956.6220703125, Val_Loss: 4071.419921875\n",
      "Epoch 1389, Train_Loss: 3956.607177734375, Val_Loss: 4071.412841796875\n",
      "Epoch 1390, Train_Loss: 3956.60107421875, Val_Loss: 4071.397216796875\n",
      "Epoch 1391, Train_Loss: 3956.592529296875, Val_Loss: 4071.379638671875\n",
      "Epoch 1392, Train_Loss: 3956.5869140625, Val_Loss: 4071.37646484375\n",
      "Epoch 1393, Train_Loss: 3956.585205078125, Val_Loss: 4071.37646484375\n",
      "Epoch 1394, Train_Loss: 3956.56787109375, Val_Loss: 4071.36328125\n",
      "Epoch 1395, Train_Loss: 3956.555419921875, Val_Loss: 4071.3544921875\n",
      "Epoch 1396, Train_Loss: 3956.49462890625, Val_Loss: 4071.275634765625\n",
      "Epoch 1397, Train_Loss: 3956.484130859375, Val_Loss: 4071.260009765625\n",
      "Epoch 1398, Train_Loss: 3956.4677734375, Val_Loss: 4071.231201171875\n",
      "Epoch 1399, Train_Loss: 3956.478271484375, Val_Loss: 4071.197509765625\n",
      "Epoch 1400, Train_Loss: 3956.448486328125, Val_Loss: 4071.1962890625\n",
      "Epoch 1401, Train_Loss: 3956.434814453125, Val_Loss: 4071.215087890625\n",
      "Epoch 1402, Train_Loss: 3956.41015625, Val_Loss: 4071.18408203125\n",
      "Epoch 1403, Train_Loss: 3956.34521484375, Val_Loss: 4071.1923828125\n",
      "Epoch 1404, Train_Loss: 3956.306396484375, Val_Loss: 4071.174072265625\n",
      "Epoch 1405, Train_Loss: 3956.347412109375, Val_Loss: 4071.1396484375\n",
      "Epoch 1406, Train_Loss: 3956.33544921875, Val_Loss: 4071.12548828125\n",
      "Epoch 1407, Train_Loss: 3956.33056640625, Val_Loss: 4071.1240234375\n",
      "Epoch 1408, Train_Loss: 3956.19482421875, Val_Loss: 4071.07958984375\n",
      "Epoch 1409, Train_Loss: 3956.185546875, Val_Loss: 4071.064453125\n",
      "Epoch 1410, Train_Loss: 3956.1728515625, Val_Loss: 4071.051513671875\n",
      "Epoch 1411, Train_Loss: 3956.155517578125, Val_Loss: 4071.0546875\n",
      "Epoch 1412, Train_Loss: 3956.1083984375, Val_Loss: 4071.0283203125\n",
      "Epoch 1413, Train_Loss: 3956.0849609375, Val_Loss: 4071.020751953125\n",
      "Epoch 1414, Train_Loss: 3956.066650390625, Val_Loss: 4070.98388671875\n",
      "Epoch 1415, Train_Loss: 3956.04296875, Val_Loss: 4070.97314453125\n",
      "Epoch 1416, Train_Loss: 3956.02978515625, Val_Loss: 4070.9580078125\n",
      "Epoch 1417, Train_Loss: 3956.002197265625, Val_Loss: 4070.9287109375\n",
      "Epoch 1418, Train_Loss: 3955.97314453125, Val_Loss: 4070.88671875\n",
      "Epoch 1419, Train_Loss: 3955.876220703125, Val_Loss: 4070.78564453125\n",
      "Epoch 1420, Train_Loss: 3955.675048828125, Val_Loss: 4070.5927734375\n",
      "Epoch 1421, Train_Loss: 3955.666259765625, Val_Loss: 4070.3583984375\n",
      "Epoch 1422, Train_Loss: 3955.681884765625, Val_Loss: 4070.3427734375\n",
      "Epoch 1423, Train_Loss: 3955.645751953125, Val_Loss: 4070.45361328125\n",
      "Epoch 1424, Train_Loss: 3955.710205078125, Val_Loss: 4070.605712890625\n",
      "Epoch 1425, Train_Loss: 3955.743896484375, Val_Loss: 4070.61083984375\n",
      "Epoch 1426, Train_Loss: 3955.715576171875, Val_Loss: 4070.572509765625\n",
      "Epoch 1427, Train_Loss: 3955.667724609375, Val_Loss: 4070.529541015625\n",
      "Epoch 1428, Train_Loss: 3955.588623046875, Val_Loss: 4070.40234375\n",
      "Epoch 1429, Train_Loss: 3955.54296875, Val_Loss: 4070.21728515625\n",
      "Epoch 1430, Train_Loss: 3955.587158203125, Val_Loss: 4070.214111328125\n",
      "Epoch 1431, Train_Loss: 3955.480224609375, Val_Loss: 4070.335693359375\n",
      "Epoch 1432, Train_Loss: 3955.552978515625, Val_Loss: 4070.4404296875\n",
      "Epoch 1433, Train_Loss: 3955.5283203125, Val_Loss: 4070.3984375\n",
      "Epoch 1434, Train_Loss: 3955.481689453125, Val_Loss: 4070.31884765625\n",
      "Epoch 1435, Train_Loss: 3955.45361328125, Val_Loss: 4070.1728515625\n",
      "Epoch 1436, Train_Loss: 3955.49951171875, Val_Loss: 4070.109130859375\n",
      "Epoch 1437, Train_Loss: 3955.409912109375, Val_Loss: 4070.114501953125\n",
      "Epoch 1438, Train_Loss: 3955.415283203125, Val_Loss: 4070.263916015625\n",
      "Epoch 1439, Train_Loss: 3955.371337890625, Val_Loss: 4070.302001953125\n",
      "Epoch 1440, Train_Loss: 3955.396484375, Val_Loss: 4070.25439453125\n",
      "Epoch 1441, Train_Loss: 3955.34130859375, Val_Loss: 4070.1767578125\n",
      "Epoch 1442, Train_Loss: 3955.324462890625, Val_Loss: 4070.162841796875\n",
      "Epoch 1443, Train_Loss: 3955.31201171875, Val_Loss: 4070.05322265625\n",
      "Epoch 1444, Train_Loss: 3955.286865234375, Val_Loss: 4070.120361328125\n",
      "Epoch 1445, Train_Loss: 3955.141845703125, Val_Loss: 4070.1279296875\n",
      "Epoch 1446, Train_Loss: 3955.14599609375, Val_Loss: 4070.14794921875\n",
      "Epoch 1447, Train_Loss: 3955.093994140625, Val_Loss: 4070.189208984375\n",
      "Epoch 1448, Train_Loss: 3955.049072265625, Val_Loss: 4070.1572265625\n",
      "Epoch 1449, Train_Loss: 3955.106201171875, Val_Loss: 4070.051513671875\n",
      "Epoch 1450, Train_Loss: 3955.041748046875, Val_Loss: 4069.9619140625\n",
      "Epoch 1451, Train_Loss: 3955.022705078125, Val_Loss: 4069.947998046875\n",
      "Epoch 1452, Train_Loss: 3955.0693359375, Val_Loss: 4070.010009765625\n",
      "Epoch 1453, Train_Loss: 3955.063232421875, Val_Loss: 4070.04150390625\n",
      "Epoch 1454, Train_Loss: 3955.05908203125, Val_Loss: 4070.0\n",
      "Epoch 1455, Train_Loss: 3955.00439453125, Val_Loss: 4069.9580078125\n",
      "Epoch 1456, Train_Loss: 3954.956298828125, Val_Loss: 4069.917236328125\n",
      "Epoch 1457, Train_Loss: 3954.949462890625, Val_Loss: 4069.8427734375\n",
      "Epoch 1458, Train_Loss: 3954.98828125, Val_Loss: 4069.887939453125\n",
      "Epoch 1459, Train_Loss: 3954.899169921875, Val_Loss: 4069.928466796875\n",
      "Epoch 1460, Train_Loss: 3954.934814453125, Val_Loss: 4069.897216796875\n",
      "Epoch 1461, Train_Loss: 3954.864501953125, Val_Loss: 4069.730712890625\n",
      "Epoch 1462, Train_Loss: 3954.857177734375, Val_Loss: 4069.697998046875\n",
      "Epoch 1463, Train_Loss: 3954.82568359375, Val_Loss: 4069.712890625\n",
      "Epoch 1464, Train_Loss: 3954.8583984375, Val_Loss: 4069.77001953125\n",
      "Epoch 1465, Train_Loss: 3954.848876953125, Val_Loss: 4069.771484375\n",
      "Epoch 1466, Train_Loss: 3954.836181640625, Val_Loss: 4069.773193359375\n",
      "Epoch 1467, Train_Loss: 3954.8115234375, Val_Loss: 4069.742919921875\n",
      "Epoch 1468, Train_Loss: 3954.785400390625, Val_Loss: 4069.698486328125\n",
      "Epoch 1469, Train_Loss: 3954.777099609375, Val_Loss: 4069.622802734375\n",
      "Epoch 1470, Train_Loss: 3954.7236328125, Val_Loss: 4069.629638671875\n",
      "Epoch 1471, Train_Loss: 3954.750732421875, Val_Loss: 4069.595947265625\n",
      "Epoch 1472, Train_Loss: 3954.720458984375, Val_Loss: 4069.6064453125\n",
      "Epoch 1473, Train_Loss: 3954.708984375, Val_Loss: 4069.584716796875\n",
      "Epoch 1474, Train_Loss: 3954.6591796875, Val_Loss: 4069.572509765625\n",
      "Epoch 1475, Train_Loss: 3954.686279296875, Val_Loss: 4069.547119140625\n",
      "Epoch 1476, Train_Loss: 3954.6533203125, Val_Loss: 4069.547607421875\n",
      "Epoch 1477, Train_Loss: 3954.620849609375, Val_Loss: 4069.518798828125\n",
      "Epoch 1478, Train_Loss: 3954.47998046875, Val_Loss: 4069.463134765625\n",
      "Epoch 1479, Train_Loss: 3954.48193359375, Val_Loss: 4069.4599609375\n",
      "Epoch 1480, Train_Loss: 3954.474609375, Val_Loss: 4069.461181640625\n",
      "Epoch 1481, Train_Loss: 3954.561767578125, Val_Loss: 4069.461669921875\n",
      "Epoch 1482, Train_Loss: 3954.528564453125, Val_Loss: 4069.464111328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1483, Train_Loss: 3954.534912109375, Val_Loss: 4069.4736328125\n",
      "Epoch 1484, Train_Loss: 3954.53857421875, Val_Loss: 4069.394775390625\n",
      "Epoch 1485, Train_Loss: 3954.427978515625, Val_Loss: 4069.34765625\n",
      "Epoch 1486, Train_Loss: 3954.389892578125, Val_Loss: 4069.313232421875\n",
      "Epoch 1487, Train_Loss: 3954.35693359375, Val_Loss: 4069.290283203125\n",
      "Epoch 1488, Train_Loss: 3954.388671875, Val_Loss: 4069.302490234375\n",
      "Epoch 1489, Train_Loss: 3954.45263671875, Val_Loss: 4069.34326171875\n",
      "Epoch 1490, Train_Loss: 3954.390869140625, Val_Loss: 4069.34228515625\n",
      "Epoch 1491, Train_Loss: 3954.3798828125, Val_Loss: 4069.333251953125\n",
      "Epoch 1492, Train_Loss: 3954.259765625, Val_Loss: 4069.19873046875\n",
      "Epoch 1493, Train_Loss: 3954.2568359375, Val_Loss: 4069.165283203125\n",
      "Epoch 1494, Train_Loss: 3954.2431640625, Val_Loss: 4069.1279296875\n",
      "Epoch 1495, Train_Loss: 3954.220458984375, Val_Loss: 4069.143310546875\n",
      "Epoch 1496, Train_Loss: 3954.23583984375, Val_Loss: 4069.12548828125\n",
      "Epoch 1497, Train_Loss: 3954.220703125, Val_Loss: 4069.11279296875\n",
      "Epoch 1498, Train_Loss: 3954.132080078125, Val_Loss: 4069.056884765625\n",
      "Epoch 1499, Train_Loss: 3954.126220703125, Val_Loss: 4069.035888671875\n",
      "Epoch 1500, Train_Loss: 3954.117919921875, Val_Loss: 4069.065673828125\n",
      "Epoch 1501, Train_Loss: 3954.094970703125, Val_Loss: 4069.06884765625\n",
      "Epoch 1502, Train_Loss: 3954.08544921875, Val_Loss: 4069.053955078125\n",
      "Epoch 1503, Train_Loss: 3954.075927734375, Val_Loss: 4069.04248046875\n",
      "Epoch 1504, Train_Loss: 3954.072265625, Val_Loss: 4069.013671875\n",
      "Epoch 1505, Train_Loss: 3954.05908203125, Val_Loss: 4069.00244140625\n",
      "Epoch 1506, Train_Loss: 3954.02099609375, Val_Loss: 4068.954345703125\n",
      "Epoch 1507, Train_Loss: 3953.997802734375, Val_Loss: 4068.938720703125\n",
      "Epoch 1508, Train_Loss: 3953.8564453125, Val_Loss: 4068.901123046875\n",
      "Epoch 1509, Train_Loss: 3953.82666015625, Val_Loss: 4068.899169921875\n",
      "Epoch 1510, Train_Loss: 3953.78369140625, Val_Loss: 4068.883544921875\n",
      "Epoch 1511, Train_Loss: 3953.7724609375, Val_Loss: 4068.872802734375\n",
      "Epoch 1512, Train_Loss: 3953.787353515625, Val_Loss: 4068.853271484375\n",
      "Epoch 1513, Train_Loss: 3953.781005859375, Val_Loss: 4068.852783203125\n",
      "Epoch 1514, Train_Loss: 3953.766357421875, Val_Loss: 4068.845947265625\n",
      "Epoch 1515, Train_Loss: 3953.69140625, Val_Loss: 4068.78076171875\n",
      "Epoch 1516, Train_Loss: 3953.68408203125, Val_Loss: 4068.739501953125\n",
      "Epoch 1517, Train_Loss: 3953.653076171875, Val_Loss: 4068.730712890625\n",
      "Epoch 1518, Train_Loss: 3953.675048828125, Val_Loss: 4068.742919921875\n",
      "Epoch 1519, Train_Loss: 3953.66162109375, Val_Loss: 4068.746337890625\n",
      "Epoch 1520, Train_Loss: 3953.599609375, Val_Loss: 4068.719970703125\n",
      "Epoch 1521, Train_Loss: 3953.5859375, Val_Loss: 4068.687255859375\n",
      "Epoch 1522, Train_Loss: 3953.56982421875, Val_Loss: 4068.6943359375\n",
      "Epoch 1523, Train_Loss: 3953.4697265625, Val_Loss: 4068.59521484375\n",
      "Epoch 1524, Train_Loss: 3953.494384765625, Val_Loss: 4068.589599609375\n",
      "Epoch 1525, Train_Loss: 3953.492919921875, Val_Loss: 4068.595947265625\n",
      "Epoch 1526, Train_Loss: 3953.46240234375, Val_Loss: 4068.575927734375\n",
      "Epoch 1527, Train_Loss: 3953.452392578125, Val_Loss: 4068.56640625\n",
      "Epoch 1528, Train_Loss: 3953.3837890625, Val_Loss: 4068.518798828125\n",
      "Epoch 1529, Train_Loss: 3953.381591796875, Val_Loss: 4068.476806640625\n",
      "Epoch 1530, Train_Loss: 3953.367431640625, Val_Loss: 4068.468017578125\n",
      "Epoch 1531, Train_Loss: 3953.36279296875, Val_Loss: 4068.4736328125\n",
      "Epoch 1532, Train_Loss: 3953.307373046875, Val_Loss: 4068.45068359375\n",
      "Epoch 1533, Train_Loss: 3953.29736328125, Val_Loss: 4068.441162109375\n",
      "Epoch 1534, Train_Loss: 3953.329345703125, Val_Loss: 4068.427978515625\n",
      "Epoch 1535, Train_Loss: 3953.33447265625, Val_Loss: 4068.439208984375\n",
      "Epoch 1536, Train_Loss: 3953.302978515625, Val_Loss: 4068.42236328125\n",
      "Epoch 1537, Train_Loss: 3953.27685546875, Val_Loss: 4068.387939453125\n",
      "Epoch 1538, Train_Loss: 3953.22021484375, Val_Loss: 4068.31591796875\n",
      "Epoch 1539, Train_Loss: 3953.200927734375, Val_Loss: 4068.320068359375\n",
      "Epoch 1540, Train_Loss: 3953.207275390625, Val_Loss: 4068.308349609375\n",
      "Epoch 1541, Train_Loss: 3953.2099609375, Val_Loss: 4068.325927734375\n",
      "Epoch 1542, Train_Loss: 3953.179931640625, Val_Loss: 4068.284912109375\n",
      "Epoch 1543, Train_Loss: 3953.135498046875, Val_Loss: 4068.23291015625\n",
      "Epoch 1544, Train_Loss: 3953.123291015625, Val_Loss: 4068.202880859375\n",
      "Epoch 1545, Train_Loss: 3953.106689453125, Val_Loss: 4068.211669921875\n",
      "Epoch 1546, Train_Loss: 3953.14111328125, Val_Loss: 4068.16552734375\n",
      "Epoch 1547, Train_Loss: 3953.06982421875, Val_Loss: 4068.111572265625\n",
      "Epoch 1548, Train_Loss: 3953.057373046875, Val_Loss: 4068.122314453125\n",
      "Epoch 1549, Train_Loss: 3953.051025390625, Val_Loss: 4068.09765625\n",
      "Epoch 1550, Train_Loss: 3953.00927734375, Val_Loss: 4068.087890625\n",
      "Epoch 1551, Train_Loss: 3953.0625, Val_Loss: 4068.08447265625\n",
      "Epoch 1552, Train_Loss: 3953.009033203125, Val_Loss: 4068.089599609375\n",
      "Epoch 1553, Train_Loss: 3953.00244140625, Val_Loss: 4068.077880859375\n",
      "Epoch 1554, Train_Loss: 3952.920166015625, Val_Loss: 4067.985595703125\n",
      "Epoch 1555, Train_Loss: 3952.926513671875, Val_Loss: 4067.946044921875\n",
      "Epoch 1556, Train_Loss: 3952.871337890625, Val_Loss: 4067.928466796875\n",
      "Epoch 1557, Train_Loss: 3952.89697265625, Val_Loss: 4067.923095703125\n",
      "Epoch 1558, Train_Loss: 3952.912353515625, Val_Loss: 4067.924072265625\n",
      "Epoch 1559, Train_Loss: 3952.87646484375, Val_Loss: 4067.91748046875\n",
      "Epoch 1560, Train_Loss: 3952.879150390625, Val_Loss: 4067.890380859375\n",
      "Epoch 1561, Train_Loss: 3952.585205078125, Val_Loss: 4067.8388671875\n",
      "Epoch 1562, Train_Loss: 3952.503662109375, Val_Loss: 4067.78271484375\n",
      "Epoch 1563, Train_Loss: 3952.49560546875, Val_Loss: 4067.708740234375\n",
      "Epoch 1564, Train_Loss: 3952.360595703125, Val_Loss: 4067.5537109375\n",
      "Epoch 1565, Train_Loss: 3952.20751953125, Val_Loss: 4067.352783203125\n",
      "Epoch 1566, Train_Loss: 3952.1796875, Val_Loss: 4067.236328125\n",
      "Epoch 1567, Train_Loss: 3952.218994140625, Val_Loss: 4067.3056640625\n",
      "Epoch 1568, Train_Loss: 3952.201171875, Val_Loss: 4067.457275390625\n",
      "Epoch 1569, Train_Loss: 3952.112548828125, Val_Loss: 4067.38330078125\n",
      "Epoch 1570, Train_Loss: 3952.082763671875, Val_Loss: 4067.148681640625\n",
      "Epoch 1571, Train_Loss: 3952.132568359375, Val_Loss: 4067.12646484375\n",
      "Epoch 1572, Train_Loss: 3951.945068359375, Val_Loss: 4067.2041015625\n",
      "Epoch 1573, Train_Loss: 3951.9365234375, Val_Loss: 4067.30712890625\n",
      "Epoch 1574, Train_Loss: 3951.925048828125, Val_Loss: 4067.33251953125\n",
      "Epoch 1575, Train_Loss: 3951.931884765625, Val_Loss: 4067.28564453125\n",
      "Epoch 1576, Train_Loss: 3951.9052734375, Val_Loss: 4067.209228515625\n",
      "Epoch 1577, Train_Loss: 3951.954345703125, Val_Loss: 4067.090087890625\n",
      "Epoch 1578, Train_Loss: 3951.84814453125, Val_Loss: 4067.0546875\n",
      "Epoch 1579, Train_Loss: 3951.83544921875, Val_Loss: 4067.073974609375\n",
      "Epoch 1580, Train_Loss: 3951.841796875, Val_Loss: 4067.162109375\n",
      "Epoch 1581, Train_Loss: 3951.811279296875, Val_Loss: 4067.188720703125\n",
      "Epoch 1582, Train_Loss: 3951.794189453125, Val_Loss: 4067.180419921875\n",
      "Epoch 1583, Train_Loss: 3951.809814453125, Val_Loss: 4067.064697265625\n",
      "Epoch 1584, Train_Loss: 3951.722900390625, Val_Loss: 4066.929931640625\n",
      "Epoch 1585, Train_Loss: 3951.6337890625, Val_Loss: 4066.91796875\n",
      "Epoch 1586, Train_Loss: 3951.67578125, Val_Loss: 4066.94287109375\n",
      "Epoch 1587, Train_Loss: 3951.646728515625, Val_Loss: 4066.946044921875\n",
      "Epoch 1588, Train_Loss: 3951.672607421875, Val_Loss: 4066.982421875\n",
      "Epoch 1589, Train_Loss: 3951.63330078125, Val_Loss: 4066.9794921875\n",
      "Epoch 1590, Train_Loss: 3951.65771484375, Val_Loss: 4066.968505859375\n",
      "Epoch 1591, Train_Loss: 3951.565185546875, Val_Loss: 4066.827880859375\n",
      "Epoch 1592, Train_Loss: 3951.5419921875, Val_Loss: 4066.808837890625\n",
      "Epoch 1593, Train_Loss: 3951.524658203125, Val_Loss: 4066.713623046875\n",
      "Epoch 1594, Train_Loss: 3951.49169921875, Val_Loss: 4066.767578125\n",
      "Epoch 1595, Train_Loss: 3951.552001953125, Val_Loss: 4066.912841796875\n",
      "Epoch 1596, Train_Loss: 3951.50439453125, Val_Loss: 4066.936279296875\n",
      "Epoch 1597, Train_Loss: 3951.496337890625, Val_Loss: 4066.91357421875\n",
      "Epoch 1598, Train_Loss: 3951.472900390625, Val_Loss: 4066.789306640625\n",
      "Epoch 1599, Train_Loss: 3951.444091796875, Val_Loss: 4066.686767578125\n",
      "Epoch 1600, Train_Loss: 3951.415771484375, Val_Loss: 4066.6064453125\n",
      "Epoch 1601, Train_Loss: 3951.375, Val_Loss: 4066.597900390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1602, Train_Loss: 3951.365966796875, Val_Loss: 4066.677978515625\n",
      "Epoch 1603, Train_Loss: 3951.3916015625, Val_Loss: 4066.63232421875\n",
      "Epoch 1604, Train_Loss: 3951.317138671875, Val_Loss: 4066.656005859375\n",
      "Epoch 1605, Train_Loss: 3951.27490234375, Val_Loss: 4066.644287109375\n",
      "Epoch 1606, Train_Loss: 3951.242919921875, Val_Loss: 4066.584716796875\n",
      "Epoch 1607, Train_Loss: 3951.235107421875, Val_Loss: 4066.565673828125\n",
      "Epoch 1608, Train_Loss: 3951.3134765625, Val_Loss: 4066.572021484375\n",
      "Epoch 1609, Train_Loss: 3951.298828125, Val_Loss: 4066.56201171875\n",
      "Epoch 1610, Train_Loss: 3951.20556640625, Val_Loss: 4066.54833984375\n",
      "Epoch 1611, Train_Loss: 3951.18212890625, Val_Loss: 4066.512451171875\n",
      "Epoch 1612, Train_Loss: 3951.178466796875, Val_Loss: 4066.49560546875\n",
      "Epoch 1613, Train_Loss: 3951.1552734375, Val_Loss: 4066.423583984375\n",
      "Epoch 1614, Train_Loss: 3951.13134765625, Val_Loss: 4066.337158203125\n",
      "Epoch 1615, Train_Loss: 3951.09814453125, Val_Loss: 4066.319580078125\n",
      "Epoch 1616, Train_Loss: 3951.101318359375, Val_Loss: 4066.366455078125\n",
      "Epoch 1617, Train_Loss: 3951.06005859375, Val_Loss: 4066.305908203125\n",
      "Epoch 1618, Train_Loss: 3951.06640625, Val_Loss: 4066.2880859375\n",
      "Epoch 1619, Train_Loss: 3951.077392578125, Val_Loss: 4066.319091796875\n",
      "Epoch 1620, Train_Loss: 3950.993408203125, Val_Loss: 4066.335205078125\n",
      "Epoch 1621, Train_Loss: 3950.95947265625, Val_Loss: 4066.24365234375\n",
      "Epoch 1622, Train_Loss: 3951.017578125, Val_Loss: 4066.216064453125\n",
      "Epoch 1623, Train_Loss: 3950.92578125, Val_Loss: 4066.215576171875\n",
      "Epoch 1624, Train_Loss: 3950.927978515625, Val_Loss: 4066.27001953125\n",
      "Epoch 1625, Train_Loss: 3950.914306640625, Val_Loss: 4066.288330078125\n",
      "Epoch 1626, Train_Loss: 3950.8828125, Val_Loss: 4066.189208984375\n",
      "Epoch 1627, Train_Loss: 3950.91845703125, Val_Loss: 4066.119140625\n",
      "Epoch 1628, Train_Loss: 3950.563720703125, Val_Loss: 4066.07763671875\n",
      "Epoch 1629, Train_Loss: 3950.561767578125, Val_Loss: 4066.109130859375\n",
      "Epoch 1630, Train_Loss: 3950.547119140625, Val_Loss: 4066.100830078125\n",
      "Epoch 1631, Train_Loss: 3950.52294921875, Val_Loss: 4066.0888671875\n",
      "Epoch 1632, Train_Loss: 3950.50341796875, Val_Loss: 4066.05517578125\n",
      "Epoch 1633, Train_Loss: 3950.492919921875, Val_Loss: 4066.050048828125\n",
      "Epoch 1634, Train_Loss: 3950.475341796875, Val_Loss: 4066.04833984375\n",
      "Epoch 1635, Train_Loss: 3950.460693359375, Val_Loss: 4065.997314453125\n",
      "Epoch 1636, Train_Loss: 3950.43603515625, Val_Loss: 4065.978515625\n",
      "Epoch 1637, Train_Loss: 3950.426025390625, Val_Loss: 4065.974365234375\n",
      "Epoch 1638, Train_Loss: 3950.431884765625, Val_Loss: 4065.905517578125\n",
      "Epoch 1639, Train_Loss: 3950.427490234375, Val_Loss: 4065.899169921875\n",
      "Epoch 1640, Train_Loss: 3950.406005859375, Val_Loss: 4065.874755859375\n",
      "Epoch 1641, Train_Loss: 3950.360107421875, Val_Loss: 4065.9091796875\n",
      "Epoch 1642, Train_Loss: 3950.324462890625, Val_Loss: 4065.822509765625\n",
      "Epoch 1643, Train_Loss: 3950.334716796875, Val_Loss: 4065.7587890625\n",
      "Epoch 1644, Train_Loss: 3950.326416015625, Val_Loss: 4065.744384765625\n",
      "Epoch 1645, Train_Loss: 3950.2734375, Val_Loss: 4065.7568359375\n",
      "Epoch 1646, Train_Loss: 3950.268798828125, Val_Loss: 4065.767578125\n",
      "Epoch 1647, Train_Loss: 3950.232177734375, Val_Loss: 4065.7548828125\n",
      "Epoch 1648, Train_Loss: 3950.21484375, Val_Loss: 4065.7392578125\n",
      "Epoch 1649, Train_Loss: 3950.191162109375, Val_Loss: 4065.70556640625\n",
      "Epoch 1650, Train_Loss: 3950.14990234375, Val_Loss: 4065.642333984375\n",
      "Epoch 1651, Train_Loss: 3950.197021484375, Val_Loss: 4065.595703125\n",
      "Epoch 1652, Train_Loss: 3950.17333984375, Val_Loss: 4065.587158203125\n",
      "Epoch 1653, Train_Loss: 3950.062255859375, Val_Loss: 4065.558349609375\n",
      "Epoch 1654, Train_Loss: 3950.14501953125, Val_Loss: 4065.568359375\n",
      "Epoch 1655, Train_Loss: 3950.094970703125, Val_Loss: 4065.580810546875\n",
      "Epoch 1656, Train_Loss: 3950.060791015625, Val_Loss: 4065.59912109375\n",
      "Epoch 1657, Train_Loss: 3950.059814453125, Val_Loss: 4065.4912109375\n",
      "Epoch 1658, Train_Loss: 3949.9638671875, Val_Loss: 4065.45556640625\n",
      "Epoch 1659, Train_Loss: 3950.001708984375, Val_Loss: 4065.470458984375\n",
      "Epoch 1660, Train_Loss: 3949.9365234375, Val_Loss: 4065.426513671875\n",
      "Epoch 1661, Train_Loss: 3949.962890625, Val_Loss: 4065.414794921875\n",
      "Epoch 1662, Train_Loss: 3949.94970703125, Val_Loss: 4065.4072265625\n",
      "Epoch 1663, Train_Loss: 3949.937744140625, Val_Loss: 4065.401611328125\n",
      "Epoch 1664, Train_Loss: 3949.841796875, Val_Loss: 4065.35791015625\n",
      "Epoch 1665, Train_Loss: 3949.90869140625, Val_Loss: 4065.372314453125\n",
      "Epoch 1666, Train_Loss: 3949.87109375, Val_Loss: 4065.361083984375\n",
      "Epoch 1667, Train_Loss: 3949.8583984375, Val_Loss: 4065.34716796875\n",
      "Epoch 1668, Train_Loss: 3949.87060546875, Val_Loss: 4065.3212890625\n",
      "Epoch 1669, Train_Loss: 3949.7646484375, Val_Loss: 4065.287109375\n",
      "Epoch 1670, Train_Loss: 3949.839599609375, Val_Loss: 4065.257568359375\n",
      "Epoch 1671, Train_Loss: 3949.758544921875, Val_Loss: 4065.100341796875\n",
      "Epoch 1672, Train_Loss: 3949.79345703125, Val_Loss: 4065.14794921875\n",
      "Epoch 1673, Train_Loss: 3949.7119140625, Val_Loss: 4065.162353515625\n",
      "Epoch 1674, Train_Loss: 3949.720703125, Val_Loss: 4065.172119140625\n",
      "Epoch 1675, Train_Loss: 3949.6787109375, Val_Loss: 4065.122314453125\n",
      "Epoch 1676, Train_Loss: 3949.6982421875, Val_Loss: 4065.072509765625\n",
      "Epoch 1677, Train_Loss: 3949.702392578125, Val_Loss: 4065.085205078125\n",
      "Epoch 1678, Train_Loss: 3949.610595703125, Val_Loss: 4065.053955078125\n",
      "Epoch 1679, Train_Loss: 3949.58447265625, Val_Loss: 4065.062744140625\n",
      "Epoch 1680, Train_Loss: 3949.573974609375, Val_Loss: 4065.07763671875\n",
      "Epoch 1681, Train_Loss: 3949.5537109375, Val_Loss: 4064.985595703125\n",
      "Epoch 1682, Train_Loss: 3949.55615234375, Val_Loss: 4064.956787109375\n",
      "Epoch 1683, Train_Loss: 3949.51806640625, Val_Loss: 4064.944091796875\n",
      "Epoch 1684, Train_Loss: 3949.508056640625, Val_Loss: 4065.0048828125\n",
      "Epoch 1685, Train_Loss: 3949.301513671875, Val_Loss: 4064.91552734375\n",
      "Epoch 1686, Train_Loss: 3949.316650390625, Val_Loss: 4064.883544921875\n",
      "Epoch 1687, Train_Loss: 3949.326171875, Val_Loss: 4064.8740234375\n",
      "Epoch 1688, Train_Loss: 3949.2841796875, Val_Loss: 4064.872802734375\n",
      "Epoch 1689, Train_Loss: 3949.257080078125, Val_Loss: 4064.8583984375\n",
      "Epoch 1690, Train_Loss: 3949.25439453125, Val_Loss: 4064.89599609375\n",
      "Epoch 1691, Train_Loss: 3949.223876953125, Val_Loss: 4064.850830078125\n",
      "Epoch 1692, Train_Loss: 3949.193115234375, Val_Loss: 4064.777099609375\n",
      "Epoch 1693, Train_Loss: 3949.2109375, Val_Loss: 4064.7431640625\n",
      "Epoch 1694, Train_Loss: 3949.193115234375, Val_Loss: 4064.738037109375\n",
      "Epoch 1695, Train_Loss: 3949.179443359375, Val_Loss: 4064.719482421875\n",
      "Epoch 1696, Train_Loss: 3949.143310546875, Val_Loss: 4064.7255859375\n",
      "Epoch 1697, Train_Loss: 3949.114501953125, Val_Loss: 4064.726806640625\n",
      "Epoch 1698, Train_Loss: 3949.114990234375, Val_Loss: 4064.693115234375\n",
      "Epoch 1699, Train_Loss: 3949.07470703125, Val_Loss: 4064.5791015625\n",
      "Epoch 1700, Train_Loss: 3949.05224609375, Val_Loss: 4064.565185546875\n",
      "Epoch 1701, Train_Loss: 3949.041259765625, Val_Loss: 4064.55322265625\n",
      "Epoch 1702, Train_Loss: 3949.03076171875, Val_Loss: 4064.535888671875\n",
      "Epoch 1703, Train_Loss: 3948.969482421875, Val_Loss: 4064.5224609375\n",
      "Epoch 1704, Train_Loss: 3948.955078125, Val_Loss: 4064.545654296875\n",
      "Epoch 1705, Train_Loss: 3948.95166015625, Val_Loss: 4064.49609375\n",
      "Epoch 1706, Train_Loss: 3948.91064453125, Val_Loss: 4064.474365234375\n",
      "Epoch 1707, Train_Loss: 3948.944091796875, Val_Loss: 4064.4580078125\n",
      "Epoch 1708, Train_Loss: 3948.927978515625, Val_Loss: 4064.437255859375\n",
      "Epoch 1709, Train_Loss: 3948.91943359375, Val_Loss: 4064.422119140625\n",
      "Epoch 1710, Train_Loss: 3948.90478515625, Val_Loss: 4064.40283203125\n",
      "Epoch 1711, Train_Loss: 3948.884521484375, Val_Loss: 4064.404052734375\n",
      "Epoch 1712, Train_Loss: 3948.81787109375, Val_Loss: 4064.3955078125\n",
      "Epoch 1713, Train_Loss: 3948.764404296875, Val_Loss: 4064.357666015625\n",
      "Epoch 1714, Train_Loss: 3948.7509765625, Val_Loss: 4064.33837890625\n",
      "Epoch 1715, Train_Loss: 3948.742431640625, Val_Loss: 4064.322021484375\n",
      "Epoch 1716, Train_Loss: 3948.78173828125, Val_Loss: 4064.2587890625\n",
      "Epoch 1717, Train_Loss: 3948.76171875, Val_Loss: 4064.2412109375\n",
      "Epoch 1718, Train_Loss: 3948.748291015625, Val_Loss: 4064.231689453125\n",
      "Epoch 1719, Train_Loss: 3948.68115234375, Val_Loss: 4064.22509765625\n",
      "Epoch 1720, Train_Loss: 3948.65380859375, Val_Loss: 4064.166015625\n",
      "Epoch 1721, Train_Loss: 3948.658203125, Val_Loss: 4064.147216796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1722, Train_Loss: 3948.629150390625, Val_Loss: 4064.129638671875\n",
      "Epoch 1723, Train_Loss: 3948.642822265625, Val_Loss: 4064.11962890625\n",
      "Epoch 1724, Train_Loss: 3948.626220703125, Val_Loss: 4064.100830078125\n",
      "Epoch 1725, Train_Loss: 3948.571044921875, Val_Loss: 4064.102783203125\n",
      "Epoch 1726, Train_Loss: 3948.549072265625, Val_Loss: 4064.075927734375\n",
      "Epoch 1727, Train_Loss: 3948.4931640625, Val_Loss: 4064.0068359375\n",
      "Epoch 1728, Train_Loss: 3948.526123046875, Val_Loss: 4063.9716796875\n",
      "Epoch 1729, Train_Loss: 3948.504638671875, Val_Loss: 4063.95751953125\n",
      "Epoch 1730, Train_Loss: 3948.45703125, Val_Loss: 4063.956298828125\n",
      "Epoch 1731, Train_Loss: 3948.41748046875, Val_Loss: 4063.944091796875\n",
      "Epoch 1732, Train_Loss: 3948.406005859375, Val_Loss: 4063.92919921875\n",
      "Epoch 1733, Train_Loss: 3948.384033203125, Val_Loss: 4063.885986328125\n",
      "Epoch 1734, Train_Loss: 3948.30712890625, Val_Loss: 4063.861083984375\n",
      "Epoch 1735, Train_Loss: 3948.2998046875, Val_Loss: 4063.853271484375\n",
      "Epoch 1736, Train_Loss: 3948.241943359375, Val_Loss: 4063.848388671875\n",
      "Epoch 1737, Train_Loss: 3948.23095703125, Val_Loss: 4063.835205078125\n",
      "Epoch 1738, Train_Loss: 3948.25927734375, Val_Loss: 4063.808837890625\n",
      "Epoch 1739, Train_Loss: 3948.3076171875, Val_Loss: 4063.7900390625\n",
      "Epoch 1740, Train_Loss: 3948.297607421875, Val_Loss: 4063.77197265625\n",
      "Epoch 1741, Train_Loss: 3948.20166015625, Val_Loss: 4063.740478515625\n",
      "Epoch 1742, Train_Loss: 3948.181396484375, Val_Loss: 4063.724365234375\n",
      "Epoch 1743, Train_Loss: 3948.168701171875, Val_Loss: 4063.71435546875\n",
      "Epoch 1744, Train_Loss: 3948.153076171875, Val_Loss: 4063.6884765625\n",
      "Epoch 1745, Train_Loss: 3948.138916015625, Val_Loss: 4063.67529296875\n",
      "Epoch 1746, Train_Loss: 3948.125732421875, Val_Loss: 4063.662353515625\n",
      "Epoch 1747, Train_Loss: 3948.1083984375, Val_Loss: 4063.644287109375\n",
      "Epoch 1748, Train_Loss: 3948.06494140625, Val_Loss: 4063.556884765625\n",
      "Epoch 1749, Train_Loss: 3948.052001953125, Val_Loss: 4063.541259765625\n",
      "Epoch 1750, Train_Loss: 3948.02197265625, Val_Loss: 4063.531982421875\n",
      "Epoch 1751, Train_Loss: 3948.004638671875, Val_Loss: 4063.507080078125\n",
      "Epoch 1752, Train_Loss: 3947.989990234375, Val_Loss: 4063.496337890625\n",
      "Epoch 1753, Train_Loss: 3947.97265625, Val_Loss: 4063.481201171875\n",
      "Epoch 1754, Train_Loss: 3947.951171875, Val_Loss: 4063.478759765625\n",
      "Epoch 1755, Train_Loss: 3947.851806640625, Val_Loss: 4063.388427734375\n",
      "Epoch 1756, Train_Loss: 3947.83642578125, Val_Loss: 4063.3720703125\n",
      "Epoch 1757, Train_Loss: 3947.822998046875, Val_Loss: 4063.36669921875\n",
      "Epoch 1758, Train_Loss: 3947.802490234375, Val_Loss: 4063.350341796875\n",
      "Epoch 1759, Train_Loss: 3947.794189453125, Val_Loss: 4063.31640625\n",
      "Epoch 1760, Train_Loss: 3947.77880859375, Val_Loss: 4063.3076171875\n",
      "Epoch 1761, Train_Loss: 3947.765380859375, Val_Loss: 4063.300048828125\n",
      "Epoch 1762, Train_Loss: 3947.717529296875, Val_Loss: 4063.268310546875\n",
      "Epoch 1763, Train_Loss: 3947.696044921875, Val_Loss: 4063.250732421875\n",
      "Epoch 1764, Train_Loss: 3947.682373046875, Val_Loss: 4063.235595703125\n",
      "Epoch 1765, Train_Loss: 3947.667236328125, Val_Loss: 4063.221923828125\n",
      "Epoch 1766, Train_Loss: 3947.6494140625, Val_Loss: 4063.20751953125\n",
      "Epoch 1767, Train_Loss: 3947.644287109375, Val_Loss: 4063.18798828125\n",
      "Epoch 1768, Train_Loss: 3947.62890625, Val_Loss: 4063.1787109375\n",
      "Epoch 1769, Train_Loss: 3947.5859375, Val_Loss: 4063.133544921875\n",
      "Epoch 1770, Train_Loss: 3947.572509765625, Val_Loss: 4063.11669921875\n",
      "Epoch 1771, Train_Loss: 3947.564453125, Val_Loss: 4063.10205078125\n",
      "Epoch 1772, Train_Loss: 3947.546875, Val_Loss: 4063.085205078125\n",
      "Epoch 1773, Train_Loss: 3947.5263671875, Val_Loss: 4063.067138671875\n",
      "Epoch 1774, Train_Loss: 3947.508544921875, Val_Loss: 4063.050048828125\n",
      "Epoch 1775, Train_Loss: 3947.488525390625, Val_Loss: 4062.988037109375\n",
      "Epoch 1776, Train_Loss: 3947.46337890625, Val_Loss: 4062.9375\n",
      "Epoch 1777, Train_Loss: 3947.447265625, Val_Loss: 4062.91796875\n",
      "Epoch 1778, Train_Loss: 3947.430908203125, Val_Loss: 4062.905517578125\n",
      "Epoch 1779, Train_Loss: 3947.42041015625, Val_Loss: 4062.888916015625\n",
      "Epoch 1780, Train_Loss: 3947.39892578125, Val_Loss: 4062.86669921875\n",
      "Epoch 1781, Train_Loss: 3947.386474609375, Val_Loss: 4062.85400390625\n",
      "Epoch 1782, Train_Loss: 3947.351318359375, Val_Loss: 4062.762451171875\n",
      "Epoch 1783, Train_Loss: 3947.331298828125, Val_Loss: 4062.739501953125\n",
      "Epoch 1784, Train_Loss: 3947.315673828125, Val_Loss: 4062.7373046875\n",
      "Epoch 1785, Train_Loss: 3947.302490234375, Val_Loss: 4062.72412109375\n",
      "Epoch 1786, Train_Loss: 3947.28369140625, Val_Loss: 4062.709228515625\n",
      "Epoch 1787, Train_Loss: 3947.273681640625, Val_Loss: 4062.694091796875\n",
      "Epoch 1788, Train_Loss: 3947.2607421875, Val_Loss: 4062.678466796875\n",
      "Epoch 1789, Train_Loss: 3947.01171875, Val_Loss: 4062.654052734375\n",
      "Epoch 1790, Train_Loss: 3946.99560546875, Val_Loss: 4062.637939453125\n",
      "Epoch 1791, Train_Loss: 3946.978759765625, Val_Loss: 4062.622314453125\n",
      "Epoch 1792, Train_Loss: 3946.827392578125, Val_Loss: 4062.600341796875\n",
      "Epoch 1793, Train_Loss: 3946.815185546875, Val_Loss: 4062.58349609375\n",
      "Epoch 1794, Train_Loss: 3946.800537109375, Val_Loss: 4062.565185546875\n",
      "Epoch 1795, Train_Loss: 3946.7939453125, Val_Loss: 4062.55517578125\n",
      "Epoch 1796, Train_Loss: 3946.67431640625, Val_Loss: 4062.516845703125\n",
      "Epoch 1797, Train_Loss: 3946.656494140625, Val_Loss: 4062.499267578125\n",
      "Epoch 1798, Train_Loss: 3946.643310546875, Val_Loss: 4062.48828125\n",
      "Epoch 1799, Train_Loss: 3946.642822265625, Val_Loss: 4062.470703125\n",
      "Epoch 1800, Train_Loss: 3946.62255859375, Val_Loss: 4062.4560546875\n",
      "Epoch 1801, Train_Loss: 3946.6083984375, Val_Loss: 4062.434814453125\n",
      "Epoch 1802, Train_Loss: 3946.5947265625, Val_Loss: 4062.427490234375\n",
      "Epoch 1803, Train_Loss: 3946.540283203125, Val_Loss: 4062.3701171875\n",
      "Epoch 1804, Train_Loss: 3946.5234375, Val_Loss: 4062.3515625\n",
      "Epoch 1805, Train_Loss: 3946.507080078125, Val_Loss: 4062.339599609375\n",
      "Epoch 1806, Train_Loss: 3946.471435546875, Val_Loss: 4062.289306640625\n",
      "Epoch 1807, Train_Loss: 3946.453857421875, Val_Loss: 4062.273193359375\n",
      "Epoch 1808, Train_Loss: 3946.383056640625, Val_Loss: 4062.264892578125\n",
      "Epoch 1809, Train_Loss: 3946.339599609375, Val_Loss: 4062.1728515625\n",
      "Epoch 1810, Train_Loss: 3946.322265625, Val_Loss: 4062.15771484375\n",
      "Epoch 1811, Train_Loss: 3946.299072265625, Val_Loss: 4062.14208984375\n",
      "Epoch 1812, Train_Loss: 3946.28759765625, Val_Loss: 4062.124755859375\n",
      "Epoch 1813, Train_Loss: 3946.275146484375, Val_Loss: 4062.10400390625\n",
      "Epoch 1814, Train_Loss: 3946.260009765625, Val_Loss: 4062.09912109375\n",
      "Epoch 1815, Train_Loss: 3946.24462890625, Val_Loss: 4062.091552734375\n",
      "Epoch 1816, Train_Loss: 3946.20947265625, Val_Loss: 4062.06396484375\n",
      "Epoch 1817, Train_Loss: 3946.2021484375, Val_Loss: 4062.040283203125\n",
      "Epoch 1818, Train_Loss: 3946.1923828125, Val_Loss: 4062.0263671875\n",
      "Epoch 1819, Train_Loss: 3946.17138671875, Val_Loss: 4062.00634765625\n",
      "Epoch 1820, Train_Loss: 3946.15673828125, Val_Loss: 4061.9951171875\n",
      "Epoch 1821, Train_Loss: 3946.138427734375, Val_Loss: 4061.969970703125\n",
      "Epoch 1822, Train_Loss: 3946.123046875, Val_Loss: 4061.956298828125\n",
      "Epoch 1823, Train_Loss: 3946.073974609375, Val_Loss: 4061.922119140625\n",
      "Epoch 1824, Train_Loss: 3946.052001953125, Val_Loss: 4061.90673828125\n",
      "Epoch 1825, Train_Loss: 3946.029296875, Val_Loss: 4061.896728515625\n",
      "Epoch 1826, Train_Loss: 3946.019287109375, Val_Loss: 4061.88232421875\n",
      "Epoch 1827, Train_Loss: 3945.995361328125, Val_Loss: 4061.87158203125\n",
      "Epoch 1828, Train_Loss: 3945.9853515625, Val_Loss: 4061.840087890625\n",
      "Epoch 1829, Train_Loss: 3945.988037109375, Val_Loss: 4061.802734375\n",
      "Epoch 1830, Train_Loss: 3945.9658203125, Val_Loss: 4061.741943359375\n",
      "Epoch 1831, Train_Loss: 3945.954345703125, Val_Loss: 4061.72509765625\n",
      "Epoch 1832, Train_Loss: 3945.947998046875, Val_Loss: 4061.72119140625\n",
      "Epoch 1833, Train_Loss: 3945.939697265625, Val_Loss: 4061.70556640625\n",
      "Epoch 1834, Train_Loss: 3945.9130859375, Val_Loss: 4061.655517578125\n",
      "Epoch 1835, Train_Loss: 3945.89306640625, Val_Loss: 4061.645263671875\n",
      "Epoch 1836, Train_Loss: 3945.85595703125, Val_Loss: 4061.53125\n",
      "Epoch 1837, Train_Loss: 3945.847412109375, Val_Loss: 4061.523681640625\n",
      "Epoch 1838, Train_Loss: 3945.8291015625, Val_Loss: 4061.512451171875\n",
      "Epoch 1839, Train_Loss: 3945.77587890625, Val_Loss: 4061.498291015625\n",
      "Epoch 1840, Train_Loss: 3945.8046875, Val_Loss: 4061.486083984375\n",
      "Epoch 1841, Train_Loss: 3945.74169921875, Val_Loss: 4061.40673828125\n",
      "Epoch 1842, Train_Loss: 3945.702392578125, Val_Loss: 4061.3671875\n",
      "Epoch 1843, Train_Loss: 3945.619140625, Val_Loss: 4061.447998046875\n",
      "Epoch 1844, Train_Loss: 3945.638916015625, Val_Loss: 4061.34326171875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1845, Train_Loss: 3945.631591796875, Val_Loss: 4061.26806640625\n",
      "Epoch 1846, Train_Loss: 3945.629638671875, Val_Loss: 4061.327880859375\n",
      "Epoch 1847, Train_Loss: 3945.610595703125, Val_Loss: 4061.3603515625\n",
      "Epoch 1848, Train_Loss: 3945.600830078125, Val_Loss: 4061.396484375\n",
      "Epoch 1849, Train_Loss: 3945.5859375, Val_Loss: 4061.399169921875\n",
      "Epoch 1850, Train_Loss: 3945.264404296875, Val_Loss: 4061.313232421875\n",
      "Epoch 1851, Train_Loss: 3945.279052734375, Val_Loss: 4061.12109375\n",
      "Epoch 1852, Train_Loss: 3945.23291015625, Val_Loss: 4061.1279296875\n",
      "Epoch 1853, Train_Loss: 3945.229248046875, Val_Loss: 4061.2373046875\n",
      "Epoch 1854, Train_Loss: 3945.21044921875, Val_Loss: 4061.3037109375\n",
      "Epoch 1855, Train_Loss: 3945.1904296875, Val_Loss: 4061.280517578125\n",
      "Epoch 1856, Train_Loss: 3945.19873046875, Val_Loss: 4061.279541015625\n",
      "Epoch 1857, Train_Loss: 3945.157958984375, Val_Loss: 4061.189208984375\n",
      "Epoch 1858, Train_Loss: 3945.0888671875, Val_Loss: 4061.1171875\n",
      "Epoch 1859, Train_Loss: 3945.0986328125, Val_Loss: 4061.065673828125\n",
      "Epoch 1860, Train_Loss: 3945.128173828125, Val_Loss: 4060.910400390625\n",
      "Epoch 1861, Train_Loss: 3945.109130859375, Val_Loss: 4060.98291015625\n",
      "Epoch 1862, Train_Loss: 3945.060546875, Val_Loss: 4061.067626953125\n",
      "Epoch 1863, Train_Loss: 3944.99072265625, Val_Loss: 4060.92919921875\n",
      "Epoch 1864, Train_Loss: 3945.01220703125, Val_Loss: 4060.794921875\n",
      "Epoch 1865, Train_Loss: 3944.9765625, Val_Loss: 4060.83251953125\n",
      "Epoch 1866, Train_Loss: 3944.965576171875, Val_Loss: 4061.00927734375\n",
      "Epoch 1867, Train_Loss: 3944.936279296875, Val_Loss: 4060.97314453125\n",
      "Epoch 1868, Train_Loss: 3944.84765625, Val_Loss: 4060.860107421875\n",
      "Epoch 1869, Train_Loss: 3944.8974609375, Val_Loss: 4060.792724609375\n",
      "Epoch 1870, Train_Loss: 3944.86865234375, Val_Loss: 4060.772705078125\n",
      "Epoch 1871, Train_Loss: 3944.826171875, Val_Loss: 4060.76611328125\n",
      "Epoch 1872, Train_Loss: 3944.8095703125, Val_Loss: 4060.824462890625\n",
      "Epoch 1873, Train_Loss: 3944.80859375, Val_Loss: 4060.83203125\n",
      "Epoch 1874, Train_Loss: 3944.736083984375, Val_Loss: 4060.80517578125\n",
      "Epoch 1875, Train_Loss: 3944.70361328125, Val_Loss: 4060.739501953125\n",
      "Epoch 1876, Train_Loss: 3944.699951171875, Val_Loss: 4060.728759765625\n",
      "Epoch 1877, Train_Loss: 3944.654052734375, Val_Loss: 4060.680908203125\n",
      "Epoch 1878, Train_Loss: 3944.6728515625, Val_Loss: 4060.6962890625\n",
      "Epoch 1879, Train_Loss: 3944.642333984375, Val_Loss: 4060.630859375\n",
      "Epoch 1880, Train_Loss: 3944.626708984375, Val_Loss: 4060.618896484375\n",
      "Epoch 1881, Train_Loss: 3944.60302734375, Val_Loss: 4060.63720703125\n",
      "Epoch 1882, Train_Loss: 3944.589599609375, Val_Loss: 4060.6103515625\n",
      "Epoch 1883, Train_Loss: 3944.592041015625, Val_Loss: 4060.52587890625\n",
      "Epoch 1884, Train_Loss: 3944.529296875, Val_Loss: 4060.537109375\n",
      "Epoch 1885, Train_Loss: 3944.528564453125, Val_Loss: 4060.57080078125\n",
      "Epoch 1886, Train_Loss: 3944.497802734375, Val_Loss: 4060.5380859375\n",
      "Epoch 1887, Train_Loss: 3944.504150390625, Val_Loss: 4060.49755859375\n",
      "Epoch 1888, Train_Loss: 3944.48193359375, Val_Loss: 4060.468505859375\n",
      "Epoch 1889, Train_Loss: 3944.46044921875, Val_Loss: 4060.423095703125\n",
      "Epoch 1890, Train_Loss: 3944.399658203125, Val_Loss: 4060.281494140625\n",
      "Epoch 1891, Train_Loss: 3944.4365234375, Val_Loss: 4060.30322265625\n",
      "Epoch 1892, Train_Loss: 3944.420654296875, Val_Loss: 4060.278076171875\n",
      "Epoch 1893, Train_Loss: 3944.375732421875, Val_Loss: 4060.306884765625\n",
      "Epoch 1894, Train_Loss: 3944.360595703125, Val_Loss: 4060.30908203125\n",
      "Epoch 1895, Train_Loss: 3944.33642578125, Val_Loss: 4060.268798828125\n",
      "Epoch 1896, Train_Loss: 3944.33837890625, Val_Loss: 4060.2568359375\n",
      "Epoch 1897, Train_Loss: 3944.248291015625, Val_Loss: 4060.19091796875\n",
      "Epoch 1898, Train_Loss: 3944.241943359375, Val_Loss: 4060.19189453125\n",
      "Epoch 1899, Train_Loss: 3944.190673828125, Val_Loss: 4060.17041015625\n",
      "Epoch 1900, Train_Loss: 3944.25048828125, Val_Loss: 4060.17529296875\n",
      "Epoch 1901, Train_Loss: 3944.253662109375, Val_Loss: 4060.187255859375\n",
      "Epoch 1902, Train_Loss: 3944.1416015625, Val_Loss: 4060.133544921875\n",
      "Epoch 1903, Train_Loss: 3944.175537109375, Val_Loss: 4060.09326171875\n",
      "Epoch 1904, Train_Loss: 3943.962158203125, Val_Loss: 4060.080810546875\n",
      "Epoch 1905, Train_Loss: 3943.9560546875, Val_Loss: 4060.0703125\n",
      "Epoch 1906, Train_Loss: 3943.93701171875, Val_Loss: 4060.034423828125\n",
      "Epoch 1907, Train_Loss: 3943.915771484375, Val_Loss: 4060.038818359375\n",
      "Epoch 1908, Train_Loss: 3943.986328125, Val_Loss: 4060.095947265625\n",
      "Epoch 1909, Train_Loss: 3943.897216796875, Val_Loss: 4060.00244140625\n",
      "Epoch 1910, Train_Loss: 3943.889892578125, Val_Loss: 4059.90283203125\n",
      "Epoch 1911, Train_Loss: 3943.887939453125, Val_Loss: 4059.875244140625\n",
      "Epoch 1912, Train_Loss: 3943.86962890625, Val_Loss: 4059.849609375\n",
      "Epoch 1913, Train_Loss: 3943.831787109375, Val_Loss: 4059.906005859375\n",
      "Epoch 1914, Train_Loss: 3943.93017578125, Val_Loss: 4059.9931640625\n",
      "Epoch 1915, Train_Loss: 3943.79052734375, Val_Loss: 4059.91357421875\n",
      "Epoch 1916, Train_Loss: 3943.780029296875, Val_Loss: 4059.812744140625\n",
      "Epoch 1917, Train_Loss: 3943.73291015625, Val_Loss: 4059.715087890625\n",
      "Epoch 1918, Train_Loss: 3943.6767578125, Val_Loss: 4059.71728515625\n",
      "Epoch 1919, Train_Loss: 3943.6904296875, Val_Loss: 4059.79443359375\n",
      "Epoch 1920, Train_Loss: 3943.664794921875, Val_Loss: 4059.706298828125\n",
      "Epoch 1921, Train_Loss: 3943.6640625, Val_Loss: 4059.664306640625\n",
      "Epoch 1922, Train_Loss: 3943.6357421875, Val_Loss: 4059.656005859375\n",
      "Epoch 1923, Train_Loss: 3943.594970703125, Val_Loss: 4059.656494140625\n",
      "Epoch 1924, Train_Loss: 3943.5927734375, Val_Loss: 4059.728759765625\n",
      "Epoch 1925, Train_Loss: 3943.579833984375, Val_Loss: 4059.6767578125\n",
      "Epoch 1926, Train_Loss: 3943.531005859375, Val_Loss: 4059.607177734375\n",
      "Epoch 1927, Train_Loss: 3943.53271484375, Val_Loss: 4059.57470703125\n",
      "Epoch 1928, Train_Loss: 3943.510009765625, Val_Loss: 4059.56689453125\n",
      "Epoch 1929, Train_Loss: 3943.50390625, Val_Loss: 4059.55712890625\n",
      "Epoch 1930, Train_Loss: 3943.4482421875, Val_Loss: 4059.481201171875\n",
      "Epoch 1931, Train_Loss: 3943.43896484375, Val_Loss: 4059.447509765625\n",
      "Epoch 1932, Train_Loss: 3943.41943359375, Val_Loss: 4059.49365234375\n",
      "Epoch 1933, Train_Loss: 3943.36767578125, Val_Loss: 4059.479248046875\n",
      "Epoch 1934, Train_Loss: 3943.403076171875, Val_Loss: 4059.46240234375\n",
      "Epoch 1935, Train_Loss: 3943.343505859375, Val_Loss: 4059.445556640625\n",
      "Epoch 1936, Train_Loss: 3943.343505859375, Val_Loss: 4059.398681640625\n",
      "Epoch 1937, Train_Loss: 3943.29833984375, Val_Loss: 4059.335693359375\n",
      "Epoch 1938, Train_Loss: 3943.314697265625, Val_Loss: 4059.339599609375\n",
      "Epoch 1939, Train_Loss: 3943.28857421875, Val_Loss: 4059.31396484375\n",
      "Epoch 1940, Train_Loss: 3943.249267578125, Val_Loss: 4059.3115234375\n",
      "Epoch 1941, Train_Loss: 3943.263427734375, Val_Loss: 4059.275146484375\n",
      "Epoch 1942, Train_Loss: 3943.223876953125, Val_Loss: 4059.2099609375\n",
      "Epoch 1943, Train_Loss: 3943.20556640625, Val_Loss: 4059.097900390625\n",
      "Epoch 1944, Train_Loss: 3943.1533203125, Val_Loss: 4059.1328125\n",
      "Epoch 1945, Train_Loss: 3943.150146484375, Val_Loss: 4059.07763671875\n",
      "Epoch 1946, Train_Loss: 3943.129638671875, Val_Loss: 4059.080078125\n",
      "Epoch 1947, Train_Loss: 3943.072509765625, Val_Loss: 4059.162353515625\n",
      "Epoch 1948, Train_Loss: 3943.055908203125, Val_Loss: 4059.115234375\n",
      "Epoch 1949, Train_Loss: 3943.06005859375, Val_Loss: 4059.043212890625\n",
      "Epoch 1950, Train_Loss: 3942.9814453125, Val_Loss: 4059.0205078125\n",
      "Epoch 1951, Train_Loss: 3942.96728515625, Val_Loss: 4059.0546875\n",
      "Epoch 1952, Train_Loss: 3942.921630859375, Val_Loss: 4059.043212890625\n",
      "Epoch 1953, Train_Loss: 3942.91162109375, Val_Loss: 4059.052001953125\n",
      "Epoch 1954, Train_Loss: 3942.9091796875, Val_Loss: 4058.954345703125\n",
      "Epoch 1955, Train_Loss: 3942.920166015625, Val_Loss: 4058.930908203125\n",
      "Epoch 1956, Train_Loss: 3942.791259765625, Val_Loss: 4058.92236328125\n",
      "Epoch 1957, Train_Loss: 3942.7373046875, Val_Loss: 4058.895263671875\n",
      "Epoch 1958, Train_Loss: 3942.70947265625, Val_Loss: 4058.859130859375\n",
      "Epoch 1959, Train_Loss: 3942.693115234375, Val_Loss: 4058.848388671875\n",
      "Epoch 1960, Train_Loss: 3942.69482421875, Val_Loss: 4058.873291015625\n",
      "Epoch 1961, Train_Loss: 3942.66845703125, Val_Loss: 4058.8583984375\n",
      "Epoch 1962, Train_Loss: 3942.6396484375, Val_Loss: 4058.8583984375\n",
      "Epoch 1963, Train_Loss: 3942.587890625, Val_Loss: 4058.755126953125\n",
      "Epoch 1964, Train_Loss: 3942.5810546875, Val_Loss: 4058.760498046875\n",
      "Epoch 1965, Train_Loss: 3942.64453125, Val_Loss: 4058.7080078125\n",
      "Epoch 1966, Train_Loss: 3942.619384765625, Val_Loss: 4058.67919921875\n",
      "Epoch 1967, Train_Loss: 3942.499267578125, Val_Loss: 4058.706787109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1968, Train_Loss: 3942.555419921875, Val_Loss: 4058.721923828125\n",
      "Epoch 1969, Train_Loss: 3942.43115234375, Val_Loss: 4058.63916015625\n",
      "Epoch 1970, Train_Loss: 3942.390380859375, Val_Loss: 4058.609619140625\n",
      "Epoch 1971, Train_Loss: 3942.4033203125, Val_Loss: 4058.55908203125\n",
      "Epoch 1972, Train_Loss: 3942.40966796875, Val_Loss: 4058.519287109375\n",
      "Epoch 1973, Train_Loss: 3942.3671875, Val_Loss: 4058.53271484375\n",
      "Epoch 1974, Train_Loss: 3942.36669921875, Val_Loss: 4058.51171875\n",
      "Epoch 1975, Train_Loss: 3942.349365234375, Val_Loss: 4058.449951171875\n",
      "Epoch 1976, Train_Loss: 3942.327392578125, Val_Loss: 4058.396484375\n",
      "Epoch 1977, Train_Loss: 3942.30712890625, Val_Loss: 4058.451904296875\n",
      "Epoch 1978, Train_Loss: 3942.24951171875, Val_Loss: 4058.486328125\n",
      "Epoch 1979, Train_Loss: 3942.2421875, Val_Loss: 4058.44091796875\n",
      "Epoch 1980, Train_Loss: 3942.23779296875, Val_Loss: 4058.377685546875\n",
      "Epoch 1981, Train_Loss: 3942.236328125, Val_Loss: 4058.3544921875\n",
      "Epoch 1982, Train_Loss: 3942.20068359375, Val_Loss: 4058.303955078125\n",
      "Epoch 1983, Train_Loss: 3942.184326171875, Val_Loss: 4058.303955078125\n",
      "Epoch 1984, Train_Loss: 3942.168212890625, Val_Loss: 4058.281982421875\n",
      "Epoch 1985, Train_Loss: 3942.12744140625, Val_Loss: 4058.26171875\n",
      "Epoch 1986, Train_Loss: 3942.103271484375, Val_Loss: 4058.266845703125\n",
      "Epoch 1987, Train_Loss: 3942.131591796875, Val_Loss: 4058.313720703125\n",
      "Epoch 1988, Train_Loss: 3942.103759765625, Val_Loss: 4058.216064453125\n",
      "Epoch 1989, Train_Loss: 3942.138916015625, Val_Loss: 4058.102783203125\n",
      "Epoch 1990, Train_Loss: 3942.056640625, Val_Loss: 4058.112060546875\n",
      "Epoch 1991, Train_Loss: 3942.005126953125, Val_Loss: 4058.1298828125\n",
      "Epoch 1992, Train_Loss: 3942.015625, Val_Loss: 4058.14208984375\n",
      "Epoch 1993, Train_Loss: 3941.974365234375, Val_Loss: 4058.11962890625\n",
      "Epoch 1994, Train_Loss: 3941.96240234375, Val_Loss: 4058.09716796875\n",
      "Epoch 1995, Train_Loss: 3941.973388671875, Val_Loss: 4057.9716796875\n",
      "Epoch 1996, Train_Loss: 3941.94970703125, Val_Loss: 4057.971923828125\n",
      "Epoch 1997, Train_Loss: 3941.924072265625, Val_Loss: 4057.955078125\n",
      "Epoch 1998, Train_Loss: 3941.87158203125, Val_Loss: 4057.983154296875\n",
      "Epoch 1999, Train_Loss: 3941.880615234375, Val_Loss: 4057.978759765625\n",
      "Epoch 2000, Train_Loss: 3941.8212890625, Val_Loss: 4057.97314453125\n",
      "Epoch 2001, Train_Loss: 3941.822509765625, Val_Loss: 4057.930908203125\n",
      "Epoch 2002, Train_Loss: 3941.583984375, Val_Loss: 4057.868408203125\n",
      "Epoch 2003, Train_Loss: 3941.5859375, Val_Loss: 4057.8583984375\n",
      "Epoch 2004, Train_Loss: 3941.51953125, Val_Loss: 4057.845703125\n",
      "Epoch 2005, Train_Loss: 3941.51123046875, Val_Loss: 4057.855712890625\n",
      "Epoch 2006, Train_Loss: 3941.51806640625, Val_Loss: 4057.8359375\n",
      "Epoch 2007, Train_Loss: 3941.495361328125, Val_Loss: 4057.757080078125\n",
      "Epoch 2008, Train_Loss: 3941.401123046875, Val_Loss: 4057.715576171875\n",
      "Epoch 2009, Train_Loss: 3941.343994140625, Val_Loss: 4057.719970703125\n",
      "Epoch 2010, Train_Loss: 3941.371826171875, Val_Loss: 4057.73193359375\n",
      "Epoch 2011, Train_Loss: 3941.328369140625, Val_Loss: 4057.731689453125\n",
      "Epoch 2012, Train_Loss: 3941.314697265625, Val_Loss: 4057.6640625\n",
      "Epoch 2013, Train_Loss: 3941.34228515625, Val_Loss: 4057.640869140625\n",
      "Epoch 2014, Train_Loss: 3941.296875, Val_Loss: 4057.6240234375\n",
      "Epoch 2015, Train_Loss: 3941.213623046875, Val_Loss: 4057.575927734375\n",
      "Epoch 2016, Train_Loss: 3941.222900390625, Val_Loss: 4057.547119140625\n",
      "Epoch 2017, Train_Loss: 3941.199951171875, Val_Loss: 4057.56201171875\n",
      "Epoch 2018, Train_Loss: 3941.186279296875, Val_Loss: 4057.542724609375\n",
      "Epoch 2019, Train_Loss: 3941.16943359375, Val_Loss: 4057.53271484375\n",
      "Epoch 2020, Train_Loss: 3941.158935546875, Val_Loss: 4057.5107421875\n",
      "Epoch 2021, Train_Loss: 3941.101806640625, Val_Loss: 4057.412841796875\n",
      "Epoch 2022, Train_Loss: 3941.08837890625, Val_Loss: 4057.395263671875\n",
      "Epoch 2023, Train_Loss: 3941.06982421875, Val_Loss: 4057.384765625\n",
      "Epoch 2024, Train_Loss: 3941.052734375, Val_Loss: 4057.38232421875\n",
      "Epoch 2025, Train_Loss: 3941.04052734375, Val_Loss: 4057.362060546875\n",
      "Epoch 2026, Train_Loss: 3940.859375, Val_Loss: 4057.323974609375\n",
      "Epoch 2027, Train_Loss: 3940.81884765625, Val_Loss: 4057.308349609375\n",
      "Epoch 2028, Train_Loss: 3940.81640625, Val_Loss: 4057.28564453125\n",
      "Epoch 2029, Train_Loss: 3940.797607421875, Val_Loss: 4057.292724609375\n",
      "Epoch 2030, Train_Loss: 3940.787109375, Val_Loss: 4057.260498046875\n",
      "Epoch 2031, Train_Loss: 3940.771728515625, Val_Loss: 4057.235107421875\n",
      "Epoch 2032, Train_Loss: 3940.76611328125, Val_Loss: 4057.2255859375\n",
      "Epoch 2033, Train_Loss: 3940.751953125, Val_Loss: 4057.212890625\n",
      "Epoch 2034, Train_Loss: 3940.71142578125, Val_Loss: 4057.158447265625\n",
      "Epoch 2035, Train_Loss: 3940.6943359375, Val_Loss: 4057.146484375\n",
      "Epoch 2036, Train_Loss: 3940.690673828125, Val_Loss: 4057.13232421875\n",
      "Epoch 2037, Train_Loss: 3940.66748046875, Val_Loss: 4057.109619140625\n",
      "Epoch 2038, Train_Loss: 3940.64404296875, Val_Loss: 4057.106689453125\n",
      "Epoch 2039, Train_Loss: 3940.607177734375, Val_Loss: 4057.05322265625\n",
      "Epoch 2040, Train_Loss: 3940.58203125, Val_Loss: 4056.9951171875\n",
      "Epoch 2041, Train_Loss: 3940.56787109375, Val_Loss: 4056.963623046875\n",
      "Epoch 2042, Train_Loss: 3940.5517578125, Val_Loss: 4056.95068359375\n",
      "Epoch 2043, Train_Loss: 3940.540283203125, Val_Loss: 4056.944091796875\n",
      "Epoch 2044, Train_Loss: 3940.526611328125, Val_Loss: 4056.9296875\n",
      "Epoch 2045, Train_Loss: 3940.5, Val_Loss: 4056.910888671875\n",
      "Epoch 2046, Train_Loss: 3940.460693359375, Val_Loss: 4056.830810546875\n",
      "Epoch 2047, Train_Loss: 3940.45068359375, Val_Loss: 4056.823486328125\n",
      "Epoch 2048, Train_Loss: 3940.447998046875, Val_Loss: 4056.80810546875\n",
      "Epoch 2049, Train_Loss: 3940.4287109375, Val_Loss: 4056.785888671875\n",
      "Epoch 2050, Train_Loss: 3940.396240234375, Val_Loss: 4056.7724609375\n",
      "Epoch 2051, Train_Loss: 3940.370849609375, Val_Loss: 4056.76123046875\n",
      "Epoch 2052, Train_Loss: 3940.36669921875, Val_Loss: 4056.742919921875\n",
      "Epoch 2053, Train_Loss: 3940.30224609375, Val_Loss: 4056.709228515625\n",
      "Epoch 2054, Train_Loss: 3940.28076171875, Val_Loss: 4056.693115234375\n",
      "Epoch 2055, Train_Loss: 3940.2568359375, Val_Loss: 4056.681884765625\n",
      "Epoch 2056, Train_Loss: 3940.23828125, Val_Loss: 4056.660400390625\n",
      "Epoch 2057, Train_Loss: 3940.227294921875, Val_Loss: 4056.634521484375\n",
      "Epoch 2058, Train_Loss: 3940.23046875, Val_Loss: 4056.619140625\n",
      "Epoch 2059, Train_Loss: 3939.897216796875, Val_Loss: 4056.587646484375\n",
      "Epoch 2060, Train_Loss: 3939.87109375, Val_Loss: 4056.583984375\n",
      "Epoch 2061, Train_Loss: 3939.8701171875, Val_Loss: 4056.555908203125\n",
      "Epoch 2062, Train_Loss: 3939.8642578125, Val_Loss: 4056.521240234375\n",
      "Epoch 2063, Train_Loss: 3939.84814453125, Val_Loss: 4056.50634765625\n",
      "Epoch 2064, Train_Loss: 3939.82763671875, Val_Loss: 4056.494873046875\n",
      "Epoch 2065, Train_Loss: 3939.7939453125, Val_Loss: 4056.436279296875\n",
      "Epoch 2066, Train_Loss: 3939.779296875, Val_Loss: 4056.41552734375\n",
      "Epoch 2067, Train_Loss: 3939.753662109375, Val_Loss: 4056.35888671875\n",
      "Epoch 2068, Train_Loss: 3939.7412109375, Val_Loss: 4056.345703125\n",
      "Epoch 2069, Train_Loss: 3939.723388671875, Val_Loss: 4056.3359375\n",
      "Epoch 2070, Train_Loss: 3939.710205078125, Val_Loss: 4056.3212890625\n",
      "Epoch 2071, Train_Loss: 3939.675048828125, Val_Loss: 4056.22314453125\n",
      "Epoch 2072, Train_Loss: 3939.665771484375, Val_Loss: 4056.2080078125\n",
      "Epoch 2073, Train_Loss: 3939.653076171875, Val_Loss: 4056.19287109375\n",
      "Epoch 2074, Train_Loss: 3939.63427734375, Val_Loss: 4056.191650390625\n",
      "Epoch 2075, Train_Loss: 3939.6123046875, Val_Loss: 4056.173583984375\n",
      "Epoch 2076, Train_Loss: 3939.59716796875, Val_Loss: 4056.155517578125\n",
      "Epoch 2077, Train_Loss: 3939.58251953125, Val_Loss: 4056.13671875\n",
      "Epoch 2078, Train_Loss: 3939.546875, Val_Loss: 4056.116455078125\n",
      "Epoch 2079, Train_Loss: 3939.5263671875, Val_Loss: 4056.093994140625\n",
      "Epoch 2080, Train_Loss: 3939.438232421875, Val_Loss: 4056.063232421875\n",
      "Epoch 2081, Train_Loss: 3939.421630859375, Val_Loss: 4056.048828125\n",
      "Epoch 2082, Train_Loss: 3939.3994140625, Val_Loss: 4056.0458984375\n",
      "Epoch 2083, Train_Loss: 3939.38525390625, Val_Loss: 4056.029541015625\n",
      "Epoch 2084, Train_Loss: 3939.338623046875, Val_Loss: 4055.974365234375\n",
      "Epoch 2085, Train_Loss: 3939.326171875, Val_Loss: 4055.96435546875\n",
      "Epoch 2086, Train_Loss: 3939.315673828125, Val_Loss: 4055.95068359375\n",
      "Epoch 2087, Train_Loss: 3939.31005859375, Val_Loss: 4055.92919921875\n",
      "Epoch 2088, Train_Loss: 3939.291015625, Val_Loss: 4055.915283203125\n",
      "Epoch 2089, Train_Loss: 3939.2763671875, Val_Loss: 4055.896728515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2090, Train_Loss: 3939.252685546875, Val_Loss: 4055.83349609375\n",
      "Epoch 2091, Train_Loss: 3939.238037109375, Val_Loss: 4055.813232421875\n",
      "Epoch 2092, Train_Loss: 3939.229736328125, Val_Loss: 4055.802001953125\n",
      "Epoch 2093, Train_Loss: 3939.209228515625, Val_Loss: 4055.78515625\n",
      "Epoch 2094, Train_Loss: 3939.17529296875, Val_Loss: 4055.7392578125\n",
      "Epoch 2095, Train_Loss: 3939.154296875, Val_Loss: 4055.717529296875\n",
      "Epoch 2096, Train_Loss: 3939.11962890625, Val_Loss: 4055.632080078125\n",
      "Epoch 2097, Train_Loss: 3939.094482421875, Val_Loss: 4055.62890625\n",
      "Epoch 2098, Train_Loss: 3939.083984375, Val_Loss: 4055.605224609375\n",
      "Epoch 2099, Train_Loss: 3939.06884765625, Val_Loss: 4055.5869140625\n",
      "Epoch 2100, Train_Loss: 3939.0478515625, Val_Loss: 4055.576904296875\n",
      "Epoch 2101, Train_Loss: 3939.03759765625, Val_Loss: 4055.551513671875\n",
      "Epoch 2102, Train_Loss: 3939.030517578125, Val_Loss: 4055.536376953125\n",
      "Epoch 2103, Train_Loss: 3938.94580078125, Val_Loss: 4055.519287109375\n",
      "Epoch 2104, Train_Loss: 3938.9619140625, Val_Loss: 4055.478759765625\n",
      "Epoch 2105, Train_Loss: 3938.9013671875, Val_Loss: 4055.47998046875\n",
      "Epoch 2106, Train_Loss: 3938.88134765625, Val_Loss: 4055.460693359375\n",
      "Epoch 2107, Train_Loss: 3938.9462890625, Val_Loss: 4055.4091796875\n",
      "Epoch 2108, Train_Loss: 3938.91796875, Val_Loss: 4055.41552734375\n",
      "Epoch 2109, Train_Loss: 3938.740234375, Val_Loss: 4055.3798828125\n",
      "Epoch 2110, Train_Loss: 3938.74609375, Val_Loss: 4055.330322265625\n",
      "Epoch 2111, Train_Loss: 3938.67919921875, Val_Loss: 4055.288330078125\n",
      "Epoch 2112, Train_Loss: 3938.686279296875, Val_Loss: 4055.247314453125\n",
      "Epoch 2113, Train_Loss: 3938.61767578125, Val_Loss: 4055.21044921875\n",
      "Epoch 2114, Train_Loss: 3938.6591796875, Val_Loss: 4055.21728515625\n",
      "Epoch 2115, Train_Loss: 3938.614501953125, Val_Loss: 4055.169189453125\n",
      "Epoch 2116, Train_Loss: 3938.611083984375, Val_Loss: 4055.16748046875\n",
      "Epoch 2117, Train_Loss: 3938.55859375, Val_Loss: 4055.1171875\n",
      "Epoch 2118, Train_Loss: 3938.5244140625, Val_Loss: 4055.07763671875\n",
      "Epoch 2119, Train_Loss: 3938.603515625, Val_Loss: 4055.151123046875\n",
      "Epoch 2120, Train_Loss: 3938.554443359375, Val_Loss: 4055.11767578125\n",
      "Epoch 2121, Train_Loss: 3938.441650390625, Val_Loss: 4055.014892578125\n",
      "Epoch 2122, Train_Loss: 3938.447021484375, Val_Loss: 4054.895263671875\n",
      "Epoch 2123, Train_Loss: 3938.388671875, Val_Loss: 4054.9052734375\n",
      "Epoch 2124, Train_Loss: 3938.45166015625, Val_Loss: 4055.067138671875\n",
      "Epoch 2125, Train_Loss: 3938.389892578125, Val_Loss: 4054.9091796875\n",
      "Epoch 2126, Train_Loss: 3938.35888671875, Val_Loss: 4054.83349609375\n",
      "Epoch 2127, Train_Loss: 3938.34228515625, Val_Loss: 4054.8935546875\n",
      "Epoch 2128, Train_Loss: 3938.37451171875, Val_Loss: 4054.936279296875\n",
      "Epoch 2129, Train_Loss: 3938.291259765625, Val_Loss: 4054.864013671875\n",
      "Epoch 2130, Train_Loss: 3938.24951171875, Val_Loss: 4054.7724609375\n",
      "Epoch 2131, Train_Loss: 3938.2861328125, Val_Loss: 4054.84765625\n",
      "Epoch 2132, Train_Loss: 3938.220703125, Val_Loss: 4054.823486328125\n",
      "Epoch 2133, Train_Loss: 3938.23046875, Val_Loss: 4054.8447265625\n",
      "Epoch 2134, Train_Loss: 3938.18701171875, Val_Loss: 4054.7958984375\n",
      "Epoch 2135, Train_Loss: 3938.146240234375, Val_Loss: 4054.704833984375\n",
      "Epoch 2136, Train_Loss: 3938.127197265625, Val_Loss: 4054.687255859375\n",
      "Epoch 2137, Train_Loss: 3938.145751953125, Val_Loss: 4054.73388671875\n",
      "Epoch 2138, Train_Loss: 3938.1298828125, Val_Loss: 4054.70556640625\n",
      "Epoch 2139, Train_Loss: 3938.126708984375, Val_Loss: 4054.73046875\n",
      "Epoch 2140, Train_Loss: 3938.078125, Val_Loss: 4054.59130859375\n",
      "Epoch 2141, Train_Loss: 3938.00830078125, Val_Loss: 4054.5419921875\n",
      "Epoch 2142, Train_Loss: 3938.0224609375, Val_Loss: 4054.53076171875\n",
      "Epoch 2143, Train_Loss: 3938.003173828125, Val_Loss: 4054.549560546875\n",
      "Epoch 2144, Train_Loss: 3938.023681640625, Val_Loss: 4054.583984375\n",
      "Epoch 2145, Train_Loss: 3937.897216796875, Val_Loss: 4054.51953125\n",
      "Epoch 2146, Train_Loss: 3937.867431640625, Val_Loss: 4054.503662109375\n",
      "Epoch 2147, Train_Loss: 3937.87109375, Val_Loss: 4054.492431640625\n",
      "Epoch 2148, Train_Loss: 3937.873779296875, Val_Loss: 4054.392333984375\n",
      "Epoch 2149, Train_Loss: 3937.858642578125, Val_Loss: 4054.374755859375\n",
      "Epoch 2150, Train_Loss: 3937.773193359375, Val_Loss: 4054.382080078125\n",
      "Epoch 2151, Train_Loss: 3937.79833984375, Val_Loss: 4054.439697265625\n",
      "Epoch 2152, Train_Loss: 3937.7861328125, Val_Loss: 4054.348876953125\n",
      "Epoch 2153, Train_Loss: 3937.717529296875, Val_Loss: 4054.335693359375\n",
      "Epoch 2154, Train_Loss: 3937.708740234375, Val_Loss: 4054.323974609375\n",
      "Epoch 2155, Train_Loss: 3937.654052734375, Val_Loss: 4054.3203125\n",
      "Epoch 2156, Train_Loss: 3937.6552734375, Val_Loss: 4054.2431640625\n",
      "Epoch 2157, Train_Loss: 3937.661376953125, Val_Loss: 4054.22998046875\n",
      "Epoch 2158, Train_Loss: 3937.653076171875, Val_Loss: 4054.252685546875\n",
      "Epoch 2159, Train_Loss: 3937.538330078125, Val_Loss: 4054.185546875\n",
      "Epoch 2160, Train_Loss: 3937.51904296875, Val_Loss: 4054.18603515625\n",
      "Epoch 2161, Train_Loss: 3937.459716796875, Val_Loss: 4054.0791015625\n",
      "Epoch 2162, Train_Loss: 3937.514892578125, Val_Loss: 4054.093994140625\n",
      "Epoch 2163, Train_Loss: 3937.471923828125, Val_Loss: 4054.095947265625\n",
      "Epoch 2164, Train_Loss: 3937.43994140625, Val_Loss: 4054.1103515625\n",
      "Epoch 2165, Train_Loss: 3937.39892578125, Val_Loss: 4054.036865234375\n",
      "Epoch 2166, Train_Loss: 3937.385986328125, Val_Loss: 4054.047119140625\n",
      "Epoch 2167, Train_Loss: 3937.357666015625, Val_Loss: 4054.08447265625\n",
      "Epoch 2168, Train_Loss: 3937.3251953125, Val_Loss: 4054.076904296875\n",
      "Epoch 2169, Train_Loss: 3937.3603515625, Val_Loss: 4053.99951171875\n",
      "Epoch 2170, Train_Loss: 3937.329345703125, Val_Loss: 4053.914794921875\n",
      "Epoch 2171, Train_Loss: 3937.3037109375, Val_Loss: 4053.910888671875\n",
      "Epoch 2172, Train_Loss: 3937.1552734375, Val_Loss: 4053.839111328125\n",
      "Epoch 2173, Train_Loss: 3937.151123046875, Val_Loss: 4053.9072265625\n",
      "Epoch 2174, Train_Loss: 3937.1318359375, Val_Loss: 4053.820068359375\n",
      "Epoch 2175, Train_Loss: 3937.107177734375, Val_Loss: 4053.785888671875\n",
      "Epoch 2176, Train_Loss: 3937.137939453125, Val_Loss: 4053.77001953125\n",
      "Epoch 2177, Train_Loss: 3937.079833984375, Val_Loss: 4053.822509765625\n",
      "Epoch 2178, Train_Loss: 3937.048583984375, Val_Loss: 4053.813720703125\n",
      "Epoch 2179, Train_Loss: 3937.057373046875, Val_Loss: 4053.731689453125\n",
      "Epoch 2180, Train_Loss: 3936.999755859375, Val_Loss: 4053.720458984375\n",
      "Epoch 2181, Train_Loss: 3936.96875, Val_Loss: 4053.705078125\n",
      "Epoch 2182, Train_Loss: 3936.969970703125, Val_Loss: 4053.701904296875\n",
      "Epoch 2183, Train_Loss: 3936.9814453125, Val_Loss: 4053.6875\n",
      "Epoch 2184, Train_Loss: 3936.950927734375, Val_Loss: 4053.6552734375\n",
      "Epoch 2185, Train_Loss: 3936.927490234375, Val_Loss: 4053.617919921875\n",
      "Epoch 2186, Train_Loss: 3936.888671875, Val_Loss: 4053.597900390625\n",
      "Epoch 2187, Train_Loss: 3936.8154296875, Val_Loss: 4053.565673828125\n",
      "Epoch 2188, Train_Loss: 3936.814697265625, Val_Loss: 4053.52490234375\n",
      "Epoch 2189, Train_Loss: 3936.79638671875, Val_Loss: 4053.549560546875\n",
      "Epoch 2190, Train_Loss: 3936.7685546875, Val_Loss: 4053.51806640625\n",
      "Epoch 2191, Train_Loss: 3936.777099609375, Val_Loss: 4053.46875\n",
      "Epoch 2192, Train_Loss: 3936.785888671875, Val_Loss: 4053.415283203125\n",
      "Epoch 2193, Train_Loss: 3936.77587890625, Val_Loss: 4053.342041015625\n",
      "Epoch 2194, Train_Loss: 3936.748779296875, Val_Loss: 4053.40478515625\n",
      "Epoch 2195, Train_Loss: 3936.7177734375, Val_Loss: 4053.447509765625\n",
      "Epoch 2196, Train_Loss: 3936.6748046875, Val_Loss: 4053.365478515625\n",
      "Epoch 2197, Train_Loss: 3936.649169921875, Val_Loss: 4053.306396484375\n",
      "Epoch 2198, Train_Loss: 3936.65380859375, Val_Loss: 4053.215087890625\n",
      "Epoch 2199, Train_Loss: 3936.633544921875, Val_Loss: 4053.211669921875\n",
      "Epoch 2200, Train_Loss: 3936.641357421875, Val_Loss: 4053.236083984375\n",
      "Epoch 2201, Train_Loss: 3936.52197265625, Val_Loss: 4053.263671875\n",
      "Epoch 2202, Train_Loss: 3936.29443359375, Val_Loss: 4053.240478515625\n",
      "Epoch 2203, Train_Loss: 3936.36376953125, Val_Loss: 4053.188720703125\n",
      "Epoch 2204, Train_Loss: 3936.28759765625, Val_Loss: 4053.118896484375\n",
      "Epoch 2205, Train_Loss: 3936.28564453125, Val_Loss: 4053.11328125\n",
      "Epoch 2206, Train_Loss: 3936.310791015625, Val_Loss: 4053.1064453125\n",
      "Epoch 2207, Train_Loss: 3936.284423828125, Val_Loss: 4053.102783203125\n",
      "Epoch 2208, Train_Loss: 3936.168212890625, Val_Loss: 4053.0625\n",
      "Epoch 2209, Train_Loss: 3936.155029296875, Val_Loss: 4053.027587890625\n",
      "Epoch 2210, Train_Loss: 3936.12353515625, Val_Loss: 4053.027587890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2211, Train_Loss: 3936.123779296875, Val_Loss: 4053.016357421875\n",
      "Epoch 2212, Train_Loss: 3936.097412109375, Val_Loss: 4053.01611328125\n",
      "Epoch 2213, Train_Loss: 3936.07763671875, Val_Loss: 4052.992919921875\n",
      "Epoch 2214, Train_Loss: 3936.040283203125, Val_Loss: 4052.923095703125\n",
      "Epoch 2215, Train_Loss: 3936.0068359375, Val_Loss: 4052.898681640625\n",
      "Epoch 2216, Train_Loss: 3935.994140625, Val_Loss: 4052.88720703125\n",
      "Epoch 2217, Train_Loss: 3935.975830078125, Val_Loss: 4052.8828125\n",
      "Epoch 2218, Train_Loss: 3935.951904296875, Val_Loss: 4052.857666015625\n",
      "Epoch 2219, Train_Loss: 3935.938720703125, Val_Loss: 4052.839111328125\n",
      "Epoch 2220, Train_Loss: 3935.88720703125, Val_Loss: 4052.74755859375\n",
      "Epoch 2221, Train_Loss: 3935.8720703125, Val_Loss: 4052.73046875\n",
      "Epoch 2222, Train_Loss: 3935.84814453125, Val_Loss: 4052.717529296875\n",
      "Epoch 2223, Train_Loss: 3935.839599609375, Val_Loss: 4052.6904296875\n",
      "Epoch 2224, Train_Loss: 3935.8203125, Val_Loss: 4052.676513671875\n",
      "Epoch 2225, Train_Loss: 3935.81201171875, Val_Loss: 4052.650390625\n",
      "Epoch 2226, Train_Loss: 3935.746826171875, Val_Loss: 4052.63916015625\n",
      "Epoch 2227, Train_Loss: 3935.760498046875, Val_Loss: 4052.61083984375\n",
      "Epoch 2228, Train_Loss: 3935.738037109375, Val_Loss: 4052.59716796875\n",
      "Epoch 2229, Train_Loss: 3935.72607421875, Val_Loss: 4052.57958984375\n",
      "Epoch 2230, Train_Loss: 3935.643310546875, Val_Loss: 4052.536865234375\n",
      "Epoch 2231, Train_Loss: 3935.64111328125, Val_Loss: 4052.507568359375\n",
      "Epoch 2232, Train_Loss: 3935.6181640625, Val_Loss: 4052.49755859375\n",
      "Epoch 2233, Train_Loss: 3935.56884765625, Val_Loss: 4052.461181640625\n",
      "Epoch 2234, Train_Loss: 3935.55419921875, Val_Loss: 4052.4462890625\n",
      "Epoch 2235, Train_Loss: 3935.538818359375, Val_Loss: 4052.434326171875\n",
      "Epoch 2236, Train_Loss: 3935.51953125, Val_Loss: 4052.4228515625\n",
      "Epoch 2237, Train_Loss: 3935.51171875, Val_Loss: 4052.389892578125\n",
      "Epoch 2238, Train_Loss: 3935.49755859375, Val_Loss: 4052.37548828125\n",
      "Epoch 2239, Train_Loss: 3935.46826171875, Val_Loss: 4052.32080078125\n",
      "Epoch 2240, Train_Loss: 3935.451416015625, Val_Loss: 4052.30712890625\n",
      "Epoch 2241, Train_Loss: 3935.443115234375, Val_Loss: 4052.2880859375\n",
      "Epoch 2242, Train_Loss: 3935.42822265625, Val_Loss: 4052.269287109375\n",
      "Epoch 2243, Train_Loss: 3935.264892578125, Val_Loss: 4052.257568359375\n",
      "Epoch 2244, Train_Loss: 3935.249267578125, Val_Loss: 4052.244384765625\n",
      "Epoch 2245, Train_Loss: 3935.22412109375, Val_Loss: 4052.1435546875\n",
      "Epoch 2246, Train_Loss: 3935.207763671875, Val_Loss: 4052.126708984375\n",
      "Epoch 2247, Train_Loss: 3935.192626953125, Val_Loss: 4052.11083984375\n",
      "Epoch 2248, Train_Loss: 3935.1826171875, Val_Loss: 4052.09326171875\n",
      "Epoch 2249, Train_Loss: 3935.17529296875, Val_Loss: 4052.069091796875\n",
      "Epoch 2250, Train_Loss: 3935.163818359375, Val_Loss: 4052.05908203125\n",
      "Epoch 2251, Train_Loss: 3935.092529296875, Val_Loss: 4052.030517578125\n",
      "Epoch 2252, Train_Loss: 3935.08154296875, Val_Loss: 4052.001708984375\n",
      "Epoch 2253, Train_Loss: 3935.071533203125, Val_Loss: 4051.985595703125\n",
      "Epoch 2254, Train_Loss: 3935.050537109375, Val_Loss: 4051.976806640625\n",
      "Epoch 2255, Train_Loss: 3935.012451171875, Val_Loss: 4051.92041015625\n",
      "Epoch 2256, Train_Loss: 3935.007568359375, Val_Loss: 4051.90087890625\n",
      "Epoch 2257, Train_Loss: 3934.692138671875, Val_Loss: 4051.857666015625\n",
      "Epoch 2258, Train_Loss: 3934.672119140625, Val_Loss: 4051.845703125\n",
      "Epoch 2259, Train_Loss: 3934.656982421875, Val_Loss: 4051.842041015625\n",
      "Epoch 2260, Train_Loss: 3934.615234375, Val_Loss: 4051.83349609375\n",
      "Epoch 2261, Train_Loss: 3934.6162109375, Val_Loss: 4051.81689453125\n",
      "Epoch 2262, Train_Loss: 3934.590087890625, Val_Loss: 4051.805908203125\n",
      "Epoch 2263, Train_Loss: 3934.572265625, Val_Loss: 4051.73681640625\n",
      "Epoch 2264, Train_Loss: 3934.564208984375, Val_Loss: 4051.70849609375\n",
      "Epoch 2265, Train_Loss: 3934.53662109375, Val_Loss: 4051.708740234375\n",
      "Epoch 2266, Train_Loss: 3934.490478515625, Val_Loss: 4051.6748046875\n",
      "Epoch 2267, Train_Loss: 3934.49365234375, Val_Loss: 4051.6572265625\n",
      "Epoch 2268, Train_Loss: 3934.4462890625, Val_Loss: 4051.56396484375\n",
      "Epoch 2269, Train_Loss: 3934.39306640625, Val_Loss: 4051.555908203125\n",
      "Epoch 2270, Train_Loss: 3934.433349609375, Val_Loss: 4051.507568359375\n",
      "Epoch 2271, Train_Loss: 3934.398193359375, Val_Loss: 4051.49365234375\n",
      "Epoch 2272, Train_Loss: 3934.35986328125, Val_Loss: 4051.494873046875\n",
      "Epoch 2273, Train_Loss: 3934.347412109375, Val_Loss: 4051.483642578125\n",
      "Epoch 2274, Train_Loss: 3934.341064453125, Val_Loss: 4051.40478515625\n",
      "Epoch 2275, Train_Loss: 3934.34228515625, Val_Loss: 4051.376708984375\n",
      "Epoch 2276, Train_Loss: 3934.283447265625, Val_Loss: 4051.322021484375\n",
      "Epoch 2277, Train_Loss: 3934.298583984375, Val_Loss: 4051.341552734375\n",
      "Epoch 2278, Train_Loss: 3934.276611328125, Val_Loss: 4051.345703125\n",
      "Epoch 2279, Train_Loss: 3934.226318359375, Val_Loss: 4051.27587890625\n",
      "Epoch 2280, Train_Loss: 3934.25830078125, Val_Loss: 4051.3115234375\n",
      "Epoch 2281, Train_Loss: 3934.18505859375, Val_Loss: 4051.283935546875\n",
      "Epoch 2282, Train_Loss: 3934.110107421875, Val_Loss: 4051.242919921875\n",
      "Epoch 2283, Train_Loss: 3934.168701171875, Val_Loss: 4051.13916015625\n",
      "Epoch 2284, Train_Loss: 3934.141357421875, Val_Loss: 4051.18603515625\n",
      "Epoch 2285, Train_Loss: 3934.069580078125, Val_Loss: 4051.178466796875\n",
      "Epoch 2286, Train_Loss: 3934.082763671875, Val_Loss: 4051.218017578125\n",
      "Epoch 2287, Train_Loss: 3934.030517578125, Val_Loss: 4051.135986328125\n",
      "Epoch 2288, Train_Loss: 3934.021484375, Val_Loss: 4051.038818359375\n",
      "Epoch 2289, Train_Loss: 3934.0986328125, Val_Loss: 4050.978759765625\n",
      "Epoch 2290, Train_Loss: 3934.00830078125, Val_Loss: 4051.087890625\n",
      "Epoch 2291, Train_Loss: 3934.040771484375, Val_Loss: 4050.960693359375\n",
      "Epoch 2292, Train_Loss: 3933.947998046875, Val_Loss: 4050.976318359375\n",
      "Epoch 2293, Train_Loss: 3933.90869140625, Val_Loss: 4050.9619140625\n",
      "Epoch 2294, Train_Loss: 3933.883056640625, Val_Loss: 4050.92724609375\n",
      "Epoch 2295, Train_Loss: 3933.872802734375, Val_Loss: 4050.850830078125\n",
      "Epoch 2296, Train_Loss: 3933.85009765625, Val_Loss: 4050.835693359375\n",
      "Epoch 2297, Train_Loss: 3933.86279296875, Val_Loss: 4050.883544921875\n",
      "Epoch 2298, Train_Loss: 3933.847412109375, Val_Loss: 4050.79638671875\n",
      "Epoch 2299, Train_Loss: 3933.771484375, Val_Loss: 4050.797119140625\n",
      "Epoch 2300, Train_Loss: 3933.801513671875, Val_Loss: 4050.903564453125\n",
      "Epoch 2301, Train_Loss: 3933.79150390625, Val_Loss: 4050.744873046875\n",
      "Epoch 2302, Train_Loss: 3933.803955078125, Val_Loss: 4050.71484375\n",
      "Epoch 2303, Train_Loss: 3933.794189453125, Val_Loss: 4050.696044921875\n",
      "Epoch 2304, Train_Loss: 3933.709716796875, Val_Loss: 4050.7236328125\n",
      "Epoch 2305, Train_Loss: 3933.546630859375, Val_Loss: 4050.778076171875\n",
      "Epoch 2306, Train_Loss: 3933.524169921875, Val_Loss: 4050.6748046875\n",
      "Epoch 2307, Train_Loss: 3933.5341796875, Val_Loss: 4050.652099609375\n",
      "Epoch 2308, Train_Loss: 3933.49853515625, Val_Loss: 4050.618408203125\n",
      "Epoch 2309, Train_Loss: 3933.465576171875, Val_Loss: 4050.634765625\n",
      "Epoch 2310, Train_Loss: 3933.401611328125, Val_Loss: 4050.643310546875\n",
      "Epoch 2311, Train_Loss: 3933.40478515625, Val_Loss: 4050.569580078125\n",
      "Epoch 2312, Train_Loss: 3933.41357421875, Val_Loss: 4050.530517578125\n",
      "Epoch 2313, Train_Loss: 3933.41162109375, Val_Loss: 4050.505615234375\n",
      "Epoch 2314, Train_Loss: 3933.379638671875, Val_Loss: 4050.49072265625\n",
      "Epoch 2315, Train_Loss: 3933.36279296875, Val_Loss: 4050.528076171875\n",
      "Epoch 2316, Train_Loss: 3933.328857421875, Val_Loss: 4050.516845703125\n",
      "Epoch 2317, Train_Loss: 3933.250244140625, Val_Loss: 4050.41357421875\n",
      "Epoch 2318, Train_Loss: 3933.23828125, Val_Loss: 4050.379150390625\n",
      "Epoch 2319, Train_Loss: 3933.20458984375, Val_Loss: 4050.361083984375\n",
      "Epoch 2320, Train_Loss: 3933.188232421875, Val_Loss: 4050.352783203125\n",
      "Epoch 2321, Train_Loss: 3933.178466796875, Val_Loss: 4050.322021484375\n",
      "Epoch 2322, Train_Loss: 3933.1611328125, Val_Loss: 4050.3046875\n",
      "Epoch 2323, Train_Loss: 3933.12353515625, Val_Loss: 4050.287109375\n",
      "Epoch 2324, Train_Loss: 3933.1025390625, Val_Loss: 4050.27685546875\n",
      "Epoch 2325, Train_Loss: 3933.0830078125, Val_Loss: 4050.26123046875\n",
      "Epoch 2326, Train_Loss: 3933.07666015625, Val_Loss: 4050.23486328125\n",
      "Epoch 2327, Train_Loss: 3933.0693359375, Val_Loss: 4050.213134765625\n",
      "Epoch 2328, Train_Loss: 3933.050048828125, Val_Loss: 4050.204833984375\n",
      "Epoch 2329, Train_Loss: 3933.01806640625, Val_Loss: 4050.102294921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2330, Train_Loss: 3932.98388671875, Val_Loss: 4050.100341796875\n",
      "Epoch 2331, Train_Loss: 3932.972412109375, Val_Loss: 4050.1201171875\n",
      "Epoch 2332, Train_Loss: 3932.96826171875, Val_Loss: 4050.064453125\n",
      "Epoch 2333, Train_Loss: 3932.9443359375, Val_Loss: 4050.048828125\n",
      "Epoch 2334, Train_Loss: 3932.9443359375, Val_Loss: 4050.0263671875\n",
      "Epoch 2335, Train_Loss: 3932.90283203125, Val_Loss: 4049.974365234375\n",
      "Epoch 2336, Train_Loss: 3932.872314453125, Val_Loss: 4049.961669921875\n",
      "Epoch 2337, Train_Loss: 3932.848388671875, Val_Loss: 4049.94287109375\n",
      "Epoch 2338, Train_Loss: 3932.837890625, Val_Loss: 4049.92919921875\n",
      "Epoch 2339, Train_Loss: 3932.790771484375, Val_Loss: 4049.90771484375\n",
      "Epoch 2340, Train_Loss: 3932.770263671875, Val_Loss: 4049.822021484375\n",
      "Epoch 2341, Train_Loss: 3932.686279296875, Val_Loss: 4049.799560546875\n",
      "Epoch 2342, Train_Loss: 3932.72509765625, Val_Loss: 4049.80126953125\n",
      "Epoch 2343, Train_Loss: 3932.68701171875, Val_Loss: 4049.8076171875\n",
      "Epoch 2344, Train_Loss: 3932.6796875, Val_Loss: 4049.7919921875\n",
      "Epoch 2345, Train_Loss: 3932.66796875, Val_Loss: 4049.76708984375\n",
      "Epoch 2346, Train_Loss: 3932.562744140625, Val_Loss: 4049.74609375\n",
      "Epoch 2347, Train_Loss: 3932.489990234375, Val_Loss: 4049.70361328125\n",
      "Epoch 2348, Train_Loss: 3932.494140625, Val_Loss: 4049.679931640625\n",
      "Epoch 2349, Train_Loss: 3932.477783203125, Val_Loss: 4049.664306640625\n",
      "Epoch 2350, Train_Loss: 3932.447021484375, Val_Loss: 4049.658447265625\n",
      "Epoch 2351, Train_Loss: 3932.420654296875, Val_Loss: 4049.6484375\n",
      "Epoch 2352, Train_Loss: 3932.29931640625, Val_Loss: 4049.6240234375\n",
      "Epoch 2353, Train_Loss: 3932.28515625, Val_Loss: 4049.595947265625\n",
      "Epoch 2354, Train_Loss: 3932.270263671875, Val_Loss: 4049.567138671875\n",
      "Epoch 2355, Train_Loss: 3932.268310546875, Val_Loss: 4049.53564453125\n",
      "Epoch 2356, Train_Loss: 3932.261962890625, Val_Loss: 4049.5283203125\n",
      "Epoch 2357, Train_Loss: 3932.23779296875, Val_Loss: 4049.505615234375\n",
      "Epoch 2358, Train_Loss: 3932.202880859375, Val_Loss: 4049.46044921875\n",
      "Epoch 2359, Train_Loss: 3932.162353515625, Val_Loss: 4049.4375\n",
      "Epoch 2360, Train_Loss: 3932.15087890625, Val_Loss: 4049.429931640625\n",
      "Epoch 2361, Train_Loss: 3932.10009765625, Val_Loss: 4049.375244140625\n",
      "Epoch 2362, Train_Loss: 3932.090087890625, Val_Loss: 4049.36328125\n",
      "Epoch 2363, Train_Loss: 3932.067138671875, Val_Loss: 4049.3427734375\n",
      "Epoch 2364, Train_Loss: 3931.959228515625, Val_Loss: 4049.264892578125\n",
      "Epoch 2365, Train_Loss: 3931.939208984375, Val_Loss: 4049.247314453125\n",
      "Epoch 2366, Train_Loss: 3931.919921875, Val_Loss: 4049.231201171875\n",
      "Epoch 2367, Train_Loss: 3931.902099609375, Val_Loss: 4049.212890625\n",
      "Epoch 2368, Train_Loss: 3931.888427734375, Val_Loss: 4049.1904296875\n",
      "Epoch 2369, Train_Loss: 3931.873779296875, Val_Loss: 4049.174072265625\n",
      "Epoch 2370, Train_Loss: 3931.843994140625, Val_Loss: 4049.151123046875\n",
      "Epoch 2371, Train_Loss: 3931.82763671875, Val_Loss: 4049.129638671875\n",
      "Epoch 2372, Train_Loss: 3931.81396484375, Val_Loss: 4049.11474609375\n",
      "Epoch 2373, Train_Loss: 3931.781494140625, Val_Loss: 4049.098876953125\n",
      "Epoch 2374, Train_Loss: 3931.76171875, Val_Loss: 4049.078857421875\n",
      "Epoch 2375, Train_Loss: 3931.715576171875, Val_Loss: 4049.03125\n",
      "Epoch 2376, Train_Loss: 3931.702880859375, Val_Loss: 4049.022705078125\n",
      "Epoch 2377, Train_Loss: 3931.685546875, Val_Loss: 4049.009521484375\n",
      "Epoch 2378, Train_Loss: 3931.66845703125, Val_Loss: 4048.9951171875\n",
      "Epoch 2379, Train_Loss: 3931.6572265625, Val_Loss: 4048.97314453125\n",
      "Epoch 2380, Train_Loss: 3931.6328125, Val_Loss: 4048.955078125\n",
      "Epoch 2381, Train_Loss: 3931.608642578125, Val_Loss: 4048.897705078125\n",
      "Epoch 2382, Train_Loss: 3931.591796875, Val_Loss: 4048.87646484375\n",
      "Epoch 2383, Train_Loss: 3931.572998046875, Val_Loss: 4048.857177734375\n",
      "Epoch 2384, Train_Loss: 3931.552490234375, Val_Loss: 4048.839111328125\n",
      "Epoch 2385, Train_Loss: 3931.53271484375, Val_Loss: 4048.8271484375\n",
      "Epoch 2386, Train_Loss: 3931.505859375, Val_Loss: 4048.771240234375\n",
      "Epoch 2387, Train_Loss: 3931.46875, Val_Loss: 4048.682861328125\n",
      "Epoch 2388, Train_Loss: 3931.4638671875, Val_Loss: 4048.674072265625\n",
      "Epoch 2389, Train_Loss: 3931.44970703125, Val_Loss: 4048.633544921875\n",
      "Epoch 2390, Train_Loss: 3931.40625, Val_Loss: 4048.629150390625\n",
      "Epoch 2391, Train_Loss: 3931.371337890625, Val_Loss: 4048.611083984375\n",
      "Epoch 2392, Train_Loss: 3931.36181640625, Val_Loss: 4048.591552734375\n",
      "Epoch 2393, Train_Loss: 3931.120849609375, Val_Loss: 4048.5771484375\n",
      "Epoch 2394, Train_Loss: 3931.0625, Val_Loss: 4048.527587890625\n",
      "Epoch 2395, Train_Loss: 3931.051513671875, Val_Loss: 4048.510009765625\n",
      "Epoch 2396, Train_Loss: 3931.03857421875, Val_Loss: 4048.494873046875\n",
      "Epoch 2397, Train_Loss: 3931.00244140625, Val_Loss: 4048.471923828125\n",
      "Epoch 2398, Train_Loss: 3930.886962890625, Val_Loss: 4048.445556640625\n",
      "Epoch 2399, Train_Loss: 3930.93359375, Val_Loss: 4048.4140625\n",
      "Epoch 2400, Train_Loss: 3930.914794921875, Val_Loss: 4048.404296875\n",
      "Epoch 2401, Train_Loss: 3930.91943359375, Val_Loss: 4048.383544921875\n",
      "Epoch 2402, Train_Loss: 3930.907470703125, Val_Loss: 4048.365478515625\n",
      "Epoch 2403, Train_Loss: 3930.8759765625, Val_Loss: 4048.35205078125\n",
      "Epoch 2404, Train_Loss: 3930.822509765625, Val_Loss: 4048.248046875\n",
      "Epoch 2405, Train_Loss: 3930.799072265625, Val_Loss: 4048.238037109375\n",
      "Epoch 2406, Train_Loss: 3930.78759765625, Val_Loss: 4048.220703125\n",
      "Epoch 2407, Train_Loss: 3930.7666015625, Val_Loss: 4048.209228515625\n",
      "Epoch 2408, Train_Loss: 3930.751953125, Val_Loss: 4048.19482421875\n",
      "Epoch 2409, Train_Loss: 3930.7314453125, Val_Loss: 4048.180908203125\n",
      "Epoch 2410, Train_Loss: 3930.6884765625, Val_Loss: 4048.0908203125\n",
      "Epoch 2411, Train_Loss: 3930.65283203125, Val_Loss: 4048.075927734375\n",
      "Epoch 2412, Train_Loss: 3930.654052734375, Val_Loss: 4048.0556640625\n",
      "Epoch 2413, Train_Loss: 3930.623046875, Val_Loss: 4048.029296875\n",
      "Epoch 2414, Train_Loss: 3930.62451171875, Val_Loss: 4048.022705078125\n",
      "Epoch 2415, Train_Loss: 3930.569580078125, Val_Loss: 4048.00634765625\n",
      "Epoch 2416, Train_Loss: 3930.542724609375, Val_Loss: 4047.965576171875\n",
      "Epoch 2417, Train_Loss: 3930.535888671875, Val_Loss: 4047.954345703125\n",
      "Epoch 2418, Train_Loss: 3930.5166015625, Val_Loss: 4047.934326171875\n",
      "Epoch 2419, Train_Loss: 3930.5390625, Val_Loss: 4047.907958984375\n",
      "Epoch 2420, Train_Loss: 3930.478271484375, Val_Loss: 4047.9033203125\n",
      "Epoch 2421, Train_Loss: 3930.50048828125, Val_Loss: 4047.783203125\n",
      "Epoch 2422, Train_Loss: 3930.421875, Val_Loss: 4047.850830078125\n",
      "Epoch 2423, Train_Loss: 3930.40966796875, Val_Loss: 4047.8359375\n",
      "Epoch 2424, Train_Loss: 3930.392578125, Val_Loss: 4047.744384765625\n",
      "Epoch 2425, Train_Loss: 3930.4296875, Val_Loss: 4047.739990234375\n",
      "Epoch 2426, Train_Loss: 3930.412353515625, Val_Loss: 4047.73291015625\n",
      "Epoch 2427, Train_Loss: 3930.385009765625, Val_Loss: 4047.666748046875\n",
      "Epoch 2428, Train_Loss: 3930.385009765625, Val_Loss: 4047.64794921875\n",
      "Epoch 2429, Train_Loss: 3930.176513671875, Val_Loss: 4047.614501953125\n",
      "Epoch 2430, Train_Loss: 3930.159912109375, Val_Loss: 4047.58251953125\n",
      "Epoch 2431, Train_Loss: 3930.268310546875, Val_Loss: 4047.62548828125\n",
      "Epoch 2432, Train_Loss: 3930.305908203125, Val_Loss: 4047.5966796875\n",
      "Epoch 2433, Train_Loss: 3930.103271484375, Val_Loss: 4047.468505859375\n",
      "Epoch 2434, Train_Loss: 3930.0712890625, Val_Loss: 4047.454833984375\n",
      "Epoch 2435, Train_Loss: 3930.246337890625, Val_Loss: 4047.49560546875\n",
      "Epoch 2436, Train_Loss: 3930.037841796875, Val_Loss: 4047.43310546875\n",
      "Epoch 2437, Train_Loss: 3930.02294921875, Val_Loss: 4047.392333984375\n",
      "Epoch 2438, Train_Loss: 3930.142578125, Val_Loss: 4047.46435546875\n",
      "Epoch 2439, Train_Loss: 3929.94189453125, Val_Loss: 4047.36328125\n",
      "Epoch 2440, Train_Loss: 3929.92724609375, Val_Loss: 4047.30078125\n",
      "Epoch 2441, Train_Loss: 3929.94384765625, Val_Loss: 4047.364013671875\n",
      "Epoch 2442, Train_Loss: 3929.8984375, Val_Loss: 4047.31005859375\n",
      "Epoch 2443, Train_Loss: 3929.852294921875, Val_Loss: 4047.2568359375\n",
      "Epoch 2444, Train_Loss: 3929.853759765625, Val_Loss: 4047.340087890625\n",
      "Epoch 2445, Train_Loss: 3929.673583984375, Val_Loss: 4047.33642578125\n",
      "Epoch 2446, Train_Loss: 3929.56103515625, Val_Loss: 4047.249267578125\n",
      "Epoch 2447, Train_Loss: 3929.481201171875, Val_Loss: 4047.19287109375\n",
      "Epoch 2448, Train_Loss: 3929.48388671875, Val_Loss: 4047.166015625\n",
      "Epoch 2449, Train_Loss: 3929.467529296875, Val_Loss: 4047.115478515625\n",
      "Epoch 2450, Train_Loss: 3929.46826171875, Val_Loss: 4047.109619140625\n",
      "Epoch 2451, Train_Loss: 3929.4052734375, Val_Loss: 4047.179931640625\n",
      "Epoch 2452, Train_Loss: 3929.393310546875, Val_Loss: 4047.168701171875\n",
      "Epoch 2453, Train_Loss: 3929.357177734375, Val_Loss: 4047.052490234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2454, Train_Loss: 3929.349853515625, Val_Loss: 4047.005615234375\n",
      "Epoch 2455, Train_Loss: 3929.38525390625, Val_Loss: 4047.05810546875\n",
      "Epoch 2456, Train_Loss: 3929.2919921875, Val_Loss: 4046.916015625\n",
      "Epoch 2457, Train_Loss: 3929.27734375, Val_Loss: 4046.873291015625\n",
      "Epoch 2458, Train_Loss: 3929.314208984375, Val_Loss: 4046.917236328125\n",
      "Epoch 2459, Train_Loss: 3929.237060546875, Val_Loss: 4046.90673828125\n",
      "Epoch 2460, Train_Loss: 3929.228271484375, Val_Loss: 4046.88525390625\n",
      "Epoch 2461, Train_Loss: 3929.201904296875, Val_Loss: 4046.864501953125\n",
      "Epoch 2462, Train_Loss: 3929.200439453125, Val_Loss: 4046.849609375\n",
      "Epoch 2463, Train_Loss: 3929.143798828125, Val_Loss: 4046.84765625\n",
      "Epoch 2464, Train_Loss: 3929.159423828125, Val_Loss: 4046.775634765625\n",
      "Epoch 2465, Train_Loss: 3929.137451171875, Val_Loss: 4046.783935546875\n",
      "Epoch 2466, Train_Loss: 3929.1181640625, Val_Loss: 4046.763671875\n",
      "Epoch 2467, Train_Loss: 3929.087646484375, Val_Loss: 4046.7392578125\n",
      "Epoch 2468, Train_Loss: 3929.017578125, Val_Loss: 4046.730712890625\n",
      "Epoch 2469, Train_Loss: 3929.03271484375, Val_Loss: 4046.68603515625\n",
      "Epoch 2470, Train_Loss: 3928.98291015625, Val_Loss: 4046.700439453125\n",
      "Epoch 2471, Train_Loss: 3929.001708984375, Val_Loss: 4046.644775390625\n",
      "Epoch 2472, Train_Loss: 3928.9833984375, Val_Loss: 4046.622314453125\n",
      "Epoch 2473, Train_Loss: 3928.9619140625, Val_Loss: 4046.61669921875\n",
      "Epoch 2474, Train_Loss: 3928.88818359375, Val_Loss: 4046.59033203125\n",
      "Epoch 2475, Train_Loss: 3928.8759765625, Val_Loss: 4046.56640625\n",
      "Epoch 2476, Train_Loss: 3928.862548828125, Val_Loss: 4046.5537109375\n",
      "Epoch 2477, Train_Loss: 3928.84716796875, Val_Loss: 4046.531982421875\n",
      "Epoch 2478, Train_Loss: 3928.828369140625, Val_Loss: 4046.508056640625\n",
      "Epoch 2479, Train_Loss: 3928.792724609375, Val_Loss: 4046.417236328125\n",
      "Epoch 2480, Train_Loss: 3928.7744140625, Val_Loss: 4046.40234375\n",
      "Epoch 2481, Train_Loss: 3928.772216796875, Val_Loss: 4046.39599609375\n",
      "Epoch 2482, Train_Loss: 3928.75390625, Val_Loss: 4046.372314453125\n",
      "Epoch 2483, Train_Loss: 3928.764892578125, Val_Loss: 4046.298828125\n",
      "Epoch 2484, Train_Loss: 3928.754638671875, Val_Loss: 4046.279296875\n",
      "Epoch 2485, Train_Loss: 3928.68994140625, Val_Loss: 4046.259521484375\n",
      "Epoch 2486, Train_Loss: 3928.627685546875, Val_Loss: 4046.25439453125\n",
      "Epoch 2487, Train_Loss: 3928.597900390625, Val_Loss: 4046.246826171875\n",
      "Epoch 2488, Train_Loss: 3928.6005859375, Val_Loss: 4046.2236328125\n",
      "Epoch 2489, Train_Loss: 3928.584716796875, Val_Loss: 4046.222412109375\n",
      "Epoch 2490, Train_Loss: 3928.392578125, Val_Loss: 4046.166015625\n",
      "Epoch 2491, Train_Loss: 3928.44580078125, Val_Loss: 4046.143310546875\n",
      "Epoch 2492, Train_Loss: 3928.4296875, Val_Loss: 4046.132080078125\n",
      "Epoch 2493, Train_Loss: 3928.4091796875, Val_Loss: 4046.115966796875\n",
      "Epoch 2494, Train_Loss: 3928.325439453125, Val_Loss: 4046.1171875\n",
      "Epoch 2495, Train_Loss: 3928.332275390625, Val_Loss: 4046.1123046875\n",
      "Epoch 2496, Train_Loss: 3928.302734375, Val_Loss: 4046.043701171875\n",
      "Epoch 2497, Train_Loss: 3928.195556640625, Val_Loss: 4046.013916015625\n",
      "Epoch 2498, Train_Loss: 3928.1787109375, Val_Loss: 4046.003173828125\n",
      "Epoch 2499, Train_Loss: 3928.1796875, Val_Loss: 4045.982421875\n",
      "Epoch 2500, Train_Loss: 3928.1640625, Val_Loss: 4045.965576171875\n",
      "Epoch 2501, Train_Loss: 3928.146240234375, Val_Loss: 4045.95068359375\n",
      "Epoch 2502, Train_Loss: 3928.09423828125, Val_Loss: 4045.861083984375\n",
      "Epoch 2503, Train_Loss: 3928.053955078125, Val_Loss: 4045.84228515625\n",
      "Epoch 2504, Train_Loss: 3928.095458984375, Val_Loss: 4045.800048828125\n",
      "Epoch 2505, Train_Loss: 3928.023681640625, Val_Loss: 4045.80517578125\n",
      "Epoch 2506, Train_Loss: 3928.029296875, Val_Loss: 4045.7880859375\n",
      "Epoch 2507, Train_Loss: 3927.97265625, Val_Loss: 4045.763916015625\n",
      "Epoch 2508, Train_Loss: 3928.00537109375, Val_Loss: 4045.728759765625\n",
      "Epoch 2509, Train_Loss: 3927.939697265625, Val_Loss: 4045.73193359375\n",
      "Epoch 2510, Train_Loss: 3927.919189453125, Val_Loss: 4045.717529296875\n",
      "Epoch 2511, Train_Loss: 3927.88525390625, Val_Loss: 4045.652099609375\n",
      "Epoch 2512, Train_Loss: 3927.917236328125, Val_Loss: 4045.6171875\n",
      "Epoch 2513, Train_Loss: 3927.866455078125, Val_Loss: 4045.580810546875\n",
      "Epoch 2514, Train_Loss: 3927.80712890625, Val_Loss: 4045.580322265625\n",
      "Epoch 2515, Train_Loss: 3927.796142578125, Val_Loss: 4045.561279296875\n",
      "Epoch 2516, Train_Loss: 3927.780029296875, Val_Loss: 4045.548828125\n",
      "Epoch 2517, Train_Loss: 3927.75634765625, Val_Loss: 4045.5458984375\n",
      "Epoch 2518, Train_Loss: 3927.78662109375, Val_Loss: 4045.455078125\n",
      "Epoch 2519, Train_Loss: 3927.7509765625, Val_Loss: 4045.43603515625\n",
      "Epoch 2520, Train_Loss: 3927.69091796875, Val_Loss: 4045.4423828125\n",
      "Epoch 2521, Train_Loss: 3927.674072265625, Val_Loss: 4045.419189453125\n",
      "Epoch 2522, Train_Loss: 3927.665771484375, Val_Loss: 4045.405517578125\n",
      "Epoch 2523, Train_Loss: 3927.643798828125, Val_Loss: 4045.369140625\n",
      "Epoch 2524, Train_Loss: 3927.61328125, Val_Loss: 4045.272705078125\n",
      "Epoch 2525, Train_Loss: 3927.5908203125, Val_Loss: 4045.271240234375\n",
      "Epoch 2526, Train_Loss: 3927.58349609375, Val_Loss: 4045.25244140625\n",
      "Epoch 2527, Train_Loss: 3927.546142578125, Val_Loss: 4045.234375\n",
      "Epoch 2528, Train_Loss: 3927.524169921875, Val_Loss: 4045.21044921875\n",
      "Epoch 2529, Train_Loss: 3927.510009765625, Val_Loss: 4045.191650390625\n",
      "Epoch 2530, Train_Loss: 3927.421142578125, Val_Loss: 4045.16357421875\n",
      "Epoch 2531, Train_Loss: 3927.41259765625, Val_Loss: 4045.147705078125\n",
      "Epoch 2532, Train_Loss: 3927.402099609375, Val_Loss: 4045.12890625\n",
      "Epoch 2533, Train_Loss: 3927.380615234375, Val_Loss: 4045.08154296875\n",
      "Epoch 2534, Train_Loss: 3927.3564453125, Val_Loss: 4045.06396484375\n",
      "Epoch 2535, Train_Loss: 3927.266845703125, Val_Loss: 4045.01806640625\n",
      "Epoch 2536, Train_Loss: 3927.23388671875, Val_Loss: 4045.003173828125\n",
      "Epoch 2537, Train_Loss: 3927.216552734375, Val_Loss: 4044.996337890625\n",
      "Epoch 2538, Train_Loss: 3927.200927734375, Val_Loss: 4044.972412109375\n",
      "Epoch 2539, Train_Loss: 3927.184814453125, Val_Loss: 4044.966796875\n",
      "Epoch 2540, Train_Loss: 3927.16162109375, Val_Loss: 4044.952392578125\n",
      "Epoch 2541, Train_Loss: 3927.130126953125, Val_Loss: 4044.890869140625\n",
      "Epoch 2542, Train_Loss: 3927.08984375, Val_Loss: 4044.88232421875\n",
      "Epoch 2543, Train_Loss: 3927.076171875, Val_Loss: 4044.8671875\n",
      "Epoch 2544, Train_Loss: 3927.056396484375, Val_Loss: 4044.84765625\n",
      "Epoch 2545, Train_Loss: 3927.000732421875, Val_Loss: 4044.805908203125\n",
      "Epoch 2546, Train_Loss: 3926.90234375, Val_Loss: 4044.7255859375\n",
      "Epoch 2547, Train_Loss: 3926.87646484375, Val_Loss: 4044.705078125\n",
      "Epoch 2548, Train_Loss: 3926.8671875, Val_Loss: 4044.689208984375\n",
      "Epoch 2549, Train_Loss: 3926.810546875, Val_Loss: 4044.669189453125\n",
      "Epoch 2550, Train_Loss: 3926.85400390625, Val_Loss: 4044.642333984375\n",
      "Epoch 2551, Train_Loss: 3926.7734375, Val_Loss: 4044.63232421875\n",
      "Epoch 2552, Train_Loss: 3926.740234375, Val_Loss: 4044.59765625\n",
      "Epoch 2553, Train_Loss: 3926.701171875, Val_Loss: 4044.532470703125\n",
      "Epoch 2554, Train_Loss: 3926.703125, Val_Loss: 4044.571533203125\n",
      "Epoch 2555, Train_Loss: 3926.677490234375, Val_Loss: 4044.494384765625\n",
      "Epoch 2556, Train_Loss: 3926.655029296875, Val_Loss: 4044.47998046875\n",
      "Epoch 2557, Train_Loss: 3926.606689453125, Val_Loss: 4044.4375\n",
      "Epoch 2558, Train_Loss: 3926.56787109375, Val_Loss: 4044.4248046875\n",
      "Epoch 2559, Train_Loss: 3926.575439453125, Val_Loss: 4044.416748046875\n",
      "Epoch 2560, Train_Loss: 3926.55908203125, Val_Loss: 4044.396728515625\n",
      "Epoch 2561, Train_Loss: 3926.541259765625, Val_Loss: 4044.3779296875\n",
      "Epoch 2562, Train_Loss: 3926.529296875, Val_Loss: 4044.3603515625\n",
      "Epoch 2563, Train_Loss: 3926.489501953125, Val_Loss: 4044.297607421875\n",
      "Epoch 2564, Train_Loss: 3926.475830078125, Val_Loss: 4044.284912109375\n",
      "Epoch 2565, Train_Loss: 3926.440185546875, Val_Loss: 4044.249267578125\n",
      "Epoch 2566, Train_Loss: 3926.420166015625, Val_Loss: 4044.23828125\n",
      "Epoch 2567, Train_Loss: 3926.406494140625, Val_Loss: 4044.224365234375\n",
      "Epoch 2568, Train_Loss: 3926.37451171875, Val_Loss: 4044.14208984375\n",
      "Epoch 2569, Train_Loss: 3926.354736328125, Val_Loss: 4044.12841796875\n",
      "Epoch 2570, Train_Loss: 3926.332275390625, Val_Loss: 4044.107177734375\n",
      "Epoch 2571, Train_Loss: 3926.317626953125, Val_Loss: 4044.08642578125\n",
      "Epoch 2572, Train_Loss: 3926.3095703125, Val_Loss: 4044.067626953125\n",
      "Epoch 2573, Train_Loss: 3926.289794921875, Val_Loss: 4044.04931640625\n",
      "Epoch 2574, Train_Loss: 3926.019775390625, Val_Loss: 4044.0263671875\n",
      "Epoch 2575, Train_Loss: 3926.0068359375, Val_Loss: 4044.0068359375\n",
      "Epoch 2576, Train_Loss: 3925.98388671875, Val_Loss: 4043.994873046875\n",
      "Epoch 2577, Train_Loss: 3925.97314453125, Val_Loss: 4043.97314453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2578, Train_Loss: 3925.945068359375, Val_Loss: 4043.921630859375\n",
      "Epoch 2579, Train_Loss: 3925.830322265625, Val_Loss: 4043.8798828125\n",
      "Epoch 2580, Train_Loss: 3925.818115234375, Val_Loss: 4043.866455078125\n",
      "Epoch 2581, Train_Loss: 3925.80029296875, Val_Loss: 4043.854736328125\n",
      "Epoch 2582, Train_Loss: 3925.78515625, Val_Loss: 4043.841552734375\n",
      "Epoch 2583, Train_Loss: 3925.76220703125, Val_Loss: 4043.82275390625\n",
      "Epoch 2584, Train_Loss: 3925.748046875, Val_Loss: 4043.8056640625\n",
      "Epoch 2585, Train_Loss: 3925.672607421875, Val_Loss: 4043.757568359375\n",
      "Epoch 2586, Train_Loss: 3925.658447265625, Val_Loss: 4043.739501953125\n",
      "Epoch 2587, Train_Loss: 3925.638916015625, Val_Loss: 4043.718505859375\n",
      "Epoch 2588, Train_Loss: 3925.626953125, Val_Loss: 4043.7060546875\n",
      "Epoch 2589, Train_Loss: 3925.607421875, Val_Loss: 4043.691650390625\n",
      "Epoch 2590, Train_Loss: 3925.55419921875, Val_Loss: 4043.5927734375\n",
      "Epoch 2591, Train_Loss: 3925.485595703125, Val_Loss: 4043.565185546875\n",
      "Epoch 2592, Train_Loss: 3925.46875, Val_Loss: 4043.56396484375\n",
      "Epoch 2593, Train_Loss: 3925.45556640625, Val_Loss: 4043.5419921875\n",
      "Epoch 2594, Train_Loss: 3925.48974609375, Val_Loss: 4043.51123046875\n",
      "Epoch 2595, Train_Loss: 3925.482421875, Val_Loss: 4043.49560546875\n",
      "Epoch 2596, Train_Loss: 3925.375, Val_Loss: 4043.490478515625\n",
      "Epoch 2597, Train_Loss: 3925.421142578125, Val_Loss: 4043.4736328125\n",
      "Epoch 2598, Train_Loss: 3925.356689453125, Val_Loss: 4043.435302734375\n",
      "Epoch 2599, Train_Loss: 3925.385986328125, Val_Loss: 4043.437255859375\n",
      "Epoch 2600, Train_Loss: 3925.3623046875, Val_Loss: 4043.423095703125\n",
      "Epoch 2601, Train_Loss: 3925.278564453125, Val_Loss: 4043.31884765625\n",
      "Epoch 2602, Train_Loss: 3925.316650390625, Val_Loss: 4043.357177734375\n",
      "Epoch 2603, Train_Loss: 3925.2900390625, Val_Loss: 4043.28955078125\n",
      "Epoch 2604, Train_Loss: 3925.2685546875, Val_Loss: 4043.271484375\n",
      "Epoch 2605, Train_Loss: 3925.1982421875, Val_Loss: 4043.248291015625\n",
      "Epoch 2606, Train_Loss: 3925.1884765625, Val_Loss: 4043.24609375\n",
      "Epoch 2607, Train_Loss: 3925.156494140625, Val_Loss: 4043.187255859375\n",
      "Epoch 2608, Train_Loss: 3925.1845703125, Val_Loss: 4043.17041015625\n",
      "Epoch 2609, Train_Loss: 3925.18212890625, Val_Loss: 4043.14208984375\n",
      "Epoch 2610, Train_Loss: 3925.10205078125, Val_Loss: 4043.1044921875\n",
      "Epoch 2611, Train_Loss: 3925.079345703125, Val_Loss: 4043.103515625\n",
      "Epoch 2612, Train_Loss: 3925.1181640625, Val_Loss: 4042.9404296875\n",
      "Epoch 2613, Train_Loss: 3925.02490234375, Val_Loss: 4042.9951171875\n",
      "Epoch 2614, Train_Loss: 3925.116455078125, Val_Loss: 4043.049560546875\n",
      "Epoch 2615, Train_Loss: 3925.077392578125, Val_Loss: 4042.8896484375\n",
      "Epoch 2616, Train_Loss: 3924.98974609375, Val_Loss: 4042.8603515625\n",
      "Epoch 2617, Train_Loss: 3925.03955078125, Val_Loss: 4042.85888671875\n",
      "Epoch 2618, Train_Loss: 3925.007568359375, Val_Loss: 4042.9912109375\n",
      "Epoch 2619, Train_Loss: 3924.957763671875, Val_Loss: 4042.818359375\n",
      "Epoch 2620, Train_Loss: 3924.872314453125, Val_Loss: 4042.773193359375\n",
      "Epoch 2621, Train_Loss: 3924.8828125, Val_Loss: 4042.87109375\n",
      "Epoch 2622, Train_Loss: 3924.850830078125, Val_Loss: 4042.80810546875\n",
      "Epoch 2623, Train_Loss: 3924.80908203125, Val_Loss: 4042.73046875\n",
      "Epoch 2624, Train_Loss: 3924.59619140625, Val_Loss: 4042.765625\n",
      "Epoch 2625, Train_Loss: 3924.475830078125, Val_Loss: 4042.682861328125\n",
      "Epoch 2626, Train_Loss: 3924.539306640625, Val_Loss: 4042.741943359375\n",
      "Epoch 2627, Train_Loss: 3924.50732421875, Val_Loss: 4042.83203125\n",
      "Epoch 2628, Train_Loss: 3924.35009765625, Val_Loss: 4042.669921875\n",
      "Epoch 2629, Train_Loss: 3924.373046875, Val_Loss: 4042.5205078125\n",
      "Epoch 2630, Train_Loss: 3924.39599609375, Val_Loss: 4042.62890625\n",
      "Epoch 2631, Train_Loss: 3924.383544921875, Val_Loss: 4042.607177734375\n",
      "Epoch 2632, Train_Loss: 3924.3505859375, Val_Loss: 4042.58447265625\n",
      "Epoch 2633, Train_Loss: 3924.247314453125, Val_Loss: 4042.533935546875\n",
      "Epoch 2634, Train_Loss: 3924.176513671875, Val_Loss: 4042.5419921875\n",
      "Epoch 2635, Train_Loss: 3924.2578125, Val_Loss: 4042.453125\n",
      "Epoch 2636, Train_Loss: 3924.281494140625, Val_Loss: 4042.47607421875\n",
      "Epoch 2637, Train_Loss: 3924.09228515625, Val_Loss: 4042.4296875\n",
      "Epoch 2638, Train_Loss: 3924.14990234375, Val_Loss: 4042.37158203125\n",
      "Epoch 2639, Train_Loss: 3924.132568359375, Val_Loss: 4042.34765625\n",
      "Epoch 2640, Train_Loss: 3924.01513671875, Val_Loss: 4042.353515625\n",
      "Epoch 2641, Train_Loss: 3924.133544921875, Val_Loss: 4042.401123046875\n",
      "Epoch 2642, Train_Loss: 3923.97705078125, Val_Loss: 4042.31201171875\n",
      "Epoch 2643, Train_Loss: 3923.97998046875, Val_Loss: 4042.2939453125\n",
      "Epoch 2644, Train_Loss: 3923.9423828125, Val_Loss: 4042.312744140625\n",
      "Epoch 2645, Train_Loss: 3923.92138671875, Val_Loss: 4042.28759765625\n",
      "Epoch 2646, Train_Loss: 3923.8583984375, Val_Loss: 4042.251220703125\n",
      "Epoch 2647, Train_Loss: 3923.9072265625, Val_Loss: 4042.135498046875\n",
      "Epoch 2648, Train_Loss: 3923.82763671875, Val_Loss: 4042.13525390625\n",
      "Epoch 2649, Train_Loss: 3923.823974609375, Val_Loss: 4042.226318359375\n",
      "Epoch 2650, Train_Loss: 3923.816162109375, Val_Loss: 4042.159912109375\n",
      "Epoch 2651, Train_Loss: 3923.7626953125, Val_Loss: 4042.052490234375\n",
      "Epoch 2652, Train_Loss: 3923.7509765625, Val_Loss: 4042.045166015625\n",
      "Epoch 2653, Train_Loss: 3923.753662109375, Val_Loss: 4042.120849609375\n",
      "Epoch 2654, Train_Loss: 3923.753173828125, Val_Loss: 4042.085205078125\n",
      "Epoch 2655, Train_Loss: 3923.695068359375, Val_Loss: 4042.0048828125\n",
      "Epoch 2656, Train_Loss: 3923.69775390625, Val_Loss: 4041.96875\n",
      "Epoch 2657, Train_Loss: 3923.645751953125, Val_Loss: 4041.87109375\n",
      "Epoch 2658, Train_Loss: 3923.63623046875, Val_Loss: 4041.89111328125\n",
      "Epoch 2659, Train_Loss: 3923.637939453125, Val_Loss: 4041.872314453125\n",
      "Epoch 2660, Train_Loss: 3923.6142578125, Val_Loss: 4041.85888671875\n",
      "Epoch 2661, Train_Loss: 3923.5771484375, Val_Loss: 4041.843994140625\n",
      "Epoch 2662, Train_Loss: 3923.578857421875, Val_Loss: 4041.828857421875\n",
      "Epoch 2663, Train_Loss: 3923.546875, Val_Loss: 4041.8271484375\n",
      "Epoch 2664, Train_Loss: 3923.509033203125, Val_Loss: 4041.786376953125\n",
      "Epoch 2665, Train_Loss: 3923.516845703125, Val_Loss: 4041.735595703125\n",
      "Epoch 2666, Train_Loss: 3923.51123046875, Val_Loss: 4041.709716796875\n",
      "Epoch 2667, Train_Loss: 3923.416748046875, Val_Loss: 4041.73681640625\n",
      "Epoch 2668, Train_Loss: 3923.307373046875, Val_Loss: 4041.72119140625\n",
      "Epoch 2669, Train_Loss: 3923.285888671875, Val_Loss: 4041.7119140625\n",
      "Epoch 2670, Train_Loss: 3923.2412109375, Val_Loss: 4041.653564453125\n",
      "Epoch 2671, Train_Loss: 3923.244384765625, Val_Loss: 4041.622802734375\n",
      "Epoch 2672, Train_Loss: 3923.228271484375, Val_Loss: 4041.604736328125\n",
      "Epoch 2673, Train_Loss: 3923.149169921875, Val_Loss: 4041.56689453125\n",
      "Epoch 2674, Train_Loss: 3923.192138671875, Val_Loss: 4041.56201171875\n",
      "Epoch 2675, Train_Loss: 3923.169921875, Val_Loss: 4041.541259765625\n",
      "Epoch 2676, Train_Loss: 3923.14599609375, Val_Loss: 4041.514404296875\n",
      "Epoch 2677, Train_Loss: 3923.0966796875, Val_Loss: 4041.489501953125\n",
      "Epoch 2678, Train_Loss: 3923.0771484375, Val_Loss: 4041.406005859375\n",
      "Epoch 2679, Train_Loss: 3923.071533203125, Val_Loss: 4041.397705078125\n",
      "Epoch 2680, Train_Loss: 3923.051513671875, Val_Loss: 4041.387939453125\n",
      "Epoch 2681, Train_Loss: 3923.0185546875, Val_Loss: 4041.3125\n",
      "Epoch 2682, Train_Loss: 3923.00390625, Val_Loss: 4041.296875\n",
      "Epoch 2683, Train_Loss: 3922.999267578125, Val_Loss: 4041.301513671875\n",
      "Epoch 2684, Train_Loss: 3922.9599609375, Val_Loss: 4041.277099609375\n",
      "Epoch 2685, Train_Loss: 3922.95166015625, Val_Loss: 4041.243896484375\n",
      "Epoch 2686, Train_Loss: 3922.91064453125, Val_Loss: 4041.214111328125\n",
      "Epoch 2687, Train_Loss: 3922.914794921875, Val_Loss: 4041.205078125\n",
      "Epoch 2688, Train_Loss: 3922.8955078125, Val_Loss: 4041.18798828125\n",
      "Epoch 2689, Train_Loss: 3922.84521484375, Val_Loss: 4041.16162109375\n",
      "Epoch 2690, Train_Loss: 3922.826416015625, Val_Loss: 4041.142333984375\n",
      "Epoch 2691, Train_Loss: 3922.821044921875, Val_Loss: 4041.131103515625\n",
      "Epoch 2692, Train_Loss: 3922.80322265625, Val_Loss: 4041.111083984375\n",
      "Epoch 2693, Train_Loss: 3922.783447265625, Val_Loss: 4041.095703125\n",
      "Epoch 2694, Train_Loss: 3922.761962890625, Val_Loss: 4041.035888671875\n",
      "Epoch 2695, Train_Loss: 3922.66796875, Val_Loss: 4041.001708984375\n",
      "Epoch 2696, Train_Loss: 3922.6376953125, Val_Loss: 4040.985107421875\n",
      "Epoch 2697, Train_Loss: 3922.6220703125, Val_Loss: 4040.97119140625\n",
      "Epoch 2698, Train_Loss: 3922.57275390625, Val_Loss: 4040.96044921875\n",
      "Epoch 2699, Train_Loss: 3922.55810546875, Val_Loss: 4040.9296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2700, Train_Loss: 3922.52490234375, Val_Loss: 4040.859130859375\n",
      "Epoch 2701, Train_Loss: 3922.511474609375, Val_Loss: 4040.83349609375\n",
      "Epoch 2702, Train_Loss: 3922.44482421875, Val_Loss: 4040.814697265625\n",
      "Epoch 2703, Train_Loss: 3922.427490234375, Val_Loss: 4040.798095703125\n",
      "Epoch 2704, Train_Loss: 3922.45068359375, Val_Loss: 4040.777099609375\n",
      "Epoch 2705, Train_Loss: 3922.333251953125, Val_Loss: 4040.751708984375\n",
      "Epoch 2706, Train_Loss: 3922.3017578125, Val_Loss: 4040.706787109375\n",
      "Epoch 2707, Train_Loss: 3922.324951171875, Val_Loss: 4040.685546875\n",
      "Epoch 2708, Train_Loss: 3922.28076171875, Val_Loss: 4040.65478515625\n",
      "Epoch 2709, Train_Loss: 3922.260009765625, Val_Loss: 4040.633544921875\n",
      "Epoch 2710, Train_Loss: 3922.173828125, Val_Loss: 4040.605712890625\n",
      "Epoch 2711, Train_Loss: 3922.12255859375, Val_Loss: 4040.59130859375\n",
      "Epoch 2712, Train_Loss: 3922.098388671875, Val_Loss: 4040.56591796875\n",
      "Epoch 2713, Train_Loss: 3922.094970703125, Val_Loss: 4040.56396484375\n",
      "Epoch 2714, Train_Loss: 3922.104736328125, Val_Loss: 4040.547119140625\n",
      "Epoch 2715, Train_Loss: 3922.046630859375, Val_Loss: 4040.469970703125\n",
      "Epoch 2716, Train_Loss: 3922.031494140625, Val_Loss: 4040.4541015625\n",
      "Epoch 2717, Train_Loss: 3921.988525390625, Val_Loss: 4040.4580078125\n",
      "Epoch 2718, Train_Loss: 3921.967041015625, Val_Loss: 4040.412109375\n",
      "Epoch 2719, Train_Loss: 3921.946533203125, Val_Loss: 4040.399658203125\n",
      "Epoch 2720, Train_Loss: 3921.9140625, Val_Loss: 4040.404296875\n",
      "Epoch 2721, Train_Loss: 3921.811767578125, Val_Loss: 4040.2939453125\n",
      "Epoch 2722, Train_Loss: 3921.7939453125, Val_Loss: 4040.273681640625\n",
      "Epoch 2723, Train_Loss: 3921.77587890625, Val_Loss: 4040.27197265625\n",
      "Epoch 2724, Train_Loss: 3921.77001953125, Val_Loss: 4040.224853515625\n",
      "Epoch 2725, Train_Loss: 3921.74951171875, Val_Loss: 4040.171630859375\n",
      "Epoch 2726, Train_Loss: 3921.7060546875, Val_Loss: 4040.189208984375\n",
      "Epoch 2727, Train_Loss: 3921.687255859375, Val_Loss: 4040.17431640625\n",
      "Epoch 2728, Train_Loss: 3921.66259765625, Val_Loss: 4040.1171875\n",
      "Epoch 2729, Train_Loss: 3921.650390625, Val_Loss: 4040.093505859375\n",
      "Epoch 2730, Train_Loss: 3921.619140625, Val_Loss: 4040.084716796875\n",
      "Epoch 2731, Train_Loss: 3921.579345703125, Val_Loss: 4040.050048828125\n",
      "Epoch 2732, Train_Loss: 3921.53466796875, Val_Loss: 4040.001220703125\n",
      "Epoch 2733, Train_Loss: 3921.517822265625, Val_Loss: 4039.9873046875\n",
      "Epoch 2734, Train_Loss: 3921.53369140625, Val_Loss: 4039.98681640625\n",
      "Epoch 2735, Train_Loss: 3921.507568359375, Val_Loss: 4039.986083984375\n",
      "Epoch 2736, Train_Loss: 3921.4619140625, Val_Loss: 4039.94873046875\n",
      "Epoch 2737, Train_Loss: 3921.430908203125, Val_Loss: 4039.87646484375\n",
      "Epoch 2738, Train_Loss: 3921.427734375, Val_Loss: 4039.8515625\n",
      "Epoch 2739, Train_Loss: 3921.407470703125, Val_Loss: 4039.83837890625\n",
      "Epoch 2740, Train_Loss: 3921.36865234375, Val_Loss: 4039.75439453125\n",
      "Epoch 2741, Train_Loss: 3921.371337890625, Val_Loss: 4039.806396484375\n",
      "Epoch 2742, Train_Loss: 3921.33154296875, Val_Loss: 4039.722412109375\n",
      "Epoch 2743, Train_Loss: 3921.2900390625, Val_Loss: 4039.67041015625\n",
      "Epoch 2744, Train_Loss: 3921.29833984375, Val_Loss: 4039.626708984375\n",
      "Epoch 2745, Train_Loss: 3921.24755859375, Val_Loss: 4039.6240234375\n",
      "Epoch 2746, Train_Loss: 3921.222412109375, Val_Loss: 4039.610107421875\n",
      "Epoch 2747, Train_Loss: 3920.95263671875, Val_Loss: 4039.590087890625\n",
      "Epoch 2748, Train_Loss: 3920.93994140625, Val_Loss: 4039.62646484375\n",
      "Epoch 2749, Train_Loss: 3920.901123046875, Val_Loss: 4039.580322265625\n",
      "Epoch 2750, Train_Loss: 3920.942138671875, Val_Loss: 4039.494384765625\n",
      "Epoch 2751, Train_Loss: 3920.878662109375, Val_Loss: 4039.510009765625\n",
      "Epoch 2752, Train_Loss: 3920.890625, Val_Loss: 4039.51123046875\n",
      "Epoch 2753, Train_Loss: 3920.784912109375, Val_Loss: 4039.436279296875\n",
      "Epoch 2754, Train_Loss: 3920.761962890625, Val_Loss: 4039.4111328125\n",
      "Epoch 2755, Train_Loss: 3920.71533203125, Val_Loss: 4039.44677734375\n",
      "Epoch 2756, Train_Loss: 3920.66796875, Val_Loss: 4039.444091796875\n",
      "Epoch 2757, Train_Loss: 3920.6669921875, Val_Loss: 4039.4072265625\n",
      "Epoch 2758, Train_Loss: 3920.612060546875, Val_Loss: 4039.35009765625\n",
      "Epoch 2759, Train_Loss: 3920.63623046875, Val_Loss: 4039.244384765625\n",
      "Epoch 2760, Train_Loss: 3920.656982421875, Val_Loss: 4039.2763671875\n",
      "Epoch 2761, Train_Loss: 3920.56787109375, Val_Loss: 4039.312744140625\n",
      "Epoch 2762, Train_Loss: 3920.6015625, Val_Loss: 4039.18994140625\n",
      "Epoch 2763, Train_Loss: 3920.491455078125, Val_Loss: 4039.115966796875\n",
      "Epoch 2764, Train_Loss: 3920.461181640625, Val_Loss: 4039.230712890625\n",
      "Epoch 2765, Train_Loss: 3920.44287109375, Val_Loss: 4039.18310546875\n",
      "Epoch 2766, Train_Loss: 3920.46435546875, Val_Loss: 4039.033203125\n",
      "Epoch 2767, Train_Loss: 3920.425537109375, Val_Loss: 4039.04248046875\n",
      "Epoch 2768, Train_Loss: 3920.404052734375, Val_Loss: 4039.10400390625\n",
      "Epoch 2769, Train_Loss: 3920.361083984375, Val_Loss: 4039.13232421875\n",
      "Epoch 2770, Train_Loss: 3920.348876953125, Val_Loss: 4038.963134765625\n",
      "Epoch 2771, Train_Loss: 3920.333251953125, Val_Loss: 4038.943115234375\n",
      "Epoch 2772, Train_Loss: 3920.29345703125, Val_Loss: 4039.040283203125\n",
      "Epoch 2773, Train_Loss: 3920.260009765625, Val_Loss: 4039.036376953125\n",
      "Epoch 2774, Train_Loss: 3920.247802734375, Val_Loss: 4038.918701171875\n",
      "Epoch 2775, Train_Loss: 3920.234130859375, Val_Loss: 4038.897705078125\n",
      "Epoch 2776, Train_Loss: 3920.219970703125, Val_Loss: 4038.8896484375\n",
      "Epoch 2777, Train_Loss: 3920.1708984375, Val_Loss: 4038.899169921875\n",
      "Epoch 2778, Train_Loss: 3920.191650390625, Val_Loss: 4038.8271484375\n",
      "Epoch 2779, Train_Loss: 3920.159912109375, Val_Loss: 4038.7705078125\n",
      "Epoch 2780, Train_Loss: 3920.095947265625, Val_Loss: 4038.768310546875\n",
      "Epoch 2781, Train_Loss: 3920.054443359375, Val_Loss: 4038.752685546875\n",
      "Epoch 2782, Train_Loss: 3920.073486328125, Val_Loss: 4038.788330078125\n",
      "Epoch 2783, Train_Loss: 3920.03759765625, Val_Loss: 4038.715087890625\n",
      "Epoch 2784, Train_Loss: 3920.1435546875, Val_Loss: 4038.679931640625\n",
      "Epoch 2785, Train_Loss: 3920.04443359375, Val_Loss: 4038.603271484375\n",
      "Epoch 2786, Train_Loss: 3919.98486328125, Val_Loss: 4038.651123046875\n",
      "Epoch 2787, Train_Loss: 3920.04345703125, Val_Loss: 4038.53125\n",
      "Epoch 2788, Train_Loss: 3920.03662109375, Val_Loss: 4038.503662109375\n",
      "Epoch 2789, Train_Loss: 3919.909423828125, Val_Loss: 4038.55712890625\n",
      "Epoch 2790, Train_Loss: 3919.79833984375, Val_Loss: 4038.552001953125\n",
      "Epoch 2791, Train_Loss: 3919.8583984375, Val_Loss: 4038.46875\n",
      "Epoch 2792, Train_Loss: 3919.865478515625, Val_Loss: 4038.446044921875\n",
      "Epoch 2793, Train_Loss: 3919.74267578125, Val_Loss: 4038.502685546875\n",
      "Epoch 2794, Train_Loss: 3919.808349609375, Val_Loss: 4038.419677734375\n",
      "Epoch 2795, Train_Loss: 3919.4970703125, Val_Loss: 4038.376708984375\n",
      "Epoch 2796, Train_Loss: 3919.4326171875, Val_Loss: 4038.4228515625\n",
      "Epoch 2797, Train_Loss: 3919.34765625, Val_Loss: 4038.42236328125\n",
      "Epoch 2798, Train_Loss: 3919.42578125, Val_Loss: 4038.3251953125\n",
      "Epoch 2799, Train_Loss: 3919.41796875, Val_Loss: 4038.310791015625\n",
      "Epoch 2800, Train_Loss: 3919.395263671875, Val_Loss: 4038.248779296875\n",
      "Epoch 2801, Train_Loss: 3919.35400390625, Val_Loss: 4038.242431640625\n",
      "Epoch 2802, Train_Loss: 3919.322021484375, Val_Loss: 4038.23046875\n",
      "Epoch 2803, Train_Loss: 3919.323486328125, Val_Loss: 4038.20556640625\n",
      "Epoch 2804, Train_Loss: 3919.309814453125, Val_Loss: 4038.197265625\n",
      "Epoch 2805, Train_Loss: 3919.271240234375, Val_Loss: 4038.1904296875\n",
      "Epoch 2806, Train_Loss: 3919.2529296875, Val_Loss: 4038.087158203125\n",
      "Epoch 2807, Train_Loss: 3919.24853515625, Val_Loss: 4038.077880859375\n",
      "Epoch 2808, Train_Loss: 3919.221435546875, Val_Loss: 4038.051513671875\n",
      "Epoch 2809, Train_Loss: 3919.185791015625, Val_Loss: 4038.033203125\n",
      "Epoch 2810, Train_Loss: 3919.173583984375, Val_Loss: 4038.008056640625\n",
      "Epoch 2811, Train_Loss: 3919.1328125, Val_Loss: 4037.986083984375\n",
      "Epoch 2812, Train_Loss: 3919.0986328125, Val_Loss: 4037.961181640625\n",
      "Epoch 2813, Train_Loss: 3919.077392578125, Val_Loss: 4037.94189453125\n",
      "Epoch 2814, Train_Loss: 3919.07373046875, Val_Loss: 4037.93994140625\n",
      "Epoch 2815, Train_Loss: 3919.0615234375, Val_Loss: 4037.908447265625\n",
      "Epoch 2816, Train_Loss: 3919.0185546875, Val_Loss: 4037.872802734375\n",
      "Epoch 2817, Train_Loss: 3918.98876953125, Val_Loss: 4037.8671875\n",
      "Epoch 2818, Train_Loss: 3918.9755859375, Val_Loss: 4037.83447265625\n",
      "Epoch 2819, Train_Loss: 3918.965087890625, Val_Loss: 4037.82080078125\n",
      "Epoch 2820, Train_Loss: 3918.941650390625, Val_Loss: 4037.766845703125\n",
      "Epoch 2821, Train_Loss: 3918.9091796875, Val_Loss: 4037.715087890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2822, Train_Loss: 3918.88720703125, Val_Loss: 4037.682373046875\n",
      "Epoch 2823, Train_Loss: 3918.869873046875, Val_Loss: 4037.68310546875\n",
      "Epoch 2824, Train_Loss: 3918.86767578125, Val_Loss: 4037.652099609375\n",
      "Epoch 2825, Train_Loss: 3918.835693359375, Val_Loss: 4037.64111328125\n",
      "Epoch 2826, Train_Loss: 3918.79443359375, Val_Loss: 4037.54833984375\n",
      "Epoch 2827, Train_Loss: 3918.779541015625, Val_Loss: 4037.54931640625\n",
      "Epoch 2828, Train_Loss: 3918.76611328125, Val_Loss: 4037.503662109375\n",
      "Epoch 2829, Train_Loss: 3918.743408203125, Val_Loss: 4037.5107421875\n",
      "Epoch 2830, Train_Loss: 3918.713134765625, Val_Loss: 4037.489990234375\n",
      "Epoch 2831, Train_Loss: 3918.685791015625, Val_Loss: 4037.466064453125\n",
      "Epoch 2832, Train_Loss: 3918.52490234375, Val_Loss: 4037.423095703125\n",
      "Epoch 2833, Train_Loss: 3918.492431640625, Val_Loss: 4037.42431640625\n",
      "Epoch 2834, Train_Loss: 3918.469482421875, Val_Loss: 4037.406494140625\n",
      "Epoch 2835, Train_Loss: 3918.453125, Val_Loss: 4037.385986328125\n",
      "Epoch 2836, Train_Loss: 3918.310791015625, Val_Loss: 4037.324462890625\n",
      "Epoch 2837, Train_Loss: 3918.276123046875, Val_Loss: 4037.317626953125\n",
      "Epoch 2838, Train_Loss: 3918.2568359375, Val_Loss: 4037.311279296875\n",
      "Epoch 2839, Train_Loss: 3918.245849609375, Val_Loss: 4037.2783203125\n",
      "Epoch 2840, Train_Loss: 3918.23779296875, Val_Loss: 4037.253662109375\n",
      "Epoch 2841, Train_Loss: 3918.20751953125, Val_Loss: 4037.246337890625\n",
      "Epoch 2842, Train_Loss: 3918.175537109375, Val_Loss: 4037.19189453125\n",
      "Epoch 2843, Train_Loss: 3918.1416015625, Val_Loss: 4037.129150390625\n",
      "Epoch 2844, Train_Loss: 3918.1376953125, Val_Loss: 4037.115966796875\n",
      "Epoch 2845, Train_Loss: 3918.103271484375, Val_Loss: 4037.1044921875\n",
      "Epoch 2846, Train_Loss: 3918.089599609375, Val_Loss: 4037.087158203125\n",
      "Epoch 2847, Train_Loss: 3918.031982421875, Val_Loss: 4036.998779296875\n",
      "Epoch 2848, Train_Loss: 3918.01611328125, Val_Loss: 4036.97998046875\n",
      "Epoch 2849, Train_Loss: 3918.0517578125, Val_Loss: 4036.9462890625\n",
      "Epoch 2850, Train_Loss: 3917.9853515625, Val_Loss: 4036.935546875\n",
      "Epoch 2851, Train_Loss: 3917.998291015625, Val_Loss: 4036.898681640625\n",
      "Epoch 2852, Train_Loss: 3917.96533203125, Val_Loss: 4036.886474609375\n",
      "Epoch 2853, Train_Loss: 3917.894287109375, Val_Loss: 4036.848876953125\n",
      "Epoch 2854, Train_Loss: 3917.923583984375, Val_Loss: 4036.845947265625\n",
      "Epoch 2855, Train_Loss: 3917.8486328125, Val_Loss: 4036.85791015625\n",
      "Epoch 2856, Train_Loss: 3917.82763671875, Val_Loss: 4036.7431640625\n",
      "Epoch 2857, Train_Loss: 3917.8291015625, Val_Loss: 4036.71240234375\n",
      "Epoch 2858, Train_Loss: 3917.8046875, Val_Loss: 4036.6953125\n",
      "Epoch 2859, Train_Loss: 3917.708251953125, Val_Loss: 4036.65283203125\n",
      "Epoch 2860, Train_Loss: 3917.789306640625, Val_Loss: 4036.680908203125\n",
      "Epoch 2861, Train_Loss: 3917.794189453125, Val_Loss: 4036.70751953125\n",
      "Epoch 2862, Train_Loss: 3917.753662109375, Val_Loss: 4036.5712890625\n",
      "Epoch 2863, Train_Loss: 3917.6181640625, Val_Loss: 4036.541259765625\n",
      "Epoch 2864, Train_Loss: 3917.59033203125, Val_Loss: 4036.52490234375\n",
      "Epoch 2865, Train_Loss: 3917.6484375, Val_Loss: 4036.52392578125\n",
      "Epoch 2866, Train_Loss: 3917.5927734375, Val_Loss: 4036.620361328125\n",
      "Epoch 2867, Train_Loss: 3917.490234375, Val_Loss: 4036.38525390625\n",
      "Epoch 2868, Train_Loss: 3917.49169921875, Val_Loss: 4036.388916015625\n",
      "Epoch 2869, Train_Loss: 3917.49853515625, Val_Loss: 4036.38330078125\n",
      "Epoch 2870, Train_Loss: 3917.437744140625, Val_Loss: 4036.36767578125\n",
      "Epoch 2871, Train_Loss: 3917.465087890625, Val_Loss: 4036.37109375\n",
      "Epoch 2872, Train_Loss: 3917.449951171875, Val_Loss: 4036.352294921875\n",
      "Epoch 2873, Train_Loss: 3917.35546875, Val_Loss: 4036.29443359375\n",
      "Epoch 2874, Train_Loss: 3917.301513671875, Val_Loss: 4036.30908203125\n",
      "Epoch 2875, Train_Loss: 3917.333984375, Val_Loss: 4036.287109375\n",
      "Epoch 2876, Train_Loss: 3917.310546875, Val_Loss: 4036.286865234375\n",
      "Epoch 2877, Train_Loss: 3917.31298828125, Val_Loss: 4036.26806640625\n",
      "Epoch 2878, Train_Loss: 3917.148681640625, Val_Loss: 4036.138427734375\n",
      "Epoch 2879, Train_Loss: 3917.1669921875, Val_Loss: 4036.138916015625\n",
      "Epoch 2880, Train_Loss: 3917.101806640625, Val_Loss: 4036.181640625\n",
      "Epoch 2881, Train_Loss: 3917.13916015625, Val_Loss: 4036.21484375\n",
      "Epoch 2882, Train_Loss: 3917.044677734375, Val_Loss: 4036.182373046875\n",
      "Epoch 2883, Train_Loss: 3917.044677734375, Val_Loss: 4036.0419921875\n",
      "Epoch 2884, Train_Loss: 3917.0263671875, Val_Loss: 4036.016845703125\n",
      "Epoch 2885, Train_Loss: 3917.041015625, Val_Loss: 4036.077880859375\n",
      "Epoch 2886, Train_Loss: 3916.955322265625, Val_Loss: 4036.001220703125\n",
      "Epoch 2887, Train_Loss: 3916.984619140625, Val_Loss: 4036.00048828125\n",
      "Epoch 2888, Train_Loss: 3916.86865234375, Val_Loss: 4035.897216796875\n",
      "Epoch 2889, Train_Loss: 3916.870361328125, Val_Loss: 4035.876708984375\n",
      "Epoch 2890, Train_Loss: 3916.784423828125, Val_Loss: 4035.865966796875\n",
      "Epoch 2891, Train_Loss: 3916.813232421875, Val_Loss: 4035.856689453125\n",
      "Epoch 2892, Train_Loss: 3916.82421875, Val_Loss: 4035.826416015625\n",
      "Epoch 2893, Train_Loss: 3916.7802734375, Val_Loss: 4035.8076171875\n",
      "Epoch 2894, Train_Loss: 3916.749267578125, Val_Loss: 4035.77685546875\n",
      "Epoch 2895, Train_Loss: 3916.67529296875, Val_Loss: 4035.791259765625\n",
      "Epoch 2896, Train_Loss: 3916.67236328125, Val_Loss: 4035.724365234375\n",
      "Epoch 2897, Train_Loss: 3916.65234375, Val_Loss: 4035.7080078125\n",
      "Epoch 2898, Train_Loss: 3916.643310546875, Val_Loss: 4035.70556640625\n",
      "Epoch 2899, Train_Loss: 3916.57470703125, Val_Loss: 4035.70361328125\n",
      "Epoch 2900, Train_Loss: 3916.578857421875, Val_Loss: 4035.622314453125\n",
      "Epoch 2901, Train_Loss: 3916.562255859375, Val_Loss: 4035.6083984375\n",
      "Epoch 2902, Train_Loss: 3916.58740234375, Val_Loss: 4035.611083984375\n",
      "Epoch 2903, Train_Loss: 3916.5439453125, Val_Loss: 4035.561279296875\n",
      "Epoch 2904, Train_Loss: 3916.506591796875, Val_Loss: 4035.538330078125\n",
      "Epoch 2905, Train_Loss: 3916.45703125, Val_Loss: 4035.465087890625\n",
      "Epoch 2906, Train_Loss: 3916.45068359375, Val_Loss: 4035.437255859375\n",
      "Epoch 2907, Train_Loss: 3916.4501953125, Val_Loss: 4035.489990234375\n",
      "Epoch 2908, Train_Loss: 3916.439208984375, Val_Loss: 4035.485107421875\n",
      "Epoch 2909, Train_Loss: 3916.306640625, Val_Loss: 4035.2939453125\n",
      "Epoch 2910, Train_Loss: 3916.288818359375, Val_Loss: 4035.28759765625\n",
      "Epoch 2911, Train_Loss: 3916.3095703125, Val_Loss: 4035.30322265625\n",
      "Epoch 2912, Train_Loss: 3916.29931640625, Val_Loss: 4035.3876953125\n",
      "Epoch 2913, Train_Loss: 3916.2080078125, Val_Loss: 4035.248779296875\n",
      "Epoch 2914, Train_Loss: 3915.961181640625, Val_Loss: 4035.18798828125\n",
      "Epoch 2915, Train_Loss: 3915.988525390625, Val_Loss: 4035.221923828125\n",
      "Epoch 2916, Train_Loss: 3915.961181640625, Val_Loss: 4035.280517578125\n",
      "Epoch 2917, Train_Loss: 3915.896728515625, Val_Loss: 4035.180908203125\n",
      "Epoch 2918, Train_Loss: 3915.883544921875, Val_Loss: 4035.102783203125\n",
      "Epoch 2919, Train_Loss: 3915.75048828125, Val_Loss: 4035.060791015625\n",
      "Epoch 2920, Train_Loss: 3915.844970703125, Val_Loss: 4035.13720703125\n",
      "Epoch 2921, Train_Loss: 3915.75, Val_Loss: 4035.069091796875\n",
      "Epoch 2922, Train_Loss: 3915.71435546875, Val_Loss: 4035.07763671875\n",
      "Epoch 2923, Train_Loss: 3915.72705078125, Val_Loss: 4035.1220703125\n",
      "Epoch 2924, Train_Loss: 3915.659912109375, Val_Loss: 4034.9873046875\n",
      "Epoch 2925, Train_Loss: 3915.596435546875, Val_Loss: 4034.919921875\n",
      "Epoch 2926, Train_Loss: 3915.617431640625, Val_Loss: 4034.97802734375\n",
      "Epoch 2927, Train_Loss: 3915.54150390625, Val_Loss: 4034.894775390625\n",
      "Epoch 2928, Train_Loss: 3915.531982421875, Val_Loss: 4034.8701171875\n",
      "Epoch 2929, Train_Loss: 3915.478759765625, Val_Loss: 4034.781494140625\n",
      "Epoch 2930, Train_Loss: 3915.493408203125, Val_Loss: 4034.85888671875\n",
      "Epoch 2931, Train_Loss: 3915.439453125, Val_Loss: 4034.7392578125\n",
      "Epoch 2932, Train_Loss: 3915.45556640625, Val_Loss: 4034.737548828125\n",
      "Epoch 2933, Train_Loss: 3915.487548828125, Val_Loss: 4034.810302734375\n",
      "Epoch 2934, Train_Loss: 3915.404296875, Val_Loss: 4034.716796875\n",
      "Epoch 2935, Train_Loss: 3915.3818359375, Val_Loss: 4034.678466796875\n",
      "Epoch 2936, Train_Loss: 3915.34814453125, Val_Loss: 4034.691162109375\n",
      "Epoch 2937, Train_Loss: 3915.293701171875, Val_Loss: 4034.6298828125\n",
      "Epoch 2938, Train_Loss: 3915.322998046875, Val_Loss: 4034.64599609375\n",
      "Epoch 2939, Train_Loss: 3915.25634765625, Val_Loss: 4034.564453125\n",
      "Epoch 2940, Train_Loss: 3915.223876953125, Val_Loss: 4034.558837890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2941, Train_Loss: 3915.216552734375, Val_Loss: 4034.556396484375\n",
      "Epoch 2942, Train_Loss: 3915.244384765625, Val_Loss: 4034.558837890625\n",
      "Epoch 2943, Train_Loss: 3915.16748046875, Val_Loss: 4034.567138671875\n",
      "Epoch 2944, Train_Loss: 3915.166259765625, Val_Loss: 4034.4580078125\n",
      "Epoch 2945, Train_Loss: 3915.120361328125, Val_Loss: 4034.417236328125\n",
      "Epoch 2946, Train_Loss: 3915.130859375, Val_Loss: 4034.421630859375\n",
      "Epoch 2947, Train_Loss: 3915.13330078125, Val_Loss: 4034.41552734375\n",
      "Epoch 2948, Train_Loss: 3915.072509765625, Val_Loss: 4034.40087890625\n",
      "Epoch 2949, Train_Loss: 3915.070556640625, Val_Loss: 4034.302490234375\n",
      "Epoch 2950, Train_Loss: 3915.033203125, Val_Loss: 4034.2900390625\n",
      "Epoch 2951, Train_Loss: 3915.027587890625, Val_Loss: 4034.253173828125\n",
      "Epoch 2952, Train_Loss: 3914.98828125, Val_Loss: 4034.25634765625\n",
      "Epoch 2953, Train_Loss: 3914.97314453125, Val_Loss: 4034.172119140625\n",
      "Epoch 2954, Train_Loss: 3914.919677734375, Val_Loss: 4034.159912109375\n",
      "Epoch 2955, Train_Loss: 3914.8974609375, Val_Loss: 4034.148681640625\n",
      "Epoch 2956, Train_Loss: 3914.848876953125, Val_Loss: 4034.142333984375\n",
      "Epoch 2957, Train_Loss: 3914.84033203125, Val_Loss: 4034.105224609375\n",
      "Epoch 2958, Train_Loss: 3914.826416015625, Val_Loss: 4034.0927734375\n",
      "Epoch 2959, Train_Loss: 3914.5537109375, Val_Loss: 4034.065185546875\n",
      "Epoch 2960, Train_Loss: 3914.46533203125, Val_Loss: 4034.04638671875\n",
      "Epoch 2961, Train_Loss: 3914.39501953125, Val_Loss: 4034.003662109375\n",
      "Epoch 2962, Train_Loss: 3914.3671875, Val_Loss: 4033.9951171875\n",
      "Epoch 2963, Train_Loss: 3914.453369140625, Val_Loss: 4034.01171875\n",
      "Epoch 2964, Train_Loss: 3914.41552734375, Val_Loss: 4033.9423828125\n",
      "Epoch 2965, Train_Loss: 3914.299072265625, Val_Loss: 4033.900390625\n",
      "Epoch 2966, Train_Loss: 3914.34375, Val_Loss: 4033.884765625\n",
      "Epoch 2967, Train_Loss: 3914.306640625, Val_Loss: 4033.907958984375\n",
      "Epoch 2968, Train_Loss: 3914.248291015625, Val_Loss: 4033.861572265625\n",
      "Epoch 2969, Train_Loss: 3914.261474609375, Val_Loss: 4033.71435546875\n",
      "Epoch 2970, Train_Loss: 3914.261474609375, Val_Loss: 4033.738037109375\n",
      "Epoch 2971, Train_Loss: 3914.247314453125, Val_Loss: 4033.77392578125\n",
      "Epoch 2972, Train_Loss: 3914.134033203125, Val_Loss: 4033.736328125\n",
      "Epoch 2973, Train_Loss: 3914.1845703125, Val_Loss: 4033.629638671875\n",
      "Epoch 2974, Train_Loss: 3914.153076171875, Val_Loss: 4033.62158203125\n",
      "Epoch 2975, Train_Loss: 3914.119873046875, Val_Loss: 4033.67041015625\n",
      "Epoch 2976, Train_Loss: 3914.112548828125, Val_Loss: 4033.593994140625\n",
      "Epoch 2977, Train_Loss: 3914.102294921875, Val_Loss: 4033.571533203125\n",
      "Epoch 2978, Train_Loss: 3914.069580078125, Val_Loss: 4033.5546875\n",
      "Epoch 2979, Train_Loss: 3914.045166015625, Val_Loss: 4033.515625\n",
      "Epoch 2980, Train_Loss: 3914.026611328125, Val_Loss: 4033.496826171875\n",
      "Epoch 2981, Train_Loss: 3913.982666015625, Val_Loss: 4033.49169921875\n",
      "Epoch 2982, Train_Loss: 3913.974853515625, Val_Loss: 4033.46923828125\n",
      "Epoch 2983, Train_Loss: 3913.94140625, Val_Loss: 4033.43310546875\n",
      "Epoch 2984, Train_Loss: 3913.924072265625, Val_Loss: 4033.38525390625\n",
      "Epoch 2985, Train_Loss: 3913.888427734375, Val_Loss: 4033.357666015625\n",
      "Epoch 2986, Train_Loss: 3913.882080078125, Val_Loss: 4033.33154296875\n",
      "Epoch 2987, Train_Loss: 3913.857421875, Val_Loss: 4033.343505859375\n",
      "Epoch 2988, Train_Loss: 3913.839599609375, Val_Loss: 4033.288330078125\n",
      "Epoch 2989, Train_Loss: 3913.781982421875, Val_Loss: 4033.22119140625\n",
      "Epoch 2990, Train_Loss: 3913.773193359375, Val_Loss: 4033.22802734375\n",
      "Epoch 2991, Train_Loss: 3913.779541015625, Val_Loss: 4033.166015625\n",
      "Epoch 2992, Train_Loss: 3913.76318359375, Val_Loss: 4033.1455078125\n",
      "Epoch 2993, Train_Loss: 3913.738037109375, Val_Loss: 4033.173095703125\n",
      "Epoch 2994, Train_Loss: 3913.745849609375, Val_Loss: 4033.056396484375\n",
      "Epoch 2995, Train_Loss: 3913.5830078125, Val_Loss: 4033.031494140625\n",
      "Epoch 2996, Train_Loss: 3913.67822265625, Val_Loss: 4033.040771484375\n",
      "Epoch 2997, Train_Loss: 3913.661376953125, Val_Loss: 4033.0615234375\n",
      "Epoch 2998, Train_Loss: 3913.64208984375, Val_Loss: 4032.983154296875\n",
      "Epoch 2999, Train_Loss: 3913.458251953125, Val_Loss: 4032.961669921875\n",
      "Epoch 3000, Train_Loss: 3913.421630859375, Val_Loss: 4032.94970703125\n",
      "Epoch 3001, Train_Loss: 3913.423095703125, Val_Loss: 4032.923583984375\n",
      "Epoch 3002, Train_Loss: 3913.39013671875, Val_Loss: 4032.935546875\n",
      "Epoch 3003, Train_Loss: 3913.23388671875, Val_Loss: 4032.882080078125\n",
      "Epoch 3004, Train_Loss: 3913.2041015625, Val_Loss: 4032.813720703125\n",
      "Epoch 3005, Train_Loss: 3913.31884765625, Val_Loss: 4032.837158203125\n",
      "Epoch 3006, Train_Loss: 3913.178466796875, Val_Loss: 4032.7939453125\n",
      "Epoch 3007, Train_Loss: 3913.31201171875, Val_Loss: 4032.801513671875\n",
      "Epoch 3008, Train_Loss: 3913.1171875, Val_Loss: 4032.76171875\n",
      "Epoch 3009, Train_Loss: 3913.0810546875, Val_Loss: 4032.674072265625\n",
      "Epoch 3010, Train_Loss: 3913.0888671875, Val_Loss: 4032.657958984375\n",
      "Epoch 3011, Train_Loss: 3913.065673828125, Val_Loss: 4032.63720703125\n",
      "Epoch 3012, Train_Loss: 3913.030517578125, Val_Loss: 4032.62841796875\n",
      "Epoch 3013, Train_Loss: 3912.9814453125, Val_Loss: 4032.627685546875\n",
      "Epoch 3014, Train_Loss: 3912.98388671875, Val_Loss: 4032.5791015625\n",
      "Epoch 3015, Train_Loss: 3912.937255859375, Val_Loss: 4032.547119140625\n",
      "Epoch 3016, Train_Loss: 3912.912841796875, Val_Loss: 4032.556396484375\n",
      "Epoch 3017, Train_Loss: 3912.8955078125, Val_Loss: 4032.478759765625\n",
      "Epoch 3018, Train_Loss: 3912.8876953125, Val_Loss: 4032.460693359375\n",
      "Epoch 3019, Train_Loss: 3912.829833984375, Val_Loss: 4032.472412109375\n",
      "Epoch 3020, Train_Loss: 3912.8232421875, Val_Loss: 4032.405517578125\n",
      "Epoch 3021, Train_Loss: 3912.77978515625, Val_Loss: 4032.3779296875\n",
      "Epoch 3022, Train_Loss: 3912.769775390625, Val_Loss: 4032.372802734375\n",
      "Epoch 3023, Train_Loss: 3912.778076171875, Val_Loss: 4032.343994140625\n",
      "Epoch 3024, Train_Loss: 3912.72216796875, Val_Loss: 4032.2880859375\n",
      "Epoch 3025, Train_Loss: 3912.71044921875, Val_Loss: 4032.248779296875\n",
      "Epoch 3026, Train_Loss: 3912.67431640625, Val_Loss: 4032.2626953125\n",
      "Epoch 3027, Train_Loss: 3912.632568359375, Val_Loss: 4032.247314453125\n",
      "Epoch 3028, Train_Loss: 3912.63134765625, Val_Loss: 4032.20166015625\n",
      "Epoch 3029, Train_Loss: 3912.58154296875, Val_Loss: 4032.153564453125\n",
      "Epoch 3030, Train_Loss: 3912.57958984375, Val_Loss: 4032.107177734375\n",
      "Epoch 3031, Train_Loss: 3912.52392578125, Val_Loss: 4032.107666015625\n",
      "Epoch 3032, Train_Loss: 3912.5537109375, Val_Loss: 4032.040771484375\n",
      "Epoch 3033, Train_Loss: 3912.5107421875, Val_Loss: 4032.05029296875\n",
      "Epoch 3034, Train_Loss: 3912.41650390625, Val_Loss: 4032.0458984375\n",
      "Epoch 3035, Train_Loss: 3912.42431640625, Val_Loss: 4031.9560546875\n",
      "Epoch 3036, Train_Loss: 3912.489990234375, Val_Loss: 4031.927978515625\n",
      "Epoch 3037, Train_Loss: 3912.38623046875, Val_Loss: 4031.918701171875\n",
      "Epoch 3038, Train_Loss: 3912.334716796875, Val_Loss: 4032.03564453125\n",
      "Epoch 3039, Train_Loss: 3912.227294921875, Val_Loss: 4031.7880859375\n",
      "Epoch 3040, Train_Loss: 3912.232177734375, Val_Loss: 4031.76513671875\n",
      "Epoch 3041, Train_Loss: 3912.25830078125, Val_Loss: 4031.850341796875\n",
      "Epoch 3042, Train_Loss: 3912.17822265625, Val_Loss: 4031.921142578125\n",
      "Epoch 3043, Train_Loss: 3912.153564453125, Val_Loss: 4031.665283203125\n",
      "Epoch 3044, Train_Loss: 3912.147216796875, Val_Loss: 4031.656005859375\n",
      "Epoch 3045, Train_Loss: 3912.06494140625, Val_Loss: 4031.707275390625\n",
      "Epoch 3046, Train_Loss: 3912.060791015625, Val_Loss: 4031.734375\n",
      "Epoch 3047, Train_Loss: 3912.067138671875, Val_Loss: 4031.724365234375\n",
      "Epoch 3048, Train_Loss: 3911.967041015625, Val_Loss: 4031.53515625\n",
      "Epoch 3049, Train_Loss: 3911.9072265625, Val_Loss: 4031.567626953125\n",
      "Epoch 3050, Train_Loss: 3911.865234375, Val_Loss: 4031.5546875\n",
      "Epoch 3051, Train_Loss: 3911.851806640625, Val_Loss: 4031.58203125\n",
      "Epoch 3052, Train_Loss: 3911.867919921875, Val_Loss: 4031.578857421875\n",
      "Epoch 3053, Train_Loss: 3911.780029296875, Val_Loss: 4031.445556640625\n",
      "Epoch 3054, Train_Loss: 3911.8203125, Val_Loss: 4031.412353515625\n",
      "Epoch 3055, Train_Loss: 3911.6904296875, Val_Loss: 4031.481689453125\n",
      "Epoch 3056, Train_Loss: 3911.755859375, Val_Loss: 4031.502685546875\n",
      "Epoch 3057, Train_Loss: 3911.716552734375, Val_Loss: 4031.385498046875\n",
      "Epoch 3058, Train_Loss: 3911.68310546875, Val_Loss: 4031.337158203125\n",
      "Epoch 3059, Train_Loss: 3911.61181640625, Val_Loss: 4031.386474609375\n",
      "Epoch 3060, Train_Loss: 3911.623291015625, Val_Loss: 4031.30322265625\n",
      "Epoch 3061, Train_Loss: 3911.5849609375, Val_Loss: 4031.308349609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3062, Train_Loss: 3911.550537109375, Val_Loss: 4031.328369140625\n",
      "Epoch 3063, Train_Loss: 3911.5185546875, Val_Loss: 4031.23681640625\n",
      "Epoch 3064, Train_Loss: 3911.558837890625, Val_Loss: 4031.20361328125\n",
      "Epoch 3065, Train_Loss: 3911.481689453125, Val_Loss: 4031.19482421875\n",
      "Epoch 3066, Train_Loss: 3911.485595703125, Val_Loss: 4031.174072265625\n",
      "Epoch 3067, Train_Loss: 3911.487060546875, Val_Loss: 4031.1279296875\n",
      "Epoch 3068, Train_Loss: 3911.45556640625, Val_Loss: 4031.065185546875\n",
      "Epoch 3069, Train_Loss: 3911.39111328125, Val_Loss: 4031.06884765625\n",
      "Epoch 3070, Train_Loss: 3911.36767578125, Val_Loss: 4031.0615234375\n",
      "Epoch 3071, Train_Loss: 3911.371826171875, Val_Loss: 4030.998291015625\n",
      "Epoch 3072, Train_Loss: 3911.372802734375, Val_Loss: 4030.963623046875\n",
      "Epoch 3073, Train_Loss: 3911.060546875, Val_Loss: 4030.9794921875\n",
      "Epoch 3074, Train_Loss: 3911.0234375, Val_Loss: 4030.95361328125\n",
      "Epoch 3075, Train_Loss: 3911.022705078125, Val_Loss: 4030.919921875\n",
      "Epoch 3076, Train_Loss: 3911.005615234375, Val_Loss: 4030.90283203125\n",
      "Epoch 3077, Train_Loss: 3910.96875, Val_Loss: 4030.890869140625\n",
      "Epoch 3078, Train_Loss: 3910.84619140625, Val_Loss: 4030.8427734375\n",
      "Epoch 3079, Train_Loss: 3910.856201171875, Val_Loss: 4030.824462890625\n",
      "Epoch 3080, Train_Loss: 3910.808349609375, Val_Loss: 4030.814453125\n",
      "Epoch 3081, Train_Loss: 3910.7822265625, Val_Loss: 4030.802490234375\n",
      "Epoch 3082, Train_Loss: 3910.826416015625, Val_Loss: 4030.73828125\n",
      "Epoch 3083, Train_Loss: 3910.73876953125, Val_Loss: 4030.660888671875\n",
      "Epoch 3084, Train_Loss: 3910.692138671875, Val_Loss: 4030.695556640625\n",
      "Epoch 3085, Train_Loss: 3910.670166015625, Val_Loss: 4030.64599609375\n",
      "Epoch 3086, Train_Loss: 3910.662353515625, Val_Loss: 4030.617919921875\n",
      "Epoch 3087, Train_Loss: 3910.670654296875, Val_Loss: 4030.61767578125\n",
      "Epoch 3088, Train_Loss: 3910.587890625, Val_Loss: 4030.53759765625\n",
      "Epoch 3089, Train_Loss: 3910.5517578125, Val_Loss: 4030.51708984375\n",
      "Epoch 3090, Train_Loss: 3910.56103515625, Val_Loss: 4030.486083984375\n",
      "Epoch 3091, Train_Loss: 3910.51025390625, Val_Loss: 4030.463134765625\n",
      "Epoch 3092, Train_Loss: 3910.5087890625, Val_Loss: 4030.44189453125\n",
      "Epoch 3093, Train_Loss: 3910.47265625, Val_Loss: 4030.43603515625\n",
      "Epoch 3094, Train_Loss: 3910.406005859375, Val_Loss: 4030.400390625\n",
      "Epoch 3095, Train_Loss: 3910.399169921875, Val_Loss: 4030.3876953125\n",
      "Epoch 3096, Train_Loss: 3910.377197265625, Val_Loss: 4030.354736328125\n",
      "Epoch 3097, Train_Loss: 3910.320556640625, Val_Loss: 4030.318115234375\n",
      "Epoch 3098, Train_Loss: 3910.30712890625, Val_Loss: 4030.2939453125\n",
      "Epoch 3099, Train_Loss: 3910.313720703125, Val_Loss: 4030.275634765625\n",
      "Epoch 3100, Train_Loss: 3910.297119140625, Val_Loss: 4030.26611328125\n",
      "Epoch 3101, Train_Loss: 3910.270263671875, Val_Loss: 4030.235595703125\n",
      "Epoch 3102, Train_Loss: 3910.247802734375, Val_Loss: 4030.173095703125\n",
      "Epoch 3103, Train_Loss: 3910.2080078125, Val_Loss: 4030.12646484375\n",
      "Epoch 3104, Train_Loss: 3910.19189453125, Val_Loss: 4030.109619140625\n",
      "Epoch 3105, Train_Loss: 3910.176025390625, Val_Loss: 4030.080322265625\n",
      "Epoch 3106, Train_Loss: 3910.1494140625, Val_Loss: 4030.07080078125\n",
      "Epoch 3107, Train_Loss: 3910.124755859375, Val_Loss: 4029.989501953125\n",
      "Epoch 3108, Train_Loss: 3910.105712890625, Val_Loss: 4029.98486328125\n",
      "Epoch 3109, Train_Loss: 3910.07958984375, Val_Loss: 4029.961181640625\n",
      "Epoch 3110, Train_Loss: 3910.07177734375, Val_Loss: 4029.943115234375\n",
      "Epoch 3111, Train_Loss: 3910.064453125, Val_Loss: 4029.9267578125\n",
      "Epoch 3112, Train_Loss: 3909.97802734375, Val_Loss: 4029.888427734375\n",
      "Epoch 3113, Train_Loss: 3909.960693359375, Val_Loss: 4029.877197265625\n",
      "Epoch 3114, Train_Loss: 3909.925048828125, Val_Loss: 4029.856689453125\n",
      "Epoch 3115, Train_Loss: 3909.908935546875, Val_Loss: 4029.839111328125\n",
      "Epoch 3116, Train_Loss: 3909.587158203125, Val_Loss: 4029.794921875\n",
      "Epoch 3117, Train_Loss: 3909.568115234375, Val_Loss: 4029.773681640625\n",
      "Epoch 3118, Train_Loss: 3909.539306640625, Val_Loss: 4029.76318359375\n",
      "Epoch 3119, Train_Loss: 3909.521484375, Val_Loss: 4029.74169921875\n",
      "Epoch 3120, Train_Loss: 3909.514404296875, Val_Loss: 4029.70361328125\n",
      "Epoch 3121, Train_Loss: 3909.47021484375, Val_Loss: 4029.646484375\n",
      "Epoch 3122, Train_Loss: 3909.44970703125, Val_Loss: 4029.6376953125\n",
      "Epoch 3123, Train_Loss: 3909.430419921875, Val_Loss: 4029.605224609375\n",
      "Epoch 3124, Train_Loss: 3909.41259765625, Val_Loss: 4029.5537109375\n",
      "Epoch 3125, Train_Loss: 3909.400146484375, Val_Loss: 4029.5400390625\n",
      "Epoch 3126, Train_Loss: 3909.35400390625, Val_Loss: 4029.4462890625\n",
      "Epoch 3127, Train_Loss: 3909.321044921875, Val_Loss: 4029.441162109375\n",
      "Epoch 3128, Train_Loss: 3909.292724609375, Val_Loss: 4029.4296875\n",
      "Epoch 3129, Train_Loss: 3909.280517578125, Val_Loss: 4029.402099609375\n",
      "Epoch 3130, Train_Loss: 3909.321533203125, Val_Loss: 4029.3759765625\n",
      "Epoch 3131, Train_Loss: 3909.216064453125, Val_Loss: 4029.360107421875\n",
      "Epoch 3132, Train_Loss: 3909.228759765625, Val_Loss: 4029.319091796875\n",
      "Epoch 3133, Train_Loss: 3909.13330078125, Val_Loss: 4029.320068359375\n",
      "Epoch 3134, Train_Loss: 3909.26806640625, Val_Loss: 4029.275146484375\n",
      "Epoch 3135, Train_Loss: 3909.076171875, Val_Loss: 4029.258056640625\n",
      "Epoch 3136, Train_Loss: 3909.114013671875, Val_Loss: 4029.16357421875\n",
      "Epoch 3137, Train_Loss: 3909.044677734375, Val_Loss: 4029.2373046875\n",
      "Epoch 3138, Train_Loss: 3909.087890625, Val_Loss: 4029.13232421875\n",
      "Epoch 3139, Train_Loss: 3909.15234375, Val_Loss: 4029.18115234375\n",
      "Epoch 3140, Train_Loss: 3909.036865234375, Val_Loss: 4029.062744140625\n",
      "Epoch 3141, Train_Loss: 3909.029052734375, Val_Loss: 4029.02490234375\n",
      "Epoch 3142, Train_Loss: 3908.99609375, Val_Loss: 4029.02490234375\n",
      "Epoch 3143, Train_Loss: 3909.037353515625, Val_Loss: 4029.03271484375\n",
      "Epoch 3144, Train_Loss: 3908.964111328125, Val_Loss: 4028.961181640625\n",
      "Epoch 3145, Train_Loss: 3908.976806640625, Val_Loss: 4028.92236328125\n",
      "Epoch 3146, Train_Loss: 3908.91162109375, Val_Loss: 4028.97412109375\n",
      "Epoch 3147, Train_Loss: 3908.90966796875, Val_Loss: 4028.81884765625\n",
      "Epoch 3148, Train_Loss: 3908.857177734375, Val_Loss: 4028.841552734375\n",
      "Epoch 3149, Train_Loss: 3908.833740234375, Val_Loss: 4028.938720703125\n",
      "Epoch 3150, Train_Loss: 3908.8154296875, Val_Loss: 4028.787109375\n",
      "Epoch 3151, Train_Loss: 3908.78759765625, Val_Loss: 4028.777099609375\n",
      "Epoch 3152, Train_Loss: 3908.80322265625, Val_Loss: 4028.80029296875\n",
      "Epoch 3153, Train_Loss: 3908.811767578125, Val_Loss: 4028.761962890625\n",
      "Epoch 3154, Train_Loss: 3908.7158203125, Val_Loss: 4028.669921875\n",
      "Epoch 3155, Train_Loss: 3908.563232421875, Val_Loss: 4028.751708984375\n",
      "Epoch 3156, Train_Loss: 3908.525146484375, Val_Loss: 4028.669189453125\n",
      "Epoch 3157, Train_Loss: 3908.56103515625, Val_Loss: 4028.677490234375\n",
      "Epoch 3158, Train_Loss: 3908.496337890625, Val_Loss: 4028.633544921875\n",
      "Epoch 3159, Train_Loss: 3908.474853515625, Val_Loss: 4028.65283203125\n",
      "Epoch 3160, Train_Loss: 3908.453125, Val_Loss: 4028.518798828125\n",
      "Epoch 3161, Train_Loss: 3908.447021484375, Val_Loss: 4028.513916015625\n",
      "Epoch 3162, Train_Loss: 3908.3955078125, Val_Loss: 4028.564697265625\n",
      "Epoch 3163, Train_Loss: 3908.432861328125, Val_Loss: 4028.530517578125\n",
      "Epoch 3164, Train_Loss: 3908.345947265625, Val_Loss: 4028.37451171875\n",
      "Epoch 3165, Train_Loss: 3908.32275390625, Val_Loss: 4028.372802734375\n",
      "Epoch 3166, Train_Loss: 3908.292236328125, Val_Loss: 4028.4130859375\n",
      "Epoch 3167, Train_Loss: 3908.25146484375, Val_Loss: 4028.396728515625\n",
      "Epoch 3168, Train_Loss: 3908.252685546875, Val_Loss: 4028.319580078125\n",
      "Epoch 3169, Train_Loss: 3908.224853515625, Val_Loss: 4028.302734375\n",
      "Epoch 3170, Train_Loss: 3908.2353515625, Val_Loss: 4028.275146484375\n",
      "Epoch 3171, Train_Loss: 3908.161865234375, Val_Loss: 4028.27685546875\n",
      "Epoch 3172, Train_Loss: 3908.0078125, Val_Loss: 4028.21728515625\n",
      "Epoch 3173, Train_Loss: 3907.978515625, Val_Loss: 4028.21435546875\n",
      "Epoch 3174, Train_Loss: 3908.1201171875, Val_Loss: 4028.196044921875\n",
      "Epoch 3175, Train_Loss: 3907.94140625, Val_Loss: 4028.151123046875\n",
      "Epoch 3176, Train_Loss: 3907.92724609375, Val_Loss: 4028.120361328125\n",
      "Epoch 3177, Train_Loss: 3908.024169921875, Val_Loss: 4028.157958984375\n",
      "Epoch 3178, Train_Loss: 3908.00146484375, Val_Loss: 4028.136474609375\n",
      "Epoch 3179, Train_Loss: 3907.894287109375, Val_Loss: 4028.037109375\n",
      "Epoch 3180, Train_Loss: 3907.824951171875, Val_Loss: 4028.012451171875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3181, Train_Loss: 3907.831298828125, Val_Loss: 4027.98291015625\n",
      "Epoch 3182, Train_Loss: 3907.75, Val_Loss: 4027.9794921875\n",
      "Epoch 3183, Train_Loss: 3907.693359375, Val_Loss: 4027.923095703125\n",
      "Epoch 3184, Train_Loss: 3907.679931640625, Val_Loss: 4027.90478515625\n",
      "Epoch 3185, Train_Loss: 3907.70068359375, Val_Loss: 4027.8203125\n",
      "Epoch 3186, Train_Loss: 3907.61865234375, Val_Loss: 4027.8076171875\n",
      "Epoch 3187, Train_Loss: 3907.667236328125, Val_Loss: 4027.8369140625\n",
      "Epoch 3188, Train_Loss: 3907.59130859375, Val_Loss: 4027.79638671875\n",
      "Epoch 3189, Train_Loss: 3907.558349609375, Val_Loss: 4027.741943359375\n",
      "Epoch 3190, Train_Loss: 3907.4892578125, Val_Loss: 4027.7451171875\n",
      "Epoch 3191, Train_Loss: 3907.524169921875, Val_Loss: 4027.71630859375\n",
      "Epoch 3192, Train_Loss: 3907.500244140625, Val_Loss: 4027.70166015625\n",
      "Epoch 3193, Train_Loss: 3907.357177734375, Val_Loss: 4027.6591796875\n",
      "Epoch 3194, Train_Loss: 3907.3203125, Val_Loss: 4027.636474609375\n",
      "Epoch 3195, Train_Loss: 3907.379638671875, Val_Loss: 4027.6171875\n",
      "Epoch 3196, Train_Loss: 3907.336669921875, Val_Loss: 4027.61328125\n",
      "Epoch 3197, Train_Loss: 3907.33251953125, Val_Loss: 4027.57958984375\n",
      "Epoch 3198, Train_Loss: 3907.228271484375, Val_Loss: 4027.516357421875\n",
      "Epoch 3199, Train_Loss: 3907.18896484375, Val_Loss: 4027.50439453125\n",
      "Epoch 3200, Train_Loss: 3907.230712890625, Val_Loss: 4027.48046875\n",
      "Epoch 3201, Train_Loss: 3907.214111328125, Val_Loss: 4027.45849609375\n",
      "Epoch 3202, Train_Loss: 3907.052001953125, Val_Loss: 4027.359619140625\n",
      "Epoch 3203, Train_Loss: 3907.021728515625, Val_Loss: 4027.359619140625\n",
      "Epoch 3204, Train_Loss: 3907.055908203125, Val_Loss: 4027.337646484375\n",
      "Epoch 3205, Train_Loss: 3906.992919921875, Val_Loss: 4027.32080078125\n",
      "Epoch 3206, Train_Loss: 3906.97314453125, Val_Loss: 4027.244873046875\n",
      "Epoch 3207, Train_Loss: 3906.979248046875, Val_Loss: 4027.22509765625\n",
      "Epoch 3208, Train_Loss: 3906.947998046875, Val_Loss: 4027.21435546875\n",
      "Epoch 3209, Train_Loss: 3906.890625, Val_Loss: 4027.197998046875\n",
      "Epoch 3210, Train_Loss: 3906.8759765625, Val_Loss: 4027.15771484375\n",
      "Epoch 3211, Train_Loss: 3906.84716796875, Val_Loss: 4027.160400390625\n",
      "Epoch 3212, Train_Loss: 3906.8115234375, Val_Loss: 4027.131591796875\n",
      "Epoch 3213, Train_Loss: 3906.78955078125, Val_Loss: 4027.0869140625\n",
      "Epoch 3214, Train_Loss: 3906.76708984375, Val_Loss: 4027.0859375\n",
      "Epoch 3215, Train_Loss: 3906.734130859375, Val_Loss: 4027.073974609375\n",
      "Epoch 3216, Train_Loss: 3906.646484375, Val_Loss: 4026.98828125\n",
      "Epoch 3217, Train_Loss: 3906.63037109375, Val_Loss: 4026.967529296875\n",
      "Epoch 3218, Train_Loss: 3906.591064453125, Val_Loss: 4026.965087890625\n",
      "Epoch 3219, Train_Loss: 3906.570556640625, Val_Loss: 4026.948486328125\n",
      "Epoch 3220, Train_Loss: 3906.57177734375, Val_Loss: 4026.910400390625\n",
      "Epoch 3221, Train_Loss: 3906.5380859375, Val_Loss: 4026.822509765625\n",
      "Epoch 3222, Train_Loss: 3906.5107421875, Val_Loss: 4026.810302734375\n",
      "Epoch 3223, Train_Loss: 3906.47900390625, Val_Loss: 4026.806884765625\n",
      "Epoch 3224, Train_Loss: 3906.484619140625, Val_Loss: 4026.770751953125\n",
      "Epoch 3225, Train_Loss: 3906.4521484375, Val_Loss: 4026.720458984375\n",
      "Epoch 3226, Train_Loss: 3906.18017578125, Val_Loss: 4026.722900390625\n",
      "Epoch 3227, Train_Loss: 3906.162353515625, Val_Loss: 4026.712890625\n",
      "Epoch 3228, Train_Loss: 3906.137939453125, Val_Loss: 4026.656494140625\n",
      "Epoch 3229, Train_Loss: 3906.124267578125, Val_Loss: 4026.63525390625\n",
      "Epoch 3230, Train_Loss: 3905.9921875, Val_Loss: 4026.58642578125\n",
      "Epoch 3231, Train_Loss: 3905.986328125, Val_Loss: 4026.59228515625\n",
      "Epoch 3232, Train_Loss: 3905.9658203125, Val_Loss: 4026.573974609375\n",
      "Epoch 3233, Train_Loss: 3905.938232421875, Val_Loss: 4026.545654296875\n",
      "Epoch 3234, Train_Loss: 3905.927734375, Val_Loss: 4026.528076171875\n",
      "Epoch 3235, Train_Loss: 3905.868408203125, Val_Loss: 4026.46484375\n",
      "Epoch 3236, Train_Loss: 3905.837646484375, Val_Loss: 4026.43994140625\n",
      "Epoch 3237, Train_Loss: 3905.811767578125, Val_Loss: 4026.42236328125\n",
      "Epoch 3238, Train_Loss: 3905.8046875, Val_Loss: 4026.40087890625\n",
      "Epoch 3239, Train_Loss: 3905.774169921875, Val_Loss: 4026.393310546875\n",
      "Epoch 3240, Train_Loss: 3905.724365234375, Val_Loss: 4026.28955078125\n",
      "Epoch 3241, Train_Loss: 3905.7060546875, Val_Loss: 4026.26953125\n",
      "Epoch 3242, Train_Loss: 3905.686767578125, Val_Loss: 4026.26708984375\n",
      "Epoch 3243, Train_Loss: 3905.665283203125, Val_Loss: 4026.176513671875\n",
      "Epoch 3244, Train_Loss: 3905.638916015625, Val_Loss: 4026.2294921875\n",
      "Epoch 3245, Train_Loss: 3905.610595703125, Val_Loss: 4026.140869140625\n",
      "Epoch 3246, Train_Loss: 3905.603271484375, Val_Loss: 4026.197509765625\n",
      "Epoch 3247, Train_Loss: 3905.622314453125, Val_Loss: 4026.06689453125\n",
      "Epoch 3248, Train_Loss: 3905.579833984375, Val_Loss: 4026.107177734375\n",
      "Epoch 3249, Train_Loss: 3905.458251953125, Val_Loss: 4026.031982421875\n",
      "Epoch 3250, Train_Loss: 3905.51318359375, Val_Loss: 4026.040283203125\n",
      "Epoch 3251, Train_Loss: 3905.458740234375, Val_Loss: 4025.99169921875\n",
      "Epoch 3252, Train_Loss: 3905.42626953125, Val_Loss: 4025.986328125\n",
      "Epoch 3253, Train_Loss: 3905.42626953125, Val_Loss: 4025.983154296875\n",
      "Epoch 3254, Train_Loss: 3905.414794921875, Val_Loss: 4025.919189453125\n",
      "Epoch 3255, Train_Loss: 3905.32470703125, Val_Loss: 4025.889892578125\n",
      "Epoch 3256, Train_Loss: 3905.322509765625, Val_Loss: 4025.88232421875\n",
      "Epoch 3257, Train_Loss: 3905.342529296875, Val_Loss: 4025.844482421875\n",
      "Epoch 3258, Train_Loss: 3905.314697265625, Val_Loss: 4025.7958984375\n",
      "Epoch 3259, Train_Loss: 3905.272216796875, Val_Loss: 4025.73876953125\n",
      "Epoch 3260, Train_Loss: 3905.293701171875, Val_Loss: 4025.73486328125\n",
      "Epoch 3261, Train_Loss: 3905.228271484375, Val_Loss: 4025.713134765625\n",
      "Epoch 3262, Train_Loss: 3905.222900390625, Val_Loss: 4025.708740234375\n",
      "Epoch 3263, Train_Loss: 3905.115478515625, Val_Loss: 4025.644775390625\n",
      "Epoch 3264, Train_Loss: 3905.134521484375, Val_Loss: 4025.668701171875\n",
      "Epoch 3265, Train_Loss: 3905.07568359375, Val_Loss: 4025.644775390625\n",
      "Epoch 3266, Train_Loss: 3905.044921875, Val_Loss: 4025.6376953125\n",
      "Epoch 3267, Train_Loss: 3905.035400390625, Val_Loss: 4025.5576171875\n",
      "Epoch 3268, Train_Loss: 3904.72265625, Val_Loss: 4025.530517578125\n",
      "Epoch 3269, Train_Loss: 3904.7177734375, Val_Loss: 4025.5751953125\n",
      "Epoch 3270, Train_Loss: 3904.671630859375, Val_Loss: 4025.520751953125\n",
      "Epoch 3271, Train_Loss: 3904.649658203125, Val_Loss: 4025.474853515625\n",
      "Epoch 3272, Train_Loss: 3904.635986328125, Val_Loss: 4025.397705078125\n",
      "Epoch 3273, Train_Loss: 3904.59619140625, Val_Loss: 4025.4267578125\n",
      "Epoch 3274, Train_Loss: 3904.561279296875, Val_Loss: 4025.404052734375\n",
      "Epoch 3275, Train_Loss: 3904.5537109375, Val_Loss: 4025.3603515625\n",
      "Epoch 3276, Train_Loss: 3904.521484375, Val_Loss: 4025.3427734375\n",
      "Epoch 3277, Train_Loss: 3904.492431640625, Val_Loss: 4025.2763671875\n",
      "Epoch 3278, Train_Loss: 3904.44580078125, Val_Loss: 4025.243896484375\n",
      "Epoch 3279, Train_Loss: 3904.442626953125, Val_Loss: 4025.231201171875\n",
      "Epoch 3280, Train_Loss: 3904.405029296875, Val_Loss: 4025.214111328125\n",
      "Epoch 3281, Train_Loss: 3904.380859375, Val_Loss: 4025.201904296875\n",
      "Epoch 3282, Train_Loss: 3904.340576171875, Val_Loss: 4025.174072265625\n",
      "Epoch 3283, Train_Loss: 3904.33544921875, Val_Loss: 4025.1376953125\n",
      "Epoch 3284, Train_Loss: 3904.310302734375, Val_Loss: 4025.12158203125\n",
      "Epoch 3285, Train_Loss: 3904.312255859375, Val_Loss: 4025.12109375\n",
      "Epoch 3286, Train_Loss: 3904.260986328125, Val_Loss: 4025.098388671875\n",
      "Epoch 3287, Train_Loss: 3904.24560546875, Val_Loss: 4025.011962890625\n",
      "Epoch 3288, Train_Loss: 3904.204833984375, Val_Loss: 4024.980712890625\n",
      "Epoch 3289, Train_Loss: 3904.190673828125, Val_Loss: 4024.989501953125\n",
      "Epoch 3290, Train_Loss: 3904.190673828125, Val_Loss: 4024.96923828125\n",
      "Epoch 3291, Train_Loss: 3904.13623046875, Val_Loss: 4024.884765625\n",
      "Epoch 3292, Train_Loss: 3904.126220703125, Val_Loss: 4024.8828125\n",
      "Epoch 3293, Train_Loss: 3904.091064453125, Val_Loss: 4024.859130859375\n",
      "Epoch 3294, Train_Loss: 3904.0771484375, Val_Loss: 4024.861083984375\n",
      "Epoch 3295, Train_Loss: 3904.054443359375, Val_Loss: 4024.843994140625\n",
      "Epoch 3296, Train_Loss: 3904.033935546875, Val_Loss: 4024.75390625\n",
      "Epoch 3297, Train_Loss: 3903.966064453125, Val_Loss: 4024.7294921875\n",
      "Epoch 3298, Train_Loss: 3903.982666015625, Val_Loss: 4024.710693359375\n",
      "Epoch 3299, Train_Loss: 3903.96240234375, Val_Loss: 4024.713134765625\n",
      "Epoch 3300, Train_Loss: 3903.90771484375, Val_Loss: 4024.673095703125\n",
      "Epoch 3301, Train_Loss: 3903.897216796875, Val_Loss: 4024.65283203125\n",
      "Epoch 3302, Train_Loss: 3903.8701171875, Val_Loss: 4024.634521484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3303, Train_Loss: 3903.85595703125, Val_Loss: 4024.60205078125\n",
      "Epoch 3304, Train_Loss: 3903.909423828125, Val_Loss: 4024.587646484375\n",
      "Epoch 3305, Train_Loss: 3903.72900390625, Val_Loss: 4024.55908203125\n",
      "Epoch 3306, Train_Loss: 3903.703369140625, Val_Loss: 4024.54638671875\n",
      "Epoch 3307, Train_Loss: 3903.688720703125, Val_Loss: 4024.52685546875\n",
      "Epoch 3308, Train_Loss: 3903.658447265625, Val_Loss: 4024.47607421875\n",
      "Epoch 3309, Train_Loss: 3903.637939453125, Val_Loss: 4024.451904296875\n",
      "Epoch 3310, Train_Loss: 3903.61474609375, Val_Loss: 4024.3896484375\n",
      "Epoch 3311, Train_Loss: 3903.587158203125, Val_Loss: 4024.37109375\n",
      "Epoch 3312, Train_Loss: 3903.572509765625, Val_Loss: 4024.3388671875\n",
      "Epoch 3313, Train_Loss: 3903.553466796875, Val_Loss: 4024.353515625\n",
      "Epoch 3314, Train_Loss: 3903.537353515625, Val_Loss: 4024.31396484375\n",
      "Epoch 3315, Train_Loss: 3903.482177734375, Val_Loss: 4024.23876953125\n",
      "Epoch 3316, Train_Loss: 3903.467041015625, Val_Loss: 4024.23046875\n",
      "Epoch 3317, Train_Loss: 3903.446044921875, Val_Loss: 4024.200439453125\n",
      "Epoch 3318, Train_Loss: 3903.408935546875, Val_Loss: 4024.168701171875\n",
      "Epoch 3319, Train_Loss: 3903.3837890625, Val_Loss: 4024.143310546875\n",
      "Epoch 3320, Train_Loss: 3903.36181640625, Val_Loss: 4024.120849609375\n",
      "Epoch 3321, Train_Loss: 3903.364990234375, Val_Loss: 4024.09716796875\n",
      "Epoch 3322, Train_Loss: 3903.32470703125, Val_Loss: 4024.0859375\n",
      "Epoch 3323, Train_Loss: 3903.308349609375, Val_Loss: 4024.0712890625\n",
      "Epoch 3324, Train_Loss: 3903.278076171875, Val_Loss: 4024.019287109375\n",
      "Epoch 3325, Train_Loss: 3903.237060546875, Val_Loss: 4024.0126953125\n",
      "Epoch 3326, Train_Loss: 3903.26220703125, Val_Loss: 4023.9423828125\n",
      "Epoch 3327, Train_Loss: 3903.20556640625, Val_Loss: 4023.939208984375\n",
      "Epoch 3328, Train_Loss: 3903.23193359375, Val_Loss: 4023.9091796875\n",
      "Epoch 3329, Train_Loss: 3903.158447265625, Val_Loss: 4023.851318359375\n",
      "Epoch 3330, Train_Loss: 3903.179931640625, Val_Loss: 4023.8291015625\n",
      "Epoch 3331, Train_Loss: 3903.111328125, Val_Loss: 4023.82080078125\n",
      "Epoch 3332, Train_Loss: 3902.979736328125, Val_Loss: 4023.788818359375\n",
      "Epoch 3333, Train_Loss: 3903.072021484375, Val_Loss: 4023.74951171875\n",
      "Epoch 3334, Train_Loss: 3902.969482421875, Val_Loss: 4023.627197265625\n",
      "Epoch 3335, Train_Loss: 3902.928955078125, Val_Loss: 4023.68310546875\n",
      "Epoch 3336, Train_Loss: 3902.8828125, Val_Loss: 4023.658447265625\n",
      "Epoch 3337, Train_Loss: 3902.872802734375, Val_Loss: 4023.631103515625\n",
      "Epoch 3338, Train_Loss: 3902.804931640625, Val_Loss: 4023.600830078125\n",
      "Epoch 3339, Train_Loss: 3902.886474609375, Val_Loss: 4023.65869140625\n",
      "Epoch 3340, Train_Loss: 3902.7939453125, Val_Loss: 4023.48681640625\n",
      "Epoch 3341, Train_Loss: 3902.8095703125, Val_Loss: 4023.496337890625\n",
      "Epoch 3342, Train_Loss: 3902.896484375, Val_Loss: 4023.644775390625\n",
      "Epoch 3343, Train_Loss: 3902.678466796875, Val_Loss: 4023.36328125\n",
      "Epoch 3344, Train_Loss: 3902.6669921875, Val_Loss: 4023.38671875\n",
      "Epoch 3345, Train_Loss: 3902.627685546875, Val_Loss: 4023.4423828125\n",
      "Epoch 3346, Train_Loss: 3902.5869140625, Val_Loss: 4023.42919921875\n",
      "Epoch 3347, Train_Loss: 3902.60107421875, Val_Loss: 4023.324462890625\n",
      "Epoch 3348, Train_Loss: 3902.5029296875, Val_Loss: 4023.415283203125\n",
      "Epoch 3349, Train_Loss: 3902.448486328125, Val_Loss: 4023.33447265625\n",
      "Epoch 3350, Train_Loss: 3902.5224609375, Val_Loss: 4023.2939453125\n",
      "Epoch 3351, Train_Loss: 3902.44287109375, Val_Loss: 4023.280029296875\n",
      "Epoch 3352, Train_Loss: 3902.31396484375, Val_Loss: 4023.295166015625\n",
      "Epoch 3353, Train_Loss: 3902.282470703125, Val_Loss: 4023.187255859375\n",
      "Epoch 3354, Train_Loss: 3902.28662109375, Val_Loss: 4023.158447265625\n",
      "Epoch 3355, Train_Loss: 3902.232177734375, Val_Loss: 4023.1640625\n",
      "Epoch 3356, Train_Loss: 3902.220703125, Val_Loss: 4023.15087890625\n",
      "Epoch 3357, Train_Loss: 3902.26416015625, Val_Loss: 4023.0791015625\n",
      "Epoch 3358, Train_Loss: 3902.15234375, Val_Loss: 4023.098388671875\n",
      "Epoch 3359, Train_Loss: 3902.154052734375, Val_Loss: 4023.085693359375\n",
      "Epoch 3360, Train_Loss: 3902.127197265625, Val_Loss: 4023.06640625\n",
      "Epoch 3361, Train_Loss: 3902.138427734375, Val_Loss: 4023.023193359375\n",
      "Epoch 3362, Train_Loss: 3902.082275390625, Val_Loss: 4023.007080078125\n",
      "Epoch 3363, Train_Loss: 3902.054931640625, Val_Loss: 4022.978759765625\n",
      "Epoch 3364, Train_Loss: 3902.041259765625, Val_Loss: 4022.959228515625\n",
      "Epoch 3365, Train_Loss: 3902.01416015625, Val_Loss: 4022.938720703125\n",
      "Epoch 3366, Train_Loss: 3901.989501953125, Val_Loss: 4022.874755859375\n",
      "Epoch 3367, Train_Loss: 3901.96337890625, Val_Loss: 4022.853515625\n",
      "Epoch 3368, Train_Loss: 3901.94873046875, Val_Loss: 4022.837646484375\n",
      "Epoch 3369, Train_Loss: 3901.919189453125, Val_Loss: 4022.7900390625\n",
      "Epoch 3370, Train_Loss: 3901.910400390625, Val_Loss: 4022.77392578125\n",
      "Epoch 3371, Train_Loss: 3901.87158203125, Val_Loss: 4022.693115234375\n",
      "Epoch 3372, Train_Loss: 3901.854736328125, Val_Loss: 4022.6787109375\n",
      "Epoch 3373, Train_Loss: 3901.828125, Val_Loss: 4022.639892578125\n",
      "Epoch 3374, Train_Loss: 3901.819091796875, Val_Loss: 4022.653564453125\n",
      "Epoch 3375, Train_Loss: 3901.54736328125, Val_Loss: 4022.618896484375\n",
      "Epoch 3376, Train_Loss: 3901.529052734375, Val_Loss: 4022.5859375\n",
      "Epoch 3377, Train_Loss: 3901.505859375, Val_Loss: 4022.58447265625\n",
      "Epoch 3378, Train_Loss: 3901.4892578125, Val_Loss: 4022.57568359375\n",
      "Epoch 3379, Train_Loss: 3901.40380859375, Val_Loss: 4022.530517578125\n",
      "Epoch 3380, Train_Loss: 3901.280029296875, Val_Loss: 4022.48876953125\n",
      "Epoch 3381, Train_Loss: 3901.270751953125, Val_Loss: 4022.48193359375\n",
      "Epoch 3382, Train_Loss: 3901.25634765625, Val_Loss: 4022.461181640625\n",
      "Epoch 3383, Train_Loss: 3901.235595703125, Val_Loss: 4022.4404296875\n",
      "Epoch 3384, Train_Loss: 3901.21142578125, Val_Loss: 4022.409912109375\n",
      "Epoch 3385, Train_Loss: 3901.151123046875, Val_Loss: 4022.340087890625\n",
      "Epoch 3386, Train_Loss: 3901.142333984375, Val_Loss: 4022.334716796875\n",
      "Epoch 3387, Train_Loss: 3901.122802734375, Val_Loss: 4022.31689453125\n",
      "Epoch 3388, Train_Loss: 3901.09716796875, Val_Loss: 4022.287109375\n",
      "Epoch 3389, Train_Loss: 3901.042724609375, Val_Loss: 4022.182861328125\n",
      "Epoch 3390, Train_Loss: 3901.012451171875, Val_Loss: 4022.155517578125\n",
      "Epoch 3391, Train_Loss: 3900.987060546875, Val_Loss: 4022.129150390625\n",
      "Epoch 3392, Train_Loss: 3900.96923828125, Val_Loss: 4022.115966796875\n",
      "Epoch 3393, Train_Loss: 3900.958984375, Val_Loss: 4022.0947265625\n",
      "Epoch 3394, Train_Loss: 3900.922119140625, Val_Loss: 4022.07568359375\n",
      "Epoch 3395, Train_Loss: 3900.896484375, Val_Loss: 4022.052001953125\n",
      "Epoch 3396, Train_Loss: 3900.89013671875, Val_Loss: 4022.036376953125\n",
      "Epoch 3397, Train_Loss: 3900.864990234375, Val_Loss: 4022.0283203125\n",
      "Epoch 3398, Train_Loss: 3900.826904296875, Val_Loss: 4021.97509765625\n",
      "Epoch 3399, Train_Loss: 3900.793701171875, Val_Loss: 4021.956787109375\n",
      "Epoch 3400, Train_Loss: 3900.777587890625, Val_Loss: 4021.94189453125\n",
      "Epoch 3401, Train_Loss: 3900.757080078125, Val_Loss: 4021.924072265625\n",
      "Epoch 3402, Train_Loss: 3900.73974609375, Val_Loss: 4021.90478515625\n",
      "Epoch 3403, Train_Loss: 3900.69384765625, Val_Loss: 4021.852294921875\n",
      "Epoch 3404, Train_Loss: 3900.6748046875, Val_Loss: 4021.82958984375\n",
      "Epoch 3405, Train_Loss: 3900.663330078125, Val_Loss: 4021.80908203125\n",
      "Epoch 3406, Train_Loss: 3900.6396484375, Val_Loss: 4021.80078125\n",
      "Epoch 3407, Train_Loss: 3900.611083984375, Val_Loss: 4021.7119140625\n",
      "Epoch 3408, Train_Loss: 3900.59423828125, Val_Loss: 4021.694091796875\n",
      "Epoch 3409, Train_Loss: 3900.56640625, Val_Loss: 4021.6416015625\n",
      "Epoch 3410, Train_Loss: 3900.55224609375, Val_Loss: 4021.620849609375\n",
      "Epoch 3411, Train_Loss: 3900.540771484375, Val_Loss: 4021.59521484375\n",
      "Epoch 3412, Train_Loss: 3900.4599609375, Val_Loss: 4021.568359375\n",
      "Epoch 3413, Train_Loss: 3900.44482421875, Val_Loss: 4021.553955078125\n",
      "Epoch 3414, Train_Loss: 3900.4248046875, Val_Loss: 4021.541259765625\n",
      "Epoch 3415, Train_Loss: 3900.462158203125, Val_Loss: 4021.516845703125\n",
      "Epoch 3416, Train_Loss: 3900.080810546875, Val_Loss: 4021.48193359375\n",
      "Epoch 3417, Train_Loss: 3900.061279296875, Val_Loss: 4021.462890625\n",
      "Epoch 3418, Train_Loss: 3900.03515625, Val_Loss: 4021.437255859375\n",
      "Epoch 3419, Train_Loss: 3899.9560546875, Val_Loss: 4021.426025390625\n",
      "Epoch 3420, Train_Loss: 3899.99462890625, Val_Loss: 4021.3916015625\n",
      "Epoch 3421, Train_Loss: 3899.96484375, Val_Loss: 4021.330810546875\n",
      "Epoch 3422, Train_Loss: 3899.87841796875, Val_Loss: 4021.328857421875\n",
      "Epoch 3423, Train_Loss: 3899.92919921875, Val_Loss: 4021.29931640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3424, Train_Loss: 3899.924560546875, Val_Loss: 4021.279296875\n",
      "Epoch 3425, Train_Loss: 3899.858642578125, Val_Loss: 4021.197265625\n",
      "Epoch 3426, Train_Loss: 3899.788330078125, Val_Loss: 4021.19287109375\n",
      "Epoch 3427, Train_Loss: 3899.840087890625, Val_Loss: 4021.151611328125\n",
      "Epoch 3428, Train_Loss: 3899.7998046875, Val_Loss: 4021.093505859375\n",
      "Epoch 3429, Train_Loss: 3899.745361328125, Val_Loss: 4021.090087890625\n",
      "Epoch 3430, Train_Loss: 3899.704345703125, Val_Loss: 4021.054443359375\n",
      "Epoch 3431, Train_Loss: 3899.716064453125, Val_Loss: 4021.0205078125\n",
      "Epoch 3432, Train_Loss: 3899.693603515625, Val_Loss: 4021.010498046875\n",
      "Epoch 3433, Train_Loss: 3899.67724609375, Val_Loss: 4021.007080078125\n",
      "Epoch 3434, Train_Loss: 3899.6337890625, Val_Loss: 4020.9619140625\n",
      "Epoch 3435, Train_Loss: 3899.6142578125, Val_Loss: 4020.945556640625\n",
      "Epoch 3436, Train_Loss: 3899.60009765625, Val_Loss: 4020.90869140625\n",
      "Epoch 3437, Train_Loss: 3899.556640625, Val_Loss: 4020.91845703125\n",
      "Epoch 3438, Train_Loss: 3899.51953125, Val_Loss: 4020.859619140625\n",
      "Epoch 3439, Train_Loss: 3899.486083984375, Val_Loss: 4020.850341796875\n",
      "Epoch 3440, Train_Loss: 3899.49267578125, Val_Loss: 4020.729248046875\n",
      "Epoch 3441, Train_Loss: 3899.487548828125, Val_Loss: 4020.80078125\n",
      "Epoch 3442, Train_Loss: 3899.42529296875, Val_Loss: 4020.7080078125\n",
      "Epoch 3443, Train_Loss: 3899.447265625, Val_Loss: 4020.744873046875\n",
      "Epoch 3444, Train_Loss: 3899.466552734375, Val_Loss: 4020.567626953125\n",
      "Epoch 3445, Train_Loss: 3899.423583984375, Val_Loss: 4020.6962890625\n",
      "Epoch 3446, Train_Loss: 3899.30322265625, Val_Loss: 4020.54638671875\n",
      "Epoch 3447, Train_Loss: 3899.350830078125, Val_Loss: 4020.558349609375\n",
      "Epoch 3448, Train_Loss: 3899.351806640625, Val_Loss: 4020.554443359375\n",
      "Epoch 3449, Train_Loss: 3899.344970703125, Val_Loss: 4020.4287109375\n",
      "Epoch 3450, Train_Loss: 3899.283203125, Val_Loss: 4020.539306640625\n",
      "Epoch 3451, Train_Loss: 3899.18994140625, Val_Loss: 4020.467529296875\n",
      "Epoch 3452, Train_Loss: 3899.259765625, Val_Loss: 4020.494384765625\n",
      "Epoch 3453, Train_Loss: 3899.0966796875, Val_Loss: 4020.448486328125\n",
      "Epoch 3454, Train_Loss: 3899.044921875, Val_Loss: 4020.39208984375\n",
      "Epoch 3455, Train_Loss: 3899.0263671875, Val_Loss: 4020.42724609375\n",
      "Epoch 3456, Train_Loss: 3898.977294921875, Val_Loss: 4020.416015625\n",
      "Epoch 3457, Train_Loss: 3898.991943359375, Val_Loss: 4020.284423828125\n",
      "Epoch 3458, Train_Loss: 3898.885009765625, Val_Loss: 4020.302734375\n",
      "Epoch 3459, Train_Loss: 3898.90966796875, Val_Loss: 4020.326904296875\n",
      "Epoch 3460, Train_Loss: 3898.88134765625, Val_Loss: 4020.26611328125\n",
      "Epoch 3461, Train_Loss: 3898.9111328125, Val_Loss: 4020.18798828125\n",
      "Epoch 3462, Train_Loss: 3898.83056640625, Val_Loss: 4020.178466796875\n",
      "Epoch 3463, Train_Loss: 3898.79736328125, Val_Loss: 4020.165283203125\n",
      "Epoch 3464, Train_Loss: 3898.78662109375, Val_Loss: 4020.127685546875\n",
      "Epoch 3465, Train_Loss: 3898.783935546875, Val_Loss: 4020.0576171875\n",
      "Epoch 3466, Train_Loss: 3898.78271484375, Val_Loss: 4020.0927734375\n",
      "Epoch 3467, Train_Loss: 3898.732421875, Val_Loss: 4020.00830078125\n",
      "Epoch 3468, Train_Loss: 3898.76171875, Val_Loss: 4020.02685546875\n",
      "Epoch 3469, Train_Loss: 3898.7236328125, Val_Loss: 4020.041259765625\n",
      "Epoch 3470, Train_Loss: 3898.697021484375, Val_Loss: 4019.945556640625\n",
      "Epoch 3471, Train_Loss: 3898.621337890625, Val_Loss: 4019.924072265625\n",
      "Epoch 3472, Train_Loss: 3898.640380859375, Val_Loss: 4019.904052734375\n",
      "Epoch 3473, Train_Loss: 3898.60400390625, Val_Loss: 4019.8876953125\n",
      "Epoch 3474, Train_Loss: 3898.58740234375, Val_Loss: 4019.88037109375\n",
      "Epoch 3475, Train_Loss: 3898.562255859375, Val_Loss: 4019.817138671875\n",
      "Epoch 3476, Train_Loss: 3898.537109375, Val_Loss: 4019.793212890625\n",
      "Epoch 3477, Train_Loss: 3898.526611328125, Val_Loss: 4019.77587890625\n",
      "Epoch 3478, Train_Loss: 3898.442626953125, Val_Loss: 4019.775146484375\n",
      "Epoch 3479, Train_Loss: 3898.410888671875, Val_Loss: 4019.7587890625\n",
      "Epoch 3480, Train_Loss: 3898.44970703125, Val_Loss: 4019.6591796875\n",
      "Epoch 3481, Train_Loss: 3898.363037109375, Val_Loss: 4019.647216796875\n",
      "Epoch 3482, Train_Loss: 3898.34375, Val_Loss: 4019.61328125\n",
      "Epoch 3483, Train_Loss: 3898.31396484375, Val_Loss: 4019.600341796875\n",
      "Epoch 3484, Train_Loss: 3898.23046875, Val_Loss: 4019.57275390625\n",
      "Epoch 3485, Train_Loss: 3898.245849609375, Val_Loss: 4019.531982421875\n",
      "Epoch 3486, Train_Loss: 3898.212158203125, Val_Loss: 4019.537109375\n",
      "Epoch 3487, Train_Loss: 3898.176025390625, Val_Loss: 4019.520751953125\n",
      "Epoch 3488, Train_Loss: 3898.19482421875, Val_Loss: 4019.449951171875\n",
      "Epoch 3489, Train_Loss: 3898.091064453125, Val_Loss: 4019.424072265625\n",
      "Epoch 3490, Train_Loss: 3898.069580078125, Val_Loss: 4019.405517578125\n",
      "Epoch 3491, Train_Loss: 3898.054443359375, Val_Loss: 4019.394775390625\n",
      "Epoch 3492, Train_Loss: 3898.04248046875, Val_Loss: 4019.367919921875\n",
      "Epoch 3493, Train_Loss: 3897.997314453125, Val_Loss: 4019.315673828125\n",
      "Epoch 3494, Train_Loss: 3897.95849609375, Val_Loss: 4019.302734375\n",
      "Epoch 3495, Train_Loss: 3897.955078125, Val_Loss: 4019.271240234375\n",
      "Epoch 3496, Train_Loss: 3897.940185546875, Val_Loss: 4019.261962890625\n",
      "Epoch 3497, Train_Loss: 3897.888427734375, Val_Loss: 4019.255126953125\n",
      "Epoch 3498, Train_Loss: 3897.782958984375, Val_Loss: 4019.169189453125\n",
      "Epoch 3499, Train_Loss: 3897.6259765625, Val_Loss: 4019.126708984375\n",
      "Epoch 3500, Train_Loss: 3897.610107421875, Val_Loss: 4019.1044921875\n",
      "Epoch 3501, Train_Loss: 3897.59912109375, Val_Loss: 4019.09033203125\n",
      "Epoch 3502, Train_Loss: 3897.553466796875, Val_Loss: 4019.060302734375\n",
      "Epoch 3503, Train_Loss: 3897.546875, Val_Loss: 4019.02880859375\n",
      "Epoch 3504, Train_Loss: 3897.550537109375, Val_Loss: 4019.00927734375\n",
      "Epoch 3505, Train_Loss: 3897.514404296875, Val_Loss: 4019.0\n",
      "Epoch 3506, Train_Loss: 3897.4970703125, Val_Loss: 4018.979248046875\n",
      "Epoch 3507, Train_Loss: 3897.45361328125, Val_Loss: 4018.939208984375\n",
      "Epoch 3508, Train_Loss: 3897.4423828125, Val_Loss: 4018.88916015625\n",
      "Epoch 3509, Train_Loss: 3897.409423828125, Val_Loss: 4018.865966796875\n",
      "Epoch 3510, Train_Loss: 3897.380126953125, Val_Loss: 4018.8544921875\n",
      "Epoch 3511, Train_Loss: 3897.351806640625, Val_Loss: 4018.790283203125\n",
      "Epoch 3512, Train_Loss: 3897.33349609375, Val_Loss: 4018.77001953125\n",
      "Epoch 3513, Train_Loss: 3897.317138671875, Val_Loss: 4018.758056640625\n",
      "Epoch 3514, Train_Loss: 3897.29150390625, Val_Loss: 4018.74365234375\n",
      "Epoch 3515, Train_Loss: 3897.26904296875, Val_Loss: 4018.65966796875\n",
      "Epoch 3516, Train_Loss: 3897.24072265625, Val_Loss: 4018.642333984375\n",
      "Epoch 3517, Train_Loss: 3897.2275390625, Val_Loss: 4018.629638671875\n",
      "Epoch 3518, Train_Loss: 3897.200927734375, Val_Loss: 4018.60009765625\n",
      "Epoch 3519, Train_Loss: 3897.184814453125, Val_Loss: 4018.5791015625\n",
      "Epoch 3520, Train_Loss: 3896.9033203125, Val_Loss: 4018.558837890625\n",
      "Epoch 3521, Train_Loss: 3896.879638671875, Val_Loss: 4018.534912109375\n",
      "Epoch 3522, Train_Loss: 3896.863525390625, Val_Loss: 4018.519287109375\n",
      "Epoch 3523, Train_Loss: 3896.849365234375, Val_Loss: 4018.49560546875\n",
      "Epoch 3524, Train_Loss: 3896.728271484375, Val_Loss: 4018.45361328125\n",
      "Epoch 3525, Train_Loss: 3896.71044921875, Val_Loss: 4018.434326171875\n",
      "Epoch 3526, Train_Loss: 3896.705810546875, Val_Loss: 4018.422119140625\n",
      "Epoch 3527, Train_Loss: 3896.68017578125, Val_Loss: 4018.359619140625\n",
      "Epoch 3528, Train_Loss: 3896.65380859375, Val_Loss: 4018.35009765625\n",
      "Epoch 3529, Train_Loss: 3896.6025390625, Val_Loss: 4018.27392578125\n",
      "Epoch 3530, Train_Loss: 3896.548095703125, Val_Loss: 4018.2705078125\n",
      "Epoch 3531, Train_Loss: 3896.560546875, Val_Loss: 4018.239990234375\n",
      "Epoch 3532, Train_Loss: 3896.505615234375, Val_Loss: 4018.236083984375\n",
      "Epoch 3533, Train_Loss: 3896.462158203125, Val_Loss: 4018.15234375\n",
      "Epoch 3534, Train_Loss: 3896.4716796875, Val_Loss: 4018.13671875\n",
      "Epoch 3535, Train_Loss: 3896.37109375, Val_Loss: 4018.087890625\n",
      "Epoch 3536, Train_Loss: 3896.32421875, Val_Loss: 4018.087890625\n",
      "Epoch 3537, Train_Loss: 3896.31689453125, Val_Loss: 4018.06591796875\n",
      "Epoch 3538, Train_Loss: 3896.304443359375, Val_Loss: 4018.019287109375\n",
      "Epoch 3539, Train_Loss: 3896.239501953125, Val_Loss: 4018.0205078125\n",
      "Epoch 3540, Train_Loss: 3896.2353515625, Val_Loss: 4017.981689453125\n",
      "Epoch 3541, Train_Loss: 3896.241455078125, Val_Loss: 4017.953125\n",
      "Epoch 3542, Train_Loss: 3896.2158203125, Val_Loss: 4017.93994140625\n",
      "Epoch 3543, Train_Loss: 3896.177734375, Val_Loss: 4017.853271484375\n",
      "Epoch 3544, Train_Loss: 3896.14306640625, Val_Loss: 4017.902099609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3545, Train_Loss: 3896.109619140625, Val_Loss: 4017.8388671875\n",
      "Epoch 3546, Train_Loss: 3896.07177734375, Val_Loss: 4017.7724609375\n",
      "Epoch 3547, Train_Loss: 3896.075439453125, Val_Loss: 4017.760498046875\n",
      "Epoch 3548, Train_Loss: 3896.039794921875, Val_Loss: 4017.74169921875\n",
      "Epoch 3549, Train_Loss: 3896.041259765625, Val_Loss: 4017.731689453125\n",
      "Epoch 3550, Train_Loss: 3896.0068359375, Val_Loss: 4017.7119140625\n",
      "Epoch 3551, Train_Loss: 3895.96240234375, Val_Loss: 4017.627197265625\n",
      "Epoch 3552, Train_Loss: 3895.99169921875, Val_Loss: 4017.62158203125\n",
      "Epoch 3553, Train_Loss: 3895.924072265625, Val_Loss: 4017.59326171875\n",
      "Epoch 3554, Train_Loss: 3895.915771484375, Val_Loss: 4017.57470703125\n",
      "Epoch 3555, Train_Loss: 3895.839599609375, Val_Loss: 4017.565673828125\n",
      "Epoch 3556, Train_Loss: 3895.801025390625, Val_Loss: 4017.51953125\n",
      "Epoch 3557, Train_Loss: 3895.79296875, Val_Loss: 4017.519287109375\n",
      "Epoch 3558, Train_Loss: 3895.81396484375, Val_Loss: 4017.498779296875\n",
      "Epoch 3559, Train_Loss: 3895.754150390625, Val_Loss: 4017.465087890625\n",
      "Epoch 3560, Train_Loss: 3895.42724609375, Val_Loss: 4017.422119140625\n",
      "Epoch 3561, Train_Loss: 3895.414306640625, Val_Loss: 4017.430419921875\n",
      "Epoch 3562, Train_Loss: 3895.394287109375, Val_Loss: 4017.38330078125\n",
      "Epoch 3563, Train_Loss: 3895.37451171875, Val_Loss: 4017.366455078125\n",
      "Epoch 3564, Train_Loss: 3895.33935546875, Val_Loss: 4017.32275390625\n",
      "Epoch 3565, Train_Loss: 3895.31884765625, Val_Loss: 4017.29833984375\n",
      "Epoch 3566, Train_Loss: 3895.302001953125, Val_Loss: 4017.23388671875\n",
      "Epoch 3567, Train_Loss: 3895.280517578125, Val_Loss: 4017.216796875\n",
      "Epoch 3568, Train_Loss: 3895.231201171875, Val_Loss: 4017.129150390625\n",
      "Epoch 3569, Train_Loss: 3895.212890625, Val_Loss: 4017.130859375\n",
      "Epoch 3570, Train_Loss: 3895.1982421875, Val_Loss: 4017.117919921875\n",
      "Epoch 3571, Train_Loss: 3895.1767578125, Val_Loss: 4017.08349609375\n",
      "Epoch 3572, Train_Loss: 3895.149658203125, Val_Loss: 4017.072021484375\n",
      "Epoch 3573, Train_Loss: 3895.115966796875, Val_Loss: 4017.0419921875\n",
      "Epoch 3574, Train_Loss: 3895.027587890625, Val_Loss: 4017.021484375\n",
      "Epoch 3575, Train_Loss: 3895.028564453125, Val_Loss: 4016.997314453125\n",
      "Epoch 3576, Train_Loss: 3895.0029296875, Val_Loss: 4016.978759765625\n",
      "Epoch 3577, Train_Loss: 3894.942626953125, Val_Loss: 4016.9287109375\n",
      "Epoch 3578, Train_Loss: 3894.933837890625, Val_Loss: 4016.91796875\n",
      "Epoch 3579, Train_Loss: 3894.923828125, Val_Loss: 4016.89208984375\n",
      "Epoch 3580, Train_Loss: 3894.912353515625, Val_Loss: 4016.881591796875\n",
      "Epoch 3581, Train_Loss: 3894.888671875, Val_Loss: 4016.850341796875\n",
      "Epoch 3582, Train_Loss: 3894.863525390625, Val_Loss: 4016.8037109375\n",
      "Epoch 3583, Train_Loss: 3894.814208984375, Val_Loss: 4016.733642578125\n",
      "Epoch 3584, Train_Loss: 3894.82275390625, Val_Loss: 4016.71630859375\n",
      "Epoch 3585, Train_Loss: 3894.79638671875, Val_Loss: 4016.701904296875\n",
      "Epoch 3586, Train_Loss: 3894.74658203125, Val_Loss: 4016.615234375\n",
      "Epoch 3587, Train_Loss: 3894.740234375, Val_Loss: 4016.611083984375\n",
      "Epoch 3588, Train_Loss: 3894.719970703125, Val_Loss: 4016.595703125\n",
      "Epoch 3589, Train_Loss: 3894.682861328125, Val_Loss: 4016.572509765625\n",
      "Epoch 3590, Train_Loss: 3894.676025390625, Val_Loss: 4016.5458984375\n",
      "Epoch 3591, Train_Loss: 3894.6591796875, Val_Loss: 4016.527099609375\n",
      "Epoch 3592, Train_Loss: 3894.6376953125, Val_Loss: 4016.50927734375\n",
      "Epoch 3593, Train_Loss: 3894.5888671875, Val_Loss: 4016.498291015625\n",
      "Epoch 3594, Train_Loss: 3894.587890625, Val_Loss: 4016.46875\n",
      "Epoch 3595, Train_Loss: 3894.4169921875, Val_Loss: 4016.432861328125\n",
      "Epoch 3596, Train_Loss: 3894.43603515625, Val_Loss: 4016.389892578125\n",
      "Epoch 3597, Train_Loss: 3894.38134765625, Val_Loss: 4016.4052734375\n",
      "Epoch 3598, Train_Loss: 3894.40234375, Val_Loss: 4016.306396484375\n",
      "Epoch 3599, Train_Loss: 3894.3505859375, Val_Loss: 4016.313232421875\n",
      "Epoch 3600, Train_Loss: 3894.32861328125, Val_Loss: 4016.241943359375\n",
      "Epoch 3601, Train_Loss: 3894.32080078125, Val_Loss: 4016.283203125\n",
      "Epoch 3602, Train_Loss: 3894.3642578125, Val_Loss: 4016.1962890625\n",
      "Epoch 3603, Train_Loss: 3894.233642578125, Val_Loss: 4016.1591796875\n",
      "Epoch 3604, Train_Loss: 3894.289306640625, Val_Loss: 4016.068359375\n",
      "Epoch 3605, Train_Loss: 3894.1953125, Val_Loss: 4016.11962890625\n",
      "Epoch 3606, Train_Loss: 3894.17822265625, Val_Loss: 4016.081298828125\n",
      "Epoch 3607, Train_Loss: 3894.2021484375, Val_Loss: 4016.014892578125\n",
      "Epoch 3608, Train_Loss: 3894.111083984375, Val_Loss: 4016.076904296875\n",
      "Epoch 3609, Train_Loss: 3894.073974609375, Val_Loss: 4016.007080078125\n",
      "Epoch 3610, Train_Loss: 3894.1162109375, Val_Loss: 4015.91845703125\n",
      "Epoch 3611, Train_Loss: 3894.04833984375, Val_Loss: 4016.021240234375\n",
      "Epoch 3612, Train_Loss: 3894.0712890625, Val_Loss: 4015.835205078125\n",
      "Epoch 3613, Train_Loss: 3894.047119140625, Val_Loss: 4015.922119140625\n",
      "Epoch 3614, Train_Loss: 3893.935546875, Val_Loss: 4015.923583984375\n",
      "Epoch 3615, Train_Loss: 3894.037841796875, Val_Loss: 4015.814697265625\n",
      "Epoch 3616, Train_Loss: 3893.940185546875, Val_Loss: 4015.9296875\n",
      "Epoch 3617, Train_Loss: 3893.998291015625, Val_Loss: 4015.72802734375\n",
      "Epoch 3618, Train_Loss: 3893.96240234375, Val_Loss: 4015.733642578125\n",
      "Epoch 3619, Train_Loss: 3893.794677734375, Val_Loss: 4015.763671875\n",
      "Epoch 3620, Train_Loss: 3893.781982421875, Val_Loss: 4015.78955078125\n",
      "Epoch 3621, Train_Loss: 3893.875244140625, Val_Loss: 4015.5771484375\n",
      "Epoch 3622, Train_Loss: 3893.816650390625, Val_Loss: 4015.598876953125\n",
      "Epoch 3623, Train_Loss: 3893.782958984375, Val_Loss: 4015.7255859375\n",
      "Epoch 3624, Train_Loss: 3893.833984375, Val_Loss: 4015.467529296875\n",
      "Epoch 3625, Train_Loss: 3893.82666015625, Val_Loss: 4015.536376953125\n",
      "Epoch 3626, Train_Loss: 3893.652099609375, Val_Loss: 4015.681884765625\n",
      "Epoch 3627, Train_Loss: 3893.654296875, Val_Loss: 4015.5126953125\n",
      "Epoch 3628, Train_Loss: 3893.671875, Val_Loss: 4015.399169921875\n",
      "Epoch 3629, Train_Loss: 3893.666259765625, Val_Loss: 4015.51953125\n",
      "Epoch 3630, Train_Loss: 3893.579345703125, Val_Loss: 4015.35009765625\n",
      "Epoch 3631, Train_Loss: 3893.508056640625, Val_Loss: 4015.42431640625\n",
      "Epoch 3632, Train_Loss: 3893.46875, Val_Loss: 4015.52001953125\n",
      "Epoch 3633, Train_Loss: 3893.474365234375, Val_Loss: 4015.388916015625\n",
      "Epoch 3634, Train_Loss: 3893.341796875, Val_Loss: 4015.221923828125\n",
      "Epoch 3635, Train_Loss: 3893.36767578125, Val_Loss: 4015.31640625\n",
      "Epoch 3636, Train_Loss: 3893.4345703125, Val_Loss: 4015.41845703125\n",
      "Epoch 3637, Train_Loss: 3893.328369140625, Val_Loss: 4015.215087890625\n",
      "Epoch 3638, Train_Loss: 3893.208251953125, Val_Loss: 4015.167236328125\n",
      "Epoch 3639, Train_Loss: 3893.18408203125, Val_Loss: 4015.203125\n",
      "Epoch 3640, Train_Loss: 3893.142333984375, Val_Loss: 4015.17431640625\n",
      "Epoch 3641, Train_Loss: 3893.15966796875, Val_Loss: 4015.0751953125\n",
      "Epoch 3642, Train_Loss: 3893.202392578125, Val_Loss: 4015.045166015625\n",
      "Epoch 3643, Train_Loss: 3893.066162109375, Val_Loss: 4015.083251953125\n",
      "Epoch 3644, Train_Loss: 3893.052001953125, Val_Loss: 4015.0712890625\n",
      "Epoch 3645, Train_Loss: 3892.98779296875, Val_Loss: 4014.985107421875\n",
      "Epoch 3646, Train_Loss: 3893.041015625, Val_Loss: 4014.983154296875\n",
      "Epoch 3647, Train_Loss: 3892.992431640625, Val_Loss: 4014.9892578125\n",
      "Epoch 3648, Train_Loss: 3892.91015625, Val_Loss: 4014.898681640625\n",
      "Epoch 3649, Train_Loss: 3892.800048828125, Val_Loss: 4014.89599609375\n",
      "Epoch 3650, Train_Loss: 3892.9091796875, Val_Loss: 4014.92041015625\n",
      "Epoch 3651, Train_Loss: 3892.896484375, Val_Loss: 4014.88916015625\n",
      "Epoch 3652, Train_Loss: 3892.8017578125, Val_Loss: 4014.78076171875\n",
      "Epoch 3653, Train_Loss: 3892.8017578125, Val_Loss: 4014.75\n",
      "Epoch 3654, Train_Loss: 3892.80859375, Val_Loss: 4014.797607421875\n",
      "Epoch 3655, Train_Loss: 3892.705322265625, Val_Loss: 4014.764892578125\n",
      "Epoch 3656, Train_Loss: 3892.662353515625, Val_Loss: 4014.680908203125\n",
      "Epoch 3657, Train_Loss: 3892.597412109375, Val_Loss: 4014.666748046875\n",
      "Epoch 3658, Train_Loss: 3892.597412109375, Val_Loss: 4014.63525390625\n",
      "Epoch 3659, Train_Loss: 3892.625732421875, Val_Loss: 4014.60009765625\n",
      "Epoch 3660, Train_Loss: 3892.595703125, Val_Loss: 4014.598876953125\n",
      "Epoch 3661, Train_Loss: 3892.25927734375, Val_Loss: 4014.599609375\n",
      "Epoch 3662, Train_Loss: 3892.281982421875, Val_Loss: 4014.518798828125\n",
      "Epoch 3663, Train_Loss: 3892.27880859375, Val_Loss: 4014.478759765625\n",
      "Epoch 3664, Train_Loss: 3892.240966796875, Val_Loss: 4014.510498046875\n",
      "Epoch 3665, Train_Loss: 3892.109375, Val_Loss: 4014.4384765625\n",
      "Epoch 3666, Train_Loss: 3892.134033203125, Val_Loss: 4014.4140625\n",
      "Epoch 3667, Train_Loss: 3892.088134765625, Val_Loss: 4014.412109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3668, Train_Loss: 3892.05712890625, Val_Loss: 4014.390869140625\n",
      "Epoch 3669, Train_Loss: 3892.02978515625, Val_Loss: 4014.315673828125\n",
      "Epoch 3670, Train_Loss: 3891.983642578125, Val_Loss: 4014.314453125\n",
      "Epoch 3671, Train_Loss: 3891.93603515625, Val_Loss: 4014.297119140625\n",
      "Epoch 3672, Train_Loss: 3891.9501953125, Val_Loss: 4014.290283203125\n",
      "Epoch 3673, Train_Loss: 3891.90625, Val_Loss: 4014.180908203125\n",
      "Epoch 3674, Train_Loss: 3891.862060546875, Val_Loss: 4014.179931640625\n",
      "Epoch 3675, Train_Loss: 3891.841796875, Val_Loss: 4014.15966796875\n",
      "Epoch 3676, Train_Loss: 3891.853759765625, Val_Loss: 4014.1201171875\n",
      "Epoch 3677, Train_Loss: 3891.80810546875, Val_Loss: 4014.1064453125\n",
      "Epoch 3678, Train_Loss: 3891.754150390625, Val_Loss: 4014.098388671875\n",
      "Epoch 3679, Train_Loss: 3891.758056640625, Val_Loss: 4014.045166015625\n",
      "Epoch 3680, Train_Loss: 3891.755859375, Val_Loss: 4013.992431640625\n",
      "Epoch 3681, Train_Loss: 3891.724853515625, Val_Loss: 4014.03076171875\n",
      "Epoch 3682, Train_Loss: 3891.687744140625, Val_Loss: 4013.980712890625\n",
      "Epoch 3683, Train_Loss: 3891.66943359375, Val_Loss: 4013.9111328125\n",
      "Epoch 3684, Train_Loss: 3891.6337890625, Val_Loss: 4013.9111328125\n",
      "Epoch 3685, Train_Loss: 3891.624755859375, Val_Loss: 4013.881591796875\n",
      "Epoch 3686, Train_Loss: 3891.594482421875, Val_Loss: 4013.817626953125\n",
      "Epoch 3687, Train_Loss: 3891.559814453125, Val_Loss: 4013.806396484375\n",
      "Epoch 3688, Train_Loss: 3891.536865234375, Val_Loss: 4013.788818359375\n",
      "Epoch 3689, Train_Loss: 3891.5556640625, Val_Loss: 4013.775634765625\n",
      "Epoch 3690, Train_Loss: 3891.517333984375, Val_Loss: 4013.689208984375\n",
      "Epoch 3691, Train_Loss: 3891.3994140625, Val_Loss: 4013.671630859375\n",
      "Epoch 3692, Train_Loss: 3891.37548828125, Val_Loss: 4013.651123046875\n",
      "Epoch 3693, Train_Loss: 3891.393798828125, Val_Loss: 4013.627197265625\n",
      "Epoch 3694, Train_Loss: 3891.373779296875, Val_Loss: 4013.61328125\n",
      "Epoch 3695, Train_Loss: 3891.264404296875, Val_Loss: 4013.58447265625\n",
      "Epoch 3696, Train_Loss: 3891.252685546875, Val_Loss: 4013.56201171875\n",
      "Epoch 3697, Train_Loss: 3891.237548828125, Val_Loss: 4013.54931640625\n",
      "Epoch 3698, Train_Loss: 3891.22509765625, Val_Loss: 4013.525146484375\n",
      "Epoch 3699, Train_Loss: 3890.898681640625, Val_Loss: 4013.486328125\n",
      "Epoch 3700, Train_Loss: 3890.8740234375, Val_Loss: 4013.470458984375\n",
      "Epoch 3701, Train_Loss: 3890.83740234375, Val_Loss: 4013.40869140625\n",
      "Epoch 3702, Train_Loss: 3890.8740234375, Val_Loss: 4013.38720703125\n",
      "Epoch 3703, Train_Loss: 3890.790283203125, Val_Loss: 4013.330810546875\n",
      "Epoch 3704, Train_Loss: 3890.7724609375, Val_Loss: 4013.315673828125\n",
      "Epoch 3705, Train_Loss: 3890.814208984375, Val_Loss: 4013.298828125\n",
      "Epoch 3706, Train_Loss: 3890.7939453125, Val_Loss: 4013.279296875\n",
      "Epoch 3707, Train_Loss: 3890.747314453125, Val_Loss: 4013.18994140625\n",
      "Epoch 3708, Train_Loss: 3890.735107421875, Val_Loss: 4013.1748046875\n",
      "Epoch 3709, Train_Loss: 3890.705322265625, Val_Loss: 4013.160400390625\n",
      "Epoch 3710, Train_Loss: 3890.6875, Val_Loss: 4013.12890625\n",
      "Epoch 3711, Train_Loss: 3890.664794921875, Val_Loss: 4013.1201171875\n",
      "Epoch 3712, Train_Loss: 3890.6298828125, Val_Loss: 4013.091552734375\n",
      "Epoch 3713, Train_Loss: 3890.598388671875, Val_Loss: 4013.0712890625\n",
      "Epoch 3714, Train_Loss: 3890.590087890625, Val_Loss: 4013.047607421875\n",
      "Epoch 3715, Train_Loss: 3890.569091796875, Val_Loss: 4013.023681640625\n",
      "Epoch 3716, Train_Loss: 3890.514404296875, Val_Loss: 4012.98681640625\n",
      "Epoch 3717, Train_Loss: 3890.5078125, Val_Loss: 4012.935302734375\n",
      "Epoch 3718, Train_Loss: 3890.4814453125, Val_Loss: 4012.921142578125\n",
      "Epoch 3719, Train_Loss: 3890.454345703125, Val_Loss: 4012.897216796875\n",
      "Epoch 3720, Train_Loss: 3890.43603515625, Val_Loss: 4012.832763671875\n",
      "Epoch 3721, Train_Loss: 3890.41552734375, Val_Loss: 4012.817626953125\n",
      "Epoch 3722, Train_Loss: 3890.397705078125, Val_Loss: 4012.805908203125\n",
      "Epoch 3723, Train_Loss: 3890.39501953125, Val_Loss: 4012.780517578125\n",
      "Epoch 3724, Train_Loss: 3890.34521484375, Val_Loss: 4012.71044921875\n",
      "Epoch 3725, Train_Loss: 3890.332763671875, Val_Loss: 4012.697265625\n",
      "Epoch 3726, Train_Loss: 3890.2646484375, Val_Loss: 4012.64990234375\n",
      "Epoch 3727, Train_Loss: 3890.23828125, Val_Loss: 4012.651611328125\n",
      "Epoch 3728, Train_Loss: 3890.218505859375, Val_Loss: 4012.634765625\n",
      "Epoch 3729, Train_Loss: 3890.140869140625, Val_Loss: 4012.600341796875\n",
      "Epoch 3730, Train_Loss: 3890.1416015625, Val_Loss: 4012.573486328125\n",
      "Epoch 3731, Train_Loss: 3890.110595703125, Val_Loss: 4012.562744140625\n",
      "Epoch 3732, Train_Loss: 3890.109619140625, Val_Loss: 4012.540283203125\n",
      "Epoch 3733, Train_Loss: 3889.9130859375, Val_Loss: 4012.46630859375\n",
      "Epoch 3734, Train_Loss: 3889.97265625, Val_Loss: 4012.486083984375\n",
      "Epoch 3735, Train_Loss: 3889.896728515625, Val_Loss: 4012.42041015625\n",
      "Epoch 3736, Train_Loss: 3889.849609375, Val_Loss: 4012.42431640625\n",
      "Epoch 3737, Train_Loss: 3889.84765625, Val_Loss: 4012.337646484375\n",
      "Epoch 3738, Train_Loss: 3889.809814453125, Val_Loss: 4012.332763671875\n",
      "Epoch 3739, Train_Loss: 3889.803955078125, Val_Loss: 4012.314697265625\n",
      "Epoch 3740, Train_Loss: 3889.766357421875, Val_Loss: 4012.3115234375\n",
      "Epoch 3741, Train_Loss: 3889.7724609375, Val_Loss: 4012.286865234375\n",
      "Epoch 3742, Train_Loss: 3889.705078125, Val_Loss: 4012.211181640625\n",
      "Epoch 3743, Train_Loss: 3889.688720703125, Val_Loss: 4012.173095703125\n",
      "Epoch 3744, Train_Loss: 3889.666748046875, Val_Loss: 4012.164794921875\n",
      "Epoch 3745, Train_Loss: 3889.63330078125, Val_Loss: 4012.1484375\n",
      "Epoch 3746, Train_Loss: 3889.607421875, Val_Loss: 4012.120361328125\n",
      "Epoch 3747, Train_Loss: 3889.583984375, Val_Loss: 4012.106689453125\n",
      "Epoch 3748, Train_Loss: 3889.572509765625, Val_Loss: 4012.0771484375\n",
      "Epoch 3749, Train_Loss: 3889.54150390625, Val_Loss: 4012.072021484375\n",
      "Epoch 3750, Train_Loss: 3889.49072265625, Val_Loss: 4012.032470703125\n",
      "Epoch 3751, Train_Loss: 3889.489013671875, Val_Loss: 4011.998046875\n",
      "Epoch 3752, Train_Loss: 3889.444091796875, Val_Loss: 4011.999267578125\n",
      "Epoch 3753, Train_Loss: 3889.458740234375, Val_Loss: 4011.926025390625\n",
      "Epoch 3754, Train_Loss: 3889.41357421875, Val_Loss: 4011.916015625\n",
      "Epoch 3755, Train_Loss: 3889.39697265625, Val_Loss: 4011.84228515625\n",
      "Epoch 3756, Train_Loss: 3889.3779296875, Val_Loss: 4011.82080078125\n",
      "Epoch 3757, Train_Loss: 3889.34716796875, Val_Loss: 4011.820068359375\n",
      "Epoch 3758, Train_Loss: 3889.326171875, Val_Loss: 4011.81689453125\n",
      "Epoch 3759, Train_Loss: 3889.281005859375, Val_Loss: 4011.72412109375\n",
      "Epoch 3760, Train_Loss: 3889.25341796875, Val_Loss: 4011.702880859375\n",
      "Epoch 3761, Train_Loss: 3889.2548828125, Val_Loss: 4011.689697265625\n",
      "Epoch 3762, Train_Loss: 3889.209228515625, Val_Loss: 4011.6591796875\n",
      "Epoch 3763, Train_Loss: 3889.09765625, Val_Loss: 4011.629638671875\n",
      "Epoch 3764, Train_Loss: 3889.07470703125, Val_Loss: 4011.609130859375\n",
      "Epoch 3765, Train_Loss: 3889.06005859375, Val_Loss: 4011.602294921875\n",
      "Epoch 3766, Train_Loss: 3889.0556640625, Val_Loss: 4011.568115234375\n",
      "Epoch 3767, Train_Loss: 3889.0537109375, Val_Loss: 4011.5400390625\n",
      "Epoch 3768, Train_Loss: 3889.015869140625, Val_Loss: 4011.5263671875\n",
      "Epoch 3769, Train_Loss: 3888.998779296875, Val_Loss: 4011.514404296875\n",
      "Epoch 3770, Train_Loss: 3888.993408203125, Val_Loss: 4011.491943359375\n",
      "Epoch 3771, Train_Loss: 3888.94775390625, Val_Loss: 4011.436767578125\n",
      "Epoch 3772, Train_Loss: 3888.926513671875, Val_Loss: 4011.376708984375\n",
      "Epoch 3773, Train_Loss: 3888.8896484375, Val_Loss: 4011.369140625\n",
      "Epoch 3774, Train_Loss: 3888.8740234375, Val_Loss: 4011.343505859375\n",
      "Epoch 3775, Train_Loss: 3888.854736328125, Val_Loss: 4011.320068359375\n",
      "Epoch 3776, Train_Loss: 3888.736572265625, Val_Loss: 4011.249267578125\n",
      "Epoch 3777, Train_Loss: 3888.736083984375, Val_Loss: 4011.221923828125\n",
      "Epoch 3778, Train_Loss: 3888.705810546875, Val_Loss: 4011.204833984375\n",
      "Epoch 3779, Train_Loss: 3888.690673828125, Val_Loss: 4011.18798828125\n",
      "Epoch 3780, Train_Loss: 3888.649169921875, Val_Loss: 4011.145263671875\n",
      "Epoch 3781, Train_Loss: 3888.6240234375, Val_Loss: 4011.13037109375\n",
      "Epoch 3782, Train_Loss: 3888.615234375, Val_Loss: 4011.110107421875\n",
      "Epoch 3783, Train_Loss: 3888.587890625, Val_Loss: 4011.089111328125\n",
      "Epoch 3784, Train_Loss: 3888.540283203125, Val_Loss: 4011.05810546875\n",
      "Epoch 3785, Train_Loss: 3888.50732421875, Val_Loss: 4011.04150390625\n",
      "Epoch 3786, Train_Loss: 3888.512451171875, Val_Loss: 4011.014892578125\n",
      "Epoch 3787, Train_Loss: 3888.489501953125, Val_Loss: 4010.994384765625\n",
      "Epoch 3788, Train_Loss: 3888.44970703125, Val_Loss: 4010.986083984375\n",
      "Epoch 3789, Train_Loss: 3888.44482421875, Val_Loss: 4010.91845703125\n",
      "Epoch 3790, Train_Loss: 3888.396728515625, Val_Loss: 4010.869140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3791, Train_Loss: 3888.38818359375, Val_Loss: 4010.84326171875\n",
      "Epoch 3792, Train_Loss: 3888.35888671875, Val_Loss: 4010.837158203125\n",
      "Epoch 3793, Train_Loss: 3888.3359375, Val_Loss: 4010.74365234375\n",
      "Epoch 3794, Train_Loss: 3888.30224609375, Val_Loss: 4010.744873046875\n",
      "Epoch 3795, Train_Loss: 3888.30126953125, Val_Loss: 4010.7099609375\n",
      "Epoch 3796, Train_Loss: 3888.262939453125, Val_Loss: 4010.7080078125\n",
      "Epoch 3797, Train_Loss: 3887.984375, Val_Loss: 4010.669677734375\n",
      "Epoch 3798, Train_Loss: 3888.0, Val_Loss: 4010.662353515625\n",
      "Epoch 3799, Train_Loss: 3887.789306640625, Val_Loss: 4010.63720703125\n",
      "Epoch 3800, Train_Loss: 3887.794677734375, Val_Loss: 4010.597900390625\n",
      "Epoch 3801, Train_Loss: 3887.843994140625, Val_Loss: 4010.594482421875\n",
      "Epoch 3802, Train_Loss: 3887.636474609375, Val_Loss: 4010.5458984375\n",
      "Epoch 3803, Train_Loss: 3887.664794921875, Val_Loss: 4010.5380859375\n",
      "Epoch 3804, Train_Loss: 3887.640625, Val_Loss: 4010.529296875\n",
      "Epoch 3805, Train_Loss: 3887.574462890625, Val_Loss: 4010.435302734375\n",
      "Epoch 3806, Train_Loss: 3887.55517578125, Val_Loss: 4010.4248046875\n",
      "Epoch 3807, Train_Loss: 3887.524658203125, Val_Loss: 4010.439208984375\n",
      "Epoch 3808, Train_Loss: 3887.4921875, Val_Loss: 4010.403564453125\n",
      "Epoch 3809, Train_Loss: 3887.487548828125, Val_Loss: 4010.355712890625\n",
      "Epoch 3810, Train_Loss: 3887.41259765625, Val_Loss: 4010.268310546875\n",
      "Epoch 3811, Train_Loss: 3887.396484375, Val_Loss: 4010.26318359375\n",
      "Epoch 3812, Train_Loss: 3887.402587890625, Val_Loss: 4010.25\n",
      "Epoch 3813, Train_Loss: 3887.36962890625, Val_Loss: 4010.216796875\n",
      "Epoch 3814, Train_Loss: 3887.326171875, Val_Loss: 4010.191162109375\n",
      "Epoch 3815, Train_Loss: 3887.310546875, Val_Loss: 4010.171142578125\n",
      "Epoch 3816, Train_Loss: 3887.30615234375, Val_Loss: 4010.15673828125\n",
      "Epoch 3817, Train_Loss: 3887.2685546875, Val_Loss: 4010.14404296875\n",
      "Epoch 3818, Train_Loss: 3887.223876953125, Val_Loss: 4010.09716796875\n",
      "Epoch 3819, Train_Loss: 3887.227783203125, Val_Loss: 4010.07080078125\n",
      "Epoch 3820, Train_Loss: 3887.1748046875, Val_Loss: 4010.049560546875\n",
      "Epoch 3821, Train_Loss: 3887.15234375, Val_Loss: 4010.048095703125\n",
      "Epoch 3822, Train_Loss: 3887.15380859375, Val_Loss: 4009.971923828125\n",
      "Epoch 3823, Train_Loss: 3887.135009765625, Val_Loss: 4009.948486328125\n",
      "Epoch 3824, Train_Loss: 3887.093994140625, Val_Loss: 4009.930419921875\n",
      "Epoch 3825, Train_Loss: 3887.076171875, Val_Loss: 4009.919677734375\n",
      "Epoch 3826, Train_Loss: 3887.078857421875, Val_Loss: 4009.865478515625\n",
      "Epoch 3827, Train_Loss: 3887.035888671875, Val_Loss: 4009.785888671875\n",
      "Epoch 3828, Train_Loss: 3886.991943359375, Val_Loss: 4009.77490234375\n",
      "Epoch 3829, Train_Loss: 3887.0048828125, Val_Loss: 4009.743896484375\n",
      "Epoch 3830, Train_Loss: 3887.0029296875, Val_Loss: 4009.71875\n",
      "Epoch 3831, Train_Loss: 3886.8974609375, Val_Loss: 4009.7080078125\n",
      "Epoch 3832, Train_Loss: 3886.885009765625, Val_Loss: 4009.688720703125\n",
      "Epoch 3833, Train_Loss: 3886.884033203125, Val_Loss: 4009.6611328125\n",
      "Epoch 3834, Train_Loss: 3886.845703125, Val_Loss: 4009.658447265625\n",
      "Epoch 3835, Train_Loss: 3886.52490234375, Val_Loss: 4009.615966796875\n",
      "Epoch 3836, Train_Loss: 3886.4453125, Val_Loss: 4009.581298828125\n",
      "Epoch 3837, Train_Loss: 3886.4013671875, Val_Loss: 4009.577880859375\n",
      "Epoch 3838, Train_Loss: 3886.38134765625, Val_Loss: 4009.5556640625\n",
      "Epoch 3839, Train_Loss: 3886.3740234375, Val_Loss: 4009.483154296875\n",
      "Epoch 3840, Train_Loss: 3886.3359375, Val_Loss: 4009.474365234375\n",
      "Epoch 3841, Train_Loss: 3886.3212890625, Val_Loss: 4009.463134765625\n",
      "Epoch 3842, Train_Loss: 3886.304931640625, Val_Loss: 4009.4443359375\n",
      "Epoch 3843, Train_Loss: 3886.253662109375, Val_Loss: 4009.34912109375\n",
      "Epoch 3844, Train_Loss: 3886.23828125, Val_Loss: 4009.343505859375\n",
      "Epoch 3845, Train_Loss: 3886.204345703125, Val_Loss: 4009.28125\n",
      "Epoch 3846, Train_Loss: 3886.1923828125, Val_Loss: 4009.2607421875\n",
      "Epoch 3847, Train_Loss: 3886.14501953125, Val_Loss: 4009.234375\n",
      "Epoch 3848, Train_Loss: 3886.12109375, Val_Loss: 4009.212890625\n",
      "Epoch 3849, Train_Loss: 3886.098388671875, Val_Loss: 4009.197509765625\n",
      "Epoch 3850, Train_Loss: 3886.083740234375, Val_Loss: 4009.185302734375\n",
      "Epoch 3851, Train_Loss: 3886.06298828125, Val_Loss: 4009.166015625\n",
      "Epoch 3852, Train_Loss: 3886.018310546875, Val_Loss: 4009.132080078125\n",
      "Epoch 3853, Train_Loss: 3885.99365234375, Val_Loss: 4009.10888671875\n",
      "Epoch 3854, Train_Loss: 3885.987548828125, Val_Loss: 4009.098388671875\n",
      "Epoch 3855, Train_Loss: 3885.935546875, Val_Loss: 4009.078857421875\n",
      "Epoch 3856, Train_Loss: 3885.917236328125, Val_Loss: 4008.998291015625\n",
      "Epoch 3857, Train_Loss: 3885.89306640625, Val_Loss: 4008.997314453125\n",
      "Epoch 3858, Train_Loss: 3885.873779296875, Val_Loss: 4008.974853515625\n",
      "Epoch 3859, Train_Loss: 3885.87939453125, Val_Loss: 4008.96630859375\n",
      "Epoch 3860, Train_Loss: 3885.84912109375, Val_Loss: 4008.872802734375\n",
      "Epoch 3861, Train_Loss: 3885.8212890625, Val_Loss: 4008.853271484375\n",
      "Epoch 3862, Train_Loss: 3885.80517578125, Val_Loss: 4008.8388671875\n",
      "Epoch 3863, Train_Loss: 3885.79052734375, Val_Loss: 4008.787109375\n",
      "Epoch 3864, Train_Loss: 3885.75732421875, Val_Loss: 4008.77001953125\n",
      "Epoch 3865, Train_Loss: 3885.7265625, Val_Loss: 4008.751708984375\n",
      "Epoch 3866, Train_Loss: 3885.711181640625, Val_Loss: 4008.728759765625\n",
      "Epoch 3867, Train_Loss: 3885.725830078125, Val_Loss: 4008.695556640625\n",
      "Epoch 3868, Train_Loss: 3885.524169921875, Val_Loss: 4008.671142578125\n",
      "Epoch 3869, Train_Loss: 3885.505859375, Val_Loss: 4008.664306640625\n",
      "Epoch 3870, Train_Loss: 3885.52099609375, Val_Loss: 4008.623291015625\n",
      "Epoch 3871, Train_Loss: 3885.47509765625, Val_Loss: 4008.63330078125\n",
      "Epoch 3872, Train_Loss: 3885.466064453125, Val_Loss: 4008.5537109375\n",
      "Epoch 3873, Train_Loss: 3885.39892578125, Val_Loss: 4008.52490234375\n",
      "Epoch 3874, Train_Loss: 3885.367431640625, Val_Loss: 4008.514404296875\n",
      "Epoch 3875, Train_Loss: 3885.375, Val_Loss: 4008.483642578125\n",
      "Epoch 3876, Train_Loss: 3885.344482421875, Val_Loss: 4008.496826171875\n",
      "Epoch 3877, Train_Loss: 3885.2900390625, Val_Loss: 4008.397216796875\n",
      "Epoch 3878, Train_Loss: 3885.295654296875, Val_Loss: 4008.3759765625\n",
      "Epoch 3879, Train_Loss: 3885.28466796875, Val_Loss: 4008.349609375\n",
      "Epoch 3880, Train_Loss: 3885.252685546875, Val_Loss: 4008.3515625\n",
      "Epoch 3881, Train_Loss: 3885.212890625, Val_Loss: 4008.277587890625\n",
      "Epoch 3882, Train_Loss: 3885.19091796875, Val_Loss: 4008.3125\n",
      "Epoch 3883, Train_Loss: 3885.164306640625, Val_Loss: 4008.234375\n",
      "Epoch 3884, Train_Loss: 3885.122802734375, Val_Loss: 4008.237548828125\n",
      "Epoch 3885, Train_Loss: 3885.101318359375, Val_Loss: 4008.18994140625\n",
      "Epoch 3886, Train_Loss: 3885.07373046875, Val_Loss: 4008.15673828125\n",
      "Epoch 3887, Train_Loss: 3885.0693359375, Val_Loss: 4008.152099609375\n",
      "Epoch 3888, Train_Loss: 3885.034912109375, Val_Loss: 4008.137939453125\n",
      "Epoch 3889, Train_Loss: 3885.036865234375, Val_Loss: 4008.0439453125\n",
      "Epoch 3890, Train_Loss: 3884.964599609375, Val_Loss: 4008.0732421875\n",
      "Epoch 3891, Train_Loss: 3884.998046875, Val_Loss: 4008.006103515625\n",
      "Epoch 3892, Train_Loss: 3884.936279296875, Val_Loss: 4008.002685546875\n",
      "Epoch 3893, Train_Loss: 3884.949462890625, Val_Loss: 4007.90966796875\n",
      "Epoch 3894, Train_Loss: 3884.891357421875, Val_Loss: 4007.916748046875\n",
      "Epoch 3895, Train_Loss: 3884.89697265625, Val_Loss: 4007.87158203125\n",
      "Epoch 3896, Train_Loss: 3884.83154296875, Val_Loss: 4007.89208984375\n",
      "Epoch 3897, Train_Loss: 3884.843505859375, Val_Loss: 4007.77685546875\n",
      "Epoch 3898, Train_Loss: 3884.724609375, Val_Loss: 4007.85888671875\n",
      "Epoch 3899, Train_Loss: 3884.740478515625, Val_Loss: 4007.718505859375\n",
      "Epoch 3900, Train_Loss: 3884.747802734375, Val_Loss: 4007.849609375\n",
      "Epoch 3901, Train_Loss: 3884.69384765625, Val_Loss: 4007.6455078125\n",
      "Epoch 3902, Train_Loss: 3884.634033203125, Val_Loss: 4007.79248046875\n",
      "Epoch 3903, Train_Loss: 3884.640380859375, Val_Loss: 4007.58447265625\n",
      "Epoch 3904, Train_Loss: 3884.59375, Val_Loss: 4007.742919921875\n",
      "Epoch 3905, Train_Loss: 3884.568115234375, Val_Loss: 4007.651611328125\n",
      "Epoch 3906, Train_Loss: 3884.57373046875, Val_Loss: 4007.5595703125\n",
      "Epoch 3907, Train_Loss: 3884.539306640625, Val_Loss: 4007.54833984375\n",
      "Epoch 3908, Train_Loss: 3884.46044921875, Val_Loss: 4007.585693359375\n",
      "Epoch 3909, Train_Loss: 3884.496826171875, Val_Loss: 4007.513916015625\n",
      "Epoch 3910, Train_Loss: 3884.2939453125, Val_Loss: 4007.45068359375\n",
      "Epoch 3911, Train_Loss: 3884.317626953125, Val_Loss: 4007.4775390625\n",
      "Epoch 3912, Train_Loss: 3884.4140625, Val_Loss: 4007.399169921875\n",
      "Epoch 3913, Train_Loss: 3884.311767578125, Val_Loss: 4007.3984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3914, Train_Loss: 3884.279296875, Val_Loss: 4007.36962890625\n",
      "Epoch 3915, Train_Loss: 3884.339111328125, Val_Loss: 4007.273681640625\n",
      "Epoch 3916, Train_Loss: 3884.18017578125, Val_Loss: 4007.35205078125\n",
      "Epoch 3917, Train_Loss: 3884.169189453125, Val_Loss: 4007.343505859375\n",
      "Epoch 3918, Train_Loss: 3884.20703125, Val_Loss: 4007.21630859375\n",
      "Epoch 3919, Train_Loss: 3884.153076171875, Val_Loss: 4007.219482421875\n",
      "Epoch 3920, Train_Loss: 3884.0517578125, Val_Loss: 4007.301513671875\n",
      "Epoch 3921, Train_Loss: 3884.19482421875, Val_Loss: 4007.176513671875\n",
      "Epoch 3922, Train_Loss: 3884.1640625, Val_Loss: 4007.114501953125\n",
      "Epoch 3923, Train_Loss: 3884.048828125, Val_Loss: 4007.19091796875\n",
      "Epoch 3924, Train_Loss: 3884.1083984375, Val_Loss: 4007.068115234375\n",
      "Epoch 3925, Train_Loss: 3884.07177734375, Val_Loss: 4007.0537109375\n",
      "Epoch 3926, Train_Loss: 3884.0009765625, Val_Loss: 4007.030517578125\n",
      "Epoch 3927, Train_Loss: 3884.040283203125, Val_Loss: 4006.94921875\n",
      "Epoch 3928, Train_Loss: 3883.997802734375, Val_Loss: 4006.943115234375\n",
      "Epoch 3929, Train_Loss: 3884.019775390625, Val_Loss: 4006.901611328125\n",
      "Epoch 3930, Train_Loss: 3883.66162109375, Val_Loss: 4006.951171875\n",
      "Epoch 3931, Train_Loss: 3883.70703125, Val_Loss: 4006.88037109375\n",
      "Epoch 3932, Train_Loss: 3883.587158203125, Val_Loss: 4006.87890625\n",
      "Epoch 3933, Train_Loss: 3883.589111328125, Val_Loss: 4006.864501953125\n",
      "Epoch 3934, Train_Loss: 3883.64697265625, Val_Loss: 4006.822509765625\n",
      "Epoch 3935, Train_Loss: 3883.458251953125, Val_Loss: 4006.8037109375\n",
      "Epoch 3936, Train_Loss: 3883.431884765625, Val_Loss: 4006.787109375\n",
      "Epoch 3937, Train_Loss: 3883.399658203125, Val_Loss: 4006.74072265625\n",
      "Epoch 3938, Train_Loss: 3883.464599609375, Val_Loss: 4006.70068359375\n",
      "Epoch 3939, Train_Loss: 3883.33935546875, Val_Loss: 4006.65478515625\n",
      "Epoch 3940, Train_Loss: 3883.329833984375, Val_Loss: 4006.64404296875\n",
      "Epoch 3941, Train_Loss: 3883.338623046875, Val_Loss: 4006.615966796875\n",
      "Epoch 3942, Train_Loss: 3883.285400390625, Val_Loss: 4006.614013671875\n",
      "Epoch 3943, Train_Loss: 3883.230224609375, Val_Loss: 4006.53076171875\n",
      "Epoch 3944, Train_Loss: 3883.14599609375, Val_Loss: 4006.470458984375\n",
      "Epoch 3945, Train_Loss: 3883.216552734375, Val_Loss: 4006.498291015625\n",
      "Epoch 3946, Train_Loss: 3883.16064453125, Val_Loss: 4006.488037109375\n",
      "Epoch 3947, Train_Loss: 3883.08203125, Val_Loss: 4006.415283203125\n",
      "Epoch 3948, Train_Loss: 3882.963134765625, Val_Loss: 4006.421630859375\n",
      "Epoch 3949, Train_Loss: 3883.007568359375, Val_Loss: 4006.390380859375\n",
      "Epoch 3950, Train_Loss: 3883.034423828125, Val_Loss: 4006.35888671875\n",
      "Epoch 3951, Train_Loss: 3882.885009765625, Val_Loss: 4006.3544921875\n",
      "Epoch 3952, Train_Loss: 3882.950927734375, Val_Loss: 4006.31591796875\n",
      "Epoch 3953, Train_Loss: 3882.8798828125, Val_Loss: 4006.250732421875\n",
      "Epoch 3954, Train_Loss: 3882.8583984375, Val_Loss: 4006.30078125\n",
      "Epoch 3955, Train_Loss: 3882.864990234375, Val_Loss: 4006.182861328125\n",
      "Epoch 3956, Train_Loss: 3882.833251953125, Val_Loss: 4006.144287109375\n",
      "Epoch 3957, Train_Loss: 3882.80224609375, Val_Loss: 4006.20068359375\n",
      "Epoch 3958, Train_Loss: 3882.807373046875, Val_Loss: 4006.135986328125\n",
      "Epoch 3959, Train_Loss: 3882.75439453125, Val_Loss: 4006.02880859375\n",
      "Epoch 3960, Train_Loss: 3882.768798828125, Val_Loss: 4006.040771484375\n",
      "Epoch 3961, Train_Loss: 3882.740234375, Val_Loss: 4006.01953125\n",
      "Epoch 3962, Train_Loss: 3882.73291015625, Val_Loss: 4005.97802734375\n",
      "Epoch 3963, Train_Loss: 3882.65185546875, Val_Loss: 4005.983642578125\n",
      "Epoch 3964, Train_Loss: 3882.62548828125, Val_Loss: 4005.9580078125\n",
      "Epoch 3965, Train_Loss: 3882.59619140625, Val_Loss: 4005.89111328125\n",
      "Epoch 3966, Train_Loss: 3882.593017578125, Val_Loss: 4005.914794921875\n",
      "Epoch 3967, Train_Loss: 3882.27392578125, Val_Loss: 4005.8544921875\n",
      "Epoch 3968, Train_Loss: 3882.246337890625, Val_Loss: 4005.855712890625\n",
      "Epoch 3969, Train_Loss: 3882.1806640625, Val_Loss: 4005.82080078125\n",
      "Epoch 3970, Train_Loss: 3882.20654296875, Val_Loss: 4005.81689453125\n",
      "Epoch 3971, Train_Loss: 3882.162353515625, Val_Loss: 4005.693115234375\n",
      "Epoch 3972, Train_Loss: 3882.177001953125, Val_Loss: 4005.751708984375\n",
      "Epoch 3973, Train_Loss: 3882.1201171875, Val_Loss: 4005.66650390625\n",
      "Epoch 3974, Train_Loss: 3882.119873046875, Val_Loss: 4005.66650390625\n",
      "Epoch 3975, Train_Loss: 3881.975830078125, Val_Loss: 4005.562744140625\n",
      "Epoch 3976, Train_Loss: 3882.0576171875, Val_Loss: 4005.593994140625\n",
      "Epoch 3977, Train_Loss: 3881.897705078125, Val_Loss: 4005.44287109375\n",
      "Epoch 3978, Train_Loss: 3882.048095703125, Val_Loss: 4005.57080078125\n",
      "Epoch 3979, Train_Loss: 3881.830322265625, Val_Loss: 4005.3984375\n",
      "Epoch 3980, Train_Loss: 3881.97998046875, Val_Loss: 4005.537109375\n",
      "Epoch 3981, Train_Loss: 3881.921875, Val_Loss: 4005.29248046875\n",
      "Epoch 3982, Train_Loss: 3881.9501953125, Val_Loss: 4005.50634765625\n",
      "Epoch 3983, Train_Loss: 3881.73388671875, Val_Loss: 4005.314453125\n",
      "Epoch 3984, Train_Loss: 3881.71337890625, Val_Loss: 4005.3984375\n",
      "Epoch 3985, Train_Loss: 3881.72802734375, Val_Loss: 4005.364013671875\n",
      "Epoch 3986, Train_Loss: 3881.67724609375, Val_Loss: 4005.25390625\n",
      "Epoch 3987, Train_Loss: 3881.739990234375, Val_Loss: 4005.35205078125\n",
      "Epoch 3988, Train_Loss: 3881.707763671875, Val_Loss: 4005.2958984375\n",
      "Epoch 3989, Train_Loss: 3881.610107421875, Val_Loss: 4005.158447265625\n",
      "Epoch 3990, Train_Loss: 3881.65576171875, Val_Loss: 4005.20751953125\n",
      "Epoch 3991, Train_Loss: 3881.5693359375, Val_Loss: 4005.083251953125\n",
      "Epoch 3992, Train_Loss: 3881.624755859375, Val_Loss: 4005.1376953125\n",
      "Epoch 3993, Train_Loss: 3881.535888671875, Val_Loss: 4005.03125\n",
      "Epoch 3994, Train_Loss: 3881.52685546875, Val_Loss: 4005.0419921875\n",
      "Epoch 3995, Train_Loss: 3881.474853515625, Val_Loss: 4005.048095703125\n",
      "Epoch 3996, Train_Loss: 3881.44189453125, Val_Loss: 4005.02197265625\n",
      "Epoch 3997, Train_Loss: 3881.409912109375, Val_Loss: 4004.992919921875\n",
      "Epoch 3998, Train_Loss: 3881.410400390625, Val_Loss: 4005.003662109375\n",
      "Epoch 3999, Train_Loss: 3881.315185546875, Val_Loss: 4004.96435546875\n",
      "Epoch 4000, Train_Loss: 3881.223876953125, Val_Loss: 4004.904296875\n",
      "Epoch 4001, Train_Loss: 3881.19580078125, Val_Loss: 4004.93408203125\n",
      "Epoch 4002, Train_Loss: 3881.171142578125, Val_Loss: 4004.806396484375\n",
      "Epoch 4003, Train_Loss: 3881.215576171875, Val_Loss: 4004.90673828125\n",
      "Epoch 4004, Train_Loss: 3881.12353515625, Val_Loss: 4004.808837890625\n",
      "Epoch 4005, Train_Loss: 3881.1181640625, Val_Loss: 4004.716064453125\n",
      "Epoch 4006, Train_Loss: 3881.14697265625, Val_Loss: 4004.81396484375\n",
      "Epoch 4007, Train_Loss: 3881.06103515625, Val_Loss: 4004.717529296875\n",
      "Epoch 4008, Train_Loss: 3881.000732421875, Val_Loss: 4004.63525390625\n",
      "Epoch 4009, Train_Loss: 3880.994384765625, Val_Loss: 4004.635498046875\n",
      "Epoch 4010, Train_Loss: 3880.92333984375, Val_Loss: 4004.54638671875\n",
      "Epoch 4011, Train_Loss: 3881.041015625, Val_Loss: 4004.662841796875\n",
      "Epoch 4012, Train_Loss: 3880.986572265625, Val_Loss: 4004.5908203125\n",
      "Epoch 4013, Train_Loss: 3880.869384765625, Val_Loss: 4004.441162109375\n",
      "Epoch 4014, Train_Loss: 3880.864501953125, Val_Loss: 4004.535888671875\n",
      "Epoch 4015, Train_Loss: 3880.850341796875, Val_Loss: 4004.638427734375\n",
      "Epoch 4016, Train_Loss: 3880.799072265625, Val_Loss: 4004.47412109375\n",
      "Epoch 4017, Train_Loss: 3880.75244140625, Val_Loss: 4004.3671875\n",
      "Epoch 4018, Train_Loss: 3880.78271484375, Val_Loss: 4004.46435546875\n",
      "Epoch 4019, Train_Loss: 3880.726318359375, Val_Loss: 4004.421630859375\n",
      "Epoch 4020, Train_Loss: 3880.672119140625, Val_Loss: 4004.284912109375\n",
      "Epoch 4021, Train_Loss: 3880.6533203125, Val_Loss: 4004.33154296875\n",
      "Epoch 4022, Train_Loss: 3880.641357421875, Val_Loss: 4004.34765625\n",
      "Epoch 4023, Train_Loss: 3880.627685546875, Val_Loss: 4004.236328125\n",
      "Epoch 4024, Train_Loss: 3880.585693359375, Val_Loss: 4004.21044921875\n",
      "Epoch 4025, Train_Loss: 3880.646484375, Val_Loss: 4004.21875\n",
      "Epoch 4026, Train_Loss: 3880.54541015625, Val_Loss: 4004.104736328125\n",
      "Epoch 4027, Train_Loss: 3880.510986328125, Val_Loss: 4004.116455078125\n",
      "Epoch 4028, Train_Loss: 3880.48779296875, Val_Loss: 4004.185546875\n",
      "Epoch 4029, Train_Loss: 3880.416015625, Val_Loss: 4004.077880859375\n",
      "Epoch 4030, Train_Loss: 3880.396728515625, Val_Loss: 4004.011962890625\n",
      "Epoch 4031, Train_Loss: 3880.39404296875, Val_Loss: 4004.064453125\n",
      "Epoch 4032, Train_Loss: 3880.3017578125, Val_Loss: 4004.03759765625\n",
      "Epoch 4033, Train_Loss: 3880.240966796875, Val_Loss: 4003.963623046875\n",
      "Epoch 4034, Train_Loss: 3880.2587890625, Val_Loss: 4003.94189453125\n",
      "Epoch 4035, Train_Loss: 3880.24462890625, Val_Loss: 4003.96923828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4036, Train_Loss: 3880.171875, Val_Loss: 4003.85400390625\n",
      "Epoch 4037, Train_Loss: 3880.145263671875, Val_Loss: 4003.843505859375\n",
      "Epoch 4038, Train_Loss: 3880.140869140625, Val_Loss: 4003.8701171875\n",
      "Epoch 4039, Train_Loss: 3880.134765625, Val_Loss: 4003.853515625\n",
      "Epoch 4040, Train_Loss: 3880.018310546875, Val_Loss: 4003.743896484375\n",
      "Epoch 4041, Train_Loss: 3879.962890625, Val_Loss: 4003.674072265625\n",
      "Epoch 4042, Train_Loss: 3879.9560546875, Val_Loss: 4003.67236328125\n",
      "Epoch 4043, Train_Loss: 3879.94921875, Val_Loss: 4003.70166015625\n",
      "Epoch 4044, Train_Loss: 3879.939208984375, Val_Loss: 4003.634765625\n",
      "Epoch 4045, Train_Loss: 3879.860595703125, Val_Loss: 4003.603515625\n",
      "Epoch 4046, Train_Loss: 3879.85791015625, Val_Loss: 4003.595947265625\n",
      "Epoch 4047, Train_Loss: 3879.860595703125, Val_Loss: 4003.55322265625\n",
      "Epoch 4048, Train_Loss: 3879.74951171875, Val_Loss: 4003.5283203125\n",
      "Epoch 4049, Train_Loss: 3879.731201171875, Val_Loss: 4003.50634765625\n",
      "Epoch 4050, Train_Loss: 3879.7978515625, Val_Loss: 4003.508056640625\n",
      "Epoch 4051, Train_Loss: 3879.766357421875, Val_Loss: 4003.4443359375\n",
      "Epoch 4052, Train_Loss: 3879.654296875, Val_Loss: 4003.416015625\n",
      "Epoch 4053, Train_Loss: 3879.645751953125, Val_Loss: 4003.388427734375\n",
      "Epoch 4054, Train_Loss: 3879.608642578125, Val_Loss: 4003.38720703125\n",
      "Epoch 4055, Train_Loss: 3879.622802734375, Val_Loss: 4003.30322265625\n",
      "Epoch 4056, Train_Loss: 3879.645263671875, Val_Loss: 4003.261962890625\n",
      "Epoch 4057, Train_Loss: 3879.623779296875, Val_Loss: 4003.2431640625\n",
      "Epoch 4058, Train_Loss: 3879.595947265625, Val_Loss: 4003.24609375\n",
      "Epoch 4059, Train_Loss: 3879.29931640625, Val_Loss: 4003.1708984375\n",
      "Epoch 4060, Train_Loss: 3879.302001953125, Val_Loss: 4003.14599609375\n",
      "Epoch 4061, Train_Loss: 3879.264404296875, Val_Loss: 4003.134033203125\n",
      "Epoch 4062, Train_Loss: 3879.259521484375, Val_Loss: 4003.13720703125\n",
      "Epoch 4063, Train_Loss: 3879.1337890625, Val_Loss: 4003.08447265625\n",
      "Epoch 4064, Train_Loss: 3879.128662109375, Val_Loss: 4003.069580078125\n",
      "Epoch 4065, Train_Loss: 3879.092529296875, Val_Loss: 4003.0595703125\n",
      "Epoch 4066, Train_Loss: 3879.071533203125, Val_Loss: 4003.03955078125\n",
      "Epoch 4067, Train_Loss: 3879.0234375, Val_Loss: 4002.97509765625\n",
      "Epoch 4068, Train_Loss: 3878.993896484375, Val_Loss: 4002.94677734375\n",
      "Epoch 4069, Train_Loss: 3878.969482421875, Val_Loss: 4002.93603515625\n",
      "Epoch 4070, Train_Loss: 3878.963134765625, Val_Loss: 4002.924072265625\n",
      "Epoch 4071, Train_Loss: 3878.91064453125, Val_Loss: 4002.83447265625\n",
      "Epoch 4072, Train_Loss: 3878.90380859375, Val_Loss: 4002.81591796875\n",
      "Epoch 4073, Train_Loss: 3878.8740234375, Val_Loss: 4002.793212890625\n",
      "Epoch 4074, Train_Loss: 3878.864501953125, Val_Loss: 4002.7763671875\n",
      "Epoch 4075, Train_Loss: 3878.8310546875, Val_Loss: 4002.752685546875\n",
      "Epoch 4076, Train_Loss: 3878.79443359375, Val_Loss: 4002.69970703125\n",
      "Epoch 4077, Train_Loss: 3878.7822265625, Val_Loss: 4002.684326171875\n",
      "Epoch 4078, Train_Loss: 3878.769775390625, Val_Loss: 4002.664306640625\n",
      "Epoch 4079, Train_Loss: 3878.715087890625, Val_Loss: 4002.613525390625\n",
      "Epoch 4080, Train_Loss: 3878.697998046875, Val_Loss: 4002.6044921875\n",
      "Epoch 4081, Train_Loss: 3878.6806640625, Val_Loss: 4002.576416015625\n",
      "Epoch 4082, Train_Loss: 3878.66015625, Val_Loss: 4002.5546875\n",
      "Epoch 4083, Train_Loss: 3878.6318359375, Val_Loss: 4002.48876953125\n",
      "Epoch 4084, Train_Loss: 3878.4658203125, Val_Loss: 4002.46630859375\n",
      "Epoch 4085, Train_Loss: 3878.471435546875, Val_Loss: 4002.44189453125\n",
      "Epoch 4086, Train_Loss: 3878.4375, Val_Loss: 4002.4384765625\n",
      "Epoch 4087, Train_Loss: 3878.405517578125, Val_Loss: 4002.34912109375\n",
      "Epoch 4088, Train_Loss: 3878.4296875, Val_Loss: 4002.314453125\n",
      "Epoch 4089, Train_Loss: 3878.388671875, Val_Loss: 4002.3095703125\n",
      "Epoch 4090, Train_Loss: 3878.3857421875, Val_Loss: 4002.24169921875\n",
      "Epoch 4091, Train_Loss: 3878.27978515625, Val_Loss: 4002.232421875\n",
      "Epoch 4092, Train_Loss: 3878.30029296875, Val_Loss: 4002.200439453125\n",
      "Epoch 4093, Train_Loss: 3878.218505859375, Val_Loss: 4002.198486328125\n",
      "Epoch 4094, Train_Loss: 3878.289794921875, Val_Loss: 4002.158447265625\n",
      "Epoch 4095, Train_Loss: 3877.923583984375, Val_Loss: 4002.1455078125\n",
      "Epoch 4096, Train_Loss: 3877.939697265625, Val_Loss: 4002.09912109375\n",
      "Epoch 4097, Train_Loss: 3877.8837890625, Val_Loss: 4002.118896484375\n",
      "Epoch 4098, Train_Loss: 3877.883544921875, Val_Loss: 4002.01611328125\n",
      "Epoch 4099, Train_Loss: 3877.83642578125, Val_Loss: 4002.051513671875\n",
      "Epoch 4100, Train_Loss: 3877.8779296875, Val_Loss: 4001.9130859375\n",
      "Epoch 4101, Train_Loss: 3877.781494140625, Val_Loss: 4002.045654296875\n",
      "Epoch 4102, Train_Loss: 3877.7578125, Val_Loss: 4001.819580078125\n",
      "Epoch 4103, Train_Loss: 3877.7001953125, Val_Loss: 4001.912109375\n",
      "Epoch 4104, Train_Loss: 3877.7783203125, Val_Loss: 4001.798828125\n",
      "Epoch 4105, Train_Loss: 3877.695556640625, Val_Loss: 4001.85595703125\n",
      "Epoch 4106, Train_Loss: 3877.674072265625, Val_Loss: 4001.76708984375\n",
      "Epoch 4107, Train_Loss: 3877.639892578125, Val_Loss: 4001.758056640625\n",
      "Epoch 4108, Train_Loss: 3877.622314453125, Val_Loss: 4001.73486328125\n",
      "Epoch 4109, Train_Loss: 3877.573974609375, Val_Loss: 4001.792724609375\n",
      "Epoch 4110, Train_Loss: 3877.528564453125, Val_Loss: 4001.63232421875\n",
      "Epoch 4111, Train_Loss: 3877.471923828125, Val_Loss: 4001.793212890625\n",
      "Epoch 4112, Train_Loss: 3877.51220703125, Val_Loss: 4001.64404296875\n",
      "Epoch 4113, Train_Loss: 3877.42822265625, Val_Loss: 4001.618408203125\n",
      "Epoch 4114, Train_Loss: 3877.45361328125, Val_Loss: 4001.60791015625\n",
      "Epoch 4115, Train_Loss: 3877.4326171875, Val_Loss: 4001.49609375\n",
      "Epoch 4116, Train_Loss: 3877.386474609375, Val_Loss: 4001.5791015625\n",
      "Epoch 4117, Train_Loss: 3877.36767578125, Val_Loss: 4001.56005859375\n",
      "Epoch 4118, Train_Loss: 3877.28857421875, Val_Loss: 4001.424072265625\n",
      "Epoch 4119, Train_Loss: 3877.3623046875, Val_Loss: 4001.388916015625\n",
      "Epoch 4120, Train_Loss: 3877.3203125, Val_Loss: 4001.447998046875\n",
      "Epoch 4121, Train_Loss: 3877.23974609375, Val_Loss: 4001.365966796875\n",
      "Epoch 4122, Train_Loss: 3877.21826171875, Val_Loss: 4001.345703125\n",
      "Epoch 4123, Train_Loss: 3877.158447265625, Val_Loss: 4001.370361328125\n",
      "Epoch 4124, Train_Loss: 3877.173828125, Val_Loss: 4001.26708984375\n",
      "Epoch 4125, Train_Loss: 3877.1611328125, Val_Loss: 4001.318115234375\n",
      "Epoch 4126, Train_Loss: 3876.9833984375, Val_Loss: 4001.217529296875\n",
      "Epoch 4127, Train_Loss: 3876.9453125, Val_Loss: 4001.267578125\n",
      "Epoch 4128, Train_Loss: 3876.933837890625, Val_Loss: 4001.2119140625\n",
      "Epoch 4129, Train_Loss: 3876.922607421875, Val_Loss: 4001.160400390625\n",
      "Epoch 4130, Train_Loss: 3876.89501953125, Val_Loss: 4001.13232421875\n",
      "Epoch 4131, Train_Loss: 3876.862548828125, Val_Loss: 4001.10595703125\n",
      "Epoch 4132, Train_Loss: 3876.841552734375, Val_Loss: 4001.07568359375\n",
      "Epoch 4133, Train_Loss: 3876.813232421875, Val_Loss: 4001.097900390625\n",
      "Epoch 4134, Train_Loss: 3876.74462890625, Val_Loss: 4000.976318359375\n",
      "Epoch 4135, Train_Loss: 3876.742919921875, Val_Loss: 4000.961669921875\n",
      "Epoch 4136, Train_Loss: 3876.73779296875, Val_Loss: 4000.97412109375\n",
      "Epoch 4137, Train_Loss: 3876.72314453125, Val_Loss: 4000.954345703125\n",
      "Epoch 4138, Train_Loss: 3876.73681640625, Val_Loss: 4000.867919921875\n",
      "Epoch 4139, Train_Loss: 3876.66064453125, Val_Loss: 4000.865234375\n",
      "Epoch 4140, Train_Loss: 3876.607177734375, Val_Loss: 4000.90673828125\n",
      "Epoch 4141, Train_Loss: 3876.57421875, Val_Loss: 4000.87451171875\n",
      "Epoch 4142, Train_Loss: 3876.5595703125, Val_Loss: 4000.793212890625\n",
      "Epoch 4143, Train_Loss: 3876.583251953125, Val_Loss: 4000.72314453125\n",
      "Epoch 4144, Train_Loss: 3876.50732421875, Val_Loss: 4000.81201171875\n",
      "Epoch 4145, Train_Loss: 3876.483642578125, Val_Loss: 4000.78515625\n",
      "Epoch 4146, Train_Loss: 3876.491943359375, Val_Loss: 4000.63720703125\n",
      "Epoch 4147, Train_Loss: 3876.453857421875, Val_Loss: 4000.6376953125\n",
      "Epoch 4148, Train_Loss: 3876.4462890625, Val_Loss: 4000.62646484375\n",
      "Epoch 4149, Train_Loss: 3876.431884765625, Val_Loss: 4000.603515625\n",
      "Epoch 4150, Train_Loss: 3876.370361328125, Val_Loss: 4000.5224609375\n",
      "Epoch 4151, Train_Loss: 3876.37158203125, Val_Loss: 4000.514892578125\n",
      "Epoch 4152, Train_Loss: 3876.34326171875, Val_Loss: 4000.488037109375\n",
      "Epoch 4153, Train_Loss: 3876.189697265625, Val_Loss: 4000.45361328125\n",
      "Epoch 4154, Train_Loss: 3876.196533203125, Val_Loss: 4000.4384765625\n",
      "Epoch 4155, Train_Loss: 3876.182861328125, Val_Loss: 4000.419921875\n",
      "Epoch 4156, Train_Loss: 3876.17822265625, Val_Loss: 4000.403564453125\n",
      "Epoch 4157, Train_Loss: 3876.097900390625, Val_Loss: 4000.361572265625\n",
      "Epoch 4158, Train_Loss: 3876.08642578125, Val_Loss: 4000.344482421875\n",
      "Epoch 4159, Train_Loss: 3876.05078125, Val_Loss: 4000.326416015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4160, Train_Loss: 3876.054931640625, Val_Loss: 4000.305908203125\n",
      "Epoch 4161, Train_Loss: 3875.996337890625, Val_Loss: 4000.23828125\n",
      "Epoch 4162, Train_Loss: 3875.9345703125, Val_Loss: 4000.18798828125\n",
      "Epoch 4163, Train_Loss: 3875.9365234375, Val_Loss: 4000.171142578125\n",
      "Epoch 4164, Train_Loss: 3875.89208984375, Val_Loss: 4000.16162109375\n",
      "Epoch 4165, Train_Loss: 3875.781982421875, Val_Loss: 4000.08154296875\n",
      "Epoch 4166, Train_Loss: 3875.77099609375, Val_Loss: 4000.07275390625\n",
      "Epoch 4167, Train_Loss: 3875.744384765625, Val_Loss: 4000.04638671875\n",
      "Epoch 4168, Train_Loss: 3875.739990234375, Val_Loss: 4000.032470703125\n",
      "Epoch 4169, Train_Loss: 3875.722900390625, Val_Loss: 3999.99072265625\n",
      "Epoch 4170, Train_Loss: 3875.672607421875, Val_Loss: 3999.9873046875\n",
      "Epoch 4171, Train_Loss: 3875.688720703125, Val_Loss: 3999.949951171875\n",
      "Epoch 4172, Train_Loss: 3875.636962890625, Val_Loss: 3999.9443359375\n",
      "Epoch 4173, Train_Loss: 3875.617919921875, Val_Loss: 3999.90283203125\n",
      "Epoch 4174, Train_Loss: 3875.629150390625, Val_Loss: 3999.88037109375\n",
      "Epoch 4175, Train_Loss: 3875.5986328125, Val_Loss: 3999.846435546875\n",
      "Epoch 4176, Train_Loss: 3875.55029296875, Val_Loss: 3999.855712890625\n",
      "Epoch 4177, Train_Loss: 3875.52001953125, Val_Loss: 3999.78515625\n",
      "Epoch 4178, Train_Loss: 3875.531005859375, Val_Loss: 3999.71240234375\n",
      "Epoch 4179, Train_Loss: 3875.509521484375, Val_Loss: 3999.686279296875\n",
      "Epoch 4180, Train_Loss: 3875.435546875, Val_Loss: 3999.638427734375\n",
      "Epoch 4181, Train_Loss: 3875.41748046875, Val_Loss: 3999.616455078125\n",
      "Epoch 4182, Train_Loss: 3875.31982421875, Val_Loss: 3999.568115234375\n",
      "Epoch 4183, Train_Loss: 3875.415283203125, Val_Loss: 3999.55322265625\n",
      "Epoch 4184, Train_Loss: 3875.0908203125, Val_Loss: 3999.59228515625\n",
      "Epoch 4185, Train_Loss: 3875.009033203125, Val_Loss: 3999.49951171875\n",
      "Epoch 4186, Train_Loss: 3874.978759765625, Val_Loss: 3999.486083984375\n",
      "Epoch 4187, Train_Loss: 3875.083984375, Val_Loss: 3999.502685546875\n",
      "Epoch 4188, Train_Loss: 3874.920654296875, Val_Loss: 3999.4130859375\n",
      "Epoch 4189, Train_Loss: 3874.906494140625, Val_Loss: 3999.40869140625\n",
      "Epoch 4190, Train_Loss: 3874.884033203125, Val_Loss: 3999.416748046875\n",
      "Epoch 4191, Train_Loss: 3874.865478515625, Val_Loss: 3999.333251953125\n",
      "Epoch 4192, Train_Loss: 3874.80419921875, Val_Loss: 3999.26953125\n",
      "Epoch 4193, Train_Loss: 3874.869384765625, Val_Loss: 3999.326904296875\n",
      "Epoch 4194, Train_Loss: 3874.813232421875, Val_Loss: 3999.2119140625\n",
      "Epoch 4195, Train_Loss: 3874.792724609375, Val_Loss: 3999.319580078125\n",
      "Epoch 4196, Train_Loss: 3874.712158203125, Val_Loss: 3999.095947265625\n",
      "Epoch 4197, Train_Loss: 3874.713134765625, Val_Loss: 3999.227294921875\n",
      "Epoch 4198, Train_Loss: 3874.677490234375, Val_Loss: 3999.050048828125\n",
      "Epoch 4199, Train_Loss: 3874.7080078125, Val_Loss: 3999.200439453125\n",
      "Epoch 4200, Train_Loss: 3874.596923828125, Val_Loss: 3998.91845703125\n",
      "Epoch 4201, Train_Loss: 3874.634521484375, Val_Loss: 3999.115966796875\n",
      "Epoch 4202, Train_Loss: 3874.606201171875, Val_Loss: 3998.979248046875\n",
      "Epoch 4203, Train_Loss: 3874.56005859375, Val_Loss: 3998.9892578125\n",
      "Epoch 4204, Train_Loss: 3874.5234375, Val_Loss: 3998.8955078125\n",
      "Epoch 4205, Train_Loss: 3874.519287109375, Val_Loss: 3999.01171875\n",
      "Epoch 4206, Train_Loss: 3874.527099609375, Val_Loss: 3998.94287109375\n",
      "Epoch 4207, Train_Loss: 3874.46630859375, Val_Loss: 3998.81005859375\n",
      "Epoch 4208, Train_Loss: 3874.4521484375, Val_Loss: 3998.80029296875\n",
      "Epoch 4209, Train_Loss: 3874.41650390625, Val_Loss: 3998.899169921875\n",
      "Epoch 4210, Train_Loss: 3874.38671875, Val_Loss: 3998.781494140625\n",
      "Epoch 4211, Train_Loss: 3874.376708984375, Val_Loss: 3998.685302734375\n",
      "Epoch 4212, Train_Loss: 3874.3505859375, Val_Loss: 3998.68359375\n",
      "Epoch 4213, Train_Loss: 3874.35009765625, Val_Loss: 3998.69091796875\n",
      "Epoch 4214, Train_Loss: 3874.342041015625, Val_Loss: 3998.68408203125\n",
      "Epoch 4215, Train_Loss: 3874.22314453125, Val_Loss: 3998.626708984375\n",
      "Epoch 4216, Train_Loss: 3874.218505859375, Val_Loss: 3998.623291015625\n",
      "Epoch 4217, Train_Loss: 3874.240966796875, Val_Loss: 3998.613525390625\n",
      "Epoch 4218, Train_Loss: 3874.04833984375, Val_Loss: 3998.561279296875\n",
      "Epoch 4219, Train_Loss: 3873.90673828125, Val_Loss: 3998.55078125\n",
      "Epoch 4220, Train_Loss: 3873.882568359375, Val_Loss: 3998.547607421875\n",
      "Epoch 4221, Train_Loss: 3873.67919921875, Val_Loss: 3998.4951171875\n",
      "Epoch 4222, Train_Loss: 3873.66357421875, Val_Loss: 3998.488037109375\n",
      "Epoch 4223, Train_Loss: 3873.810302734375, Val_Loss: 3998.465576171875\n",
      "Epoch 4224, Train_Loss: 3873.614990234375, Val_Loss: 3998.412109375\n",
      "Epoch 4225, Train_Loss: 3873.6201171875, Val_Loss: 3998.375244140625\n",
      "Epoch 4226, Train_Loss: 3873.572021484375, Val_Loss: 3998.388916015625\n",
      "Epoch 4227, Train_Loss: 3873.580322265625, Val_Loss: 3998.302001953125\n",
      "Epoch 4228, Train_Loss: 3873.502685546875, Val_Loss: 3998.222412109375\n",
      "Epoch 4229, Train_Loss: 3873.494873046875, Val_Loss: 3998.207275390625\n",
      "Epoch 4230, Train_Loss: 3873.507568359375, Val_Loss: 3998.234375\n",
      "Epoch 4231, Train_Loss: 3873.4140625, Val_Loss: 3998.17919921875\n",
      "Epoch 4232, Train_Loss: 3873.42041015625, Val_Loss: 3998.15478515625\n",
      "Epoch 4233, Train_Loss: 3873.3837890625, Val_Loss: 3998.149169921875\n",
      "Epoch 4234, Train_Loss: 3873.341796875, Val_Loss: 3998.1171875\n",
      "Epoch 4235, Train_Loss: 3873.337646484375, Val_Loss: 3998.089599609375\n",
      "Epoch 4236, Train_Loss: 3873.322998046875, Val_Loss: 3998.069580078125\n",
      "Epoch 4237, Train_Loss: 3873.33447265625, Val_Loss: 3998.064697265625\n",
      "Epoch 4238, Train_Loss: 3873.2607421875, Val_Loss: 3997.9775390625\n",
      "Epoch 4239, Train_Loss: 3873.243408203125, Val_Loss: 3997.9599609375\n",
      "Epoch 4240, Train_Loss: 3873.21142578125, Val_Loss: 3997.9619140625\n",
      "Epoch 4241, Train_Loss: 3873.19580078125, Val_Loss: 3997.926513671875\n",
      "Epoch 4242, Train_Loss: 3873.18994140625, Val_Loss: 3997.8203125\n",
      "Epoch 4243, Train_Loss: 3873.157958984375, Val_Loss: 3997.8232421875\n",
      "Epoch 4244, Train_Loss: 3873.127197265625, Val_Loss: 3997.8046875\n",
      "Epoch 4245, Train_Loss: 3873.122802734375, Val_Loss: 3997.755615234375\n",
      "Epoch 4246, Train_Loss: 3873.083984375, Val_Loss: 3997.721923828125\n",
      "Epoch 4247, Train_Loss: 3873.05322265625, Val_Loss: 3997.71630859375\n",
      "Epoch 4248, Train_Loss: 3873.041259765625, Val_Loss: 3997.698486328125\n",
      "Epoch 4249, Train_Loss: 3872.869384765625, Val_Loss: 3997.676025390625\n",
      "Epoch 4250, Train_Loss: 3872.84521484375, Val_Loss: 3997.656494140625\n",
      "Epoch 4251, Train_Loss: 3872.840576171875, Val_Loss: 3997.6201171875\n",
      "Epoch 4252, Train_Loss: 3872.81640625, Val_Loss: 3997.61669921875\n",
      "Epoch 4253, Train_Loss: 3872.791748046875, Val_Loss: 3997.55908203125\n",
      "Epoch 4254, Train_Loss: 3872.70068359375, Val_Loss: 3997.5087890625\n",
      "Epoch 4255, Train_Loss: 3872.678955078125, Val_Loss: 3997.490478515625\n",
      "Epoch 4256, Train_Loss: 3872.657470703125, Val_Loss: 3997.50830078125\n",
      "Epoch 4257, Train_Loss: 3872.605224609375, Val_Loss: 3997.38916015625\n",
      "Epoch 4258, Train_Loss: 3872.587890625, Val_Loss: 3997.367919921875\n",
      "Epoch 4259, Train_Loss: 3872.563232421875, Val_Loss: 3997.376708984375\n",
      "Epoch 4260, Train_Loss: 3872.506591796875, Val_Loss: 3997.3115234375\n",
      "Epoch 4261, Train_Loss: 3872.494140625, Val_Loss: 3997.275634765625\n",
      "Epoch 4262, Train_Loss: 3872.48095703125, Val_Loss: 3997.251953125\n",
      "Epoch 4263, Train_Loss: 3872.42919921875, Val_Loss: 3997.226318359375\n",
      "Epoch 4264, Train_Loss: 3872.4375, Val_Loss: 3997.2255859375\n",
      "Epoch 4265, Train_Loss: 3872.360595703125, Val_Loss: 3997.191162109375\n",
      "Epoch 4266, Train_Loss: 3872.3291015625, Val_Loss: 3997.15283203125\n",
      "Epoch 4267, Train_Loss: 3872.357421875, Val_Loss: 3997.1708984375\n",
      "Epoch 4268, Train_Loss: 3872.289306640625, Val_Loss: 3997.049560546875\n",
      "Epoch 4269, Train_Loss: 3872.305419921875, Val_Loss: 3997.093505859375\n",
      "Epoch 4270, Train_Loss: 3872.246337890625, Val_Loss: 3997.021240234375\n",
      "Epoch 4271, Train_Loss: 3872.24365234375, Val_Loss: 3997.058837890625\n",
      "Epoch 4272, Train_Loss: 3872.261962890625, Val_Loss: 3996.864501953125\n",
      "Epoch 4273, Train_Loss: 3872.202392578125, Val_Loss: 3997.018310546875\n",
      "Epoch 4274, Train_Loss: 3872.2294921875, Val_Loss: 3996.789306640625\n",
      "Epoch 4275, Train_Loss: 3872.153564453125, Val_Loss: 3996.93115234375\n",
      "Epoch 4276, Train_Loss: 3872.1083984375, Val_Loss: 3996.792724609375\n",
      "Epoch 4277, Train_Loss: 3872.05712890625, Val_Loss: 3996.80712890625\n",
      "Epoch 4278, Train_Loss: 3872.08740234375, Val_Loss: 3996.728759765625\n",
      "Epoch 4279, Train_Loss: 3872.044921875, Val_Loss: 3996.794921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4280, Train_Loss: 3871.90087890625, Val_Loss: 3996.69970703125\n",
      "Epoch 4281, Train_Loss: 3871.90771484375, Val_Loss: 3996.74609375\n",
      "Epoch 4282, Train_Loss: 3871.859130859375, Val_Loss: 3996.669921875\n",
      "Epoch 4283, Train_Loss: 3871.80810546875, Val_Loss: 3996.671142578125\n",
      "Epoch 4284, Train_Loss: 3871.916259765625, Val_Loss: 3996.753173828125\n",
      "Epoch 4285, Train_Loss: 3871.774658203125, Val_Loss: 3996.587158203125\n",
      "Epoch 4286, Train_Loss: 3871.732177734375, Val_Loss: 3996.580322265625\n",
      "Epoch 4287, Train_Loss: 3871.626953125, Val_Loss: 3996.51123046875\n",
      "Epoch 4288, Train_Loss: 3871.601806640625, Val_Loss: 3996.481689453125\n",
      "Epoch 4289, Train_Loss: 3871.580810546875, Val_Loss: 3996.5048828125\n",
      "Epoch 4290, Train_Loss: 3871.544189453125, Val_Loss: 3996.4951171875\n",
      "Epoch 4291, Train_Loss: 3871.595947265625, Val_Loss: 3996.349609375\n",
      "Epoch 4292, Train_Loss: 3871.510498046875, Val_Loss: 3996.457275390625\n",
      "Epoch 4293, Train_Loss: 3871.508056640625, Val_Loss: 3996.4404296875\n",
      "Epoch 4294, Train_Loss: 3871.5107421875, Val_Loss: 3996.30810546875\n",
      "Epoch 4295, Train_Loss: 3871.424072265625, Val_Loss: 3996.363525390625\n",
      "Epoch 4296, Train_Loss: 3871.4326171875, Val_Loss: 3996.25390625\n",
      "Epoch 4297, Train_Loss: 3871.406982421875, Val_Loss: 3996.277587890625\n",
      "Epoch 4298, Train_Loss: 3871.364990234375, Val_Loss: 3996.285888671875\n",
      "Epoch 4299, Train_Loss: 3871.364013671875, Val_Loss: 3996.197265625\n",
      "Epoch 4300, Train_Loss: 3871.3046875, Val_Loss: 3996.1572265625\n",
      "Epoch 4301, Train_Loss: 3871.2958984375, Val_Loss: 3996.16357421875\n",
      "Epoch 4302, Train_Loss: 3871.27734375, Val_Loss: 3996.181640625\n",
      "Epoch 4303, Train_Loss: 3871.249267578125, Val_Loss: 3996.069580078125\n",
      "Epoch 4304, Train_Loss: 3871.236572265625, Val_Loss: 3996.006103515625\n",
      "Epoch 4305, Train_Loss: 3871.20361328125, Val_Loss: 3996.041259765625\n",
      "Epoch 4306, Train_Loss: 3870.898681640625, Val_Loss: 3996.02685546875\n",
      "Epoch 4307, Train_Loss: 3870.886474609375, Val_Loss: 3995.99169921875\n",
      "Epoch 4308, Train_Loss: 3870.90478515625, Val_Loss: 3995.888427734375\n",
      "Epoch 4309, Train_Loss: 3870.893798828125, Val_Loss: 3995.92724609375\n",
      "Epoch 4310, Train_Loss: 3870.713134765625, Val_Loss: 3995.934814453125\n",
      "Epoch 4311, Train_Loss: 3870.715087890625, Val_Loss: 3995.914794921875\n",
      "Epoch 4312, Train_Loss: 3870.732421875, Val_Loss: 3995.82470703125\n",
      "Epoch 4313, Train_Loss: 3870.689453125, Val_Loss: 3995.821533203125\n",
      "Epoch 4314, Train_Loss: 3870.6318359375, Val_Loss: 3995.760009765625\n",
      "Epoch 4315, Train_Loss: 3870.6015625, Val_Loss: 3995.752685546875\n",
      "Epoch 4316, Train_Loss: 3870.598876953125, Val_Loss: 3995.735595703125\n",
      "Epoch 4317, Train_Loss: 3870.58740234375, Val_Loss: 3995.716064453125\n",
      "Epoch 4318, Train_Loss: 3870.561279296875, Val_Loss: 3995.614501953125\n",
      "Epoch 4319, Train_Loss: 3870.53857421875, Val_Loss: 3995.6201171875\n",
      "Epoch 4320, Train_Loss: 3870.474853515625, Val_Loss: 3995.595947265625\n",
      "Epoch 4321, Train_Loss: 3870.44384765625, Val_Loss: 3995.56884765625\n",
      "Epoch 4322, Train_Loss: 3870.427978515625, Val_Loss: 3995.539306640625\n",
      "Epoch 4323, Train_Loss: 3870.470947265625, Val_Loss: 3995.507080078125\n",
      "Epoch 4324, Train_Loss: 3870.447509765625, Val_Loss: 3995.50048828125\n",
      "Epoch 4325, Train_Loss: 3870.40966796875, Val_Loss: 3995.462890625\n",
      "Epoch 4326, Train_Loss: 3870.384033203125, Val_Loss: 3995.439697265625\n",
      "Epoch 4327, Train_Loss: 3870.373046875, Val_Loss: 3995.424072265625\n",
      "Epoch 4328, Train_Loss: 3870.3427734375, Val_Loss: 3995.366455078125\n",
      "Epoch 4329, Train_Loss: 3870.31396484375, Val_Loss: 3995.308349609375\n",
      "Epoch 4330, Train_Loss: 3870.289794921875, Val_Loss: 3995.290771484375\n",
      "Epoch 4331, Train_Loss: 3870.279296875, Val_Loss: 3995.268310546875\n",
      "Epoch 4332, Train_Loss: 3870.2578125, Val_Loss: 3995.275634765625\n",
      "Epoch 4333, Train_Loss: 3870.2216796875, Val_Loss: 3995.18310546875\n",
      "Epoch 4334, Train_Loss: 3870.20166015625, Val_Loss: 3995.166748046875\n",
      "Epoch 4335, Train_Loss: 3870.185546875, Val_Loss: 3995.14990234375\n",
      "Epoch 4336, Train_Loss: 3870.089599609375, Val_Loss: 3995.117919921875\n",
      "Epoch 4337, Train_Loss: 3870.066650390625, Val_Loss: 3995.1064453125\n",
      "Epoch 4338, Train_Loss: 3870.045166015625, Val_Loss: 3995.08642578125\n",
      "Epoch 4339, Train_Loss: 3870.038330078125, Val_Loss: 3995.058837890625\n",
      "Epoch 4340, Train_Loss: 3869.696044921875, Val_Loss: 3995.02392578125\n",
      "Epoch 4341, Train_Loss: 3869.686279296875, Val_Loss: 3995.001953125\n",
      "Epoch 4342, Train_Loss: 3869.685546875, Val_Loss: 3994.97119140625\n",
      "Epoch 4343, Train_Loss: 3869.650634765625, Val_Loss: 3994.972412109375\n",
      "Epoch 4344, Train_Loss: 3869.60400390625, Val_Loss: 3994.912353515625\n",
      "Epoch 4345, Train_Loss: 3869.593994140625, Val_Loss: 3994.853515625\n",
      "Epoch 4346, Train_Loss: 3869.600341796875, Val_Loss: 3994.830810546875\n",
      "Epoch 4347, Train_Loss: 3869.52685546875, Val_Loss: 3994.75244140625\n",
      "Epoch 4348, Train_Loss: 3869.5126953125, Val_Loss: 3994.740478515625\n",
      "Epoch 4349, Train_Loss: 3869.502197265625, Val_Loss: 3994.722412109375\n",
      "Epoch 4350, Train_Loss: 3869.484375, Val_Loss: 3994.694091796875\n",
      "Epoch 4351, Train_Loss: 3869.43994140625, Val_Loss: 3994.677978515625\n",
      "Epoch 4352, Train_Loss: 3869.2685546875, Val_Loss: 3994.64794921875\n",
      "Epoch 4353, Train_Loss: 3869.259521484375, Val_Loss: 3994.6220703125\n",
      "Epoch 4354, Train_Loss: 3869.2333984375, Val_Loss: 3994.607666015625\n",
      "Epoch 4355, Train_Loss: 3869.187744140625, Val_Loss: 3994.57470703125\n",
      "Epoch 4356, Train_Loss: 3869.18896484375, Val_Loss: 3994.546875\n",
      "Epoch 4357, Train_Loss: 3869.1669921875, Val_Loss: 3994.53759765625\n",
      "Epoch 4358, Train_Loss: 3869.14599609375, Val_Loss: 3994.4755859375\n",
      "Epoch 4359, Train_Loss: 3869.120361328125, Val_Loss: 3994.441650390625\n",
      "Epoch 4360, Train_Loss: 3869.085693359375, Val_Loss: 3994.38720703125\n",
      "Epoch 4361, Train_Loss: 3869.07568359375, Val_Loss: 3994.38916015625\n",
      "Epoch 4362, Train_Loss: 3869.033203125, Val_Loss: 3994.29931640625\n",
      "Epoch 4363, Train_Loss: 3869.03125, Val_Loss: 3994.283935546875\n",
      "Epoch 4364, Train_Loss: 3869.020751953125, Val_Loss: 3994.25830078125\n",
      "Epoch 4365, Train_Loss: 3868.9951171875, Val_Loss: 3994.246337890625\n",
      "Epoch 4366, Train_Loss: 3868.971435546875, Val_Loss: 3994.23681640625\n",
      "Epoch 4367, Train_Loss: 3868.955078125, Val_Loss: 3994.197509765625\n",
      "Epoch 4368, Train_Loss: 3868.92431640625, Val_Loss: 3994.166748046875\n",
      "Epoch 4369, Train_Loss: 3868.913330078125, Val_Loss: 3994.162353515625\n",
      "Epoch 4370, Train_Loss: 3868.726318359375, Val_Loss: 3994.129150390625\n",
      "Epoch 4371, Train_Loss: 3868.720458984375, Val_Loss: 3994.0908203125\n",
      "Epoch 4372, Train_Loss: 3868.758056640625, Val_Loss: 3994.056884765625\n",
      "Epoch 4373, Train_Loss: 3868.662353515625, Val_Loss: 3994.02490234375\n",
      "Epoch 4374, Train_Loss: 3868.648681640625, Val_Loss: 3994.00244140625\n",
      "Epoch 4375, Train_Loss: 3868.700439453125, Val_Loss: 3993.927490234375\n",
      "Epoch 4376, Train_Loss: 3868.6962890625, Val_Loss: 3993.927490234375\n",
      "Epoch 4377, Train_Loss: 3868.549072265625, Val_Loss: 3993.8759765625\n",
      "Epoch 4378, Train_Loss: 3868.60302734375, Val_Loss: 3993.830322265625\n",
      "Epoch 4379, Train_Loss: 3868.607177734375, Val_Loss: 3993.799560546875\n",
      "Epoch 4380, Train_Loss: 3868.49609375, Val_Loss: 3993.82275390625\n",
      "Epoch 4381, Train_Loss: 3868.44384765625, Val_Loss: 3993.78076171875\n",
      "Epoch 4382, Train_Loss: 3868.443359375, Val_Loss: 3993.728759765625\n",
      "Epoch 4383, Train_Loss: 3868.403564453125, Val_Loss: 3993.750732421875\n",
      "Epoch 4384, Train_Loss: 3868.37841796875, Val_Loss: 3993.731201171875\n",
      "Epoch 4385, Train_Loss: 3868.361083984375, Val_Loss: 3993.656005859375\n",
      "Epoch 4386, Train_Loss: 3868.3359375, Val_Loss: 3993.635498046875\n",
      "Epoch 4387, Train_Loss: 3868.255615234375, Val_Loss: 3993.673095703125\n",
      "Epoch 4388, Train_Loss: 3868.22998046875, Val_Loss: 3993.587646484375\n",
      "Epoch 4389, Train_Loss: 3868.256103515625, Val_Loss: 3993.529296875\n",
      "Epoch 4390, Train_Loss: 3868.226806640625, Val_Loss: 3993.518310546875\n",
      "Epoch 4391, Train_Loss: 3868.162353515625, Val_Loss: 3993.53271484375\n",
      "Epoch 4392, Train_Loss: 3868.09912109375, Val_Loss: 3993.4091796875\n",
      "Epoch 4393, Train_Loss: 3868.13720703125, Val_Loss: 3993.37548828125\n",
      "Epoch 4394, Train_Loss: 3868.04931640625, Val_Loss: 3993.370849609375\n",
      "Epoch 4395, Train_Loss: 3867.981201171875, Val_Loss: 3993.35888671875\n",
      "Epoch 4396, Train_Loss: 3867.954833984375, Val_Loss: 3993.3369140625\n",
      "Epoch 4397, Train_Loss: 3867.923583984375, Val_Loss: 3993.325927734375\n",
      "Epoch 4398, Train_Loss: 3867.940185546875, Val_Loss: 3993.3125\n",
      "Epoch 4399, Train_Loss: 3867.89111328125, Val_Loss: 3993.2587890625\n",
      "Epoch 4400, Train_Loss: 3867.80517578125, Val_Loss: 3993.24755859375\n",
      "Epoch 4401, Train_Loss: 3867.816162109375, Val_Loss: 3993.244873046875\n",
      "Epoch 4402, Train_Loss: 3867.822998046875, Val_Loss: 3993.189208984375\n",
      "Epoch 4403, Train_Loss: 3867.745849609375, Val_Loss: 3993.144775390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4404, Train_Loss: 3867.71142578125, Val_Loss: 3993.1298828125\n",
      "Epoch 4405, Train_Loss: 3867.731689453125, Val_Loss: 3993.098388671875\n",
      "Epoch 4406, Train_Loss: 3867.58447265625, Val_Loss: 3993.0224609375\n",
      "Epoch 4407, Train_Loss: 3867.6044921875, Val_Loss: 3992.998779296875\n",
      "Epoch 4408, Train_Loss: 3867.59326171875, Val_Loss: 3992.978759765625\n",
      "Epoch 4409, Train_Loss: 3867.57568359375, Val_Loss: 3992.925537109375\n",
      "Epoch 4410, Train_Loss: 3867.54052734375, Val_Loss: 3992.901123046875\n",
      "Epoch 4411, Train_Loss: 3867.516845703125, Val_Loss: 3992.88671875\n",
      "Epoch 4412, Train_Loss: 3867.4921875, Val_Loss: 3992.86474609375\n",
      "Epoch 4413, Train_Loss: 3867.443115234375, Val_Loss: 3992.8720703125\n",
      "Epoch 4414, Train_Loss: 3867.43505859375, Val_Loss: 3992.81689453125\n",
      "Epoch 4415, Train_Loss: 3867.36328125, Val_Loss: 3992.963134765625\n",
      "Epoch 4416, Train_Loss: 3867.43212890625, Val_Loss: 3992.638916015625\n",
      "Epoch 4417, Train_Loss: 3868.02783203125, Val_Loss: 3993.929931640625\n",
      "Epoch 4418, Train_Loss: 3867.594482421875, Val_Loss: 3992.54638671875\n",
      "Epoch 4419, Train_Loss: 3867.331298828125, Val_Loss: 3992.65087890625\n",
      "Epoch 4420, Train_Loss: 3867.8349609375, Val_Loss: 3993.743896484375\n",
      "Epoch 4421, Train_Loss: 3867.338134765625, Val_Loss: 3993.08349609375\n",
      "Epoch 4422, Train_Loss: 3867.26708984375, Val_Loss: 3992.280517578125\n",
      "Epoch 4423, Train_Loss: 3867.120849609375, Val_Loss: 3992.05712890625\n",
      "Epoch 4424, Train_Loss: 3867.10546875, Val_Loss: 3992.481689453125\n",
      "Epoch 4425, Train_Loss: 3866.976806640625, Val_Loss: 3992.627197265625\n",
      "Epoch 4426, Train_Loss: 3866.8330078125, Val_Loss: 3992.17236328125\n",
      "Epoch 4427, Train_Loss: 3866.922119140625, Val_Loss: 3991.951171875\n",
      "Epoch 4428, Train_Loss: 3866.75927734375, Val_Loss: 3991.983154296875\n",
      "Epoch 4429, Train_Loss: 3866.5869140625, Val_Loss: 3992.3720703125\n",
      "Epoch 4430, Train_Loss: 3866.527587890625, Val_Loss: 3992.3046875\n",
      "Epoch 4431, Train_Loss: 3866.555419921875, Val_Loss: 3992.165283203125\n",
      "Epoch 4432, Train_Loss: 3866.50927734375, Val_Loss: 3992.18408203125\n",
      "Epoch 4433, Train_Loss: 3866.49560546875, Val_Loss: 3992.55322265625\n",
      "Epoch 4434, Train_Loss: 3866.585693359375, Val_Loss: 3992.60791015625\n",
      "Epoch 4435, Train_Loss: 3866.3232421875, Val_Loss: 3992.08349609375\n",
      "Epoch 4436, Train_Loss: 3866.298583984375, Val_Loss: 3991.728759765625\n",
      "Epoch 4437, Train_Loss: 3866.361083984375, Val_Loss: 3991.6953125\n",
      "Epoch 4438, Train_Loss: 3866.28515625, Val_Loss: 3991.8896484375\n",
      "Epoch 4439, Train_Loss: 3866.327392578125, Val_Loss: 3991.924072265625\n",
      "Epoch 4440, Train_Loss: 3866.296142578125, Val_Loss: 3991.684326171875\n",
      "Epoch 4441, Train_Loss: 3866.281982421875, Val_Loss: 3991.5751953125\n",
      "Epoch 4442, Train_Loss: 3866.198486328125, Val_Loss: 3991.662353515625\n",
      "Epoch 4443, Train_Loss: 3866.166748046875, Val_Loss: 3991.8359375\n",
      "Epoch 4444, Train_Loss: 3866.196533203125, Val_Loss: 3991.99169921875\n",
      "Epoch 4445, Train_Loss: 3866.062255859375, Val_Loss: 3991.873291015625\n",
      "Epoch 4446, Train_Loss: 3866.0498046875, Val_Loss: 3991.70751953125\n",
      "Epoch 4447, Train_Loss: 3866.025146484375, Val_Loss: 3991.59326171875\n",
      "Epoch 4448, Train_Loss: 3865.994873046875, Val_Loss: 3991.647705078125\n",
      "Epoch 4449, Train_Loss: 3865.9794921875, Val_Loss: 3991.65966796875\n",
      "Epoch 4450, Train_Loss: 3865.982177734375, Val_Loss: 3991.5439453125\n",
      "Epoch 4451, Train_Loss: 3865.962890625, Val_Loss: 3991.333251953125\n",
      "Epoch 4452, Train_Loss: 3865.933349609375, Val_Loss: 3991.265625\n",
      "Epoch 4453, Train_Loss: 3865.982177734375, Val_Loss: 3991.32080078125\n",
      "Epoch 4454, Train_Loss: 3865.85107421875, Val_Loss: 3991.405517578125\n",
      "Epoch 4455, Train_Loss: 3865.68603515625, Val_Loss: 3991.4111328125\n",
      "Epoch 4456, Train_Loss: 3865.711181640625, Val_Loss: 3991.348388671875\n",
      "Epoch 4457, Train_Loss: 3865.707763671875, Val_Loss: 3991.289306640625\n",
      "Epoch 4458, Train_Loss: 3865.4130859375, Val_Loss: 3991.2724609375\n",
      "Epoch 4459, Train_Loss: 3865.29150390625, Val_Loss: 3991.285888671875\n",
      "Epoch 4460, Train_Loss: 3865.24658203125, Val_Loss: 3991.2451171875\n",
      "Epoch 4461, Train_Loss: 3865.263427734375, Val_Loss: 3991.109619140625\n",
      "Epoch 4462, Train_Loss: 3865.214599609375, Val_Loss: 3991.034423828125\n",
      "Epoch 4463, Train_Loss: 3865.28125, Val_Loss: 3991.075927734375\n",
      "Epoch 4464, Train_Loss: 3865.220458984375, Val_Loss: 3991.064453125\n",
      "Epoch 4465, Train_Loss: 3865.1962890625, Val_Loss: 3991.021484375\n",
      "Epoch 4466, Train_Loss: 3865.13330078125, Val_Loss: 3991.0068359375\n",
      "Epoch 4467, Train_Loss: 3865.149658203125, Val_Loss: 3990.94970703125\n",
      "Epoch 4468, Train_Loss: 3865.105712890625, Val_Loss: 3990.956298828125\n",
      "Epoch 4469, Train_Loss: 3865.091552734375, Val_Loss: 3990.948486328125\n",
      "Epoch 4470, Train_Loss: 3865.05322265625, Val_Loss: 3990.914794921875\n",
      "Epoch 4471, Train_Loss: 3864.99755859375, Val_Loss: 3990.864501953125\n",
      "Epoch 4472, Train_Loss: 3864.963134765625, Val_Loss: 3990.7900390625\n",
      "Epoch 4473, Train_Loss: 3864.9169921875, Val_Loss: 3990.786376953125\n",
      "Epoch 4474, Train_Loss: 3864.9541015625, Val_Loss: 3990.76953125\n",
      "Epoch 4475, Train_Loss: 3864.93310546875, Val_Loss: 3990.766845703125\n",
      "Epoch 4476, Train_Loss: 3864.91162109375, Val_Loss: 3990.6611328125\n",
      "Epoch 4477, Train_Loss: 3864.85986328125, Val_Loss: 3990.651123046875\n",
      "Epoch 4478, Train_Loss: 3864.83984375, Val_Loss: 3990.62548828125\n",
      "Epoch 4479, Train_Loss: 3864.80126953125, Val_Loss: 3990.544921875\n",
      "Epoch 4480, Train_Loss: 3864.79736328125, Val_Loss: 3990.541259765625\n",
      "Epoch 4481, Train_Loss: 3864.75927734375, Val_Loss: 3990.5087890625\n",
      "Epoch 4482, Train_Loss: 3864.73193359375, Val_Loss: 3990.476806640625\n",
      "Epoch 4483, Train_Loss: 3864.697509765625, Val_Loss: 3990.451904296875\n",
      "Epoch 4484, Train_Loss: 3864.692138671875, Val_Loss: 3990.43310546875\n",
      "Epoch 4485, Train_Loss: 3864.59521484375, Val_Loss: 3990.40478515625\n",
      "Epoch 4486, Train_Loss: 3864.42919921875, Val_Loss: 3990.36962890625\n",
      "Epoch 4487, Train_Loss: 3864.4375, Val_Loss: 3990.3583984375\n",
      "Epoch 4488, Train_Loss: 3864.38671875, Val_Loss: 3990.34912109375\n",
      "Epoch 4489, Train_Loss: 3864.346435546875, Val_Loss: 3990.33447265625\n",
      "Epoch 4490, Train_Loss: 3864.3359375, Val_Loss: 3990.250732421875\n",
      "Epoch 4491, Train_Loss: 3864.339599609375, Val_Loss: 3990.21923828125\n",
      "Epoch 4492, Train_Loss: 3864.30078125, Val_Loss: 3990.195556640625\n",
      "Epoch 4493, Train_Loss: 3864.261474609375, Val_Loss: 3990.1123046875\n",
      "Epoch 4494, Train_Loss: 3864.224853515625, Val_Loss: 3990.0703125\n",
      "Epoch 4495, Train_Loss: 3864.1875, Val_Loss: 3990.073486328125\n",
      "Epoch 4496, Train_Loss: 3864.185546875, Val_Loss: 3990.0419921875\n",
      "Epoch 4497, Train_Loss: 3864.159423828125, Val_Loss: 3990.02880859375\n",
      "Epoch 4498, Train_Loss: 3864.140869140625, Val_Loss: 3990.00244140625\n",
      "Epoch 4499, Train_Loss: 3864.105224609375, Val_Loss: 3989.96484375\n",
      "Epoch 4500, Train_Loss: 3864.046142578125, Val_Loss: 3989.930908203125\n",
      "Epoch 4501, Train_Loss: 3864.030517578125, Val_Loss: 3989.912109375\n",
      "Epoch 4502, Train_Loss: 3864.011962890625, Val_Loss: 3989.894775390625\n",
      "Epoch 4503, Train_Loss: 3864.0078125, Val_Loss: 3989.883544921875\n",
      "Epoch 4504, Train_Loss: 3863.9638671875, Val_Loss: 3989.808349609375\n",
      "Epoch 4505, Train_Loss: 3863.927734375, Val_Loss: 3989.810302734375\n",
      "Epoch 4506, Train_Loss: 3863.92138671875, Val_Loss: 3989.772705078125\n",
      "Epoch 4507, Train_Loss: 3863.8984375, Val_Loss: 3989.6953125\n",
      "Epoch 4508, Train_Loss: 3863.87646484375, Val_Loss: 3989.671142578125\n",
      "Epoch 4509, Train_Loss: 3863.82763671875, Val_Loss: 3989.6484375\n",
      "Epoch 4510, Train_Loss: 3863.809814453125, Val_Loss: 3989.634033203125\n",
      "Epoch 4511, Train_Loss: 3863.734375, Val_Loss: 3989.576416015625\n",
      "Epoch 4512, Train_Loss: 3863.739013671875, Val_Loss: 3989.5576171875\n",
      "Epoch 4513, Train_Loss: 3863.701416015625, Val_Loss: 3989.5439453125\n",
      "Epoch 4514, Train_Loss: 3863.624755859375, Val_Loss: 3989.513916015625\n",
      "Epoch 4515, Train_Loss: 3863.595947265625, Val_Loss: 3989.4755859375\n",
      "Epoch 4516, Train_Loss: 3863.602294921875, Val_Loss: 3989.458740234375\n",
      "Epoch 4517, Train_Loss: 3863.515869140625, Val_Loss: 3989.452392578125\n",
      "Epoch 4518, Train_Loss: 3863.500244140625, Val_Loss: 3989.379638671875\n",
      "Epoch 4519, Train_Loss: 3863.470703125, Val_Loss: 3989.34912109375\n",
      "Epoch 4520, Train_Loss: 3863.45556640625, Val_Loss: 3989.434326171875\n",
      "Epoch 4521, Train_Loss: 3863.4853515625, Val_Loss: 3989.337646484375\n",
      "Epoch 4522, Train_Loss: 3863.266357421875, Val_Loss: 3989.1787109375\n",
      "Epoch 4523, Train_Loss: 3863.266357421875, Val_Loss: 3989.388916015625\n",
      "Epoch 4524, Train_Loss: 3863.350341796875, Val_Loss: 3989.234375\n",
      "Epoch 4525, Train_Loss: 3863.242431640625, Val_Loss: 3989.044921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4526, Train_Loss: 3863.203125, Val_Loss: 3989.2216796875\n",
      "Epoch 4527, Train_Loss: 3863.1611328125, Val_Loss: 3989.2587890625\n",
      "Epoch 4528, Train_Loss: 3863.14697265625, Val_Loss: 3989.025634765625\n",
      "Epoch 4529, Train_Loss: 3863.21923828125, Val_Loss: 3989.0703125\n",
      "Epoch 4530, Train_Loss: 3863.18212890625, Val_Loss: 3989.0927734375\n",
      "Epoch 4531, Train_Loss: 3863.042236328125, Val_Loss: 3989.03759765625\n",
      "Epoch 4532, Train_Loss: 3863.044677734375, Val_Loss: 3989.185302734375\n",
      "Epoch 4533, Train_Loss: 3862.990234375, Val_Loss: 3988.94189453125\n",
      "Epoch 4534, Train_Loss: 3862.98876953125, Val_Loss: 3988.842041015625\n",
      "Epoch 4535, Train_Loss: 3862.99462890625, Val_Loss: 3989.01708984375\n",
      "Epoch 4536, Train_Loss: 3863.0478515625, Val_Loss: 3988.845947265625\n",
      "Epoch 4537, Train_Loss: 3862.93994140625, Val_Loss: 3988.687255859375\n",
      "Epoch 4538, Train_Loss: 3862.914306640625, Val_Loss: 3988.94921875\n",
      "Epoch 4539, Train_Loss: 3862.6669921875, Val_Loss: 3988.859619140625\n",
      "Epoch 4540, Train_Loss: 3862.63671875, Val_Loss: 3988.66845703125\n",
      "Epoch 4541, Train_Loss: 3862.62548828125, Val_Loss: 3988.8095703125\n",
      "Epoch 4542, Train_Loss: 3862.583251953125, Val_Loss: 3988.881591796875\n",
      "Epoch 4543, Train_Loss: 3862.411865234375, Val_Loss: 3988.673583984375\n",
      "Epoch 4544, Train_Loss: 3862.4873046875, Val_Loss: 3988.724853515625\n",
      "Epoch 4545, Train_Loss: 3862.446533203125, Val_Loss: 3988.665283203125\n",
      "Epoch 4546, Train_Loss: 3862.34326171875, Val_Loss: 3988.570068359375\n",
      "Epoch 4547, Train_Loss: 3862.335205078125, Val_Loss: 3988.69091796875\n",
      "Epoch 4548, Train_Loss: 3862.40234375, Val_Loss: 3988.537109375\n",
      "Epoch 4549, Train_Loss: 3862.248779296875, Val_Loss: 3988.423583984375\n",
      "Epoch 4550, Train_Loss: 3862.245849609375, Val_Loss: 3988.569580078125\n",
      "Epoch 4551, Train_Loss: 3862.263427734375, Val_Loss: 3988.488037109375\n",
      "Epoch 4552, Train_Loss: 3862.198974609375, Val_Loss: 3988.326904296875\n",
      "Epoch 4553, Train_Loss: 3862.19970703125, Val_Loss: 3988.452880859375\n",
      "Epoch 4554, Train_Loss: 3862.27197265625, Val_Loss: 3988.6376953125\n",
      "Epoch 4555, Train_Loss: 3862.08544921875, Val_Loss: 3988.286865234375\n",
      "Epoch 4556, Train_Loss: 3862.061279296875, Val_Loss: 3988.218017578125\n",
      "Epoch 4557, Train_Loss: 3862.2138671875, Val_Loss: 3988.522705078125\n",
      "Epoch 4558, Train_Loss: 3862.0654296875, Val_Loss: 3988.330078125\n",
      "Epoch 4559, Train_Loss: 3862.124267578125, Val_Loss: 3988.038818359375\n",
      "Epoch 4560, Train_Loss: 3862.003662109375, Val_Loss: 3988.129638671875\n",
      "Epoch 4561, Train_Loss: 3862.037841796875, Val_Loss: 3988.460693359375\n",
      "Epoch 4562, Train_Loss: 3862.010986328125, Val_Loss: 3988.18603515625\n",
      "Epoch 4563, Train_Loss: 3862.111083984375, Val_Loss: 3988.023193359375\n",
      "Epoch 4564, Train_Loss: 3861.92041015625, Val_Loss: 3988.116455078125\n",
      "Epoch 4565, Train_Loss: 3862.060302734375, Val_Loss: 3988.544921875\n",
      "Epoch 4566, Train_Loss: 3861.860595703125, Val_Loss: 3988.06640625\n",
      "Epoch 4567, Train_Loss: 3861.96484375, Val_Loss: 3987.85009765625\n",
      "Epoch 4568, Train_Loss: 3861.828125, Val_Loss: 3987.85791015625\n",
      "Epoch 4569, Train_Loss: 3861.92724609375, Val_Loss: 3988.28955078125\n",
      "Epoch 4570, Train_Loss: 3861.809814453125, Val_Loss: 3988.117919921875\n",
      "Epoch 4571, Train_Loss: 3861.845458984375, Val_Loss: 3987.688720703125\n",
      "Epoch 4572, Train_Loss: 3861.737548828125, Val_Loss: 3987.7060546875\n",
      "Epoch 4573, Train_Loss: 3861.495361328125, Val_Loss: 3988.0888671875\n",
      "Epoch 4574, Train_Loss: 3861.44091796875, Val_Loss: 3988.134765625\n",
      "Epoch 4575, Train_Loss: 3861.294189453125, Val_Loss: 3987.813232421875\n",
      "Epoch 4576, Train_Loss: 3861.386962890625, Val_Loss: 3987.741943359375\n",
      "Epoch 4577, Train_Loss: 3861.2744140625, Val_Loss: 3987.9052734375\n",
      "Epoch 4578, Train_Loss: 3861.416259765625, Val_Loss: 3988.248291015625\n",
      "Epoch 4579, Train_Loss: 3861.302001953125, Val_Loss: 3987.6943359375\n",
      "Epoch 4580, Train_Loss: 3861.23388671875, Val_Loss: 3987.431884765625\n",
      "Epoch 4581, Train_Loss: 3861.278564453125, Val_Loss: 3987.525634765625\n",
      "Epoch 4582, Train_Loss: 3861.2119140625, Val_Loss: 3987.627685546875\n",
      "Epoch 4583, Train_Loss: 3861.1611328125, Val_Loss: 3987.465087890625\n",
      "Epoch 4584, Train_Loss: 3861.119140625, Val_Loss: 3987.51708984375\n",
      "Epoch 4585, Train_Loss: 3861.1474609375, Val_Loss: 3987.69873046875\n",
      "Epoch 4586, Train_Loss: 3861.13818359375, Val_Loss: 3987.533203125\n",
      "Epoch 4587, Train_Loss: 3861.113525390625, Val_Loss: 3987.532470703125\n",
      "Epoch 4588, Train_Loss: 3861.0234375, Val_Loss: 3987.61669921875\n",
      "Epoch 4589, Train_Loss: 3861.049072265625, Val_Loss: 3987.593994140625\n",
      "Epoch 4590, Train_Loss: 3860.999755859375, Val_Loss: 3987.370361328125\n",
      "Epoch 4591, Train_Loss: 3860.990966796875, Val_Loss: 3987.344482421875\n",
      "Epoch 4592, Train_Loss: 3860.9951171875, Val_Loss: 3987.448486328125\n",
      "Epoch 4593, Train_Loss: 3860.95263671875, Val_Loss: 3987.292724609375\n",
      "Epoch 4594, Train_Loss: 3860.919677734375, Val_Loss: 3987.28271484375\n",
      "Epoch 4595, Train_Loss: 3860.89697265625, Val_Loss: 3987.3251953125\n",
      "Epoch 4596, Train_Loss: 3860.848388671875, Val_Loss: 3987.211181640625\n",
      "Epoch 4597, Train_Loss: 3860.850341796875, Val_Loss: 3987.1962890625\n",
      "Epoch 4598, Train_Loss: 3860.810546875, Val_Loss: 3987.29931640625\n",
      "Epoch 4599, Train_Loss: 3860.779052734375, Val_Loss: 3987.209716796875\n",
      "Epoch 4600, Train_Loss: 3860.759765625, Val_Loss: 3987.189208984375\n",
      "Epoch 4601, Train_Loss: 3860.75244140625, Val_Loss: 3987.215576171875\n",
      "Epoch 4602, Train_Loss: 3860.740478515625, Val_Loss: 3987.11962890625\n",
      "Epoch 4603, Train_Loss: 3860.571044921875, Val_Loss: 3987.072021484375\n",
      "Epoch 4604, Train_Loss: 3860.584228515625, Val_Loss: 3987.05517578125\n",
      "Epoch 4605, Train_Loss: 3860.56103515625, Val_Loss: 3986.986328125\n",
      "Epoch 4606, Train_Loss: 3860.562744140625, Val_Loss: 3987.02197265625\n",
      "Epoch 4607, Train_Loss: 3860.520263671875, Val_Loss: 3986.92236328125\n",
      "Epoch 4608, Train_Loss: 3860.4951171875, Val_Loss: 3986.932373046875\n",
      "Epoch 4609, Train_Loss: 3860.50341796875, Val_Loss: 3986.965087890625\n",
      "Epoch 4610, Train_Loss: 3860.43017578125, Val_Loss: 3986.9423828125\n",
      "Epoch 4611, Train_Loss: 3860.42236328125, Val_Loss: 3986.872314453125\n",
      "Epoch 4612, Train_Loss: 3860.399658203125, Val_Loss: 3986.839111328125\n",
      "Epoch 4613, Train_Loss: 3860.3740234375, Val_Loss: 3986.808837890625\n",
      "Epoch 4614, Train_Loss: 3860.3740234375, Val_Loss: 3986.8076171875\n",
      "Epoch 4615, Train_Loss: 3860.3369140625, Val_Loss: 3986.74755859375\n",
      "Epoch 4616, Train_Loss: 3860.302734375, Val_Loss: 3986.711181640625\n",
      "Epoch 4617, Train_Loss: 3860.275634765625, Val_Loss: 3986.69091796875\n",
      "Epoch 4618, Train_Loss: 3860.22900390625, Val_Loss: 3986.66162109375\n",
      "Epoch 4619, Train_Loss: 3860.2451171875, Val_Loss: 3986.676025390625\n",
      "Epoch 4620, Train_Loss: 3860.197998046875, Val_Loss: 3986.667236328125\n",
      "Epoch 4621, Train_Loss: 3860.2080078125, Val_Loss: 3986.684814453125\n",
      "Epoch 4622, Train_Loss: 3860.1923828125, Val_Loss: 3986.578369140625\n",
      "Epoch 4623, Train_Loss: 3860.13720703125, Val_Loss: 3986.577880859375\n",
      "Epoch 4624, Train_Loss: 3860.145263671875, Val_Loss: 3986.5400390625\n",
      "Epoch 4625, Train_Loss: 3860.12255859375, Val_Loss: 3986.517578125\n",
      "Epoch 4626, Train_Loss: 3860.076904296875, Val_Loss: 3986.45556640625\n",
      "Epoch 4627, Train_Loss: 3860.07958984375, Val_Loss: 3986.43359375\n",
      "Epoch 4628, Train_Loss: 3860.040771484375, Val_Loss: 3986.434814453125\n",
      "Epoch 4629, Train_Loss: 3860.02294921875, Val_Loss: 3986.434814453125\n",
      "Epoch 4630, Train_Loss: 3859.962890625, Val_Loss: 3986.401123046875\n",
      "Epoch 4631, Train_Loss: 3859.94189453125, Val_Loss: 3986.382080078125\n",
      "Epoch 4632, Train_Loss: 3859.9140625, Val_Loss: 3986.3779296875\n",
      "Epoch 4633, Train_Loss: 3859.9365234375, Val_Loss: 3986.365234375\n",
      "Epoch 4634, Train_Loss: 3859.8486328125, Val_Loss: 3986.322021484375\n",
      "Epoch 4635, Train_Loss: 3859.822509765625, Val_Loss: 3986.3046875\n",
      "Epoch 4636, Train_Loss: 3859.81396484375, Val_Loss: 3986.30029296875\n",
      "Epoch 4637, Train_Loss: 3859.8076171875, Val_Loss: 3986.269287109375\n",
      "Epoch 4638, Train_Loss: 3859.773193359375, Val_Loss: 3986.21923828125\n",
      "Epoch 4639, Train_Loss: 3859.739501953125, Val_Loss: 3986.2041015625\n",
      "Epoch 4640, Train_Loss: 3859.708740234375, Val_Loss: 3986.21630859375\n",
      "Epoch 4641, Train_Loss: 3859.687744140625, Val_Loss: 3986.194091796875\n",
      "Epoch 4642, Train_Loss: 3859.573486328125, Val_Loss: 3986.129150390625\n",
      "Epoch 4643, Train_Loss: 3859.579833984375, Val_Loss: 3986.094482421875\n",
      "Epoch 4644, Train_Loss: 3859.564697265625, Val_Loss: 3986.075927734375\n",
      "Epoch 4645, Train_Loss: 3859.546630859375, Val_Loss: 3986.063232421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4646, Train_Loss: 3859.4970703125, Val_Loss: 3986.05810546875\n",
      "Epoch 4647, Train_Loss: 3859.481201171875, Val_Loss: 3986.038818359375\n",
      "Epoch 4648, Train_Loss: 3859.4755859375, Val_Loss: 3986.014404296875\n",
      "Epoch 4649, Train_Loss: 3859.445068359375, Val_Loss: 3985.963623046875\n",
      "Epoch 4650, Train_Loss: 3859.43408203125, Val_Loss: 3985.94677734375\n",
      "Epoch 4651, Train_Loss: 3859.422607421875, Val_Loss: 3985.9208984375\n",
      "Epoch 4652, Train_Loss: 3859.40673828125, Val_Loss: 3985.912353515625\n",
      "Epoch 4653, Train_Loss: 3859.374267578125, Val_Loss: 3985.848876953125\n",
      "Epoch 4654, Train_Loss: 3859.36767578125, Val_Loss: 3985.82958984375\n",
      "Epoch 4655, Train_Loss: 3859.318359375, Val_Loss: 3985.828857421875\n",
      "Epoch 4656, Train_Loss: 3859.295654296875, Val_Loss: 3985.818359375\n",
      "Epoch 4657, Train_Loss: 3859.284912109375, Val_Loss: 3985.737548828125\n",
      "Epoch 4658, Train_Loss: 3859.2626953125, Val_Loss: 3985.722412109375\n",
      "Epoch 4659, Train_Loss: 3859.21923828125, Val_Loss: 3985.721923828125\n",
      "Epoch 4660, Train_Loss: 3859.206787109375, Val_Loss: 3985.699951171875\n",
      "Epoch 4661, Train_Loss: 3858.93798828125, Val_Loss: 3985.6796875\n",
      "Epoch 4662, Train_Loss: 3858.92578125, Val_Loss: 3985.6630859375\n",
      "Epoch 4663, Train_Loss: 3858.89501953125, Val_Loss: 3985.651123046875\n",
      "Epoch 4664, Train_Loss: 3858.943115234375, Val_Loss: 3985.630859375\n",
      "Epoch 4665, Train_Loss: 3858.826416015625, Val_Loss: 3985.59912109375\n",
      "Epoch 4666, Train_Loss: 3858.820068359375, Val_Loss: 3985.58251953125\n",
      "Epoch 4667, Train_Loss: 3858.79736328125, Val_Loss: 3985.56884765625\n",
      "Epoch 4668, Train_Loss: 3858.7900390625, Val_Loss: 3985.552001953125\n",
      "Epoch 4669, Train_Loss: 3858.76708984375, Val_Loss: 3985.479248046875\n",
      "Epoch 4670, Train_Loss: 3858.748291015625, Val_Loss: 3985.46728515625\n",
      "Epoch 4671, Train_Loss: 3858.726806640625, Val_Loss: 3985.451904296875\n",
      "Epoch 4672, Train_Loss: 3858.720458984375, Val_Loss: 3985.419921875\n",
      "Epoch 4673, Train_Loss: 3858.66796875, Val_Loss: 3985.348876953125\n",
      "Epoch 4674, Train_Loss: 3858.6533203125, Val_Loss: 3985.3271484375\n",
      "Epoch 4675, Train_Loss: 3858.6376953125, Val_Loss: 3985.31005859375\n",
      "Epoch 4676, Train_Loss: 3858.625732421875, Val_Loss: 3985.30908203125\n",
      "Epoch 4677, Train_Loss: 3858.5810546875, Val_Loss: 3985.2900390625\n",
      "Epoch 4678, Train_Loss: 3858.561279296875, Val_Loss: 3985.271240234375\n",
      "Epoch 4679, Train_Loss: 3858.540771484375, Val_Loss: 3985.25048828125\n",
      "Epoch 4680, Train_Loss: 3858.532958984375, Val_Loss: 3985.235595703125\n",
      "Epoch 4681, Train_Loss: 3858.489501953125, Val_Loss: 3985.197509765625\n",
      "Epoch 4682, Train_Loss: 3858.4873046875, Val_Loss: 3985.19091796875\n",
      "Epoch 4683, Train_Loss: 3858.475830078125, Val_Loss: 3985.15966796875\n",
      "Epoch 4684, Train_Loss: 3858.453369140625, Val_Loss: 3985.112060546875\n",
      "Epoch 4685, Train_Loss: 3858.43115234375, Val_Loss: 3985.09716796875\n",
      "Epoch 4686, Train_Loss: 3858.42138671875, Val_Loss: 3985.078369140625\n",
      "Epoch 4687, Train_Loss: 3858.4033203125, Val_Loss: 3985.067626953125\n",
      "Epoch 4688, Train_Loss: 3858.369384765625, Val_Loss: 3984.98291015625\n",
      "Epoch 4689, Train_Loss: 3858.35205078125, Val_Loss: 3984.9775390625\n",
      "Epoch 4690, Train_Loss: 3858.33837890625, Val_Loss: 3984.953125\n",
      "Epoch 4691, Train_Loss: 3858.335205078125, Val_Loss: 3984.92529296875\n",
      "Epoch 4692, Train_Loss: 3858.255615234375, Val_Loss: 3984.910400390625\n",
      "Epoch 4693, Train_Loss: 3858.251953125, Val_Loss: 3984.901123046875\n",
      "Epoch 4694, Train_Loss: 3858.228515625, Val_Loss: 3984.8876953125\n",
      "Epoch 4695, Train_Loss: 3858.211669921875, Val_Loss: 3984.87109375\n",
      "Epoch 4696, Train_Loss: 3857.888916015625, Val_Loss: 3984.833251953125\n",
      "Epoch 4697, Train_Loss: 3857.873291015625, Val_Loss: 3984.805908203125\n",
      "Epoch 4698, Train_Loss: 3857.860595703125, Val_Loss: 3984.799560546875\n",
      "Epoch 4699, Train_Loss: 3857.838623046875, Val_Loss: 3984.77880859375\n",
      "Epoch 4700, Train_Loss: 3857.808837890625, Val_Loss: 3984.70849609375\n",
      "Epoch 4701, Train_Loss: 3857.797607421875, Val_Loss: 3984.69970703125\n",
      "Epoch 4702, Train_Loss: 3857.7705078125, Val_Loss: 3984.702880859375\n",
      "Epoch 4703, Train_Loss: 3857.752685546875, Val_Loss: 3984.689697265625\n",
      "Epoch 4704, Train_Loss: 3857.69775390625, Val_Loss: 3984.622314453125\n",
      "Epoch 4705, Train_Loss: 3857.68505859375, Val_Loss: 3984.59521484375\n",
      "Epoch 4706, Train_Loss: 3857.671875, Val_Loss: 3984.580078125\n",
      "Epoch 4707, Train_Loss: 3857.661376953125, Val_Loss: 3984.5732421875\n",
      "Epoch 4708, Train_Loss: 3857.621826171875, Val_Loss: 3984.54833984375\n",
      "Epoch 4709, Train_Loss: 3857.616943359375, Val_Loss: 3984.5283203125\n",
      "Epoch 4710, Train_Loss: 3857.59765625, Val_Loss: 3984.496826171875\n",
      "Epoch 4711, Train_Loss: 3857.553466796875, Val_Loss: 3984.468017578125\n",
      "Epoch 4712, Train_Loss: 3857.5341796875, Val_Loss: 3984.461181640625\n",
      "Epoch 4713, Train_Loss: 3857.52099609375, Val_Loss: 3984.451171875\n",
      "Epoch 4714, Train_Loss: 3857.51123046875, Val_Loss: 3984.41357421875\n",
      "Epoch 4715, Train_Loss: 3857.594482421875, Val_Loss: 3984.33447265625\n",
      "Epoch 4716, Train_Loss: 3857.4638671875, Val_Loss: 3984.339111328125\n",
      "Epoch 4717, Train_Loss: 3857.4423828125, Val_Loss: 3984.325927734375\n",
      "Epoch 4718, Train_Loss: 3857.551513671875, Val_Loss: 3984.2958984375\n",
      "Epoch 4719, Train_Loss: 3857.517822265625, Val_Loss: 3984.216796875\n",
      "Epoch 4720, Train_Loss: 3857.503662109375, Val_Loss: 3984.199951171875\n",
      "Epoch 4721, Train_Loss: 3857.487548828125, Val_Loss: 3984.19873046875\n",
      "Epoch 4722, Train_Loss: 3857.46826171875, Val_Loss: 3984.181884765625\n",
      "Epoch 4723, Train_Loss: 3857.4404296875, Val_Loss: 3984.157958984375\n",
      "Epoch 4724, Train_Loss: 3857.42578125, Val_Loss: 3984.144775390625\n",
      "Epoch 4725, Train_Loss: 3857.3984375, Val_Loss: 3984.142333984375\n",
      "Epoch 4726, Train_Loss: 3857.385986328125, Val_Loss: 3984.131591796875\n",
      "Epoch 4727, Train_Loss: 3857.220947265625, Val_Loss: 3984.0966796875\n",
      "Epoch 4728, Train_Loss: 3857.2099609375, Val_Loss: 3984.083251953125\n",
      "Epoch 4729, Train_Loss: 3857.200439453125, Val_Loss: 3984.053955078125\n",
      "Epoch 4730, Train_Loss: 3857.17529296875, Val_Loss: 3984.05029296875\n",
      "Epoch 4731, Train_Loss: 3857.1748046875, Val_Loss: 3983.97509765625\n",
      "Epoch 4732, Train_Loss: 3857.1474609375, Val_Loss: 3983.971923828125\n",
      "Epoch 4733, Train_Loss: 3857.140625, Val_Loss: 3983.960693359375\n",
      "Epoch 4734, Train_Loss: 3857.111328125, Val_Loss: 3983.93798828125\n",
      "Epoch 4735, Train_Loss: 3857.05517578125, Val_Loss: 3983.845947265625\n",
      "Epoch 4736, Train_Loss: 3857.034912109375, Val_Loss: 3983.82958984375\n",
      "Epoch 4737, Train_Loss: 3857.021728515625, Val_Loss: 3983.826416015625\n",
      "Epoch 4738, Train_Loss: 3857.01171875, Val_Loss: 3983.793701171875\n",
      "Epoch 4739, Train_Loss: 3856.99755859375, Val_Loss: 3983.78076171875\n",
      "Epoch 4740, Train_Loss: 3856.955078125, Val_Loss: 3983.780029296875\n",
      "Epoch 4741, Train_Loss: 3856.9384765625, Val_Loss: 3983.760498046875\n",
      "Epoch 4742, Train_Loss: 3856.893798828125, Val_Loss: 3983.72998046875\n",
      "Epoch 4743, Train_Loss: 3856.87646484375, Val_Loss: 3983.720458984375\n",
      "Epoch 4744, Train_Loss: 3856.869384765625, Val_Loss: 3983.70751953125\n",
      "Epoch 4745, Train_Loss: 3856.812255859375, Val_Loss: 3983.70166015625\n",
      "Epoch 4746, Train_Loss: 3856.784912109375, Val_Loss: 3983.635498046875\n",
      "Epoch 4747, Train_Loss: 3856.763427734375, Val_Loss: 3983.613525390625\n",
      "Epoch 4748, Train_Loss: 3856.73681640625, Val_Loss: 3983.607666015625\n",
      "Epoch 4749, Train_Loss: 3856.71728515625, Val_Loss: 3983.60400390625\n",
      "Epoch 4750, Train_Loss: 3856.68798828125, Val_Loss: 3983.518798828125\n",
      "Epoch 4751, Train_Loss: 3856.66748046875, Val_Loss: 3983.51171875\n",
      "Epoch 4752, Train_Loss: 3856.656005859375, Val_Loss: 3983.486083984375\n",
      "Epoch 4753, Train_Loss: 3856.642333984375, Val_Loss: 3983.472412109375\n",
      "Epoch 4754, Train_Loss: 3856.58740234375, Val_Loss: 3983.430908203125\n",
      "Epoch 4755, Train_Loss: 3856.5751953125, Val_Loss: 3983.42431640625\n",
      "Epoch 4756, Train_Loss: 3856.597900390625, Val_Loss: 3983.393310546875\n",
      "Epoch 4757, Train_Loss: 3856.504150390625, Val_Loss: 3983.357177734375\n",
      "Epoch 4758, Train_Loss: 3856.4794921875, Val_Loss: 3983.34228515625\n",
      "Epoch 4759, Train_Loss: 3856.5361328125, Val_Loss: 3983.326416015625\n",
      "Epoch 4760, Train_Loss: 3856.520263671875, Val_Loss: 3983.30810546875\n",
      "Epoch 4761, Train_Loss: 3856.4951171875, Val_Loss: 3983.247314453125\n",
      "Epoch 4762, Train_Loss: 3856.47265625, Val_Loss: 3983.227294921875\n",
      "Epoch 4763, Train_Loss: 3856.44580078125, Val_Loss: 3983.232421875\n",
      "Epoch 4764, Train_Loss: 3856.43603515625, Val_Loss: 3983.218017578125\n",
      "Epoch 4765, Train_Loss: 3856.3310546875, Val_Loss: 3983.134521484375\n",
      "Epoch 4766, Train_Loss: 3856.30224609375, Val_Loss: 3983.11962890625\n",
      "Epoch 4767, Train_Loss: 3856.287109375, Val_Loss: 3983.098876953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4768, Train_Loss: 3856.277587890625, Val_Loss: 3983.085693359375\n",
      "Epoch 4769, Train_Loss: 3856.238525390625, Val_Loss: 3983.063720703125\n",
      "Epoch 4770, Train_Loss: 3856.217529296875, Val_Loss: 3983.046875\n",
      "Epoch 4771, Train_Loss: 3856.20166015625, Val_Loss: 3983.02587890625\n",
      "Epoch 4772, Train_Loss: 3856.19921875, Val_Loss: 3983.007568359375\n",
      "Epoch 4773, Train_Loss: 3856.161376953125, Val_Loss: 3982.9755859375\n",
      "Epoch 4774, Train_Loss: 3856.150390625, Val_Loss: 3982.9599609375\n",
      "Epoch 4775, Train_Loss: 3856.123046875, Val_Loss: 3982.937255859375\n",
      "Epoch 4776, Train_Loss: 3856.1064453125, Val_Loss: 3982.879150390625\n",
      "Epoch 4777, Train_Loss: 3856.08056640625, Val_Loss: 3982.879638671875\n",
      "Epoch 4778, Train_Loss: 3856.05029296875, Val_Loss: 3982.8583984375\n",
      "Epoch 4779, Train_Loss: 3856.04296875, Val_Loss: 3982.846435546875\n",
      "Epoch 4780, Train_Loss: 3855.959716796875, Val_Loss: 3982.780029296875\n",
      "Epoch 4781, Train_Loss: 3855.94189453125, Val_Loss: 3982.766845703125\n",
      "Epoch 4782, Train_Loss: 3855.9267578125, Val_Loss: 3982.751708984375\n",
      "Epoch 4783, Train_Loss: 3855.9130859375, Val_Loss: 3982.733154296875\n",
      "Epoch 4784, Train_Loss: 3855.629150390625, Val_Loss: 3982.713623046875\n",
      "Epoch 4785, Train_Loss: 3855.630126953125, Val_Loss: 3982.6953125\n",
      "Epoch 4786, Train_Loss: 3855.59716796875, Val_Loss: 3982.677490234375\n",
      "Epoch 4787, Train_Loss: 3855.596435546875, Val_Loss: 3982.65966796875\n",
      "Epoch 4788, Train_Loss: 3855.48388671875, Val_Loss: 3982.627685546875\n",
      "Epoch 4789, Train_Loss: 3855.474365234375, Val_Loss: 3982.60400390625\n",
      "Epoch 4790, Train_Loss: 3855.431396484375, Val_Loss: 3982.580810546875\n",
      "Epoch 4791, Train_Loss: 3855.386474609375, Val_Loss: 3982.50927734375\n",
      "Epoch 4792, Train_Loss: 3855.3701171875, Val_Loss: 3982.498291015625\n",
      "Epoch 4793, Train_Loss: 3855.350830078125, Val_Loss: 3982.484375\n",
      "Epoch 4794, Train_Loss: 3855.32470703125, Val_Loss: 3982.467529296875\n",
      "Epoch 4795, Train_Loss: 3855.28125, Val_Loss: 3982.402099609375\n",
      "Epoch 4796, Train_Loss: 3855.274169921875, Val_Loss: 3982.386474609375\n",
      "Epoch 4797, Train_Loss: 3855.26123046875, Val_Loss: 3982.370361328125\n",
      "Epoch 4798, Train_Loss: 3855.237060546875, Val_Loss: 3982.370849609375\n",
      "Epoch 4799, Train_Loss: 3855.19580078125, Val_Loss: 3982.343994140625\n",
      "Epoch 4800, Train_Loss: 3855.194580078125, Val_Loss: 3982.32080078125\n",
      "Epoch 4801, Train_Loss: 3855.17431640625, Val_Loss: 3982.302001953125\n",
      "Epoch 4802, Train_Loss: 3855.152099609375, Val_Loss: 3982.296875\n",
      "Epoch 4803, Train_Loss: 3855.1181640625, Val_Loss: 3982.255126953125\n",
      "Epoch 4804, Train_Loss: 3855.100341796875, Val_Loss: 3982.2451171875\n",
      "Epoch 4805, Train_Loss: 3855.020263671875, Val_Loss: 3982.236083984375\n",
      "Epoch 4806, Train_Loss: 3855.050048828125, Val_Loss: 3982.16650390625\n",
      "Epoch 4807, Train_Loss: 3854.97509765625, Val_Loss: 3982.147216796875\n",
      "Epoch 4808, Train_Loss: 3854.9609375, Val_Loss: 3982.126708984375\n",
      "Epoch 4809, Train_Loss: 3854.9443359375, Val_Loss: 3982.115478515625\n",
      "Epoch 4810, Train_Loss: 3854.9189453125, Val_Loss: 3982.027099609375\n",
      "Epoch 4811, Train_Loss: 3854.883544921875, Val_Loss: 3982.02880859375\n",
      "Epoch 4812, Train_Loss: 3854.87353515625, Val_Loss: 3982.013916015625\n",
      "Epoch 4813, Train_Loss: 3854.859375, Val_Loss: 3981.99072265625\n",
      "Epoch 4814, Train_Loss: 3854.792236328125, Val_Loss: 3981.979248046875\n",
      "Epoch 4815, Train_Loss: 3854.764404296875, Val_Loss: 3981.967529296875\n",
      "Epoch 4816, Train_Loss: 3854.743408203125, Val_Loss: 3981.956787109375\n",
      "Epoch 4817, Train_Loss: 3854.729248046875, Val_Loss: 3981.941650390625\n",
      "Epoch 4818, Train_Loss: 3854.399169921875, Val_Loss: 3981.916015625\n",
      "Epoch 4819, Train_Loss: 3854.3876953125, Val_Loss: 3981.893310546875\n",
      "Epoch 4820, Train_Loss: 3854.432373046875, Val_Loss: 3981.861083984375\n",
      "Epoch 4821, Train_Loss: 3854.40185546875, Val_Loss: 3981.814697265625\n",
      "Epoch 4822, Train_Loss: 3854.3876953125, Val_Loss: 3981.798095703125\n",
      "Epoch 4823, Train_Loss: 3854.38037109375, Val_Loss: 3981.764892578125\n",
      "Epoch 4824, Train_Loss: 3854.36376953125, Val_Loss: 3981.753662109375\n",
      "Epoch 4825, Train_Loss: 3854.319091796875, Val_Loss: 3981.671142578125\n",
      "Epoch 4826, Train_Loss: 3854.326904296875, Val_Loss: 3981.64208984375\n",
      "Epoch 4827, Train_Loss: 3854.312255859375, Val_Loss: 3981.62548828125\n",
      "Epoch 4828, Train_Loss: 3854.2861328125, Val_Loss: 3981.60888671875\n",
      "Epoch 4829, Train_Loss: 3854.24951171875, Val_Loss: 3981.59326171875\n",
      "Epoch 4830, Train_Loss: 3854.226806640625, Val_Loss: 3981.57080078125\n",
      "Epoch 4831, Train_Loss: 3854.209716796875, Val_Loss: 3981.558349609375\n",
      "Epoch 4832, Train_Loss: 3854.1953125, Val_Loss: 3981.545166015625\n",
      "Epoch 4833, Train_Loss: 3854.147705078125, Val_Loss: 3981.5244140625\n",
      "Epoch 4834, Train_Loss: 3854.148193359375, Val_Loss: 3981.5126953125\n",
      "Epoch 4835, Train_Loss: 3854.12255859375, Val_Loss: 3981.49609375\n",
      "Epoch 4836, Train_Loss: 3854.098388671875, Val_Loss: 3981.43359375\n",
      "Epoch 4837, Train_Loss: 3854.081787109375, Val_Loss: 3981.42724609375\n",
      "Epoch 4838, Train_Loss: 3854.076416015625, Val_Loss: 3981.4033203125\n",
      "Epoch 4839, Train_Loss: 3854.05126953125, Val_Loss: 3981.39404296875\n",
      "Epoch 4840, Train_Loss: 3854.017333984375, Val_Loss: 3981.31201171875\n",
      "Epoch 4841, Train_Loss: 3854.004150390625, Val_Loss: 3981.2958984375\n",
      "Epoch 4842, Train_Loss: 3853.99951171875, Val_Loss: 3981.28125\n",
      "Epoch 4843, Train_Loss: 3853.980224609375, Val_Loss: 3981.27587890625\n",
      "Epoch 4844, Train_Loss: 3853.970703125, Val_Loss: 3981.235595703125\n",
      "Epoch 4845, Train_Loss: 3853.947021484375, Val_Loss: 3981.21875\n",
      "Epoch 4846, Train_Loss: 3853.930419921875, Val_Loss: 3981.201171875\n",
      "Epoch 4847, Train_Loss: 3853.7841796875, Val_Loss: 3981.16162109375\n",
      "Epoch 4848, Train_Loss: 3853.74853515625, Val_Loss: 3981.162109375\n",
      "Epoch 4849, Train_Loss: 3853.74267578125, Val_Loss: 3981.139892578125\n",
      "Epoch 4850, Train_Loss: 3853.739990234375, Val_Loss: 3981.134521484375\n",
      "Epoch 4851, Train_Loss: 3853.707763671875, Val_Loss: 3981.081298828125\n",
      "Epoch 4852, Train_Loss: 3853.679443359375, Val_Loss: 3981.0546875\n",
      "Epoch 4853, Train_Loss: 3853.66845703125, Val_Loss: 3981.045654296875\n",
      "Epoch 4854, Train_Loss: 3853.638916015625, Val_Loss: 3981.052734375\n",
      "Epoch 4855, Train_Loss: 3853.595947265625, Val_Loss: 3980.94677734375\n",
      "Epoch 4856, Train_Loss: 3853.595458984375, Val_Loss: 3980.941650390625\n",
      "Epoch 4857, Train_Loss: 3853.5712890625, Val_Loss: 3980.935546875\n",
      "Epoch 4858, Train_Loss: 3853.558837890625, Val_Loss: 3980.898681640625\n",
      "Epoch 4859, Train_Loss: 3853.538818359375, Val_Loss: 3980.87451171875\n",
      "Epoch 4860, Train_Loss: 3853.523681640625, Val_Loss: 3980.85888671875\n",
      "Epoch 4861, Train_Loss: 3853.616943359375, Val_Loss: 3980.82763671875\n",
      "Epoch 4862, Train_Loss: 3853.572509765625, Val_Loss: 3980.790771484375\n",
      "Epoch 4863, Train_Loss: 3853.56201171875, Val_Loss: 3980.780517578125\n",
      "Epoch 4864, Train_Loss: 3853.546630859375, Val_Loss: 3980.7626953125\n",
      "Epoch 4865, Train_Loss: 3853.536376953125, Val_Loss: 3980.749267578125\n",
      "Epoch 4866, Train_Loss: 3853.5107421875, Val_Loss: 3980.694091796875\n",
      "Epoch 4867, Train_Loss: 3853.439453125, Val_Loss: 3980.68603515625\n",
      "Epoch 4868, Train_Loss: 3853.470703125, Val_Loss: 3980.66357421875\n",
      "Epoch 4869, Train_Loss: 3853.4462890625, Val_Loss: 3980.6591796875\n",
      "Epoch 4870, Train_Loss: 3853.370849609375, Val_Loss: 3980.5947265625\n",
      "Epoch 4871, Train_Loss: 3853.393310546875, Val_Loss: 3980.5703125\n",
      "Epoch 4872, Train_Loss: 3853.375, Val_Loss: 3980.55126953125\n",
      "Epoch 4873, Train_Loss: 3853.259765625, Val_Loss: 3980.53515625\n",
      "Epoch 4874, Train_Loss: 3853.2529296875, Val_Loss: 3980.520751953125\n",
      "Epoch 4875, Train_Loss: 3853.23486328125, Val_Loss: 3980.513916015625\n",
      "Epoch 4876, Train_Loss: 3853.2470703125, Val_Loss: 3980.481689453125\n",
      "Epoch 4877, Train_Loss: 3853.14697265625, Val_Loss: 3980.4443359375\n",
      "Epoch 4878, Train_Loss: 3853.127685546875, Val_Loss: 3980.447509765625\n",
      "Epoch 4879, Train_Loss: 3853.13232421875, Val_Loss: 3980.406494140625\n",
      "Epoch 4880, Train_Loss: 3853.114501953125, Val_Loss: 3980.38671875\n",
      "Epoch 4881, Train_Loss: 3853.0810546875, Val_Loss: 3980.344482421875\n",
      "Epoch 4882, Train_Loss: 3853.0537109375, Val_Loss: 3980.317626953125\n",
      "Epoch 4883, Train_Loss: 3853.0322265625, Val_Loss: 3980.313232421875\n",
      "Epoch 4884, Train_Loss: 3852.9326171875, Val_Loss: 3980.236328125\n",
      "Epoch 4885, Train_Loss: 3852.91748046875, Val_Loss: 3980.22119140625\n",
      "Epoch 4886, Train_Loss: 3852.88232421875, Val_Loss: 3980.19970703125\n",
      "Epoch 4887, Train_Loss: 3852.843505859375, Val_Loss: 3980.186279296875\n",
      "Epoch 4888, Train_Loss: 3852.81787109375, Val_Loss: 3980.18115234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4889, Train_Loss: 3852.785400390625, Val_Loss: 3980.15478515625\n",
      "Epoch 4890, Train_Loss: 3852.764404296875, Val_Loss: 3980.139892578125\n",
      "Epoch 4891, Train_Loss: 3852.776611328125, Val_Loss: 3980.147705078125\n",
      "Epoch 4892, Train_Loss: 3852.71875, Val_Loss: 3980.084716796875\n",
      "Epoch 4893, Train_Loss: 3852.70068359375, Val_Loss: 3980.069091796875\n",
      "Epoch 4894, Train_Loss: 3852.698486328125, Val_Loss: 3980.065185546875\n",
      "Epoch 4895, Train_Loss: 3852.671630859375, Val_Loss: 3979.9951171875\n",
      "Epoch 4896, Train_Loss: 3852.671630859375, Val_Loss: 3979.966064453125\n",
      "Epoch 4897, Train_Loss: 3852.656982421875, Val_Loss: 3979.94970703125\n",
      "Epoch 4898, Train_Loss: 3852.646484375, Val_Loss: 3979.9375\n",
      "Epoch 4899, Train_Loss: 3852.614501953125, Val_Loss: 3979.861572265625\n",
      "Epoch 4900, Train_Loss: 3852.597900390625, Val_Loss: 3979.84326171875\n",
      "Epoch 4901, Train_Loss: 3852.58740234375, Val_Loss: 3979.839599609375\n",
      "Epoch 4902, Train_Loss: 3852.570556640625, Val_Loss: 3979.8232421875\n",
      "Epoch 4903, Train_Loss: 3852.299072265625, Val_Loss: 3979.8056640625\n",
      "Epoch 4904, Train_Loss: 3852.28076171875, Val_Loss: 3979.784423828125\n",
      "Epoch 4905, Train_Loss: 3852.260009765625, Val_Loss: 3979.77587890625\n",
      "Epoch 4906, Train_Loss: 3852.24267578125, Val_Loss: 3979.771484375\n",
      "Epoch 4907, Train_Loss: 3852.134521484375, Val_Loss: 3979.738037109375\n",
      "Epoch 4908, Train_Loss: 3852.12109375, Val_Loss: 3979.72412109375\n",
      "Epoch 4909, Train_Loss: 3852.099609375, Val_Loss: 3979.705078125\n",
      "Epoch 4910, Train_Loss: 3852.048095703125, Val_Loss: 3979.638916015625\n",
      "Epoch 4911, Train_Loss: 3852.037109375, Val_Loss: 3979.632080078125\n",
      "Epoch 4912, Train_Loss: 3852.023681640625, Val_Loss: 3979.617919921875\n",
      "Epoch 4913, Train_Loss: 3851.997802734375, Val_Loss: 3979.603271484375\n",
      "Epoch 4914, Train_Loss: 3851.96435546875, Val_Loss: 3979.51318359375\n",
      "Epoch 4915, Train_Loss: 3851.95751953125, Val_Loss: 3979.50439453125\n",
      "Epoch 4916, Train_Loss: 3851.940673828125, Val_Loss: 3979.489990234375\n",
      "Epoch 4917, Train_Loss: 3851.9267578125, Val_Loss: 3979.479248046875\n",
      "Epoch 4918, Train_Loss: 3851.883056640625, Val_Loss: 3979.4580078125\n",
      "Epoch 4919, Train_Loss: 3851.865478515625, Val_Loss: 3979.430908203125\n",
      "Epoch 4920, Train_Loss: 3851.847412109375, Val_Loss: 3979.43115234375\n",
      "Epoch 4921, Train_Loss: 3851.80712890625, Val_Loss: 3979.389892578125\n",
      "Epoch 4922, Train_Loss: 3851.74169921875, Val_Loss: 3979.37548828125\n",
      "Epoch 4923, Train_Loss: 3851.730224609375, Val_Loss: 3979.377685546875\n",
      "Epoch 4924, Train_Loss: 3851.7841796875, Val_Loss: 3979.363525390625\n",
      "Epoch 4925, Train_Loss: 3851.693603515625, Val_Loss: 3979.280029296875\n",
      "Epoch 4926, Train_Loss: 3851.696533203125, Val_Loss: 3979.262451171875\n",
      "Epoch 4927, Train_Loss: 3851.67578125, Val_Loss: 3979.2763671875\n",
      "Epoch 4928, Train_Loss: 3851.622314453125, Val_Loss: 3979.1748046875\n",
      "Epoch 4929, Train_Loss: 3851.636962890625, Val_Loss: 3979.15234375\n",
      "Epoch 4930, Train_Loss: 3851.632080078125, Val_Loss: 3979.171142578125\n",
      "Epoch 4931, Train_Loss: 3851.56103515625, Val_Loss: 3979.1103515625\n",
      "Epoch 4932, Train_Loss: 3851.513671875, Val_Loss: 3979.080322265625\n",
      "Epoch 4933, Train_Loss: 3851.531494140625, Val_Loss: 3979.118896484375\n",
      "Epoch 4934, Train_Loss: 3851.45654296875, Val_Loss: 3979.0771484375\n",
      "Epoch 4935, Train_Loss: 3851.4619140625, Val_Loss: 3979.05126953125\n",
      "Epoch 4936, Train_Loss: 3851.156494140625, Val_Loss: 3979.036376953125\n",
      "Epoch 4937, Train_Loss: 3851.111328125, Val_Loss: 3979.017578125\n",
      "Epoch 4938, Train_Loss: 3851.106201171875, Val_Loss: 3979.001708984375\n",
      "Epoch 4939, Train_Loss: 3851.06982421875, Val_Loss: 3978.936767578125\n",
      "Epoch 4940, Train_Loss: 3851.037353515625, Val_Loss: 3978.936767578125\n",
      "Epoch 4941, Train_Loss: 3851.05908203125, Val_Loss: 3978.939697265625\n",
      "Epoch 4942, Train_Loss: 3851.00537109375, Val_Loss: 3978.91650390625\n",
      "Epoch 4943, Train_Loss: 3850.98095703125, Val_Loss: 3978.80078125\n",
      "Epoch 4944, Train_Loss: 3850.978759765625, Val_Loss: 3978.844482421875\n",
      "Epoch 4945, Train_Loss: 3850.94775390625, Val_Loss: 3978.784912109375\n",
      "Epoch 4946, Train_Loss: 3850.949462890625, Val_Loss: 3978.74169921875\n",
      "Epoch 4947, Train_Loss: 3850.892822265625, Val_Loss: 3978.748291015625\n",
      "Epoch 4948, Train_Loss: 3850.893310546875, Val_Loss: 3978.77001953125\n",
      "Epoch 4949, Train_Loss: 3850.877197265625, Val_Loss: 3978.684326171875\n",
      "Epoch 4950, Train_Loss: 3850.806884765625, Val_Loss: 3978.66845703125\n",
      "Epoch 4951, Train_Loss: 3850.8330078125, Val_Loss: 3978.669189453125\n",
      "Epoch 4952, Train_Loss: 3850.798095703125, Val_Loss: 3978.624755859375\n",
      "Epoch 4953, Train_Loss: 3850.81787109375, Val_Loss: 3978.640380859375\n",
      "Epoch 4954, Train_Loss: 3850.74560546875, Val_Loss: 3978.565185546875\n",
      "Epoch 4955, Train_Loss: 3850.734619140625, Val_Loss: 3978.545654296875\n",
      "Epoch 4956, Train_Loss: 3850.748291015625, Val_Loss: 3978.5576171875\n",
      "Epoch 4957, Train_Loss: 3850.708984375, Val_Loss: 3978.53369140625\n",
      "Epoch 4958, Train_Loss: 3850.65185546875, Val_Loss: 3978.439697265625\n",
      "Epoch 4959, Train_Loss: 3850.6728515625, Val_Loss: 3978.466796875\n",
      "Epoch 4960, Train_Loss: 3850.671142578125, Val_Loss: 3978.446044921875\n",
      "Epoch 4961, Train_Loss: 3850.6650390625, Val_Loss: 3978.4052734375\n",
      "Epoch 4962, Train_Loss: 3850.577880859375, Val_Loss: 3978.408447265625\n",
      "Epoch 4963, Train_Loss: 3850.58740234375, Val_Loss: 3978.4091796875\n",
      "Epoch 4964, Train_Loss: 3850.613525390625, Val_Loss: 3978.357177734375\n",
      "Epoch 4965, Train_Loss: 3850.478271484375, Val_Loss: 3978.348388671875\n",
      "Epoch 4966, Train_Loss: 3850.43701171875, Val_Loss: 3978.32763671875\n",
      "Epoch 4967, Train_Loss: 3850.431640625, Val_Loss: 3978.277099609375\n",
      "Epoch 4968, Train_Loss: 3850.423828125, Val_Loss: 3978.308837890625\n",
      "Epoch 4969, Train_Loss: 3850.412841796875, Val_Loss: 3978.20849609375\n",
      "Epoch 4970, Train_Loss: 3850.3720703125, Val_Loss: 3978.196044921875\n",
      "Epoch 4971, Train_Loss: 3850.376708984375, Val_Loss: 3978.20849609375\n",
      "Epoch 4972, Train_Loss: 3850.33642578125, Val_Loss: 3978.110107421875\n",
      "Epoch 4973, Train_Loss: 3850.29345703125, Val_Loss: 3978.074462890625\n",
      "Epoch 4974, Train_Loss: 3850.311767578125, Val_Loss: 3978.104736328125\n",
      "Epoch 4975, Train_Loss: 3850.306640625, Val_Loss: 3978.072021484375\n",
      "Epoch 4976, Train_Loss: 3850.224853515625, Val_Loss: 3978.0400390625\n",
      "Epoch 4977, Train_Loss: 3850.21533203125, Val_Loss: 3978.038818359375\n",
      "Epoch 4978, Train_Loss: 3850.2294921875, Val_Loss: 3978.031494140625\n",
      "Epoch 4979, Train_Loss: 3850.170166015625, Val_Loss: 3977.994873046875\n",
      "Epoch 4980, Train_Loss: 3850.140869140625, Val_Loss: 3977.971923828125\n",
      "Epoch 4981, Train_Loss: 3850.130859375, Val_Loss: 3977.9541015625\n",
      "Epoch 4982, Train_Loss: 3850.12158203125, Val_Loss: 3977.954345703125\n",
      "Epoch 4983, Train_Loss: 3850.09326171875, Val_Loss: 3977.890380859375\n",
      "Epoch 4984, Train_Loss: 3850.07763671875, Val_Loss: 3977.842041015625\n",
      "Epoch 4985, Train_Loss: 3850.060791015625, Val_Loss: 3977.82275390625\n",
      "Epoch 4986, Train_Loss: 3850.05078125, Val_Loss: 3977.8408203125\n",
      "Epoch 4987, Train_Loss: 3850.018798828125, Val_Loss: 3977.752685546875\n",
      "Epoch 4988, Train_Loss: 3849.99951171875, Val_Loss: 3977.73046875\n",
      "Epoch 4989, Train_Loss: 3849.97900390625, Val_Loss: 3977.71728515625\n",
      "Epoch 4990, Train_Loss: 3849.9140625, Val_Loss: 3977.709228515625\n",
      "Epoch 4991, Train_Loss: 3849.903076171875, Val_Loss: 3977.699951171875\n",
      "Epoch 4992, Train_Loss: 3849.85693359375, Val_Loss: 3977.6767578125\n",
      "Epoch 4993, Train_Loss: 3849.87109375, Val_Loss: 3977.66552734375\n",
      "Epoch 4994, Train_Loss: 3849.78564453125, Val_Loss: 3977.63671875\n",
      "Epoch 4995, Train_Loss: 3849.76904296875, Val_Loss: 3977.622314453125\n",
      "Epoch 4996, Train_Loss: 3849.7626953125, Val_Loss: 3977.595703125\n",
      "Epoch 4997, Train_Loss: 3849.72900390625, Val_Loss: 3977.5380859375\n",
      "Epoch 4998, Train_Loss: 3849.725830078125, Val_Loss: 3977.528076171875\n",
      "Epoch 4999, Train_Loss: 3849.69921875, Val_Loss: 3977.512451171875\n",
      "Epoch 5000, Train_Loss: 3849.675048828125, Val_Loss: 3977.4931640625\n",
      "Epoch 5001, Train_Loss: 3849.688720703125, Val_Loss: 3977.39599609375\n",
      "Epoch 5002, Train_Loss: 3849.682373046875, Val_Loss: 3977.38916015625\n",
      "Epoch 5003, Train_Loss: 3849.665283203125, Val_Loss: 3977.374755859375\n",
      "Epoch 5004, Train_Loss: 3849.65478515625, Val_Loss: 3977.349609375\n",
      "Epoch 5005, Train_Loss: 3849.59228515625, Val_Loss: 3977.334716796875\n",
      "Epoch 5006, Train_Loss: 3849.5771484375, Val_Loss: 3977.3291015625\n",
      "Epoch 5007, Train_Loss: 3849.578857421875, Val_Loss: 3977.310791015625\n",
      "Epoch 5008, Train_Loss: 3849.52685546875, Val_Loss: 3977.277099609375\n",
      "Epoch 5009, Train_Loss: 3849.509033203125, Val_Loss: 3977.26171875\n",
      "Epoch 5010, Train_Loss: 3849.49169921875, Val_Loss: 3977.263671875\n",
      "Epoch 5011, Train_Loss: 3849.472412109375, Val_Loss: 3977.246337890625\n",
      "Epoch 5012, Train_Loss: 3849.444091796875, Val_Loss: 3977.197265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5013, Train_Loss: 3849.42822265625, Val_Loss: 3977.175537109375\n",
      "Epoch 5014, Train_Loss: 3849.416748046875, Val_Loss: 3977.150390625\n",
      "Epoch 5015, Train_Loss: 3849.388916015625, Val_Loss: 3977.0791015625\n",
      "Epoch 5016, Train_Loss: 3849.364501953125, Val_Loss: 3977.06689453125\n",
      "Epoch 5017, Train_Loss: 3849.352294921875, Val_Loss: 3977.039306640625\n",
      "Epoch 5018, Train_Loss: 3849.338134765625, Val_Loss: 3977.03271484375\n",
      "Epoch 5019, Train_Loss: 3849.066162109375, Val_Loss: 3976.998291015625\n",
      "Epoch 5020, Train_Loss: 3849.10400390625, Val_Loss: 3976.969482421875\n",
      "Epoch 5021, Train_Loss: 3849.0830078125, Val_Loss: 3976.9462890625\n",
      "Epoch 5022, Train_Loss: 3849.04736328125, Val_Loss: 3976.967529296875\n",
      "Epoch 5023, Train_Loss: 3848.956298828125, Val_Loss: 3976.91748046875\n",
      "Epoch 5024, Train_Loss: 3848.958740234375, Val_Loss: 3976.899658203125\n",
      "Epoch 5025, Train_Loss: 3848.872802734375, Val_Loss: 3976.895263671875\n",
      "Epoch 5026, Train_Loss: 3848.871826171875, Val_Loss: 3976.828857421875\n",
      "Epoch 5027, Train_Loss: 3848.82470703125, Val_Loss: 3976.817138671875\n",
      "Epoch 5028, Train_Loss: 3848.749755859375, Val_Loss: 3976.824462890625\n",
      "Epoch 5029, Train_Loss: 3848.796875, Val_Loss: 3976.798095703125\n",
      "Epoch 5030, Train_Loss: 3848.754638671875, Val_Loss: 3976.71484375\n",
      "Epoch 5031, Train_Loss: 3848.727294921875, Val_Loss: 3976.706787109375\n",
      "Epoch 5032, Train_Loss: 3848.721923828125, Val_Loss: 3976.68115234375\n",
      "Epoch 5033, Train_Loss: 3848.697509765625, Val_Loss: 3976.65673828125\n",
      "Epoch 5034, Train_Loss: 3848.661865234375, Val_Loss: 3976.64404296875\n",
      "Epoch 5035, Train_Loss: 3848.656982421875, Val_Loss: 3976.624755859375\n",
      "Epoch 5036, Train_Loss: 3848.662841796875, Val_Loss: 3976.600341796875\n",
      "Epoch 5037, Train_Loss: 3848.61767578125, Val_Loss: 3976.570068359375\n",
      "Epoch 5038, Train_Loss: 3848.614501953125, Val_Loss: 3976.556884765625\n",
      "Epoch 5039, Train_Loss: 3848.585205078125, Val_Loss: 3976.539306640625\n",
      "Epoch 5040, Train_Loss: 3848.55810546875, Val_Loss: 3976.4775390625\n",
      "Epoch 5041, Train_Loss: 3848.55126953125, Val_Loss: 3976.476806640625\n",
      "Epoch 5042, Train_Loss: 3848.534912109375, Val_Loss: 3976.459716796875\n",
      "Epoch 5043, Train_Loss: 3848.513427734375, Val_Loss: 3976.44970703125\n",
      "Epoch 5044, Train_Loss: 3848.485595703125, Val_Loss: 3976.37353515625\n",
      "Epoch 5045, Train_Loss: 3848.469482421875, Val_Loss: 3976.369140625\n",
      "Epoch 5046, Train_Loss: 3848.456298828125, Val_Loss: 3976.3544921875\n",
      "Epoch 5047, Train_Loss: 3848.391845703125, Val_Loss: 3976.31396484375\n",
      "Epoch 5048, Train_Loss: 3848.3671875, Val_Loss: 3976.315673828125\n",
      "Epoch 5049, Train_Loss: 3848.35302734375, Val_Loss: 3976.297607421875\n",
      "Epoch 5050, Train_Loss: 3848.347412109375, Val_Loss: 3976.268310546875\n",
      "Epoch 5051, Train_Loss: 3848.02001953125, Val_Loss: 3976.24365234375\n",
      "Epoch 5052, Train_Loss: 3848.0078125, Val_Loss: 3976.236328125\n",
      "Epoch 5053, Train_Loss: 3847.997314453125, Val_Loss: 3976.19189453125\n",
      "Epoch 5054, Train_Loss: 3847.978515625, Val_Loss: 3976.1708984375\n",
      "Epoch 5055, Train_Loss: 3847.946533203125, Val_Loss: 3976.129638671875\n",
      "Epoch 5056, Train_Loss: 3847.937255859375, Val_Loss: 3976.102294921875\n",
      "Epoch 5057, Train_Loss: 3847.9150390625, Val_Loss: 3976.093505859375\n",
      "Epoch 5058, Train_Loss: 3847.875244140625, Val_Loss: 3976.01708984375\n",
      "Epoch 5059, Train_Loss: 3847.8642578125, Val_Loss: 3976.001708984375\n",
      "Epoch 5060, Train_Loss: 3847.847412109375, Val_Loss: 3975.9794921875\n",
      "Epoch 5061, Train_Loss: 3847.823486328125, Val_Loss: 3975.9619140625\n",
      "Epoch 5062, Train_Loss: 3847.783203125, Val_Loss: 3975.93310546875\n",
      "Epoch 5063, Train_Loss: 3847.718505859375, Val_Loss: 3975.941162109375\n",
      "Epoch 5064, Train_Loss: 3847.697265625, Val_Loss: 3975.926025390625\n",
      "Epoch 5065, Train_Loss: 3847.65771484375, Val_Loss: 3975.88720703125\n",
      "Epoch 5066, Train_Loss: 3847.638916015625, Val_Loss: 3975.87548828125\n",
      "Epoch 5067, Train_Loss: 3847.6416015625, Val_Loss: 3975.867919921875\n",
      "Epoch 5068, Train_Loss: 3847.623291015625, Val_Loss: 3975.852294921875\n",
      "Epoch 5069, Train_Loss: 3847.593994140625, Val_Loss: 3975.77001953125\n",
      "Epoch 5070, Train_Loss: 3847.578857421875, Val_Loss: 3975.751953125\n",
      "Epoch 5071, Train_Loss: 3847.5625, Val_Loss: 3975.757568359375\n",
      "Epoch 5072, Train_Loss: 3847.559326171875, Val_Loss: 3975.689697265625\n",
      "Epoch 5073, Train_Loss: 3847.48974609375, Val_Loss: 3975.640380859375\n",
      "Epoch 5074, Train_Loss: 3847.41943359375, Val_Loss: 3975.63525390625\n",
      "Epoch 5075, Train_Loss: 3847.487548828125, Val_Loss: 3975.629638671875\n",
      "Epoch 5076, Train_Loss: 3847.46142578125, Val_Loss: 3975.614501953125\n",
      "Epoch 5077, Train_Loss: 3847.388427734375, Val_Loss: 3975.60400390625\n",
      "Epoch 5078, Train_Loss: 3847.359619140625, Val_Loss: 3975.581298828125\n",
      "Epoch 5079, Train_Loss: 3847.215087890625, Val_Loss: 3975.54443359375\n",
      "Epoch 5080, Train_Loss: 3847.25634765625, Val_Loss: 3975.5224609375\n",
      "Epoch 5081, Train_Loss: 3847.235595703125, Val_Loss: 3975.5244140625\n",
      "Epoch 5082, Train_Loss: 3847.163330078125, Val_Loss: 3975.525146484375\n",
      "Epoch 5083, Train_Loss: 3847.136962890625, Val_Loss: 3975.466064453125\n",
      "Epoch 5084, Train_Loss: 3847.11083984375, Val_Loss: 3975.445556640625\n",
      "Epoch 5085, Train_Loss: 3847.16943359375, Val_Loss: 3975.434326171875\n",
      "Epoch 5086, Train_Loss: 3847.05517578125, Val_Loss: 3975.349609375\n",
      "Epoch 5087, Train_Loss: 3847.04638671875, Val_Loss: 3975.34228515625\n",
      "Epoch 5088, Train_Loss: 3847.029052734375, Val_Loss: 3975.332763671875\n",
      "Epoch 5089, Train_Loss: 3847.07666015625, Val_Loss: 3975.306396484375\n",
      "Epoch 5090, Train_Loss: 3846.987060546875, Val_Loss: 3975.27490234375\n",
      "Epoch 5091, Train_Loss: 3846.9619140625, Val_Loss: 3975.259521484375\n",
      "Epoch 5092, Train_Loss: 3846.928466796875, Val_Loss: 3975.233642578125\n",
      "Epoch 5093, Train_Loss: 3846.884033203125, Val_Loss: 3975.215576171875\n",
      "Epoch 5094, Train_Loss: 3846.869873046875, Val_Loss: 3975.204345703125\n",
      "Epoch 5095, Train_Loss: 3846.92578125, Val_Loss: 3975.187255859375\n",
      "Epoch 5096, Train_Loss: 3846.8515625, Val_Loss: 3975.181640625\n",
      "Epoch 5097, Train_Loss: 3846.813720703125, Val_Loss: 3975.109130859375\n",
      "Epoch 5098, Train_Loss: 3846.790771484375, Val_Loss: 3975.10400390625\n",
      "Epoch 5099, Train_Loss: 3846.76708984375, Val_Loss: 3975.098876953125\n",
      "Epoch 5100, Train_Loss: 3846.736083984375, Val_Loss: 3975.023193359375\n",
      "Epoch 5101, Train_Loss: 3846.790283203125, Val_Loss: 3975.003173828125\n",
      "Epoch 5102, Train_Loss: 3846.768798828125, Val_Loss: 3974.992431640625\n",
      "Epoch 5103, Train_Loss: 3846.756103515625, Val_Loss: 3974.97998046875\n",
      "Epoch 5104, Train_Loss: 3846.676513671875, Val_Loss: 3974.9560546875\n",
      "Epoch 5105, Train_Loss: 3846.673828125, Val_Loss: 3974.943115234375\n",
      "Epoch 5106, Train_Loss: 3846.683837890625, Val_Loss: 3974.9296875\n",
      "Epoch 5107, Train_Loss: 3846.582763671875, Val_Loss: 3974.90087890625\n",
      "Epoch 5108, Train_Loss: 3846.58203125, Val_Loss: 3974.86669921875\n",
      "Epoch 5109, Train_Loss: 3846.58642578125, Val_Loss: 3974.861083984375\n",
      "Epoch 5110, Train_Loss: 3846.577880859375, Val_Loss: 3974.8408203125\n",
      "Epoch 5111, Train_Loss: 3846.54345703125, Val_Loss: 3974.783935546875\n",
      "Epoch 5112, Train_Loss: 3846.511962890625, Val_Loss: 3974.783203125\n",
      "Epoch 5113, Train_Loss: 3846.492919921875, Val_Loss: 3974.775634765625\n",
      "Epoch 5114, Train_Loss: 3846.3857421875, Val_Loss: 3974.69091796875\n",
      "Epoch 5115, Train_Loss: 3846.3662109375, Val_Loss: 3974.66650390625\n",
      "Epoch 5116, Train_Loss: 3846.347900390625, Val_Loss: 3974.669921875\n",
      "Epoch 5117, Train_Loss: 3846.3359375, Val_Loss: 3974.6630859375\n",
      "Epoch 5118, Train_Loss: 3846.2939453125, Val_Loss: 3974.643310546875\n",
      "Epoch 5119, Train_Loss: 3846.3369140625, Val_Loss: 3974.6123046875\n",
      "Epoch 5120, Train_Loss: 3846.260498046875, Val_Loss: 3974.605712890625\n",
      "Epoch 5121, Train_Loss: 3846.224853515625, Val_Loss: 3974.569091796875\n",
      "Epoch 5122, Train_Loss: 3846.2138671875, Val_Loss: 3974.550048828125\n",
      "Epoch 5123, Train_Loss: 3846.263427734375, Val_Loss: 3974.538330078125\n",
      "Epoch 5124, Train_Loss: 3846.185546875, Val_Loss: 3974.52392578125\n",
      "Epoch 5125, Train_Loss: 3846.16455078125, Val_Loss: 3974.44189453125\n",
      "Epoch 5126, Train_Loss: 3846.147705078125, Val_Loss: 3974.4287109375\n",
      "Epoch 5127, Train_Loss: 3846.134521484375, Val_Loss: 3974.426513671875\n",
      "Epoch 5128, Train_Loss: 3846.105224609375, Val_Loss: 3974.342041015625\n",
      "Epoch 5129, Train_Loss: 3846.0849609375, Val_Loss: 3974.330078125\n",
      "Epoch 5130, Train_Loss: 3846.068115234375, Val_Loss: 3974.3095703125\n",
      "Epoch 5131, Train_Loss: 3846.05859375, Val_Loss: 3974.308349609375\n",
      "Epoch 5132, Train_Loss: 3845.775634765625, Val_Loss: 3974.285888671875\n",
      "Epoch 5133, Train_Loss: 3845.761474609375, Val_Loss: 3974.26953125\n",
      "Epoch 5134, Train_Loss: 3845.7275390625, Val_Loss: 3974.273193359375\n",
      "Epoch 5135, Train_Loss: 3845.61181640625, Val_Loss: 3974.233642578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5136, Train_Loss: 3845.59765625, Val_Loss: 3974.228759765625\n",
      "Epoch 5137, Train_Loss: 3845.580322265625, Val_Loss: 3974.206787109375\n",
      "Epoch 5138, Train_Loss: 3845.588134765625, Val_Loss: 3974.173095703125\n",
      "Epoch 5139, Train_Loss: 3845.5283203125, Val_Loss: 3974.1279296875\n",
      "Epoch 5140, Train_Loss: 3845.51416015625, Val_Loss: 3974.111572265625\n",
      "Epoch 5141, Train_Loss: 3845.4970703125, Val_Loss: 3974.08837890625\n",
      "Epoch 5142, Train_Loss: 3845.445068359375, Val_Loss: 3974.021484375\n",
      "Epoch 5143, Train_Loss: 3845.5556640625, Val_Loss: 3973.97314453125\n",
      "Epoch 5144, Train_Loss: 3845.53955078125, Val_Loss: 3973.953125\n",
      "Epoch 5145, Train_Loss: 3845.524658203125, Val_Loss: 3973.950439453125\n",
      "Epoch 5146, Train_Loss: 3845.48486328125, Val_Loss: 3973.929931640625\n",
      "Epoch 5147, Train_Loss: 3845.47119140625, Val_Loss: 3973.919189453125\n",
      "Epoch 5148, Train_Loss: 3845.451416015625, Val_Loss: 3973.899658203125\n",
      "Epoch 5149, Train_Loss: 3845.415771484375, Val_Loss: 3973.861572265625\n",
      "Epoch 5150, Train_Loss: 3845.401123046875, Val_Loss: 3973.85400390625\n",
      "Epoch 5151, Train_Loss: 3845.380126953125, Val_Loss: 3973.853515625\n",
      "Epoch 5152, Train_Loss: 3845.358642578125, Val_Loss: 3973.834716796875\n",
      "Epoch 5153, Train_Loss: 3845.331298828125, Val_Loss: 3973.768310546875\n",
      "Epoch 5154, Train_Loss: 3845.32080078125, Val_Loss: 3973.751708984375\n",
      "Epoch 5155, Train_Loss: 3845.313720703125, Val_Loss: 3973.73681640625\n",
      "Epoch 5156, Train_Loss: 3845.27734375, Val_Loss: 3973.662353515625\n",
      "Epoch 5157, Train_Loss: 3845.25634765625, Val_Loss: 3973.6484375\n",
      "Epoch 5158, Train_Loss: 3845.249755859375, Val_Loss: 3973.632080078125\n",
      "Epoch 5159, Train_Loss: 3845.178955078125, Val_Loss: 3973.5947265625\n",
      "Epoch 5160, Train_Loss: 3845.162109375, Val_Loss: 3973.580322265625\n",
      "Epoch 5161, Train_Loss: 3845.135498046875, Val_Loss: 3973.578857421875\n",
      "Epoch 5162, Train_Loss: 3845.122314453125, Val_Loss: 3973.567138671875\n",
      "Epoch 5163, Train_Loss: 3844.811767578125, Val_Loss: 3973.53564453125\n",
      "Epoch 5164, Train_Loss: 3844.78955078125, Val_Loss: 3973.528076171875\n",
      "Epoch 5165, Train_Loss: 3844.770263671875, Val_Loss: 3973.518310546875\n",
      "Epoch 5166, Train_Loss: 3844.741943359375, Val_Loss: 3973.459228515625\n",
      "Epoch 5167, Train_Loss: 3844.726806640625, Val_Loss: 3973.4423828125\n",
      "Epoch 5168, Train_Loss: 3844.658203125, Val_Loss: 3973.444091796875\n",
      "Epoch 5169, Train_Loss: 3844.642822265625, Val_Loss: 3973.425537109375\n",
      "Epoch 5170, Train_Loss: 3844.597900390625, Val_Loss: 3973.342041015625\n",
      "Epoch 5171, Train_Loss: 3844.587890625, Val_Loss: 3973.32275390625\n",
      "Epoch 5172, Train_Loss: 3844.58251953125, Val_Loss: 3973.313232421875\n",
      "Epoch 5173, Train_Loss: 3844.53857421875, Val_Loss: 3973.28076171875\n",
      "Epoch 5174, Train_Loss: 3844.520751953125, Val_Loss: 3973.256103515625\n",
      "Epoch 5175, Train_Loss: 3844.501953125, Val_Loss: 3973.248291015625\n",
      "Epoch 5176, Train_Loss: 3844.5087890625, Val_Loss: 3973.22314453125\n",
      "Epoch 5177, Train_Loss: 3844.460693359375, Val_Loss: 3973.172119140625\n",
      "Epoch 5178, Train_Loss: 3844.441162109375, Val_Loss: 3973.158447265625\n",
      "Epoch 5179, Train_Loss: 3844.42919921875, Val_Loss: 3973.146484375\n",
      "Epoch 5180, Train_Loss: 3844.404541015625, Val_Loss: 3973.1044921875\n",
      "Epoch 5181, Train_Loss: 3844.390380859375, Val_Loss: 3973.08154296875\n",
      "Epoch 5182, Train_Loss: 3844.370849609375, Val_Loss: 3973.063232421875\n",
      "Epoch 5183, Train_Loss: 3844.34375, Val_Loss: 3972.98388671875\n",
      "Epoch 5184, Train_Loss: 3844.3310546875, Val_Loss: 3972.974853515625\n",
      "Epoch 5185, Train_Loss: 3844.302734375, Val_Loss: 3972.97802734375\n",
      "Epoch 5186, Train_Loss: 3844.296142578125, Val_Loss: 3972.954345703125\n",
      "Epoch 5187, Train_Loss: 3844.27880859375, Val_Loss: 3972.93359375\n",
      "Epoch 5188, Train_Loss: 3844.2568359375, Val_Loss: 3972.919189453125\n",
      "Epoch 5189, Train_Loss: 3844.2490234375, Val_Loss: 3972.899658203125\n",
      "Epoch 5190, Train_Loss: 3844.09423828125, Val_Loss: 3972.865966796875\n",
      "Epoch 5191, Train_Loss: 3844.07958984375, Val_Loss: 3972.853271484375\n",
      "Epoch 5192, Train_Loss: 3844.061767578125, Val_Loss: 3972.844482421875\n",
      "Epoch 5193, Train_Loss: 3844.055419921875, Val_Loss: 3972.813232421875\n",
      "Epoch 5194, Train_Loss: 3844.026123046875, Val_Loss: 3972.755615234375\n",
      "Epoch 5195, Train_Loss: 3843.97216796875, Val_Loss: 3972.75048828125\n",
      "Epoch 5196, Train_Loss: 3843.96533203125, Val_Loss: 3972.73681640625\n",
      "Epoch 5197, Train_Loss: 3843.9150390625, Val_Loss: 3972.66748046875\n",
      "Epoch 5198, Train_Loss: 3843.90087890625, Val_Loss: 3972.644287109375\n",
      "Epoch 5199, Train_Loss: 3843.8857421875, Val_Loss: 3972.634765625\n",
      "Epoch 5200, Train_Loss: 3843.8046875, Val_Loss: 3972.607177734375\n",
      "Epoch 5201, Train_Loss: 3843.787353515625, Val_Loss: 3972.601318359375\n",
      "Epoch 5202, Train_Loss: 3843.772705078125, Val_Loss: 3972.587646484375\n",
      "Epoch 5203, Train_Loss: 3843.752685546875, Val_Loss: 3972.549560546875\n",
      "Epoch 5204, Train_Loss: 3843.729736328125, Val_Loss: 3972.512451171875\n",
      "Epoch 5205, Train_Loss: 3843.7138671875, Val_Loss: 3972.493896484375\n",
      "Epoch 5206, Train_Loss: 3843.69970703125, Val_Loss: 3972.489990234375\n",
      "Epoch 5207, Train_Loss: 3843.679931640625, Val_Loss: 3972.426513671875\n",
      "Epoch 5208, Train_Loss: 3843.621826171875, Val_Loss: 3972.39404296875\n",
      "Epoch 5209, Train_Loss: 3843.61328125, Val_Loss: 3972.37548828125\n",
      "Epoch 5210, Train_Loss: 3843.603759765625, Val_Loss: 3972.363525390625\n",
      "Epoch 5211, Train_Loss: 3843.578125, Val_Loss: 3972.285888671875\n",
      "Epoch 5212, Train_Loss: 3843.551513671875, Val_Loss: 3972.279296875\n",
      "Epoch 5213, Train_Loss: 3843.539306640625, Val_Loss: 3972.262451171875\n",
      "Epoch 5214, Train_Loss: 3843.46923828125, Val_Loss: 3972.25439453125\n",
      "Epoch 5215, Train_Loss: 3843.45458984375, Val_Loss: 3972.239990234375\n",
      "Epoch 5216, Train_Loss: 3843.42626953125, Val_Loss: 3972.226806640625\n",
      "Epoch 5217, Train_Loss: 3843.4423828125, Val_Loss: 3972.222900390625\n",
      "Epoch 5218, Train_Loss: 3843.342529296875, Val_Loss: 3972.18994140625\n",
      "Epoch 5219, Train_Loss: 3843.34130859375, Val_Loss: 3972.184326171875\n",
      "Epoch 5220, Train_Loss: 3843.3271484375, Val_Loss: 3972.162353515625\n",
      "Epoch 5221, Train_Loss: 3843.31201171875, Val_Loss: 3972.101318359375\n",
      "Epoch 5222, Train_Loss: 3843.266845703125, Val_Loss: 3972.0859375\n",
      "Epoch 5223, Train_Loss: 3843.255859375, Val_Loss: 3972.073974609375\n",
      "Epoch 5224, Train_Loss: 3843.15478515625, Val_Loss: 3971.996826171875\n",
      "Epoch 5225, Train_Loss: 3843.13916015625, Val_Loss: 3971.98388671875\n",
      "Epoch 5226, Train_Loss: 3843.115966796875, Val_Loss: 3971.95361328125\n",
      "Epoch 5227, Train_Loss: 3843.116455078125, Val_Loss: 3971.923095703125\n",
      "Epoch 5228, Train_Loss: 3843.08203125, Val_Loss: 3971.897705078125\n",
      "Epoch 5229, Train_Loss: 3843.056396484375, Val_Loss: 3971.88232421875\n",
      "Epoch 5230, Train_Loss: 3843.044677734375, Val_Loss: 3971.855224609375\n",
      "Epoch 5231, Train_Loss: 3843.00439453125, Val_Loss: 3971.832763671875\n",
      "Epoch 5232, Train_Loss: 3842.9833984375, Val_Loss: 3971.83154296875\n",
      "Epoch 5233, Train_Loss: 3842.972412109375, Val_Loss: 3971.818115234375\n",
      "Epoch 5234, Train_Loss: 3842.955810546875, Val_Loss: 3971.804443359375\n",
      "Epoch 5235, Train_Loss: 3842.921630859375, Val_Loss: 3971.753662109375\n",
      "Epoch 5236, Train_Loss: 3842.90380859375, Val_Loss: 3971.739501953125\n",
      "Epoch 5237, Train_Loss: 3842.884033203125, Val_Loss: 3971.71435546875\n",
      "Epoch 5238, Train_Loss: 3842.847900390625, Val_Loss: 3971.65966796875\n",
      "Epoch 5239, Train_Loss: 3842.89697265625, Val_Loss: 3971.61083984375\n",
      "Epoch 5240, Train_Loss: 3842.88916015625, Val_Loss: 3971.6044921875\n",
      "Epoch 5241, Train_Loss: 3842.6103515625, Val_Loss: 3971.607666015625\n",
      "Epoch 5242, Train_Loss: 3842.6181640625, Val_Loss: 3971.54638671875\n",
      "Epoch 5243, Train_Loss: 3842.60107421875, Val_Loss: 3971.540283203125\n",
      "Epoch 5244, Train_Loss: 3842.58154296875, Val_Loss: 3971.552734375\n",
      "Epoch 5245, Train_Loss: 3842.470458984375, Val_Loss: 3971.478515625\n",
      "Epoch 5246, Train_Loss: 3842.463623046875, Val_Loss: 3971.47314453125\n",
      "Epoch 5247, Train_Loss: 3842.442138671875, Val_Loss: 3971.49951171875\n",
      "Epoch 5248, Train_Loss: 3842.3955078125, Val_Loss: 3971.40869140625\n",
      "Epoch 5249, Train_Loss: 3842.3876953125, Val_Loss: 3971.38720703125\n",
      "Epoch 5250, Train_Loss: 3842.355712890625, Val_Loss: 3971.3876953125\n",
      "Epoch 5251, Train_Loss: 3842.313720703125, Val_Loss: 3971.325927734375\n",
      "Epoch 5252, Train_Loss: 3842.290771484375, Val_Loss: 3971.306884765625\n",
      "Epoch 5253, Train_Loss: 3842.274658203125, Val_Loss: 3971.290771484375\n",
      "Epoch 5254, Train_Loss: 3842.268310546875, Val_Loss: 3971.292724609375\n",
      "Epoch 5255, Train_Loss: 3842.233154296875, Val_Loss: 3971.2451171875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5256, Train_Loss: 3842.244140625, Val_Loss: 3971.210693359375\n",
      "Epoch 5257, Train_Loss: 3842.204345703125, Val_Loss: 3971.215087890625\n",
      "Epoch 5258, Train_Loss: 3842.1630859375, Val_Loss: 3971.1630859375\n",
      "Epoch 5259, Train_Loss: 3842.17919921875, Val_Loss: 3971.135986328125\n",
      "Epoch 5260, Train_Loss: 3842.14892578125, Val_Loss: 3971.12646484375\n",
      "Epoch 5261, Train_Loss: 3842.13037109375, Val_Loss: 3971.115478515625\n",
      "Epoch 5262, Train_Loss: 3842.10205078125, Val_Loss: 3971.042724609375\n",
      "Epoch 5263, Train_Loss: 3842.109619140625, Val_Loss: 3970.992431640625\n",
      "Epoch 5264, Train_Loss: 3842.061767578125, Val_Loss: 3971.01171875\n",
      "Epoch 5265, Train_Loss: 3842.046630859375, Val_Loss: 3970.9248046875\n",
      "Epoch 5266, Train_Loss: 3842.04296875, Val_Loss: 3970.894775390625\n",
      "Epoch 5267, Train_Loss: 3842.025146484375, Val_Loss: 3970.90869140625\n",
      "Epoch 5268, Train_Loss: 3841.923583984375, Val_Loss: 3970.9111328125\n",
      "Epoch 5269, Train_Loss: 3841.946044921875, Val_Loss: 3970.87890625\n",
      "Epoch 5270, Train_Loss: 3841.92724609375, Val_Loss: 3970.83642578125\n",
      "Epoch 5271, Train_Loss: 3841.897705078125, Val_Loss: 3970.84228515625\n",
      "Epoch 5272, Train_Loss: 3841.572998046875, Val_Loss: 3970.817138671875\n",
      "Epoch 5273, Train_Loss: 3841.57177734375, Val_Loss: 3970.783203125\n",
      "Epoch 5274, Train_Loss: 3841.5537109375, Val_Loss: 3970.78076171875\n",
      "Epoch 5275, Train_Loss: 3841.525146484375, Val_Loss: 3970.718505859375\n",
      "Epoch 5276, Train_Loss: 3841.5185546875, Val_Loss: 3970.70751953125\n",
      "Epoch 5277, Train_Loss: 3841.4912109375, Val_Loss: 3970.689208984375\n",
      "Epoch 5278, Train_Loss: 3841.4541015625, Val_Loss: 3970.609130859375\n",
      "Epoch 5279, Train_Loss: 3841.44873046875, Val_Loss: 3970.591552734375\n",
      "Epoch 5280, Train_Loss: 3841.431396484375, Val_Loss: 3970.575927734375\n",
      "Epoch 5281, Train_Loss: 3841.408935546875, Val_Loss: 3970.55517578125\n",
      "Epoch 5282, Train_Loss: 3841.37451171875, Val_Loss: 3970.543701171875\n",
      "Epoch 5283, Train_Loss: 3841.36181640625, Val_Loss: 3970.51806640625\n",
      "Epoch 5284, Train_Loss: 3841.316650390625, Val_Loss: 3970.51513671875\n",
      "Epoch 5285, Train_Loss: 3841.28369140625, Val_Loss: 3970.50390625\n",
      "Epoch 5286, Train_Loss: 3841.281005859375, Val_Loss: 3970.4912109375\n",
      "Epoch 5287, Train_Loss: 3841.2626953125, Val_Loss: 3970.452880859375\n",
      "Epoch 5288, Train_Loss: 3841.235595703125, Val_Loss: 3970.403564453125\n",
      "Epoch 5289, Train_Loss: 3841.2255859375, Val_Loss: 3970.386474609375\n",
      "Epoch 5290, Train_Loss: 3841.20361328125, Val_Loss: 3970.368896484375\n",
      "Epoch 5291, Train_Loss: 3841.199462890625, Val_Loss: 3970.36474609375\n",
      "Epoch 5292, Train_Loss: 3841.281005859375, Val_Loss: 3970.262451171875\n",
      "Epoch 5293, Train_Loss: 3841.278564453125, Val_Loss: 3970.233154296875\n",
      "Epoch 5294, Train_Loss: 3841.245361328125, Val_Loss: 3970.22607421875\n",
      "Epoch 5295, Train_Loss: 3841.237060546875, Val_Loss: 3970.2255859375\n",
      "Epoch 5296, Train_Loss: 3841.209228515625, Val_Loss: 3970.191162109375\n",
      "Epoch 5297, Train_Loss: 3841.207763671875, Val_Loss: 3970.18408203125\n",
      "Epoch 5298, Train_Loss: 3841.04052734375, Val_Loss: 3970.146484375\n",
      "Epoch 5299, Train_Loss: 3841.033203125, Val_Loss: 3970.1572265625\n",
      "Epoch 5300, Train_Loss: 3841.020263671875, Val_Loss: 3970.158447265625\n",
      "Epoch 5301, Train_Loss: 3840.9970703125, Val_Loss: 3970.113525390625\n",
      "Epoch 5302, Train_Loss: 3840.972900390625, Val_Loss: 3970.046875\n",
      "Epoch 5303, Train_Loss: 3840.955322265625, Val_Loss: 3970.048095703125\n",
      "Epoch 5304, Train_Loss: 3840.955078125, Val_Loss: 3970.03271484375\n",
      "Epoch 5305, Train_Loss: 3840.896484375, Val_Loss: 3969.96044921875\n",
      "Epoch 5306, Train_Loss: 3840.876708984375, Val_Loss: 3969.938720703125\n",
      "Epoch 5307, Train_Loss: 3840.877685546875, Val_Loss: 3969.9248046875\n",
      "Epoch 5308, Train_Loss: 3840.858154296875, Val_Loss: 3969.906005859375\n",
      "Epoch 5309, Train_Loss: 3840.829345703125, Val_Loss: 3969.865478515625\n",
      "Epoch 5310, Train_Loss: 3840.804931640625, Val_Loss: 3969.850341796875\n",
      "Epoch 5311, Train_Loss: 3840.790283203125, Val_Loss: 3969.850341796875\n",
      "Epoch 5312, Train_Loss: 3840.74658203125, Val_Loss: 3969.815185546875\n",
      "Epoch 5313, Train_Loss: 3840.732421875, Val_Loss: 3969.802001953125\n",
      "Epoch 5314, Train_Loss: 3840.718994140625, Val_Loss: 3969.784423828125\n",
      "Epoch 5315, Train_Loss: 3840.69677734375, Val_Loss: 3969.72607421875\n",
      "Epoch 5316, Train_Loss: 3840.634521484375, Val_Loss: 3969.729248046875\n",
      "Epoch 5317, Train_Loss: 3840.615234375, Val_Loss: 3969.713623046875\n",
      "Epoch 5318, Train_Loss: 3840.598388671875, Val_Loss: 3969.70361328125\n",
      "Epoch 5319, Train_Loss: 3840.549072265625, Val_Loss: 3969.6259765625\n",
      "Epoch 5320, Train_Loss: 3840.5302734375, Val_Loss: 3969.60009765625\n",
      "Epoch 5321, Train_Loss: 3840.518798828125, Val_Loss: 3969.5966796875\n",
      "Epoch 5322, Train_Loss: 3840.454345703125, Val_Loss: 3969.56591796875\n",
      "Epoch 5323, Train_Loss: 3840.4453125, Val_Loss: 3969.5546875\n",
      "Epoch 5324, Train_Loss: 3840.456787109375, Val_Loss: 3969.53125\n",
      "Epoch 5325, Train_Loss: 3840.363525390625, Val_Loss: 3969.4794921875\n",
      "Epoch 5326, Train_Loss: 3840.34375, Val_Loss: 3969.482421875\n",
      "Epoch 5327, Train_Loss: 3840.357421875, Val_Loss: 3969.457275390625\n",
      "Epoch 5328, Train_Loss: 3840.30712890625, Val_Loss: 3969.419677734375\n",
      "Epoch 5329, Train_Loss: 3840.2861328125, Val_Loss: 3969.40087890625\n",
      "Epoch 5330, Train_Loss: 3840.270263671875, Val_Loss: 3969.382080078125\n",
      "Epoch 5331, Train_Loss: 3840.232177734375, Val_Loss: 3969.37841796875\n",
      "Epoch 5332, Train_Loss: 3840.167236328125, Val_Loss: 3969.296875\n",
      "Epoch 5333, Train_Loss: 3840.112060546875, Val_Loss: 3969.28759765625\n",
      "Epoch 5334, Train_Loss: 3840.16552734375, Val_Loss: 3969.2548828125\n",
      "Epoch 5335, Train_Loss: 3840.082275390625, Val_Loss: 3969.24560546875\n",
      "Epoch 5336, Train_Loss: 3840.064453125, Val_Loss: 3969.250732421875\n",
      "Epoch 5337, Train_Loss: 3840.04296875, Val_Loss: 3969.217529296875\n",
      "Epoch 5338, Train_Loss: 3840.0634765625, Val_Loss: 3969.167236328125\n",
      "Epoch 5339, Train_Loss: 3840.05712890625, Val_Loss: 3969.131591796875\n",
      "Epoch 5340, Train_Loss: 3839.98583984375, Val_Loss: 3969.139892578125\n",
      "Epoch 5341, Train_Loss: 3839.9794921875, Val_Loss: 3969.13330078125\n",
      "Epoch 5342, Train_Loss: 3839.94921875, Val_Loss: 3969.063232421875\n",
      "Epoch 5343, Train_Loss: 3839.975341796875, Val_Loss: 3969.034912109375\n",
      "Epoch 5344, Train_Loss: 3839.94873046875, Val_Loss: 3969.03125\n",
      "Epoch 5345, Train_Loss: 3839.87646484375, Val_Loss: 3968.967529296875\n",
      "Epoch 5346, Train_Loss: 3839.902099609375, Val_Loss: 3968.941650390625\n",
      "Epoch 5347, Train_Loss: 3839.880126953125, Val_Loss: 3968.921630859375\n",
      "Epoch 5348, Train_Loss: 3839.557373046875, Val_Loss: 3968.91552734375\n",
      "Epoch 5349, Train_Loss: 3839.5, Val_Loss: 3968.9228515625\n",
      "Epoch 5350, Train_Loss: 3839.536865234375, Val_Loss: 3968.884033203125\n",
      "Epoch 5351, Train_Loss: 3839.5322265625, Val_Loss: 3968.868896484375\n",
      "Epoch 5352, Train_Loss: 3839.4189453125, Val_Loss: 3968.837890625\n",
      "Epoch 5353, Train_Loss: 3839.40625, Val_Loss: 3968.833984375\n",
      "Epoch 5354, Train_Loss: 3839.399658203125, Val_Loss: 3968.821533203125\n",
      "Epoch 5355, Train_Loss: 3839.302490234375, Val_Loss: 3968.733154296875\n",
      "Epoch 5356, Train_Loss: 3839.285400390625, Val_Loss: 3968.72998046875\n",
      "Epoch 5357, Train_Loss: 3839.27783203125, Val_Loss: 3968.72412109375\n",
      "Epoch 5358, Train_Loss: 3839.23193359375, Val_Loss: 3968.6298828125\n",
      "Epoch 5359, Train_Loss: 3839.207763671875, Val_Loss: 3968.6279296875\n",
      "Epoch 5360, Train_Loss: 3839.214599609375, Val_Loss: 3968.61279296875\n",
      "Epoch 5361, Train_Loss: 3839.21337890625, Val_Loss: 3968.607177734375\n",
      "Epoch 5362, Train_Loss: 3839.16064453125, Val_Loss: 3968.574462890625\n",
      "Epoch 5363, Train_Loss: 3839.123291015625, Val_Loss: 3968.565185546875\n",
      "Epoch 5364, Train_Loss: 3839.123291015625, Val_Loss: 3968.565673828125\n",
      "Epoch 5365, Train_Loss: 3839.080322265625, Val_Loss: 3968.529541015625\n",
      "Epoch 5366, Train_Loss: 3839.056884765625, Val_Loss: 3968.5048828125\n",
      "Epoch 5367, Train_Loss: 3839.041748046875, Val_Loss: 3968.4912109375\n",
      "Epoch 5368, Train_Loss: 3839.028564453125, Val_Loss: 3968.439208984375\n",
      "Epoch 5369, Train_Loss: 3839.012939453125, Val_Loss: 3968.426025390625\n",
      "Epoch 5370, Train_Loss: 3839.00439453125, Val_Loss: 3968.405517578125\n",
      "Epoch 5371, Train_Loss: 3838.977783203125, Val_Loss: 3968.3759765625\n",
      "Epoch 5372, Train_Loss: 3838.955810546875, Val_Loss: 3968.292724609375\n",
      "Epoch 5373, Train_Loss: 3838.96875, Val_Loss: 3968.28515625\n",
      "Epoch 5374, Train_Loss: 3838.96142578125, Val_Loss: 3968.27197265625\n",
      "Epoch 5375, Train_Loss: 3838.883544921875, Val_Loss: 3968.25244140625\n",
      "Epoch 5376, Train_Loss: 3838.836669921875, Val_Loss: 3968.253173828125\n",
      "Epoch 5377, Train_Loss: 3838.822021484375, Val_Loss: 3968.233642578125\n",
      "Epoch 5378, Train_Loss: 3838.52197265625, Val_Loss: 3968.201904296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5379, Train_Loss: 3838.474609375, Val_Loss: 3968.16162109375\n",
      "Epoch 5380, Train_Loss: 3838.441162109375, Val_Loss: 3968.155517578125\n",
      "Epoch 5381, Train_Loss: 3838.446533203125, Val_Loss: 3968.127685546875\n",
      "Epoch 5382, Train_Loss: 3838.3955078125, Val_Loss: 3968.0869140625\n",
      "Epoch 5383, Train_Loss: 3838.384521484375, Val_Loss: 3968.080322265625\n",
      "Epoch 5384, Train_Loss: 3838.404541015625, Val_Loss: 3968.080810546875\n",
      "Epoch 5385, Train_Loss: 3838.39599609375, Val_Loss: 3967.96875\n",
      "Epoch 5386, Train_Loss: 3838.385009765625, Val_Loss: 3967.95556640625\n",
      "Epoch 5387, Train_Loss: 3838.33544921875, Val_Loss: 3967.97314453125\n",
      "Epoch 5388, Train_Loss: 3838.275146484375, Val_Loss: 3967.91748046875\n",
      "Epoch 5389, Train_Loss: 3838.253173828125, Val_Loss: 3967.9033203125\n",
      "Epoch 5390, Train_Loss: 3838.3359375, Val_Loss: 3967.9033203125\n",
      "Epoch 5391, Train_Loss: 3838.198974609375, Val_Loss: 3967.85400390625\n",
      "Epoch 5392, Train_Loss: 3838.181884765625, Val_Loss: 3967.84130859375\n",
      "Epoch 5393, Train_Loss: 3838.253662109375, Val_Loss: 3967.827880859375\n",
      "Epoch 5394, Train_Loss: 3838.2509765625, Val_Loss: 3967.7451171875\n",
      "Epoch 5395, Train_Loss: 3838.146240234375, Val_Loss: 3967.73291015625\n",
      "Epoch 5396, Train_Loss: 3838.134033203125, Val_Loss: 3967.722900390625\n",
      "Epoch 5397, Train_Loss: 3838.21630859375, Val_Loss: 3967.71728515625\n",
      "Epoch 5398, Train_Loss: 3838.098388671875, Val_Loss: 3967.61767578125\n",
      "Epoch 5399, Train_Loss: 3838.078125, Val_Loss: 3967.619140625\n",
      "Epoch 5400, Train_Loss: 3838.14990234375, Val_Loss: 3967.61767578125\n",
      "Epoch 5401, Train_Loss: 3838.054931640625, Val_Loss: 3967.576904296875\n",
      "Epoch 5402, Train_Loss: 3838.016845703125, Val_Loss: 3967.564697265625\n",
      "Epoch 5403, Train_Loss: 3838.10693359375, Val_Loss: 3967.59521484375\n",
      "Epoch 5404, Train_Loss: 3837.8779296875, Val_Loss: 3967.502685546875\n",
      "Epoch 5405, Train_Loss: 3837.83349609375, Val_Loss: 3967.498779296875\n",
      "Epoch 5406, Train_Loss: 3837.909912109375, Val_Loss: 3967.55078125\n",
      "Epoch 5407, Train_Loss: 3837.85693359375, Val_Loss: 3967.4248046875\n",
      "Epoch 5408, Train_Loss: 3837.797119140625, Val_Loss: 3967.40478515625\n",
      "Epoch 5409, Train_Loss: 3837.852783203125, Val_Loss: 3967.46630859375\n",
      "Epoch 5410, Train_Loss: 3837.758544921875, Val_Loss: 3967.343505859375\n",
      "Epoch 5411, Train_Loss: 3837.737060546875, Val_Loss: 3967.275634765625\n",
      "Epoch 5412, Train_Loss: 3837.760498046875, Val_Loss: 3967.372802734375\n",
      "Epoch 5413, Train_Loss: 3837.67919921875, Val_Loss: 3967.234375\n",
      "Epoch 5414, Train_Loss: 3837.667236328125, Val_Loss: 3967.23486328125\n",
      "Epoch 5415, Train_Loss: 3837.709716796875, Val_Loss: 3967.3046875\n",
      "Epoch 5416, Train_Loss: 3837.6337890625, Val_Loss: 3967.1484375\n",
      "Epoch 5417, Train_Loss: 3837.557373046875, Val_Loss: 3967.17431640625\n",
      "Epoch 5418, Train_Loss: 3837.642578125, Val_Loss: 3967.255126953125\n",
      "Epoch 5419, Train_Loss: 3837.6845703125, Val_Loss: 3967.048095703125\n",
      "Epoch 5420, Train_Loss: 3837.661376953125, Val_Loss: 3967.065185546875\n",
      "Epoch 5421, Train_Loss: 3837.579345703125, Val_Loss: 3967.207275390625\n",
      "Epoch 5422, Train_Loss: 3837.621826171875, Val_Loss: 3966.96728515625\n",
      "Epoch 5423, Train_Loss: 3837.62109375, Val_Loss: 3966.978759765625\n",
      "Epoch 5424, Train_Loss: 3837.459716796875, Val_Loss: 3967.05908203125\n",
      "Epoch 5425, Train_Loss: 3837.557373046875, Val_Loss: 3966.89599609375\n",
      "Epoch 5426, Train_Loss: 3837.5322265625, Val_Loss: 3966.941162109375\n",
      "Epoch 5427, Train_Loss: 3837.374267578125, Val_Loss: 3966.968017578125\n",
      "Epoch 5428, Train_Loss: 3837.45654296875, Val_Loss: 3966.89599609375\n",
      "Epoch 5429, Train_Loss: 3837.429931640625, Val_Loss: 3966.897216796875\n",
      "Epoch 5430, Train_Loss: 3837.46875, Val_Loss: 3966.934326171875\n",
      "Epoch 5431, Train_Loss: 3837.359619140625, Val_Loss: 3966.785888671875\n",
      "Epoch 5432, Train_Loss: 3837.3603515625, Val_Loss: 3966.86962890625\n",
      "Epoch 5433, Train_Loss: 3837.284912109375, Val_Loss: 3966.934326171875\n",
      "Epoch 5434, Train_Loss: 3837.32080078125, Val_Loss: 3966.69970703125\n",
      "Epoch 5435, Train_Loss: 3837.293212890625, Val_Loss: 3966.71484375\n",
      "Epoch 5436, Train_Loss: 3837.328857421875, Val_Loss: 3966.83251953125\n",
      "Epoch 5437, Train_Loss: 3837.157958984375, Val_Loss: 3966.689697265625\n",
      "Epoch 5438, Train_Loss: 3837.15283203125, Val_Loss: 3966.593994140625\n",
      "Epoch 5439, Train_Loss: 3837.163818359375, Val_Loss: 3966.67431640625\n",
      "Epoch 5440, Train_Loss: 3837.1181640625, Val_Loss: 3966.649658203125\n",
      "Epoch 5441, Train_Loss: 3837.05419921875, Val_Loss: 3966.5791015625\n",
      "Epoch 5442, Train_Loss: 3837.09130859375, Val_Loss: 3966.57763671875\n",
      "Epoch 5443, Train_Loss: 3837.078369140625, Val_Loss: 3966.612060546875\n",
      "Epoch 5444, Train_Loss: 3837.05224609375, Val_Loss: 3966.555908203125\n",
      "Epoch 5445, Train_Loss: 3836.986083984375, Val_Loss: 3966.512451171875\n",
      "Epoch 5446, Train_Loss: 3837.034423828125, Val_Loss: 3966.531494140625\n",
      "Epoch 5447, Train_Loss: 3837.000244140625, Val_Loss: 3966.47509765625\n",
      "Epoch 5448, Train_Loss: 3836.955078125, Val_Loss: 3966.43798828125\n",
      "Epoch 5449, Train_Loss: 3836.856201171875, Val_Loss: 3966.44189453125\n",
      "Epoch 5450, Train_Loss: 3836.927978515625, Val_Loss: 3966.377685546875\n",
      "Epoch 5451, Train_Loss: 3836.877197265625, Val_Loss: 3966.353515625\n",
      "Epoch 5452, Train_Loss: 3836.848876953125, Val_Loss: 3966.317626953125\n",
      "Epoch 5453, Train_Loss: 3836.576904296875, Val_Loss: 3966.3056640625\n",
      "Epoch 5454, Train_Loss: 3836.55859375, Val_Loss: 3966.318115234375\n",
      "Epoch 5455, Train_Loss: 3836.48779296875, Val_Loss: 3966.28271484375\n",
      "Epoch 5456, Train_Loss: 3836.50146484375, Val_Loss: 3966.26123046875\n",
      "Epoch 5457, Train_Loss: 3836.4091796875, Val_Loss: 3966.250732421875\n",
      "Epoch 5458, Train_Loss: 3836.399658203125, Val_Loss: 3966.21044921875\n",
      "Epoch 5459, Train_Loss: 3836.3798828125, Val_Loss: 3966.197509765625\n",
      "Epoch 5460, Train_Loss: 3836.362548828125, Val_Loss: 3966.135498046875\n",
      "Epoch 5461, Train_Loss: 3836.317138671875, Val_Loss: 3966.127197265625\n",
      "Epoch 5462, Train_Loss: 3836.291259765625, Val_Loss: 3966.132080078125\n",
      "Epoch 5463, Train_Loss: 3836.276611328125, Val_Loss: 3966.0419921875\n",
      "Epoch 5464, Train_Loss: 3836.20947265625, Val_Loss: 3966.00048828125\n",
      "Epoch 5465, Train_Loss: 3836.191162109375, Val_Loss: 3965.998779296875\n",
      "Epoch 5466, Train_Loss: 3836.220703125, Val_Loss: 3965.999267578125\n",
      "Epoch 5467, Train_Loss: 3836.1650390625, Val_Loss: 3965.98046875\n",
      "Epoch 5468, Train_Loss: 3836.1630859375, Val_Loss: 3965.96435546875\n",
      "Epoch 5469, Train_Loss: 3836.14501953125, Val_Loss: 3965.953125\n",
      "Epoch 5470, Train_Loss: 3836.10205078125, Val_Loss: 3965.888427734375\n",
      "Epoch 5471, Train_Loss: 3836.096435546875, Val_Loss: 3965.899658203125\n",
      "Epoch 5472, Train_Loss: 3836.082763671875, Val_Loss: 3965.8798828125\n",
      "Epoch 5473, Train_Loss: 3836.048828125, Val_Loss: 3965.793212890625\n",
      "Epoch 5474, Train_Loss: 3836.04150390625, Val_Loss: 3965.771240234375\n",
      "Epoch 5475, Train_Loss: 3836.033935546875, Val_Loss: 3965.76513671875\n",
      "Epoch 5476, Train_Loss: 3835.980224609375, Val_Loss: 3965.68603515625\n",
      "Epoch 5477, Train_Loss: 3835.97265625, Val_Loss: 3965.66162109375\n",
      "Epoch 5478, Train_Loss: 3835.958740234375, Val_Loss: 3965.655517578125\n",
      "Epoch 5479, Train_Loss: 3835.8916015625, Val_Loss: 3965.62890625\n",
      "Epoch 5480, Train_Loss: 3835.8935546875, Val_Loss: 3965.589599609375\n",
      "Epoch 5481, Train_Loss: 3835.887939453125, Val_Loss: 3965.600830078125\n",
      "Epoch 5482, Train_Loss: 3835.522216796875, Val_Loss: 3965.55029296875\n",
      "Epoch 5483, Train_Loss: 3835.531982421875, Val_Loss: 3965.54833984375\n",
      "Epoch 5484, Train_Loss: 3835.5341796875, Val_Loss: 3965.534912109375\n",
      "Epoch 5485, Train_Loss: 3835.508544921875, Val_Loss: 3965.478759765625\n",
      "Epoch 5486, Train_Loss: 3835.494873046875, Val_Loss: 3965.455078125\n",
      "Epoch 5487, Train_Loss: 3835.4853515625, Val_Loss: 3965.44970703125\n",
      "Epoch 5488, Train_Loss: 3835.446044921875, Val_Loss: 3965.401611328125\n",
      "Epoch 5489, Train_Loss: 3835.40478515625, Val_Loss: 3965.3212890625\n",
      "Epoch 5490, Train_Loss: 3835.414306640625, Val_Loss: 3965.337646484375\n",
      "Epoch 5491, Train_Loss: 3835.395263671875, Val_Loss: 3965.32958984375\n",
      "Epoch 5492, Train_Loss: 3835.352783203125, Val_Loss: 3965.299560546875\n",
      "Epoch 5493, Train_Loss: 3835.34033203125, Val_Loss: 3965.289306640625\n",
      "Epoch 5494, Train_Loss: 3835.27001953125, Val_Loss: 3965.293212890625\n",
      "Epoch 5495, Train_Loss: 3835.22509765625, Val_Loss: 3965.248291015625\n",
      "Epoch 5496, Train_Loss: 3835.20263671875, Val_Loss: 3965.248779296875\n",
      "Epoch 5497, Train_Loss: 3835.21240234375, Val_Loss: 3965.219482421875\n",
      "Epoch 5498, Train_Loss: 3835.192626953125, Val_Loss: 3965.218505859375\n",
      "Epoch 5499, Train_Loss: 3835.15234375, Val_Loss: 3965.1220703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5500, Train_Loss: 3835.15234375, Val_Loss: 3965.134033203125\n",
      "Epoch 5501, Train_Loss: 3835.148681640625, Val_Loss: 3965.1064453125\n",
      "Epoch 5502, Train_Loss: 3835.103515625, Val_Loss: 3965.010009765625\n",
      "Epoch 5503, Train_Loss: 3835.095703125, Val_Loss: 3965.0400390625\n",
      "Epoch 5504, Train_Loss: 3835.0546875, Val_Loss: 3964.96923828125\n",
      "Epoch 5505, Train_Loss: 3835.043212890625, Val_Loss: 3964.986083984375\n",
      "Epoch 5506, Train_Loss: 3835.01904296875, Val_Loss: 3964.972412109375\n",
      "Epoch 5507, Train_Loss: 3835.01220703125, Val_Loss: 3964.906494140625\n",
      "Epoch 5508, Train_Loss: 3834.858154296875, Val_Loss: 3964.930908203125\n",
      "Epoch 5509, Train_Loss: 3834.8388671875, Val_Loss: 3964.888916015625\n",
      "Epoch 5510, Train_Loss: 3834.81787109375, Val_Loss: 3964.903564453125\n",
      "Epoch 5511, Train_Loss: 3834.81591796875, Val_Loss: 3964.9091796875\n",
      "Epoch 5512, Train_Loss: 3834.80126953125, Val_Loss: 3964.85205078125\n",
      "Epoch 5513, Train_Loss: 3834.7607421875, Val_Loss: 3964.86669921875\n",
      "Epoch 5514, Train_Loss: 3834.771728515625, Val_Loss: 3964.892822265625\n",
      "Epoch 5515, Train_Loss: 3834.735595703125, Val_Loss: 3964.75634765625\n",
      "Epoch 5516, Train_Loss: 3834.7294921875, Val_Loss: 3964.758056640625\n",
      "Epoch 5517, Train_Loss: 3834.681396484375, Val_Loss: 3964.795166015625\n",
      "Epoch 5518, Train_Loss: 3834.707763671875, Val_Loss: 3964.67041015625\n",
      "Epoch 5519, Train_Loss: 3834.6630859375, Val_Loss: 3964.811279296875\n",
      "Epoch 5520, Train_Loss: 3834.67236328125, Val_Loss: 3964.70849609375\n",
      "Epoch 5521, Train_Loss: 3834.665283203125, Val_Loss: 3964.734375\n",
      "Epoch 5522, Train_Loss: 3834.65869140625, Val_Loss: 3964.7275390625\n",
      "Epoch 5523, Train_Loss: 3834.613037109375, Val_Loss: 3964.682861328125\n",
      "Epoch 5524, Train_Loss: 3834.601318359375, Val_Loss: 3964.68408203125\n",
      "Epoch 5525, Train_Loss: 3834.590576171875, Val_Loss: 3964.680908203125\n",
      "Epoch 5526, Train_Loss: 3834.57763671875, Val_Loss: 3964.65234375\n",
      "Epoch 5527, Train_Loss: 3834.566162109375, Val_Loss: 3964.673095703125\n",
      "Epoch 5528, Train_Loss: 3834.5595703125, Val_Loss: 3964.65771484375\n",
      "Epoch 5529, Train_Loss: 3834.527587890625, Val_Loss: 3964.635986328125\n",
      "Epoch 5530, Train_Loss: 3834.541259765625, Val_Loss: 3964.6416015625\n",
      "Epoch 5531, Train_Loss: 3834.528564453125, Val_Loss: 3964.633544921875\n",
      "Epoch 5532, Train_Loss: 3834.4833984375, Val_Loss: 3964.54443359375\n",
      "Epoch 5533, Train_Loss: 3834.48193359375, Val_Loss: 3964.573974609375\n",
      "Epoch 5534, Train_Loss: 3834.454345703125, Val_Loss: 3964.5615234375\n",
      "Epoch 5535, Train_Loss: 3834.51708984375, Val_Loss: 3964.512451171875\n",
      "Epoch 5536, Train_Loss: 3834.43798828125, Val_Loss: 3964.5419921875\n",
      "Epoch 5537, Train_Loss: 3834.5078125, Val_Loss: 3964.5283203125\n",
      "Epoch 5538, Train_Loss: 3834.4931640625, Val_Loss: 3964.560302734375\n",
      "Epoch 5539, Train_Loss: 3834.484619140625, Val_Loss: 3964.55078125\n",
      "Epoch 5540, Train_Loss: 3834.525146484375, Val_Loss: 3964.485595703125\n",
      "Epoch 5541, Train_Loss: 3834.4169921875, Val_Loss: 3964.51611328125\n",
      "Epoch 5542, Train_Loss: 3834.451171875, Val_Loss: 3964.4462890625\n",
      "Epoch 5543, Train_Loss: 3834.427490234375, Val_Loss: 3964.449951171875\n",
      "Epoch 5544, Train_Loss: 3834.43359375, Val_Loss: 3964.439697265625\n",
      "Epoch 5545, Train_Loss: 3834.438232421875, Val_Loss: 3964.43115234375\n",
      "Epoch 5546, Train_Loss: 3834.4677734375, Val_Loss: 3964.41650390625\n",
      "Epoch 5547, Train_Loss: 3834.4345703125, Val_Loss: 3964.421142578125\n",
      "Epoch 5548, Train_Loss: 3834.4326171875, Val_Loss: 3964.40283203125\n",
      "Epoch 5549, Train_Loss: 3834.412841796875, Val_Loss: 3964.408447265625\n",
      "Epoch 5550, Train_Loss: 3834.407958984375, Val_Loss: 3964.394775390625\n",
      "Epoch 5551, Train_Loss: 3834.330078125, Val_Loss: 3964.322509765625\n",
      "Epoch 5552, Train_Loss: 3834.276611328125, Val_Loss: 3964.359619140625\n",
      "Epoch 5553, Train_Loss: 3834.2587890625, Val_Loss: 3964.289306640625\n",
      "Epoch 5554, Train_Loss: 3834.2626953125, Val_Loss: 3964.34228515625\n",
      "Epoch 5555, Train_Loss: 3834.2421875, Val_Loss: 3964.30322265625\n",
      "Epoch 5556, Train_Loss: 3834.251220703125, Val_Loss: 3964.347900390625\n",
      "Epoch 5557, Train_Loss: 3834.24853515625, Val_Loss: 3964.31884765625\n",
      "Epoch 5558, Train_Loss: 3834.2275390625, Val_Loss: 3964.293701171875\n",
      "Epoch 5559, Train_Loss: 3834.21337890625, Val_Loss: 3964.333984375\n",
      "Epoch 5560, Train_Loss: 3834.2275390625, Val_Loss: 3964.189208984375\n",
      "Epoch 5561, Train_Loss: 3834.15966796875, Val_Loss: 3964.305908203125\n",
      "Epoch 5562, Train_Loss: 3834.173828125, Val_Loss: 3964.2216796875\n",
      "Epoch 5563, Train_Loss: 3834.171875, Val_Loss: 3964.250732421875\n",
      "Epoch 5564, Train_Loss: 3834.17919921875, Val_Loss: 3964.22802734375\n",
      "Epoch 5565, Train_Loss: 3834.173828125, Val_Loss: 3964.211181640625\n",
      "Epoch 5566, Train_Loss: 3834.162109375, Val_Loss: 3964.20166015625\n",
      "Epoch 5567, Train_Loss: 3834.148681640625, Val_Loss: 3964.191650390625\n",
      "Epoch 5568, Train_Loss: 3834.134033203125, Val_Loss: 3964.182861328125\n",
      "Epoch 5569, Train_Loss: 3834.1142578125, Val_Loss: 3964.11767578125\n",
      "Epoch 5570, Train_Loss: 3834.079345703125, Val_Loss: 3964.084716796875\n",
      "Epoch 5571, Train_Loss: 3834.0673828125, Val_Loss: 3964.126708984375\n",
      "Epoch 5572, Train_Loss: 3834.076416015625, Val_Loss: 3964.051513671875\n",
      "Epoch 5573, Train_Loss: 3834.051025390625, Val_Loss: 3964.10791015625\n",
      "Epoch 5574, Train_Loss: 3834.041259765625, Val_Loss: 3964.084716796875\n",
      "Epoch 5575, Train_Loss: 3834.046875, Val_Loss: 3964.10009765625\n",
      "Epoch 5576, Train_Loss: 3834.02880859375, Val_Loss: 3964.10791015625\n",
      "Epoch 5577, Train_Loss: 3834.145263671875, Val_Loss: 3964.016845703125\n",
      "Epoch 5578, Train_Loss: 3833.9833984375, Val_Loss: 3964.131591796875\n",
      "Epoch 5579, Train_Loss: 3833.9169921875, Val_Loss: 3964.001708984375\n",
      "Epoch 5580, Train_Loss: 3833.88916015625, Val_Loss: 3964.048095703125\n",
      "Epoch 5581, Train_Loss: 3833.8603515625, Val_Loss: 3964.083251953125\n",
      "Epoch 5582, Train_Loss: 3833.986572265625, Val_Loss: 3963.935302734375\n",
      "Epoch 5583, Train_Loss: 3834.062744140625, Val_Loss: 3964.050048828125\n",
      "Epoch 5584, Train_Loss: 3834.05908203125, Val_Loss: 3964.031494140625\n",
      "Epoch 5585, Train_Loss: 3834.156494140625, Val_Loss: 3963.926513671875\n",
      "Epoch 5586, Train_Loss: 3833.996826171875, Val_Loss: 3964.060302734375\n",
      "Epoch 5587, Train_Loss: 3834.15087890625, Val_Loss: 3963.968505859375\n",
      "Epoch 5588, Train_Loss: 3834.14306640625, Val_Loss: 3963.859130859375\n",
      "Epoch 5589, Train_Loss: 3833.961669921875, Val_Loss: 3964.002685546875\n",
      "Epoch 5590, Train_Loss: 3834.08740234375, Val_Loss: 3963.815673828125\n",
      "Epoch 5591, Train_Loss: 3834.0888671875, Val_Loss: 3963.89208984375\n",
      "Epoch 5592, Train_Loss: 3833.923828125, Val_Loss: 3963.9296875\n",
      "Epoch 5593, Train_Loss: 3834.046142578125, Val_Loss: 3963.8251953125\n",
      "Epoch 5594, Train_Loss: 3834.027099609375, Val_Loss: 3963.92529296875\n",
      "Epoch 5595, Train_Loss: 3834.04833984375, Val_Loss: 3963.884033203125\n",
      "Epoch 5596, Train_Loss: 3834.022216796875, Val_Loss: 3963.79248046875\n",
      "Epoch 5597, Train_Loss: 3833.97412109375, Val_Loss: 3963.897705078125\n",
      "Epoch 5598, Train_Loss: 3834.0087890625, Val_Loss: 3963.8251953125\n",
      "Epoch 5599, Train_Loss: 3834.00439453125, Val_Loss: 3963.795654296875\n",
      "Epoch 5600, Train_Loss: 3833.94677734375, Val_Loss: 3963.8798828125\n",
      "Epoch 5601, Train_Loss: 3833.99609375, Val_Loss: 3963.792724609375\n",
      "Epoch 5602, Train_Loss: 3833.96875, Val_Loss: 3963.815673828125\n",
      "Epoch 5603, Train_Loss: 3833.9296875, Val_Loss: 3963.8427734375\n",
      "Epoch 5604, Train_Loss: 3833.97314453125, Val_Loss: 3963.69970703125\n",
      "Epoch 5605, Train_Loss: 3833.948974609375, Val_Loss: 3963.789306640625\n",
      "Epoch 5606, Train_Loss: 3833.9423828125, Val_Loss: 3963.790771484375\n",
      "Epoch 5607, Train_Loss: 3833.923583984375, Val_Loss: 3963.61328125\n",
      "Epoch 5608, Train_Loss: 3833.892578125, Val_Loss: 3963.7255859375\n",
      "Epoch 5609, Train_Loss: 3833.90283203125, Val_Loss: 3963.68408203125\n",
      "Epoch 5610, Train_Loss: 3833.90869140625, Val_Loss: 3963.636474609375\n",
      "Epoch 5611, Train_Loss: 3833.8359375, Val_Loss: 3963.7568359375\n",
      "Epoch 5612, Train_Loss: 3833.883544921875, Val_Loss: 3963.62353515625\n",
      "Epoch 5613, Train_Loss: 3833.851318359375, Val_Loss: 3963.656005859375\n",
      "Epoch 5614, Train_Loss: 3833.817626953125, Val_Loss: 3963.72998046875\n",
      "Epoch 5615, Train_Loss: 3833.758056640625, Val_Loss: 3963.595947265625\n",
      "Epoch 5616, Train_Loss: 3833.533447265625, Val_Loss: 3963.654052734375\n",
      "Epoch 5617, Train_Loss: 3833.530029296875, Val_Loss: 3963.616455078125\n",
      "Epoch 5618, Train_Loss: 3833.45458984375, Val_Loss: 3963.5419921875\n",
      "Epoch 5619, Train_Loss: 3833.486083984375, Val_Loss: 3963.669677734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5620, Train_Loss: 3833.52294921875, Val_Loss: 3963.581298828125\n",
      "Epoch 5621, Train_Loss: 3833.50146484375, Val_Loss: 3963.568359375\n",
      "Epoch 5622, Train_Loss: 3833.490966796875, Val_Loss: 3963.63037109375\n",
      "Epoch 5623, Train_Loss: 3833.451904296875, Val_Loss: 3963.48291015625\n",
      "Epoch 5624, Train_Loss: 3833.49072265625, Val_Loss: 3963.605712890625\n",
      "Epoch 5625, Train_Loss: 3833.40478515625, Val_Loss: 3963.498779296875\n",
      "Epoch 5626, Train_Loss: 3833.367431640625, Val_Loss: 3963.418701171875\n",
      "Epoch 5627, Train_Loss: 3833.3798828125, Val_Loss: 3963.533203125\n",
      "Epoch 5628, Train_Loss: 3833.414306640625, Val_Loss: 3963.395263671875\n",
      "Epoch 5629, Train_Loss: 3833.357421875, Val_Loss: 3963.46923828125\n",
      "Epoch 5630, Train_Loss: 3833.40771484375, Val_Loss: 3963.46484375\n",
      "Epoch 5631, Train_Loss: 3833.419921875, Val_Loss: 3963.436279296875\n",
      "Epoch 5632, Train_Loss: 3833.34033203125, Val_Loss: 3963.478515625\n",
      "Epoch 5633, Train_Loss: 3833.33203125, Val_Loss: 3963.35400390625\n",
      "Epoch 5634, Train_Loss: 3833.28857421875, Val_Loss: 3963.527587890625\n",
      "Epoch 5635, Train_Loss: 3833.276611328125, Val_Loss: 3963.342041015625\n",
      "Epoch 5636, Train_Loss: 3833.333251953125, Val_Loss: 3963.389892578125\n",
      "Epoch 5637, Train_Loss: 3833.261962890625, Val_Loss: 3963.51171875\n",
      "Epoch 5638, Train_Loss: 3833.24658203125, Val_Loss: 3963.32470703125\n",
      "Epoch 5639, Train_Loss: 3833.286376953125, Val_Loss: 3963.400390625\n",
      "Epoch 5640, Train_Loss: 3833.28564453125, Val_Loss: 3963.335205078125\n",
      "Epoch 5641, Train_Loss: 3833.257568359375, Val_Loss: 3963.37548828125\n",
      "Epoch 5642, Train_Loss: 3833.21923828125, Val_Loss: 3963.28369140625\n",
      "Epoch 5643, Train_Loss: 3833.208984375, Val_Loss: 3963.323486328125\n",
      "Epoch 5644, Train_Loss: 3833.18408203125, Val_Loss: 3963.2412109375\n",
      "Epoch 5645, Train_Loss: 3833.252685546875, Val_Loss: 3963.280029296875\n",
      "Epoch 5646, Train_Loss: 3833.152099609375, Val_Loss: 3963.18310546875\n",
      "Epoch 5647, Train_Loss: 3833.163818359375, Val_Loss: 3963.312744140625\n",
      "Epoch 5648, Train_Loss: 3833.220458984375, Val_Loss: 3963.24755859375\n",
      "Epoch 5649, Train_Loss: 3833.20947265625, Val_Loss: 3963.268310546875\n",
      "Epoch 5650, Train_Loss: 3833.19140625, Val_Loss: 3963.257080078125\n",
      "Epoch 5651, Train_Loss: 3833.192626953125, Val_Loss: 3963.23486328125\n",
      "Epoch 5652, Train_Loss: 3833.1240234375, Val_Loss: 3963.2744140625\n",
      "Epoch 5653, Train_Loss: 3833.024658203125, Val_Loss: 3963.185546875\n",
      "Epoch 5654, Train_Loss: 3833.0166015625, Val_Loss: 3963.211669921875\n",
      "Epoch 5655, Train_Loss: 3833.015869140625, Val_Loss: 3963.17529296875\n",
      "Epoch 5656, Train_Loss: 3832.9365234375, Val_Loss: 3963.23486328125\n",
      "Epoch 5657, Train_Loss: 3832.958984375, Val_Loss: 3963.12451171875\n",
      "Epoch 5658, Train_Loss: 3832.9228515625, Val_Loss: 3963.199951171875\n",
      "Epoch 5659, Train_Loss: 3832.968994140625, Val_Loss: 3963.1484375\n",
      "Epoch 5660, Train_Loss: 3832.869384765625, Val_Loss: 3963.106689453125\n",
      "Epoch 5661, Train_Loss: 3832.886474609375, Val_Loss: 3963.213134765625\n",
      "Epoch 5662, Train_Loss: 3832.91162109375, Val_Loss: 3963.064453125\n",
      "Epoch 5663, Train_Loss: 3832.787353515625, Val_Loss: 3963.001220703125\n",
      "Epoch 5664, Train_Loss: 3832.83154296875, Val_Loss: 3963.126708984375\n",
      "Epoch 5665, Train_Loss: 3832.827392578125, Val_Loss: 3963.067626953125\n",
      "Epoch 5666, Train_Loss: 3832.7705078125, Val_Loss: 3963.005615234375\n",
      "Epoch 5667, Train_Loss: 3832.77978515625, Val_Loss: 3963.087646484375\n",
      "Epoch 5668, Train_Loss: 3832.78955078125, Val_Loss: 3963.073974609375\n",
      "Epoch 5669, Train_Loss: 3832.720703125, Val_Loss: 3962.99951171875\n",
      "Epoch 5670, Train_Loss: 3832.7568359375, Val_Loss: 3963.06640625\n",
      "Epoch 5671, Train_Loss: 3832.73291015625, Val_Loss: 3963.03271484375\n",
      "Epoch 5672, Train_Loss: 3832.673828125, Val_Loss: 3962.9404296875\n",
      "Epoch 5673, Train_Loss: 3832.693115234375, Val_Loss: 3963.0283203125\n",
      "Epoch 5674, Train_Loss: 3832.718017578125, Val_Loss: 3962.998779296875\n",
      "Epoch 5675, Train_Loss: 3832.654052734375, Val_Loss: 3962.919921875\n",
      "Epoch 5676, Train_Loss: 3832.62890625, Val_Loss: 3963.018798828125\n",
      "Epoch 5677, Train_Loss: 3832.691650390625, Val_Loss: 3962.976806640625\n",
      "Epoch 5678, Train_Loss: 3832.599365234375, Val_Loss: 3962.8671875\n",
      "Epoch 5679, Train_Loss: 3832.611328125, Val_Loss: 3962.99560546875\n",
      "Epoch 5680, Train_Loss: 3832.65771484375, Val_Loss: 3962.87890625\n",
      "Epoch 5681, Train_Loss: 3832.558349609375, Val_Loss: 3962.802490234375\n",
      "Epoch 5682, Train_Loss: 3832.564697265625, Val_Loss: 3962.910400390625\n",
      "Epoch 5683, Train_Loss: 3832.61279296875, Val_Loss: 3962.862060546875\n",
      "Epoch 5684, Train_Loss: 3832.531005859375, Val_Loss: 3962.751220703125\n",
      "Epoch 5685, Train_Loss: 3832.551025390625, Val_Loss: 3962.8935546875\n",
      "Epoch 5686, Train_Loss: 3832.599609375, Val_Loss: 3962.8564453125\n",
      "Epoch 5687, Train_Loss: 3832.496337890625, Val_Loss: 3962.785888671875\n",
      "Epoch 5688, Train_Loss: 3832.549072265625, Val_Loss: 3962.86669921875\n",
      "Epoch 5689, Train_Loss: 3832.501708984375, Val_Loss: 3962.803955078125\n",
      "Epoch 5690, Train_Loss: 3832.416259765625, Val_Loss: 3962.75927734375\n",
      "Epoch 5691, Train_Loss: 3832.428955078125, Val_Loss: 3962.813232421875\n",
      "Epoch 5692, Train_Loss: 3832.508056640625, Val_Loss: 3962.77197265625\n",
      "Epoch 5693, Train_Loss: 3832.410400390625, Val_Loss: 3962.719970703125\n",
      "Epoch 5694, Train_Loss: 3832.471435546875, Val_Loss: 3962.797119140625\n",
      "Epoch 5695, Train_Loss: 3832.4892578125, Val_Loss: 3962.77490234375\n",
      "Epoch 5696, Train_Loss: 3832.4140625, Val_Loss: 3962.6708984375\n",
      "Epoch 5697, Train_Loss: 3832.452392578125, Val_Loss: 3962.7607421875\n",
      "Epoch 5698, Train_Loss: 3832.37255859375, Val_Loss: 3962.6796875\n",
      "Epoch 5699, Train_Loss: 3832.308349609375, Val_Loss: 3962.5791015625\n",
      "Epoch 5700, Train_Loss: 3832.29443359375, Val_Loss: 3962.69482421875\n",
      "Epoch 5701, Train_Loss: 3832.29052734375, Val_Loss: 3962.62890625\n",
      "Epoch 5702, Train_Loss: 3832.25146484375, Val_Loss: 3962.552734375\n",
      "Epoch 5703, Train_Loss: 3832.309326171875, Val_Loss: 3962.714111328125\n",
      "Epoch 5704, Train_Loss: 3832.25732421875, Val_Loss: 3962.568115234375\n",
      "Epoch 5705, Train_Loss: 3832.2626953125, Val_Loss: 3962.63720703125\n",
      "Epoch 5706, Train_Loss: 3832.266357421875, Val_Loss: 3962.583251953125\n",
      "Epoch 5707, Train_Loss: 3832.200439453125, Val_Loss: 3962.53076171875\n",
      "Epoch 5708, Train_Loss: 3832.2470703125, Val_Loss: 3962.631591796875\n",
      "Epoch 5709, Train_Loss: 3832.16015625, Val_Loss: 3962.4931640625\n",
      "Epoch 5710, Train_Loss: 3832.245361328125, Val_Loss: 3962.577880859375\n",
      "Epoch 5711, Train_Loss: 3832.216796875, Val_Loss: 3962.5859375\n",
      "Epoch 5712, Train_Loss: 3832.145751953125, Val_Loss: 3962.463134765625\n",
      "Epoch 5713, Train_Loss: 3832.212158203125, Val_Loss: 3962.56396484375\n",
      "Epoch 5714, Train_Loss: 3832.15771484375, Val_Loss: 3962.505126953125\n",
      "Epoch 5715, Train_Loss: 3832.11474609375, Val_Loss: 3962.46728515625\n",
      "Epoch 5716, Train_Loss: 3832.166748046875, Val_Loss: 3962.492431640625\n",
      "Epoch 5717, Train_Loss: 3832.168212890625, Val_Loss: 3962.335693359375\n",
      "Epoch 5718, Train_Loss: 3832.1357421875, Val_Loss: 3962.467529296875\n",
      "Epoch 5719, Train_Loss: 3832.11962890625, Val_Loss: 3962.37158203125\n",
      "Epoch 5720, Train_Loss: 3832.0810546875, Val_Loss: 3962.426025390625\n",
      "Epoch 5721, Train_Loss: 3832.06201171875, Val_Loss: 3962.449951171875\n",
      "Epoch 5722, Train_Loss: 3832.109375, Val_Loss: 3962.406494140625\n",
      "Epoch 5723, Train_Loss: 3832.036865234375, Val_Loss: 3962.444091796875\n",
      "Epoch 5724, Train_Loss: 3832.098876953125, Val_Loss: 3962.396484375\n",
      "Epoch 5725, Train_Loss: 3831.984619140625, Val_Loss: 3962.384521484375\n",
      "Epoch 5726, Train_Loss: 3831.98388671875, Val_Loss: 3962.382080078125\n",
      "Epoch 5727, Train_Loss: 3831.99072265625, Val_Loss: 3962.339111328125\n",
      "Epoch 5728, Train_Loss: 3831.96826171875, Val_Loss: 3962.342041015625\n",
      "Epoch 5729, Train_Loss: 3831.9765625, Val_Loss: 3962.33203125\n",
      "Epoch 5730, Train_Loss: 3832.177001953125, Val_Loss: 3962.330322265625\n",
      "Epoch 5731, Train_Loss: 3832.17333984375, Val_Loss: 3962.301513671875\n",
      "Epoch 5732, Train_Loss: 3832.160400390625, Val_Loss: 3962.298828125\n",
      "Epoch 5733, Train_Loss: 3832.13720703125, Val_Loss: 3962.31005859375\n",
      "Epoch 5734, Train_Loss: 3832.116943359375, Val_Loss: 3962.232421875\n",
      "Epoch 5735, Train_Loss: 3832.10009765625, Val_Loss: 3962.2236328125\n",
      "Epoch 5736, Train_Loss: 3832.090576171875, Val_Loss: 3962.21484375\n",
      "Epoch 5737, Train_Loss: 3832.079345703125, Val_Loss: 3962.200439453125\n",
      "Epoch 5738, Train_Loss: 3832.073486328125, Val_Loss: 3962.1943359375\n",
      "Epoch 5739, Train_Loss: 3832.0625, Val_Loss: 3962.231689453125\n",
      "Epoch 5740, Train_Loss: 3832.04248046875, Val_Loss: 3962.2255859375\n",
      "Epoch 5741, Train_Loss: 3832.04443359375, Val_Loss: 3962.21728515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5742, Train_Loss: 3832.0419921875, Val_Loss: 3962.202392578125\n",
      "Epoch 5743, Train_Loss: 3831.9970703125, Val_Loss: 3962.181884765625\n",
      "Epoch 5744, Train_Loss: 3831.990966796875, Val_Loss: 3962.1728515625\n",
      "Epoch 5745, Train_Loss: 3831.996337890625, Val_Loss: 3962.1728515625\n",
      "Epoch 5746, Train_Loss: 3831.965576171875, Val_Loss: 3962.169921875\n",
      "Epoch 5747, Train_Loss: 3831.96630859375, Val_Loss: 3962.1484375\n",
      "Epoch 5748, Train_Loss: 3831.971923828125, Val_Loss: 3962.138916015625\n",
      "Epoch 5749, Train_Loss: 3831.949462890625, Val_Loss: 3962.1220703125\n",
      "Epoch 5750, Train_Loss: 3831.94873046875, Val_Loss: 3962.105712890625\n",
      "Epoch 5751, Train_Loss: 3831.947021484375, Val_Loss: 3962.110107421875\n",
      "Epoch 5752, Train_Loss: 3831.930419921875, Val_Loss: 3962.0087890625\n",
      "Epoch 5753, Train_Loss: 3831.922119140625, Val_Loss: 3962.005126953125\n",
      "Epoch 5754, Train_Loss: 3831.887451171875, Val_Loss: 3962.00439453125\n",
      "Epoch 5755, Train_Loss: 3831.813720703125, Val_Loss: 3961.992919921875\n",
      "Epoch 5756, Train_Loss: 3831.85498046875, Val_Loss: 3961.9775390625\n",
      "Epoch 5757, Train_Loss: 3831.8466796875, Val_Loss: 3961.986083984375\n",
      "Epoch 5758, Train_Loss: 3831.84228515625, Val_Loss: 3961.951904296875\n",
      "Epoch 5759, Train_Loss: 3831.825927734375, Val_Loss: 3961.94921875\n",
      "Epoch 5760, Train_Loss: 3831.819580078125, Val_Loss: 3961.92919921875\n",
      "Epoch 5761, Train_Loss: 3831.505126953125, Val_Loss: 3961.922119140625\n",
      "Epoch 5762, Train_Loss: 3831.49658203125, Val_Loss: 3961.92236328125\n",
      "Epoch 5763, Train_Loss: 3831.4619140625, Val_Loss: 3961.894287109375\n",
      "Epoch 5764, Train_Loss: 3831.4248046875, Val_Loss: 3961.930419921875\n",
      "Epoch 5765, Train_Loss: 3831.526611328125, Val_Loss: 3961.8759765625\n",
      "Epoch 5766, Train_Loss: 3831.55810546875, Val_Loss: 3961.865234375\n",
      "Epoch 5767, Train_Loss: 3831.52685546875, Val_Loss: 3961.87841796875\n",
      "Epoch 5768, Train_Loss: 3831.4892578125, Val_Loss: 3961.849609375\n",
      "Epoch 5769, Train_Loss: 3831.5048828125, Val_Loss: 3961.853515625\n",
      "Epoch 5770, Train_Loss: 3831.39794921875, Val_Loss: 3961.804443359375\n",
      "Epoch 5771, Train_Loss: 3831.3662109375, Val_Loss: 3961.746826171875\n",
      "Epoch 5772, Train_Loss: 3831.42041015625, Val_Loss: 3961.752685546875\n",
      "Epoch 5773, Train_Loss: 3831.40771484375, Val_Loss: 3961.75390625\n",
      "Epoch 5774, Train_Loss: 3831.3447265625, Val_Loss: 3961.74951171875\n",
      "Epoch 5775, Train_Loss: 3831.35302734375, Val_Loss: 3961.771484375\n",
      "Epoch 5776, Train_Loss: 3831.376708984375, Val_Loss: 3961.772705078125\n",
      "Epoch 5777, Train_Loss: 3831.41650390625, Val_Loss: 3961.7412109375\n",
      "Epoch 5778, Train_Loss: 3831.375732421875, Val_Loss: 3961.711669921875\n",
      "Epoch 5779, Train_Loss: 3831.327392578125, Val_Loss: 3961.7216796875\n",
      "Epoch 5780, Train_Loss: 3831.35400390625, Val_Loss: 3961.70751953125\n",
      "Epoch 5781, Train_Loss: 3831.302490234375, Val_Loss: 3961.67919921875\n",
      "Epoch 5782, Train_Loss: 3831.303955078125, Val_Loss: 3961.7080078125\n",
      "Epoch 5783, Train_Loss: 3831.299560546875, Val_Loss: 3961.65673828125\n",
      "Epoch 5784, Train_Loss: 3831.296630859375, Val_Loss: 3961.681884765625\n",
      "Epoch 5785, Train_Loss: 3831.281494140625, Val_Loss: 3961.637939453125\n",
      "Epoch 5786, Train_Loss: 3831.2958984375, Val_Loss: 3961.649169921875\n",
      "Epoch 5787, Train_Loss: 3831.26123046875, Val_Loss: 3961.563232421875\n",
      "Epoch 5788, Train_Loss: 3831.254638671875, Val_Loss: 3961.574462890625\n",
      "Epoch 5789, Train_Loss: 3831.218994140625, Val_Loss: 3961.57763671875\n",
      "Epoch 5790, Train_Loss: 3831.223388671875, Val_Loss: 3961.55517578125\n",
      "Epoch 5791, Train_Loss: 3831.22705078125, Val_Loss: 3961.580322265625\n",
      "Epoch 5792, Train_Loss: 3831.190673828125, Val_Loss: 3961.5556640625\n",
      "Epoch 5793, Train_Loss: 3831.200439453125, Val_Loss: 3961.558837890625\n",
      "Epoch 5794, Train_Loss: 3831.1640625, Val_Loss: 3961.53955078125\n",
      "Epoch 5795, Train_Loss: 3831.18310546875, Val_Loss: 3961.5556640625\n",
      "Epoch 5796, Train_Loss: 3831.01318359375, Val_Loss: 3961.5244140625\n",
      "Epoch 5797, Train_Loss: 3831.0, Val_Loss: 3961.498046875\n",
      "Epoch 5798, Train_Loss: 3830.99951171875, Val_Loss: 3961.525634765625\n",
      "Epoch 5799, Train_Loss: 3830.98974609375, Val_Loss: 3961.484375\n",
      "Epoch 5800, Train_Loss: 3830.95556640625, Val_Loss: 3961.498046875\n",
      "Epoch 5801, Train_Loss: 3830.9375, Val_Loss: 3961.486328125\n",
      "Epoch 5802, Train_Loss: 3830.960693359375, Val_Loss: 3961.437255859375\n",
      "Epoch 5803, Train_Loss: 3830.95166015625, Val_Loss: 3961.469970703125\n",
      "Epoch 5804, Train_Loss: 3830.939453125, Val_Loss: 3961.427490234375\n",
      "Epoch 5805, Train_Loss: 3830.8798828125, Val_Loss: 3961.372314453125\n",
      "Epoch 5806, Train_Loss: 3830.85986328125, Val_Loss: 3961.37548828125\n",
      "Epoch 5807, Train_Loss: 3830.88330078125, Val_Loss: 3961.3369140625\n",
      "Epoch 5808, Train_Loss: 3830.835693359375, Val_Loss: 3961.36083984375\n",
      "Epoch 5809, Train_Loss: 3830.857177734375, Val_Loss: 3961.35888671875\n",
      "Epoch 5810, Train_Loss: 3830.83740234375, Val_Loss: 3961.3564453125\n",
      "Epoch 5811, Train_Loss: 3830.80322265625, Val_Loss: 3961.369140625\n",
      "Epoch 5812, Train_Loss: 3830.822998046875, Val_Loss: 3961.3291015625\n",
      "Epoch 5813, Train_Loss: 3830.7646484375, Val_Loss: 3961.335693359375\n",
      "Epoch 5814, Train_Loss: 3830.76123046875, Val_Loss: 3961.313232421875\n",
      "Epoch 5815, Train_Loss: 3830.751220703125, Val_Loss: 3961.306884765625\n",
      "Epoch 5816, Train_Loss: 3830.724609375, Val_Loss: 3961.326416015625\n",
      "Epoch 5817, Train_Loss: 3830.75439453125, Val_Loss: 3961.27490234375\n",
      "Epoch 5818, Train_Loss: 3830.72509765625, Val_Loss: 3961.283203125\n",
      "Epoch 5819, Train_Loss: 3830.71142578125, Val_Loss: 3961.26318359375\n",
      "Epoch 5820, Train_Loss: 3830.727294921875, Val_Loss: 3961.249267578125\n",
      "Epoch 5821, Train_Loss: 3830.696533203125, Val_Loss: 3961.268310546875\n",
      "Epoch 5822, Train_Loss: 3830.819580078125, Val_Loss: 3961.138427734375\n",
      "Epoch 5823, Train_Loss: 3830.6552734375, Val_Loss: 3961.178466796875\n",
      "Epoch 5824, Train_Loss: 3830.80517578125, Val_Loss: 3961.12841796875\n",
      "Epoch 5825, Train_Loss: 3830.63623046875, Val_Loss: 3961.1728515625\n",
      "Epoch 5826, Train_Loss: 3830.7841796875, Val_Loss: 3961.140869140625\n",
      "Epoch 5827, Train_Loss: 3830.73291015625, Val_Loss: 3961.1708984375\n",
      "Epoch 5828, Train_Loss: 3830.764404296875, Val_Loss: 3961.138427734375\n",
      "Epoch 5829, Train_Loss: 3830.74609375, Val_Loss: 3961.15869140625\n",
      "Epoch 5830, Train_Loss: 3830.7509765625, Val_Loss: 3961.140380859375\n",
      "Epoch 5831, Train_Loss: 3830.662841796875, Val_Loss: 3961.11474609375\n",
      "Epoch 5832, Train_Loss: 3830.639892578125, Val_Loss: 3961.105224609375\n",
      "Epoch 5833, Train_Loss: 3830.672607421875, Val_Loss: 3961.106689453125\n",
      "Epoch 5834, Train_Loss: 3830.64794921875, Val_Loss: 3961.115478515625\n",
      "Epoch 5835, Train_Loss: 3830.722412109375, Val_Loss: 3961.06005859375\n",
      "Epoch 5836, Train_Loss: 3830.6474609375, Val_Loss: 3961.06640625\n",
      "Epoch 5837, Train_Loss: 3830.637939453125, Val_Loss: 3961.073974609375\n",
      "Epoch 5838, Train_Loss: 3830.6748046875, Val_Loss: 3961.036865234375\n",
      "Epoch 5839, Train_Loss: 3830.57470703125, Val_Loss: 3961.021484375\n",
      "Epoch 5840, Train_Loss: 3830.55908203125, Val_Loss: 3960.9599609375\n",
      "Epoch 5841, Train_Loss: 3830.514404296875, Val_Loss: 3960.974365234375\n",
      "Epoch 5842, Train_Loss: 3830.50732421875, Val_Loss: 3960.97998046875\n",
      "Epoch 5843, Train_Loss: 3830.54296875, Val_Loss: 3960.923583984375\n",
      "Epoch 5844, Train_Loss: 3830.49169921875, Val_Loss: 3960.99365234375\n",
      "Epoch 5845, Train_Loss: 3830.492919921875, Val_Loss: 3960.96923828125\n",
      "Epoch 5846, Train_Loss: 3830.516845703125, Val_Loss: 3960.935302734375\n",
      "Epoch 5847, Train_Loss: 3830.460205078125, Val_Loss: 3960.9931640625\n",
      "Epoch 5848, Train_Loss: 3830.475830078125, Val_Loss: 3960.881103515625\n",
      "Epoch 5849, Train_Loss: 3830.44189453125, Val_Loss: 3960.906494140625\n",
      "Epoch 5850, Train_Loss: 3830.413818359375, Val_Loss: 3960.941650390625\n",
      "Epoch 5851, Train_Loss: 3830.452880859375, Val_Loss: 3960.840087890625\n",
      "Epoch 5852, Train_Loss: 3830.42041015625, Val_Loss: 3960.8798828125\n",
      "Epoch 5853, Train_Loss: 3830.4248046875, Val_Loss: 3960.9228515625\n",
      "Epoch 5854, Train_Loss: 3830.365966796875, Val_Loss: 3960.81591796875\n",
      "Epoch 5855, Train_Loss: 3830.38818359375, Val_Loss: 3960.842041015625\n",
      "Epoch 5856, Train_Loss: 3830.40869140625, Val_Loss: 3960.84228515625\n",
      "Epoch 5857, Train_Loss: 3830.32275390625, Val_Loss: 3960.70849609375\n",
      "Epoch 5858, Train_Loss: 3830.30419921875, Val_Loss: 3960.757568359375\n",
      "Epoch 5859, Train_Loss: 3830.35009765625, Val_Loss: 3960.848388671875\n",
      "Epoch 5860, Train_Loss: 3830.375, Val_Loss: 3960.63916015625\n",
      "Epoch 5861, Train_Loss: 3830.2958984375, Val_Loss: 3960.798095703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5862, Train_Loss: 3830.33544921875, Val_Loss: 3960.814697265625\n",
      "Epoch 5863, Train_Loss: 3830.33740234375, Val_Loss: 3960.65087890625\n",
      "Epoch 5864, Train_Loss: 3830.2509765625, Val_Loss: 3960.773681640625\n",
      "Epoch 5865, Train_Loss: 3830.21630859375, Val_Loss: 3960.77685546875\n",
      "Epoch 5866, Train_Loss: 3830.203125, Val_Loss: 3960.614013671875\n",
      "Epoch 5867, Train_Loss: 3830.13134765625, Val_Loss: 3960.70751953125\n",
      "Epoch 5868, Train_Loss: 3830.1826171875, Val_Loss: 3960.796875\n",
      "Epoch 5869, Train_Loss: 3830.31884765625, Val_Loss: 3960.61474609375\n",
      "Epoch 5870, Train_Loss: 3830.35400390625, Val_Loss: 3960.634033203125\n",
      "Epoch 5871, Train_Loss: 3830.314697265625, Val_Loss: 3960.80517578125\n",
      "Epoch 5872, Train_Loss: 3830.373046875, Val_Loss: 3960.572021484375\n",
      "Epoch 5873, Train_Loss: 3830.328857421875, Val_Loss: 3960.6201171875\n",
      "Epoch 5874, Train_Loss: 3830.248779296875, Val_Loss: 3960.718505859375\n",
      "Epoch 5875, Train_Loss: 3830.30419921875, Val_Loss: 3960.484375\n",
      "Epoch 5876, Train_Loss: 3830.25927734375, Val_Loss: 3960.5439453125\n",
      "Epoch 5877, Train_Loss: 3830.232177734375, Val_Loss: 3960.700439453125\n",
      "Epoch 5878, Train_Loss: 3830.247802734375, Val_Loss: 3960.52685546875\n",
      "Epoch 5879, Train_Loss: 3830.1953125, Val_Loss: 3960.522705078125\n",
      "Epoch 5880, Train_Loss: 3830.175537109375, Val_Loss: 3960.6376953125\n",
      "Epoch 5881, Train_Loss: 3830.21923828125, Val_Loss: 3960.52587890625\n",
      "Epoch 5882, Train_Loss: 3830.193603515625, Val_Loss: 3960.493896484375\n",
      "Epoch 5883, Train_Loss: 3830.156494140625, Val_Loss: 3960.545654296875\n",
      "Epoch 5884, Train_Loss: 3830.133544921875, Val_Loss: 3960.44677734375\n",
      "Epoch 5885, Train_Loss: 3830.14990234375, Val_Loss: 3960.49072265625\n",
      "Epoch 5886, Train_Loss: 3830.11865234375, Val_Loss: 3960.5595703125\n",
      "Epoch 5887, Train_Loss: 3830.122314453125, Val_Loss: 3960.466064453125\n",
      "Epoch 5888, Train_Loss: 3830.1494140625, Val_Loss: 3960.44482421875\n",
      "Epoch 5889, Train_Loss: 3830.09130859375, Val_Loss: 3960.50927734375\n",
      "Epoch 5890, Train_Loss: 3830.099853515625, Val_Loss: 3960.474853515625\n",
      "Epoch 5891, Train_Loss: 3830.080810546875, Val_Loss: 3960.369140625\n",
      "Epoch 5892, Train_Loss: 3830.088623046875, Val_Loss: 3960.404296875\n",
      "Epoch 5893, Train_Loss: 3830.08056640625, Val_Loss: 3960.3984375\n",
      "Epoch 5894, Train_Loss: 3830.07421875, Val_Loss: 3960.330078125\n",
      "Epoch 5895, Train_Loss: 3830.057373046875, Val_Loss: 3960.3896484375\n",
      "Epoch 5896, Train_Loss: 3830.059814453125, Val_Loss: 3960.377685546875\n",
      "Epoch 5897, Train_Loss: 3830.0419921875, Val_Loss: 3960.360107421875\n",
      "Epoch 5898, Train_Loss: 3830.04443359375, Val_Loss: 3960.361083984375\n",
      "Epoch 5899, Train_Loss: 3830.02099609375, Val_Loss: 3960.35791015625\n",
      "Epoch 5900, Train_Loss: 3829.70751953125, Val_Loss: 3960.32470703125\n",
      "Epoch 5901, Train_Loss: 3829.695556640625, Val_Loss: 3960.308837890625\n",
      "Epoch 5902, Train_Loss: 3829.68212890625, Val_Loss: 3960.328857421875\n",
      "Epoch 5903, Train_Loss: 3829.66845703125, Val_Loss: 3960.300048828125\n",
      "Epoch 5904, Train_Loss: 3829.656494140625, Val_Loss: 3960.275634765625\n",
      "Epoch 5905, Train_Loss: 3829.6953125, Val_Loss: 3960.294921875\n",
      "Epoch 5906, Train_Loss: 3829.631103515625, Val_Loss: 3960.29443359375\n",
      "Epoch 5907, Train_Loss: 3829.610595703125, Val_Loss: 3960.27001953125\n",
      "Epoch 5908, Train_Loss: 3829.585205078125, Val_Loss: 3960.2236328125\n",
      "Epoch 5909, Train_Loss: 3829.586669921875, Val_Loss: 3960.201171875\n",
      "Epoch 5910, Train_Loss: 3829.590087890625, Val_Loss: 3960.17919921875\n",
      "Epoch 5911, Train_Loss: 3829.555419921875, Val_Loss: 3960.191650390625\n",
      "Epoch 5912, Train_Loss: 3829.561767578125, Val_Loss: 3960.205078125\n",
      "Epoch 5913, Train_Loss: 3829.557373046875, Val_Loss: 3960.180419921875\n",
      "Epoch 5914, Train_Loss: 3829.518798828125, Val_Loss: 3960.20068359375\n",
      "Epoch 5915, Train_Loss: 3829.539794921875, Val_Loss: 3960.168701171875\n",
      "Epoch 5916, Train_Loss: 3829.513671875, Val_Loss: 3960.152099609375\n",
      "Epoch 5917, Train_Loss: 3829.47705078125, Val_Loss: 3960.1552734375\n",
      "Epoch 5918, Train_Loss: 3829.485107421875, Val_Loss: 3960.131591796875\n",
      "Epoch 5919, Train_Loss: 3829.440673828125, Val_Loss: 3960.09326171875\n",
      "Epoch 5920, Train_Loss: 3829.438720703125, Val_Loss: 3960.13720703125\n",
      "Epoch 5921, Train_Loss: 3829.465576171875, Val_Loss: 3960.100341796875\n",
      "Epoch 5922, Train_Loss: 3829.41748046875, Val_Loss: 3960.0556640625\n",
      "Epoch 5923, Train_Loss: 3829.421630859375, Val_Loss: 3960.10400390625\n",
      "Epoch 5924, Train_Loss: 3829.413330078125, Val_Loss: 3960.08203125\n",
      "Epoch 5925, Train_Loss: 3829.387939453125, Val_Loss: 3959.965087890625\n",
      "Epoch 5926, Train_Loss: 3829.378662109375, Val_Loss: 3959.998779296875\n",
      "Epoch 5927, Train_Loss: 3829.373779296875, Val_Loss: 3959.981689453125\n",
      "Epoch 5928, Train_Loss: 3829.359375, Val_Loss: 3959.939208984375\n",
      "Epoch 5929, Train_Loss: 3829.338134765625, Val_Loss: 3960.006103515625\n",
      "Epoch 5930, Train_Loss: 3829.337646484375, Val_Loss: 3959.9892578125\n",
      "Epoch 5931, Train_Loss: 3829.31640625, Val_Loss: 3959.95849609375\n",
      "Epoch 5932, Train_Loss: 3829.3056640625, Val_Loss: 3959.992431640625\n",
      "Epoch 5933, Train_Loss: 3829.17236328125, Val_Loss: 3959.94873046875\n",
      "Epoch 5934, Train_Loss: 3829.128173828125, Val_Loss: 3959.92236328125\n",
      "Epoch 5935, Train_Loss: 3829.127685546875, Val_Loss: 3959.951904296875\n",
      "Epoch 5936, Train_Loss: 3829.140625, Val_Loss: 3959.939208984375\n",
      "Epoch 5937, Train_Loss: 3829.127197265625, Val_Loss: 3959.88916015625\n",
      "Epoch 5938, Train_Loss: 3829.11328125, Val_Loss: 3959.921630859375\n",
      "Epoch 5939, Train_Loss: 3829.0986328125, Val_Loss: 3959.918701171875\n",
      "Epoch 5940, Train_Loss: 3829.1083984375, Val_Loss: 3959.864501953125\n",
      "Epoch 5941, Train_Loss: 3829.098388671875, Val_Loss: 3959.8896484375\n",
      "Epoch 5942, Train_Loss: 3829.056884765625, Val_Loss: 3959.823486328125\n",
      "Epoch 5943, Train_Loss: 3829.049072265625, Val_Loss: 3959.78271484375\n",
      "Epoch 5944, Train_Loss: 3829.031982421875, Val_Loss: 3959.7900390625\n",
      "Epoch 5945, Train_Loss: 3829.024169921875, Val_Loss: 3959.786865234375\n",
      "Epoch 5946, Train_Loss: 3829.023193359375, Val_Loss: 3959.7724609375\n",
      "Epoch 5947, Train_Loss: 3829.004638671875, Val_Loss: 3959.7919921875\n",
      "Epoch 5948, Train_Loss: 3829.003173828125, Val_Loss: 3959.764404296875\n",
      "Epoch 5949, Train_Loss: 3828.987548828125, Val_Loss: 3959.760009765625\n",
      "Epoch 5950, Train_Loss: 3828.9580078125, Val_Loss: 3959.737548828125\n",
      "Epoch 5951, Train_Loss: 3828.926025390625, Val_Loss: 3959.719482421875\n",
      "Epoch 5952, Train_Loss: 3828.924072265625, Val_Loss: 3959.736083984375\n",
      "Epoch 5953, Train_Loss: 3828.914306640625, Val_Loss: 3959.709716796875\n",
      "Epoch 5954, Train_Loss: 3828.920654296875, Val_Loss: 3959.694091796875\n",
      "Epoch 5955, Train_Loss: 3828.912109375, Val_Loss: 3959.680419921875\n",
      "Epoch 5956, Train_Loss: 3828.8662109375, Val_Loss: 3959.685546875\n",
      "Epoch 5957, Train_Loss: 3828.872802734375, Val_Loss: 3959.6796875\n",
      "Epoch 5958, Train_Loss: 3828.837158203125, Val_Loss: 3959.675537109375\n",
      "Epoch 5959, Train_Loss: 3828.833251953125, Val_Loss: 3959.598388671875\n",
      "Epoch 5960, Train_Loss: 3828.80712890625, Val_Loss: 3959.59033203125\n",
      "Epoch 5961, Train_Loss: 3828.821044921875, Val_Loss: 3959.573974609375\n",
      "Epoch 5962, Train_Loss: 3828.8720703125, Val_Loss: 3959.563720703125\n",
      "Epoch 5963, Train_Loss: 3828.857177734375, Val_Loss: 3959.597900390625\n",
      "Epoch 5964, Train_Loss: 3828.846923828125, Val_Loss: 3959.585693359375\n",
      "Epoch 5965, Train_Loss: 3828.8408203125, Val_Loss: 3959.594482421875\n",
      "Epoch 5966, Train_Loss: 3828.883544921875, Val_Loss: 3959.553955078125\n",
      "Epoch 5967, Train_Loss: 3828.769775390625, Val_Loss: 3959.56005859375\n",
      "Epoch 5968, Train_Loss: 3828.770751953125, Val_Loss: 3959.529296875\n",
      "Epoch 5969, Train_Loss: 3828.7998046875, Val_Loss: 3959.51806640625\n",
      "Epoch 5970, Train_Loss: 3828.76416015625, Val_Loss: 3959.555908203125\n",
      "Epoch 5971, Train_Loss: 3828.82763671875, Val_Loss: 3959.494873046875\n",
      "Epoch 5972, Train_Loss: 3828.79296875, Val_Loss: 3959.50244140625\n",
      "Epoch 5973, Train_Loss: 3828.77685546875, Val_Loss: 3959.490478515625\n",
      "Epoch 5974, Train_Loss: 3828.768798828125, Val_Loss: 3959.489501953125\n",
      "Epoch 5975, Train_Loss: 3828.6728515625, Val_Loss: 3959.406494140625\n",
      "Epoch 5976, Train_Loss: 3828.664794921875, Val_Loss: 3959.40771484375\n",
      "Epoch 5977, Train_Loss: 3828.66064453125, Val_Loss: 3959.399658203125\n",
      "Epoch 5978, Train_Loss: 3828.642578125, Val_Loss: 3959.400390625\n",
      "Epoch 5979, Train_Loss: 3828.631103515625, Val_Loss: 3959.388427734375\n",
      "Epoch 5980, Train_Loss: 3828.616455078125, Val_Loss: 3959.422119140625\n",
      "Epoch 5981, Train_Loss: 3828.614013671875, Val_Loss: 3959.396728515625\n",
      "Epoch 5982, Train_Loss: 3828.60302734375, Val_Loss: 3959.392822265625\n",
      "Epoch 5983, Train_Loss: 3828.600341796875, Val_Loss: 3959.384765625\n",
      "Epoch 5984, Train_Loss: 3828.58056640625, Val_Loss: 3959.348876953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5985, Train_Loss: 3828.571044921875, Val_Loss: 3959.34033203125\n",
      "Epoch 5986, Train_Loss: 3828.56201171875, Val_Loss: 3959.337158203125\n",
      "Epoch 5987, Train_Loss: 3828.551513671875, Val_Loss: 3959.32958984375\n",
      "Epoch 5988, Train_Loss: 3828.536376953125, Val_Loss: 3959.33203125\n",
      "Epoch 5989, Train_Loss: 3828.581787109375, Val_Loss: 3959.283203125\n",
      "Epoch 5990, Train_Loss: 3828.578857421875, Val_Loss: 3959.269287109375\n",
      "Epoch 5991, Train_Loss: 3828.51806640625, Val_Loss: 3959.283203125\n",
      "Epoch 5992, Train_Loss: 3828.54541015625, Val_Loss: 3959.194091796875\n",
      "Epoch 5993, Train_Loss: 3828.528564453125, Val_Loss: 3959.180908203125\n",
      "Epoch 5994, Train_Loss: 3828.51318359375, Val_Loss: 3959.187255859375\n",
      "Epoch 5995, Train_Loss: 3828.514892578125, Val_Loss: 3959.171142578125\n",
      "Epoch 5996, Train_Loss: 3828.50439453125, Val_Loss: 3959.19287109375\n",
      "Epoch 5997, Train_Loss: 3828.479736328125, Val_Loss: 3959.200439453125\n",
      "Epoch 5998, Train_Loss: 3828.471923828125, Val_Loss: 3959.18994140625\n",
      "Epoch 5999, Train_Loss: 3828.45068359375, Val_Loss: 3959.179931640625\n",
      "Epoch 6000, Train_Loss: 3828.34033203125, Val_Loss: 3959.16552734375\n",
      "Epoch 6001, Train_Loss: 3828.33447265625, Val_Loss: 3959.160888671875\n",
      "Epoch 6002, Train_Loss: 3828.330810546875, Val_Loss: 3959.148681640625\n",
      "Epoch 6003, Train_Loss: 3828.313720703125, Val_Loss: 3959.131591796875\n",
      "Epoch 6004, Train_Loss: 3828.528076171875, Val_Loss: 3959.13525390625\n",
      "Epoch 6005, Train_Loss: 3828.521484375, Val_Loss: 3959.125244140625\n",
      "Epoch 6006, Train_Loss: 3828.5126953125, Val_Loss: 3959.105712890625\n",
      "Epoch 6007, Train_Loss: 3828.500244140625, Val_Loss: 3959.105712890625\n",
      "Epoch 6008, Train_Loss: 3828.4931640625, Val_Loss: 3959.097900390625\n",
      "Epoch 6009, Train_Loss: 3828.45556640625, Val_Loss: 3959.0244140625\n",
      "Epoch 6010, Train_Loss: 3828.5615234375, Val_Loss: 3959.001953125\n",
      "Epoch 6011, Train_Loss: 3828.552734375, Val_Loss: 3959.0\n",
      "Epoch 6012, Train_Loss: 3828.53466796875, Val_Loss: 3958.983154296875\n",
      "Epoch 6013, Train_Loss: 3828.531982421875, Val_Loss: 3959.001708984375\n",
      "Epoch 6014, Train_Loss: 3828.5205078125, Val_Loss: 3958.9873046875\n",
      "Epoch 6015, Train_Loss: 3828.511474609375, Val_Loss: 3959.0\n",
      "Epoch 6016, Train_Loss: 3828.509765625, Val_Loss: 3958.9931640625\n",
      "Epoch 6017, Train_Loss: 3828.46484375, Val_Loss: 3958.970458984375\n",
      "Epoch 6018, Train_Loss: 3828.46044921875, Val_Loss: 3958.96630859375\n",
      "Epoch 6019, Train_Loss: 3828.448486328125, Val_Loss: 3958.961181640625\n",
      "Epoch 6020, Train_Loss: 3828.4482421875, Val_Loss: 3958.954345703125\n",
      "Epoch 6021, Train_Loss: 3828.406982421875, Val_Loss: 3958.94873046875\n",
      "Epoch 6022, Train_Loss: 3828.427490234375, Val_Loss: 3958.906494140625\n",
      "Epoch 6023, Train_Loss: 3828.42578125, Val_Loss: 3958.912841796875\n",
      "Epoch 6024, Train_Loss: 3828.43408203125, Val_Loss: 3958.936279296875\n",
      "Epoch 6025, Train_Loss: 3828.4033203125, Val_Loss: 3958.83203125\n",
      "Epoch 6026, Train_Loss: 3828.394287109375, Val_Loss: 3958.8203125\n",
      "Epoch 6027, Train_Loss: 3828.387451171875, Val_Loss: 3958.84326171875\n",
      "Epoch 6028, Train_Loss: 3828.37939453125, Val_Loss: 3958.797119140625\n",
      "Epoch 6029, Train_Loss: 3828.37646484375, Val_Loss: 3958.8251953125\n",
      "Epoch 6030, Train_Loss: 3828.37451171875, Val_Loss: 3958.852783203125\n",
      "Epoch 6031, Train_Loss: 3828.357421875, Val_Loss: 3958.79931640625\n",
      "Epoch 6032, Train_Loss: 3828.3486328125, Val_Loss: 3958.791259765625\n",
      "Epoch 6033, Train_Loss: 3828.04345703125, Val_Loss: 3958.810302734375\n",
      "Epoch 6034, Train_Loss: 3828.01513671875, Val_Loss: 3958.75830078125\n",
      "Epoch 6035, Train_Loss: 3828.013916015625, Val_Loss: 3958.735107421875\n",
      "Epoch 6036, Train_Loss: 3828.012939453125, Val_Loss: 3958.768798828125\n",
      "Epoch 6037, Train_Loss: 3828.0302734375, Val_Loss: 3958.712890625\n",
      "Epoch 6038, Train_Loss: 3828.023681640625, Val_Loss: 3958.715087890625\n",
      "Epoch 6039, Train_Loss: 3828.030517578125, Val_Loss: 3958.73388671875\n",
      "Epoch 6040, Train_Loss: 3828.0048828125, Val_Loss: 3958.699951171875\n",
      "Epoch 6041, Train_Loss: 3827.9638671875, Val_Loss: 3958.629150390625\n",
      "Epoch 6042, Train_Loss: 3827.980224609375, Val_Loss: 3958.64794921875\n",
      "Epoch 6043, Train_Loss: 3827.85400390625, Val_Loss: 3958.624755859375\n",
      "Epoch 6044, Train_Loss: 3827.84619140625, Val_Loss: 3958.617919921875\n",
      "Epoch 6045, Train_Loss: 3827.94677734375, Val_Loss: 3958.627685546875\n",
      "Epoch 6046, Train_Loss: 3827.875732421875, Val_Loss: 3958.63916015625\n",
      "Epoch 6047, Train_Loss: 3827.859375, Val_Loss: 3958.631591796875\n",
      "Epoch 6048, Train_Loss: 3827.86474609375, Val_Loss: 3958.649658203125\n",
      "Epoch 6049, Train_Loss: 3827.837646484375, Val_Loss: 3958.596435546875\n",
      "Epoch 6050, Train_Loss: 3827.81298828125, Val_Loss: 3958.592041015625\n",
      "Epoch 6051, Train_Loss: 3827.791748046875, Val_Loss: 3958.582763671875\n",
      "Epoch 6052, Train_Loss: 3827.772216796875, Val_Loss: 3958.56396484375\n",
      "Epoch 6053, Train_Loss: 3827.78076171875, Val_Loss: 3958.59326171875\n",
      "Epoch 6054, Train_Loss: 3827.727783203125, Val_Loss: 3958.51708984375\n",
      "Epoch 6055, Train_Loss: 3827.756103515625, Val_Loss: 3958.543701171875\n",
      "Epoch 6056, Train_Loss: 3827.7646484375, Val_Loss: 3958.5380859375\n",
      "Epoch 6057, Train_Loss: 3827.694580078125, Val_Loss: 3958.503662109375\n",
      "Epoch 6058, Train_Loss: 3827.751220703125, Val_Loss: 3958.469482421875\n",
      "Epoch 6059, Train_Loss: 3827.669677734375, Val_Loss: 3958.4267578125\n",
      "Epoch 6060, Train_Loss: 3827.724609375, Val_Loss: 3958.42041015625\n",
      "Epoch 6061, Train_Loss: 3827.7158203125, Val_Loss: 3958.41796875\n",
      "Epoch 6062, Train_Loss: 3827.634765625, Val_Loss: 3958.451904296875\n",
      "Epoch 6063, Train_Loss: 3827.61669921875, Val_Loss: 3958.439208984375\n",
      "Epoch 6064, Train_Loss: 3827.677001953125, Val_Loss: 3958.4267578125\n",
      "Epoch 6065, Train_Loss: 3827.67578125, Val_Loss: 3958.41552734375\n",
      "Epoch 6066, Train_Loss: 3827.450439453125, Val_Loss: 3958.406005859375\n",
      "Epoch 6067, Train_Loss: 3827.508544921875, Val_Loss: 3958.39990234375\n",
      "Epoch 6068, Train_Loss: 3827.49462890625, Val_Loss: 3958.398681640625\n",
      "Epoch 6069, Train_Loss: 3827.422607421875, Val_Loss: 3958.381103515625\n",
      "Epoch 6070, Train_Loss: 3827.488037109375, Val_Loss: 3958.38330078125\n",
      "Epoch 6071, Train_Loss: 3827.405517578125, Val_Loss: 3958.357666015625\n",
      "Epoch 6072, Train_Loss: 3827.39013671875, Val_Loss: 3958.353515625\n",
      "Epoch 6073, Train_Loss: 3827.447265625, Val_Loss: 3958.352294921875\n",
      "Epoch 6074, Train_Loss: 3827.354736328125, Val_Loss: 3958.259521484375\n",
      "Epoch 6075, Train_Loss: 3827.409912109375, Val_Loss: 3958.2724609375\n",
      "Epoch 6076, Train_Loss: 3827.330810546875, Val_Loss: 3958.251953125\n",
      "Epoch 6077, Train_Loss: 3827.32470703125, Val_Loss: 3958.227294921875\n",
      "Epoch 6078, Train_Loss: 3827.382080078125, Val_Loss: 3958.27001953125\n",
      "Epoch 6079, Train_Loss: 3827.315673828125, Val_Loss: 3958.230712890625\n",
      "Epoch 6080, Train_Loss: 3827.31005859375, Val_Loss: 3958.2451171875\n",
      "Epoch 6081, Train_Loss: 3827.363037109375, Val_Loss: 3958.23193359375\n",
      "Epoch 6082, Train_Loss: 3827.253173828125, Val_Loss: 3958.19189453125\n",
      "Epoch 6083, Train_Loss: 3827.249755859375, Val_Loss: 3958.199951171875\n",
      "Epoch 6084, Train_Loss: 3827.239501953125, Val_Loss: 3958.222412109375\n",
      "Epoch 6085, Train_Loss: 3827.219482421875, Val_Loss: 3958.17724609375\n",
      "Epoch 6086, Train_Loss: 3827.2841796875, Val_Loss: 3958.197509765625\n",
      "Epoch 6087, Train_Loss: 3827.215576171875, Val_Loss: 3958.184326171875\n",
      "Epoch 6088, Train_Loss: 3827.157470703125, Val_Loss: 3958.13037109375\n",
      "Epoch 6089, Train_Loss: 3827.242431640625, Val_Loss: 3958.176025390625\n",
      "Epoch 6090, Train_Loss: 3827.117431640625, Val_Loss: 3958.0732421875\n",
      "Epoch 6091, Train_Loss: 3827.109130859375, Val_Loss: 3958.067138671875\n",
      "Epoch 6092, Train_Loss: 3827.2109375, Val_Loss: 3958.078369140625\n",
      "Epoch 6093, Train_Loss: 3827.09521484375, Val_Loss: 3958.06201171875\n",
      "Epoch 6094, Train_Loss: 3827.09521484375, Val_Loss: 3958.060302734375\n",
      "Epoch 6095, Train_Loss: 3827.1484375, Val_Loss: 3958.123291015625\n",
      "Epoch 6096, Train_Loss: 3827.072265625, Val_Loss: 3958.061279296875\n",
      "Epoch 6097, Train_Loss: 3827.093505859375, Val_Loss: 3958.052490234375\n",
      "Epoch 6098, Train_Loss: 3827.056396484375, Val_Loss: 3958.063720703125\n",
      "Epoch 6099, Train_Loss: 3827.01513671875, Val_Loss: 3958.01171875\n",
      "Epoch 6100, Train_Loss: 3826.98388671875, Val_Loss: 3957.988037109375\n",
      "Epoch 6101, Train_Loss: 3827.059814453125, Val_Loss: 3958.035888671875\n",
      "Epoch 6102, Train_Loss: 3827.027099609375, Val_Loss: 3957.9716796875\n",
      "Epoch 6103, Train_Loss: 3826.980224609375, Val_Loss: 3957.9580078125\n",
      "Epoch 6104, Train_Loss: 3827.044921875, Val_Loss: 3958.013671875\n",
      "Epoch 6105, Train_Loss: 3826.96630859375, Val_Loss: 3957.9619140625\n",
      "Epoch 6106, Train_Loss: 3826.870849609375, Val_Loss: 3957.884521484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6107, Train_Loss: 3826.857421875, Val_Loss: 3957.9111328125\n",
      "Epoch 6108, Train_Loss: 3826.8515625, Val_Loss: 3957.8515625\n",
      "Epoch 6109, Train_Loss: 3826.86376953125, Val_Loss: 3957.8671875\n",
      "Epoch 6110, Train_Loss: 3826.852294921875, Val_Loss: 3957.901611328125\n",
      "Epoch 6111, Train_Loss: 3826.816162109375, Val_Loss: 3957.87646484375\n",
      "Epoch 6112, Train_Loss: 3826.833984375, Val_Loss: 3957.872802734375\n",
      "Epoch 6113, Train_Loss: 3826.8251953125, Val_Loss: 3957.865966796875\n",
      "Epoch 6114, Train_Loss: 3826.796630859375, Val_Loss: 3957.839111328125\n",
      "Epoch 6115, Train_Loss: 3826.77587890625, Val_Loss: 3957.815673828125\n",
      "Epoch 6116, Train_Loss: 3826.77587890625, Val_Loss: 3957.817626953125\n",
      "Epoch 6117, Train_Loss: 3826.76171875, Val_Loss: 3957.8037109375\n",
      "Epoch 6118, Train_Loss: 3826.755126953125, Val_Loss: 3957.76171875\n",
      "Epoch 6119, Train_Loss: 3826.7880859375, Val_Loss: 3957.793212890625\n",
      "Epoch 6120, Train_Loss: 3826.768798828125, Val_Loss: 3957.785888671875\n",
      "Epoch 6121, Train_Loss: 3826.75439453125, Val_Loss: 3957.753662109375\n",
      "Epoch 6122, Train_Loss: 3826.747802734375, Val_Loss: 3957.7099609375\n",
      "Epoch 6123, Train_Loss: 3826.73828125, Val_Loss: 3957.7041015625\n",
      "Epoch 6124, Train_Loss: 3826.704345703125, Val_Loss: 3957.667236328125\n",
      "Epoch 6125, Train_Loss: 3826.696044921875, Val_Loss: 3957.6767578125\n",
      "Epoch 6126, Train_Loss: 3826.670654296875, Val_Loss: 3957.699951171875\n",
      "Epoch 6127, Train_Loss: 3826.66552734375, Val_Loss: 3957.6904296875\n",
      "Epoch 6128, Train_Loss: 3826.66015625, Val_Loss: 3957.662109375\n",
      "Epoch 6129, Train_Loss: 3826.658447265625, Val_Loss: 3957.674072265625\n",
      "Epoch 6130, Train_Loss: 3826.54296875, Val_Loss: 3957.6591796875\n",
      "Epoch 6131, Train_Loss: 3826.535888671875, Val_Loss: 3957.656494140625\n",
      "Epoch 6132, Train_Loss: 3826.53271484375, Val_Loss: 3957.6396484375\n",
      "Epoch 6133, Train_Loss: 3826.51904296875, Val_Loss: 3957.62841796875\n",
      "Epoch 6134, Train_Loss: 3826.725830078125, Val_Loss: 3957.644287109375\n",
      "Epoch 6135, Train_Loss: 3826.727294921875, Val_Loss: 3957.595703125\n",
      "Epoch 6136, Train_Loss: 3826.71923828125, Val_Loss: 3957.593994140625\n",
      "Epoch 6137, Train_Loss: 3826.69287109375, Val_Loss: 3957.620361328125\n",
      "Epoch 6138, Train_Loss: 3826.660400390625, Val_Loss: 3957.513671875\n",
      "Epoch 6139, Train_Loss: 3826.64599609375, Val_Loss: 3957.51806640625\n",
      "Epoch 6140, Train_Loss: 3826.63134765625, Val_Loss: 3957.53076171875\n",
      "Epoch 6141, Train_Loss: 3826.595947265625, Val_Loss: 3957.447265625\n",
      "Epoch 6142, Train_Loss: 3826.6298828125, Val_Loss: 3957.534912109375\n",
      "Epoch 6143, Train_Loss: 3826.614990234375, Val_Loss: 3957.530029296875\n",
      "Epoch 6144, Train_Loss: 3826.5654296875, Val_Loss: 3957.47119140625\n",
      "Epoch 6145, Train_Loss: 3826.59130859375, Val_Loss: 3957.47412109375\n",
      "Epoch 6146, Train_Loss: 3826.55322265625, Val_Loss: 3957.500732421875\n",
      "Epoch 6147, Train_Loss: 3826.52490234375, Val_Loss: 3957.395263671875\n",
      "Epoch 6148, Train_Loss: 3826.517578125, Val_Loss: 3957.53076171875\n",
      "Epoch 6149, Train_Loss: 3826.4814453125, Val_Loss: 3957.40478515625\n",
      "Epoch 6150, Train_Loss: 3826.478271484375, Val_Loss: 3957.41357421875\n",
      "Epoch 6151, Train_Loss: 3826.55859375, Val_Loss: 3957.48876953125\n",
      "Epoch 6152, Train_Loss: 3826.557861328125, Val_Loss: 3957.27490234375\n",
      "Epoch 6153, Train_Loss: 3826.4990234375, Val_Loss: 3957.44189453125\n",
      "Epoch 6154, Train_Loss: 3826.460693359375, Val_Loss: 3957.373291015625\n",
      "Epoch 6155, Train_Loss: 3826.535888671875, Val_Loss: 3957.231201171875\n",
      "Epoch 6156, Train_Loss: 3826.4404296875, Val_Loss: 3957.37158203125\n",
      "Epoch 6157, Train_Loss: 3826.451416015625, Val_Loss: 3957.353515625\n",
      "Epoch 6158, Train_Loss: 3826.514404296875, Val_Loss: 3957.24609375\n",
      "Epoch 6159, Train_Loss: 3826.41259765625, Val_Loss: 3957.396728515625\n",
      "Epoch 6160, Train_Loss: 3826.40625, Val_Loss: 3957.36767578125\n",
      "Epoch 6161, Train_Loss: 3826.481689453125, Val_Loss: 3957.220458984375\n",
      "Epoch 6162, Train_Loss: 3826.079345703125, Val_Loss: 3957.337646484375\n",
      "Epoch 6163, Train_Loss: 3826.145263671875, Val_Loss: 3957.3232421875\n",
      "Epoch 6164, Train_Loss: 3826.14794921875, Val_Loss: 3957.1904296875\n",
      "Epoch 6165, Train_Loss: 3826.106689453125, Val_Loss: 3957.342041015625\n",
      "Epoch 6166, Train_Loss: 3826.1435546875, Val_Loss: 3957.21240234375\n",
      "Epoch 6167, Train_Loss: 3826.13232421875, Val_Loss: 3957.2041015625\n",
      "Epoch 6168, Train_Loss: 3826.126708984375, Val_Loss: 3957.285888671875\n",
      "Epoch 6169, Train_Loss: 3826.124755859375, Val_Loss: 3957.23193359375\n",
      "Epoch 6170, Train_Loss: 3826.077880859375, Val_Loss: 3957.13037109375\n",
      "Epoch 6171, Train_Loss: 3826.086669921875, Val_Loss: 3957.173095703125\n",
      "Epoch 6172, Train_Loss: 3826.0771484375, Val_Loss: 3957.119140625\n",
      "Epoch 6173, Train_Loss: 3826.06591796875, Val_Loss: 3957.137939453125\n",
      "Epoch 6174, Train_Loss: 3826.071044921875, Val_Loss: 3957.1748046875\n",
      "Epoch 6175, Train_Loss: 3826.049560546875, Val_Loss: 3957.105224609375\n",
      "Epoch 6176, Train_Loss: 3826.02099609375, Val_Loss: 3957.15283203125\n",
      "Epoch 6177, Train_Loss: 3826.02880859375, Val_Loss: 3957.160400390625\n",
      "Epoch 6178, Train_Loss: 3825.9912109375, Val_Loss: 3957.04150390625\n",
      "Epoch 6179, Train_Loss: 3825.9931640625, Val_Loss: 3957.15087890625\n",
      "Epoch 6180, Train_Loss: 3825.975830078125, Val_Loss: 3957.08251953125\n",
      "Epoch 6181, Train_Loss: 3825.966796875, Val_Loss: 3957.074462890625\n",
      "Epoch 6182, Train_Loss: 3825.976806640625, Val_Loss: 3957.1220703125\n",
      "Epoch 6183, Train_Loss: 3825.947509765625, Val_Loss: 3957.009521484375\n",
      "Epoch 6184, Train_Loss: 3825.96533203125, Val_Loss: 3957.072021484375\n",
      "Epoch 6185, Train_Loss: 3825.94140625, Val_Loss: 3957.01171875\n",
      "Epoch 6186, Train_Loss: 3825.920654296875, Val_Loss: 3956.977294921875\n",
      "Epoch 6187, Train_Loss: 3825.89599609375, Val_Loss: 3956.923583984375\n",
      "Epoch 6188, Train_Loss: 3825.89892578125, Val_Loss: 3956.966796875\n",
      "Epoch 6189, Train_Loss: 3825.873291015625, Val_Loss: 3956.91748046875\n",
      "Epoch 6190, Train_Loss: 3825.873046875, Val_Loss: 3956.989990234375\n",
      "Epoch 6191, Train_Loss: 3825.830322265625, Val_Loss: 3956.93115234375\n",
      "Epoch 6192, Train_Loss: 3825.8427734375, Val_Loss: 3956.959716796875\n",
      "Epoch 6193, Train_Loss: 3825.82861328125, Val_Loss: 3956.94091796875\n",
      "Epoch 6194, Train_Loss: 3825.687255859375, Val_Loss: 3956.954833984375\n",
      "Epoch 6195, Train_Loss: 3825.68359375, Val_Loss: 3956.8583984375\n",
      "Epoch 6196, Train_Loss: 3825.69677734375, Val_Loss: 3956.9453125\n",
      "Epoch 6197, Train_Loss: 3825.670166015625, Val_Loss: 3956.872802734375\n",
      "Epoch 6198, Train_Loss: 3825.654296875, Val_Loss: 3956.895263671875\n",
      "Epoch 6199, Train_Loss: 3825.63623046875, Val_Loss: 3956.8876953125\n",
      "Epoch 6200, Train_Loss: 3825.759033203125, Val_Loss: 3956.80126953125\n",
      "Epoch 6201, Train_Loss: 3825.6103515625, Val_Loss: 3956.894287109375\n",
      "Epoch 6202, Train_Loss: 3825.70068359375, Val_Loss: 3956.75830078125\n",
      "Epoch 6203, Train_Loss: 3825.5615234375, Val_Loss: 3956.793212890625\n",
      "Epoch 6204, Train_Loss: 3825.68798828125, Val_Loss: 3956.727294921875\n",
      "Epoch 6205, Train_Loss: 3825.554443359375, Val_Loss: 3956.81396484375\n",
      "Epoch 6206, Train_Loss: 3825.662109375, Val_Loss: 3956.760498046875\n",
      "Epoch 6207, Train_Loss: 3825.644287109375, Val_Loss: 3956.766357421875\n",
      "Epoch 6208, Train_Loss: 3825.64013671875, Val_Loss: 3956.74755859375\n",
      "Epoch 6209, Train_Loss: 3825.62353515625, Val_Loss: 3956.7724609375\n",
      "Epoch 6210, Train_Loss: 3825.59228515625, Val_Loss: 3956.6767578125\n",
      "Epoch 6211, Train_Loss: 3825.572265625, Val_Loss: 3956.732421875\n",
      "Epoch 6212, Train_Loss: 3825.58251953125, Val_Loss: 3956.6953125\n",
      "Epoch 6213, Train_Loss: 3825.564453125, Val_Loss: 3956.71875\n",
      "Epoch 6214, Train_Loss: 3825.564208984375, Val_Loss: 3956.672119140625\n",
      "Epoch 6215, Train_Loss: 3825.553955078125, Val_Loss: 3956.7099609375\n",
      "Epoch 6216, Train_Loss: 3825.543212890625, Val_Loss: 3956.65234375\n",
      "Epoch 6217, Train_Loss: 3825.521728515625, Val_Loss: 3956.6884765625\n",
      "Epoch 6218, Train_Loss: 3825.5126953125, Val_Loss: 3956.583984375\n",
      "Epoch 6219, Train_Loss: 3825.487060546875, Val_Loss: 3956.626708984375\n",
      "Epoch 6220, Train_Loss: 3825.504150390625, Val_Loss: 3956.56591796875\n",
      "Epoch 6221, Train_Loss: 3825.47314453125, Val_Loss: 3956.611083984375\n",
      "Epoch 6222, Train_Loss: 3825.48486328125, Val_Loss: 3956.5888671875\n",
      "Epoch 6223, Train_Loss: 3825.447265625, Val_Loss: 3956.62451171875\n",
      "Epoch 6224, Train_Loss: 3825.46630859375, Val_Loss: 3956.57763671875\n",
      "Epoch 6225, Train_Loss: 3825.466796875, Val_Loss: 3956.616455078125\n",
      "Epoch 6226, Train_Loss: 3825.40673828125, Val_Loss: 3956.509521484375\n",
      "Epoch 6227, Train_Loss: 3825.381591796875, Val_Loss: 3956.578369140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6228, Train_Loss: 3825.400146484375, Val_Loss: 3956.52001953125\n",
      "Epoch 6229, Train_Loss: 3825.42236328125, Val_Loss: 3956.530517578125\n",
      "Epoch 6230, Train_Loss: 3825.429931640625, Val_Loss: 3956.476806640625\n",
      "Epoch 6231, Train_Loss: 3825.391845703125, Val_Loss: 3956.548828125\n",
      "Epoch 6232, Train_Loss: 3825.38330078125, Val_Loss: 3956.453125\n",
      "Epoch 6233, Train_Loss: 3825.2861328125, Val_Loss: 3956.439697265625\n",
      "Epoch 6234, Train_Loss: 3825.274169921875, Val_Loss: 3956.432861328125\n",
      "Epoch 6235, Train_Loss: 3825.257568359375, Val_Loss: 3956.357177734375\n",
      "Epoch 6236, Train_Loss: 3825.262451171875, Val_Loss: 3956.4453125\n",
      "Epoch 6237, Train_Loss: 3825.2431640625, Val_Loss: 3956.386474609375\n",
      "Epoch 6238, Train_Loss: 3825.252685546875, Val_Loss: 3956.444091796875\n",
      "Epoch 6239, Train_Loss: 3825.2060546875, Val_Loss: 3956.421630859375\n",
      "Epoch 6240, Train_Loss: 3825.20703125, Val_Loss: 3956.379150390625\n",
      "Epoch 6241, Train_Loss: 3825.269775390625, Val_Loss: 3956.468017578125\n",
      "Epoch 6242, Train_Loss: 3825.312744140625, Val_Loss: 3955.887939453125\n",
      "Epoch 6243, Train_Loss: 3825.34814453125, Val_Loss: 3957.053955078125\n",
      "Epoch 6244, Train_Loss: 3825.468994140625, Val_Loss: 3955.850341796875\n",
      "Epoch 6245, Train_Loss: 3825.129638671875, Val_Loss: 3956.407958984375\n",
      "Epoch 6246, Train_Loss: 3825.486572265625, Val_Loss: 3957.5048828125\n",
      "Epoch 6247, Train_Loss: 3825.9130859375, Val_Loss: 3955.877197265625\n",
      "Epoch 6248, Train_Loss: 3825.164794921875, Val_Loss: 3955.90234375\n",
      "Epoch 6249, Train_Loss: 3825.90869140625, Val_Loss: 3958.36669921875\n",
      "Epoch 6250, Train_Loss: 3825.091552734375, Val_Loss: 3956.087646484375\n",
      "Epoch 6251, Train_Loss: 3825.56201171875, Val_Loss: 3955.7080078125\n",
      "Epoch 6252, Train_Loss: 3825.097412109375, Val_Loss: 3956.080078125\n",
      "Epoch 6253, Train_Loss: 3825.48681640625, Val_Loss: 3957.751953125\n",
      "Epoch 6254, Train_Loss: 3825.081787109375, Val_Loss: 3956.233154296875\n",
      "Epoch 6255, Train_Loss: 3825.21875, Val_Loss: 3955.735107421875\n",
      "Epoch 6256, Train_Loss: 3825.119140625, Val_Loss: 3955.934326171875\n",
      "Epoch 6257, Train_Loss: 3825.109375, Val_Loss: 3957.01806640625\n",
      "Epoch 6258, Train_Loss: 3824.88134765625, Val_Loss: 3956.52880859375\n",
      "Epoch 6259, Train_Loss: 3825.128662109375, Val_Loss: 3955.7255859375\n",
      "Epoch 6260, Train_Loss: 3825.115966796875, Val_Loss: 3955.72412109375\n",
      "Epoch 6261, Train_Loss: 3825.09716796875, Val_Loss: 3956.44873046875\n",
      "Epoch 6262, Train_Loss: 3825.234375, Val_Loss: 3956.72412109375\n",
      "Epoch 6263, Train_Loss: 3824.992431640625, Val_Loss: 3955.91162109375\n",
      "Epoch 6264, Train_Loss: 3825.142578125, Val_Loss: 3955.716064453125\n",
      "Epoch 6265, Train_Loss: 3825.0263671875, Val_Loss: 3955.986328125\n",
      "Epoch 6266, Train_Loss: 3825.22314453125, Val_Loss: 3956.677490234375\n",
      "Epoch 6267, Train_Loss: 3825.030029296875, Val_Loss: 3956.02685546875\n",
      "Epoch 6268, Train_Loss: 3824.98974609375, Val_Loss: 3955.686279296875\n",
      "Epoch 6269, Train_Loss: 3824.989501953125, Val_Loss: 3955.87109375\n",
      "Epoch 6270, Train_Loss: 3825.064697265625, Val_Loss: 3956.343505859375\n",
      "Epoch 6271, Train_Loss: 3824.95947265625, Val_Loss: 3956.07568359375\n",
      "Epoch 6272, Train_Loss: 3824.9736328125, Val_Loss: 3955.817138671875\n",
      "Epoch 6273, Train_Loss: 3824.93505859375, Val_Loss: 3955.810791015625\n",
      "Epoch 6274, Train_Loss: 3824.908447265625, Val_Loss: 3956.047119140625\n",
      "Epoch 6275, Train_Loss: 3825.0224609375, Val_Loss: 3956.218505859375\n",
      "Epoch 6276, Train_Loss: 3824.83203125, Val_Loss: 3955.7919921875\n",
      "Epoch 6277, Train_Loss: 3824.859130859375, Val_Loss: 3955.71630859375\n",
      "Epoch 6278, Train_Loss: 3824.879150390625, Val_Loss: 3955.9208984375\n",
      "Epoch 6279, Train_Loss: 3824.93798828125, Val_Loss: 3956.27001953125\n",
      "Epoch 6280, Train_Loss: 3824.849365234375, Val_Loss: 3955.868896484375\n",
      "Epoch 6281, Train_Loss: 3824.839599609375, Val_Loss: 3955.637939453125\n",
      "Epoch 6282, Train_Loss: 3824.780029296875, Val_Loss: 3955.7451171875\n",
      "Epoch 6283, Train_Loss: 3824.85302734375, Val_Loss: 3956.07958984375\n",
      "Epoch 6284, Train_Loss: 3824.764404296875, Val_Loss: 3955.794921875\n",
      "Epoch 6285, Train_Loss: 3824.802001953125, Val_Loss: 3955.71630859375\n",
      "Epoch 6286, Train_Loss: 3824.760009765625, Val_Loss: 3955.779296875\n",
      "Epoch 6287, Train_Loss: 3824.73779296875, Val_Loss: 3955.99169921875\n",
      "Epoch 6288, Train_Loss: 3824.457275390625, Val_Loss: 3955.7275390625\n",
      "Epoch 6289, Train_Loss: 3824.4951171875, Val_Loss: 3955.660400390625\n",
      "Epoch 6290, Train_Loss: 3824.390625, Val_Loss: 3955.814697265625\n",
      "Epoch 6291, Train_Loss: 3824.42822265625, Val_Loss: 3955.96484375\n",
      "Epoch 6292, Train_Loss: 3824.493896484375, Val_Loss: 3955.5732421875\n",
      "Epoch 6293, Train_Loss: 3824.48681640625, Val_Loss: 3955.541259765625\n",
      "Epoch 6294, Train_Loss: 3824.40771484375, Val_Loss: 3955.7724609375\n",
      "Epoch 6295, Train_Loss: 3824.529052734375, Val_Loss: 3956.073486328125\n",
      "Epoch 6296, Train_Loss: 3824.41796875, Val_Loss: 3955.57763671875\n",
      "Epoch 6297, Train_Loss: 3824.4248046875, Val_Loss: 3955.454833984375\n",
      "Epoch 6298, Train_Loss: 3824.39794921875, Val_Loss: 3955.58203125\n",
      "Epoch 6299, Train_Loss: 3824.45947265625, Val_Loss: 3955.928466796875\n",
      "Epoch 6300, Train_Loss: 3824.348388671875, Val_Loss: 3955.6787109375\n",
      "Epoch 6301, Train_Loss: 3824.36474609375, Val_Loss: 3955.508056640625\n",
      "Epoch 6302, Train_Loss: 3824.37646484375, Val_Loss: 3955.5625\n",
      "Epoch 6303, Train_Loss: 3824.2998046875, Val_Loss: 3955.81689453125\n",
      "Epoch 6304, Train_Loss: 3824.3134765625, Val_Loss: 3955.532470703125\n",
      "Epoch 6305, Train_Loss: 3824.27197265625, Val_Loss: 3955.482421875\n",
      "Epoch 6306, Train_Loss: 3824.296630859375, Val_Loss: 3955.60400390625\n",
      "Epoch 6307, Train_Loss: 3824.24267578125, Val_Loss: 3955.788330078125\n",
      "Epoch 6308, Train_Loss: 3824.260986328125, Val_Loss: 3955.434326171875\n",
      "Epoch 6309, Train_Loss: 3824.29638671875, Val_Loss: 3955.373291015625\n",
      "Epoch 6310, Train_Loss: 3824.259765625, Val_Loss: 3955.585693359375\n",
      "Epoch 6311, Train_Loss: 3824.299560546875, Val_Loss: 3955.795166015625\n",
      "Epoch 6312, Train_Loss: 3824.25537109375, Val_Loss: 3955.360107421875\n",
      "Epoch 6313, Train_Loss: 3824.21826171875, Val_Loss: 3955.2607421875\n",
      "Epoch 6314, Train_Loss: 3824.200439453125, Val_Loss: 3955.428466796875\n",
      "Epoch 6315, Train_Loss: 3824.236328125, Val_Loss: 3955.7744140625\n",
      "Epoch 6316, Train_Loss: 3824.234130859375, Val_Loss: 3955.478759765625\n",
      "Epoch 6317, Train_Loss: 3824.194091796875, Val_Loss: 3955.310791015625\n",
      "Epoch 6318, Train_Loss: 3824.1845703125, Val_Loss: 3955.39794921875\n",
      "Epoch 6319, Train_Loss: 3824.022705078125, Val_Loss: 3955.63037109375\n",
      "Epoch 6320, Train_Loss: 3824.0146484375, Val_Loss: 3955.36083984375\n",
      "Epoch 6321, Train_Loss: 3823.98291015625, Val_Loss: 3955.3076171875\n",
      "Epoch 6322, Train_Loss: 3824.052001953125, Val_Loss: 3955.392333984375\n",
      "Epoch 6323, Train_Loss: 3824.0283203125, Val_Loss: 3955.60595703125\n",
      "Epoch 6324, Train_Loss: 3823.950439453125, Val_Loss: 3955.28515625\n",
      "Epoch 6325, Train_Loss: 3824.10302734375, Val_Loss: 3955.20556640625\n",
      "Epoch 6326, Train_Loss: 3824.001708984375, Val_Loss: 3955.37890625\n",
      "Epoch 6327, Train_Loss: 3823.9765625, Val_Loss: 3955.58642578125\n",
      "Epoch 6328, Train_Loss: 3823.91650390625, Val_Loss: 3955.232421875\n",
      "Epoch 6329, Train_Loss: 3824.0322265625, Val_Loss: 3955.1064453125\n",
      "Epoch 6330, Train_Loss: 3823.8837890625, Val_Loss: 3955.25927734375\n",
      "Epoch 6331, Train_Loss: 3823.9306640625, Val_Loss: 3955.534423828125\n",
      "Epoch 6332, Train_Loss: 3823.8642578125, Val_Loss: 3955.23828125\n",
      "Epoch 6333, Train_Loss: 3823.967041015625, Val_Loss: 3955.12646484375\n",
      "Epoch 6334, Train_Loss: 3823.796875, Val_Loss: 3955.242431640625\n",
      "Epoch 6335, Train_Loss: 3823.823486328125, Val_Loss: 3955.49609375\n",
      "Epoch 6336, Train_Loss: 3823.943359375, Val_Loss: 3955.14990234375\n",
      "Epoch 6337, Train_Loss: 3823.9013671875, Val_Loss: 3955.109130859375\n",
      "Epoch 6338, Train_Loss: 3823.865234375, Val_Loss: 3955.213623046875\n",
      "Epoch 6339, Train_Loss: 3823.810791015625, Val_Loss: 3955.456298828125\n",
      "Epoch 6340, Train_Loss: 3823.882080078125, Val_Loss: 3955.119140625\n",
      "Epoch 6341, Train_Loss: 3823.888916015625, Val_Loss: 3955.029296875\n",
      "Epoch 6342, Train_Loss: 3823.807861328125, Val_Loss: 3955.118896484375\n",
      "Epoch 6343, Train_Loss: 3823.820068359375, Val_Loss: 3955.377685546875\n",
      "Epoch 6344, Train_Loss: 3823.849609375, Val_Loss: 3955.0380859375\n",
      "Epoch 6345, Train_Loss: 3823.83740234375, Val_Loss: 3954.930419921875\n",
      "Epoch 6346, Train_Loss: 3823.8203125, Val_Loss: 3955.109130859375\n",
      "Epoch 6347, Train_Loss: 3823.753173828125, Val_Loss: 3955.357666015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6348, Train_Loss: 3823.82080078125, Val_Loss: 3955.03515625\n",
      "Epoch 6349, Train_Loss: 3823.837890625, Val_Loss: 3954.93359375\n",
      "Epoch 6350, Train_Loss: 3823.7451171875, Val_Loss: 3955.056884765625\n",
      "Epoch 6351, Train_Loss: 3823.6318359375, Val_Loss: 3955.284423828125\n",
      "Epoch 6352, Train_Loss: 3823.747314453125, Val_Loss: 3954.996337890625\n",
      "Epoch 6353, Train_Loss: 3823.784423828125, Val_Loss: 3954.903564453125\n",
      "Epoch 6354, Train_Loss: 3823.826171875, Val_Loss: 3955.033203125\n",
      "Epoch 6355, Train_Loss: 3823.670166015625, Val_Loss: 3955.302490234375\n",
      "Epoch 6356, Train_Loss: 3823.720947265625, Val_Loss: 3954.961181640625\n",
      "Epoch 6357, Train_Loss: 3823.619384765625, Val_Loss: 3954.80908203125\n",
      "Epoch 6358, Train_Loss: 3823.635986328125, Val_Loss: 3954.925537109375\n",
      "Epoch 6359, Train_Loss: 3823.537353515625, Val_Loss: 3955.2060546875\n",
      "Epoch 6360, Train_Loss: 3823.622314453125, Val_Loss: 3954.86962890625\n",
      "Epoch 6361, Train_Loss: 3823.578125, Val_Loss: 3954.813720703125\n",
      "Epoch 6362, Train_Loss: 3823.62353515625, Val_Loss: 3954.928466796875\n",
      "Epoch 6363, Train_Loss: 3823.49560546875, Val_Loss: 3955.177490234375\n",
      "Epoch 6364, Train_Loss: 3823.593505859375, Val_Loss: 3954.874755859375\n",
      "Epoch 6365, Train_Loss: 3823.51318359375, Val_Loss: 3954.761962890625\n",
      "Epoch 6366, Train_Loss: 3823.5625, Val_Loss: 3954.90234375\n",
      "Epoch 6367, Train_Loss: 3823.5859375, Val_Loss: 3955.165283203125\n",
      "Epoch 6368, Train_Loss: 3823.488525390625, Val_Loss: 3954.84326171875\n",
      "Epoch 6369, Train_Loss: 3823.529541015625, Val_Loss: 3954.70751953125\n",
      "Epoch 6370, Train_Loss: 3823.522216796875, Val_Loss: 3954.84130859375\n",
      "Epoch 6371, Train_Loss: 3823.539306640625, Val_Loss: 3955.0927734375\n",
      "Epoch 6372, Train_Loss: 3823.482177734375, Val_Loss: 3954.8115234375\n",
      "Epoch 6373, Train_Loss: 3823.47998046875, Val_Loss: 3954.624755859375\n",
      "Epoch 6374, Train_Loss: 3823.50048828125, Val_Loss: 3954.753662109375\n",
      "Epoch 6375, Train_Loss: 3823.500244140625, Val_Loss: 3955.00830078125\n",
      "Epoch 6376, Train_Loss: 3823.410400390625, Val_Loss: 3954.74560546875\n",
      "Epoch 6377, Train_Loss: 3823.437255859375, Val_Loss: 3954.636474609375\n",
      "Epoch 6378, Train_Loss: 3823.4208984375, Val_Loss: 3954.778076171875\n",
      "Epoch 6379, Train_Loss: 3823.450927734375, Val_Loss: 3955.031494140625\n",
      "Epoch 6380, Train_Loss: 3823.2958984375, Val_Loss: 3954.68310546875\n",
      "Epoch 6381, Train_Loss: 3823.30322265625, Val_Loss: 3954.59033203125\n",
      "Epoch 6382, Train_Loss: 3823.28125, Val_Loss: 3954.699951171875\n",
      "Epoch 6383, Train_Loss: 3823.30078125, Val_Loss: 3954.961181640625\n",
      "Epoch 6384, Train_Loss: 3823.467041015625, Val_Loss: 3954.654052734375\n",
      "Epoch 6385, Train_Loss: 3823.49560546875, Val_Loss: 3954.567626953125\n",
      "Epoch 6386, Train_Loss: 3823.474853515625, Val_Loss: 3954.70166015625\n",
      "Epoch 6387, Train_Loss: 3823.50732421875, Val_Loss: 3954.95849609375\n",
      "Epoch 6388, Train_Loss: 3823.399658203125, Val_Loss: 3954.53564453125\n",
      "Epoch 6389, Train_Loss: 3823.432373046875, Val_Loss: 3954.453125\n",
      "Epoch 6390, Train_Loss: 3823.428955078125, Val_Loss: 3954.568115234375\n",
      "Epoch 6391, Train_Loss: 3823.44482421875, Val_Loss: 3954.828857421875\n",
      "Epoch 6392, Train_Loss: 3823.356689453125, Val_Loss: 3954.564697265625\n",
      "Epoch 6393, Train_Loss: 3823.39697265625, Val_Loss: 3954.469482421875\n",
      "Epoch 6394, Train_Loss: 3823.39306640625, Val_Loss: 3954.589111328125\n",
      "Epoch 6395, Train_Loss: 3823.41650390625, Val_Loss: 3954.84130859375\n",
      "Epoch 6396, Train_Loss: 3823.298095703125, Val_Loss: 3954.505615234375\n",
      "Epoch 6397, Train_Loss: 3823.328857421875, Val_Loss: 3954.40087890625\n",
      "Epoch 6398, Train_Loss: 3823.3359375, Val_Loss: 3954.536376953125\n",
      "Epoch 6399, Train_Loss: 3823.30322265625, Val_Loss: 3954.7939453125\n",
      "Epoch 6400, Train_Loss: 3823.27099609375, Val_Loss: 3954.485595703125\n",
      "Epoch 6401, Train_Loss: 3823.3134765625, Val_Loss: 3954.384033203125\n",
      "Epoch 6402, Train_Loss: 3823.322998046875, Val_Loss: 3954.5068359375\n",
      "Epoch 6403, Train_Loss: 3823.251708984375, Val_Loss: 3954.701904296875\n",
      "Epoch 6404, Train_Loss: 3823.21923828125, Val_Loss: 3954.388427734375\n",
      "Epoch 6405, Train_Loss: 3823.264892578125, Val_Loss: 3954.288818359375\n",
      "Epoch 6406, Train_Loss: 3823.2890625, Val_Loss: 3954.41552734375\n",
      "Epoch 6407, Train_Loss: 3823.252685546875, Val_Loss: 3954.73046875\n",
      "Epoch 6408, Train_Loss: 3823.2158203125, Val_Loss: 3954.38037109375\n",
      "Epoch 6409, Train_Loss: 3823.232177734375, Val_Loss: 3954.280029296875\n",
      "Epoch 6410, Train_Loss: 3823.187744140625, Val_Loss: 3954.39404296875\n",
      "Epoch 6411, Train_Loss: 3822.914306640625, Val_Loss: 3954.6455078125\n",
      "Epoch 6412, Train_Loss: 3822.8623046875, Val_Loss: 3954.34521484375\n",
      "Epoch 6413, Train_Loss: 3822.86376953125, Val_Loss: 3954.259521484375\n",
      "Epoch 6414, Train_Loss: 3822.8740234375, Val_Loss: 3954.360107421875\n",
      "Epoch 6415, Train_Loss: 3822.93408203125, Val_Loss: 3954.5751953125\n",
      "Epoch 6416, Train_Loss: 3822.82275390625, Val_Loss: 3954.312744140625\n",
      "Epoch 6417, Train_Loss: 3822.848388671875, Val_Loss: 3954.202880859375\n",
      "Epoch 6418, Train_Loss: 3822.89111328125, Val_Loss: 3954.2744140625\n",
      "Epoch 6419, Train_Loss: 3822.872802734375, Val_Loss: 3954.534912109375\n",
      "Epoch 6420, Train_Loss: 3822.780517578125, Val_Loss: 3954.199951171875\n",
      "Epoch 6421, Train_Loss: 3822.801025390625, Val_Loss: 3954.104736328125\n",
      "Epoch 6422, Train_Loss: 3822.802001953125, Val_Loss: 3954.290283203125\n",
      "Epoch 6423, Train_Loss: 3822.849609375, Val_Loss: 3954.563232421875\n",
      "Epoch 6424, Train_Loss: 3822.74951171875, Val_Loss: 3954.169677734375\n",
      "Epoch 6425, Train_Loss: 3822.80859375, Val_Loss: 3954.069580078125\n",
      "Epoch 6426, Train_Loss: 3822.685546875, Val_Loss: 3954.211669921875\n",
      "Epoch 6427, Train_Loss: 3822.78173828125, Val_Loss: 3954.50439453125\n",
      "Epoch 6428, Train_Loss: 3822.655029296875, Val_Loss: 3954.15673828125\n",
      "Epoch 6429, Train_Loss: 3822.717041015625, Val_Loss: 3954.0595703125\n",
      "Epoch 6430, Train_Loss: 3822.689453125, Val_Loss: 3954.1943359375\n",
      "Epoch 6431, Train_Loss: 3822.767822265625, Val_Loss: 3954.4755859375\n",
      "Epoch 6432, Train_Loss: 3822.64501953125, Val_Loss: 3954.115478515625\n",
      "Epoch 6433, Train_Loss: 3822.69384765625, Val_Loss: 3954.021240234375\n",
      "Epoch 6434, Train_Loss: 3822.650390625, Val_Loss: 3954.098388671875\n",
      "Epoch 6435, Train_Loss: 3822.7255859375, Val_Loss: 3954.3876953125\n",
      "Epoch 6436, Train_Loss: 3822.58740234375, Val_Loss: 3954.034423828125\n",
      "Epoch 6437, Train_Loss: 3822.65380859375, Val_Loss: 3953.951904296875\n",
      "Epoch 6438, Train_Loss: 3822.56494140625, Val_Loss: 3954.078369140625\n",
      "Epoch 6439, Train_Loss: 3822.69091796875, Val_Loss: 3954.357666015625\n",
      "Epoch 6440, Train_Loss: 3822.54345703125, Val_Loss: 3954.05126953125\n",
      "Epoch 6441, Train_Loss: 3822.46533203125, Val_Loss: 3953.927978515625\n",
      "Epoch 6442, Train_Loss: 3822.420654296875, Val_Loss: 3954.046875\n",
      "Epoch 6443, Train_Loss: 3822.5, Val_Loss: 3954.2880859375\n",
      "Epoch 6444, Train_Loss: 3822.37890625, Val_Loss: 3953.994873046875\n",
      "Epoch 6445, Train_Loss: 3822.439208984375, Val_Loss: 3953.888427734375\n",
      "Epoch 6446, Train_Loss: 3822.365234375, Val_Loss: 3954.017578125\n",
      "Epoch 6447, Train_Loss: 3822.466064453125, Val_Loss: 3954.28076171875\n",
      "Epoch 6448, Train_Loss: 3822.338623046875, Val_Loss: 3953.9716796875\n",
      "Epoch 6449, Train_Loss: 3822.35302734375, Val_Loss: 3953.797119140625\n",
      "Epoch 6450, Train_Loss: 3822.29248046875, Val_Loss: 3953.9384765625\n",
      "Epoch 6451, Train_Loss: 3822.372314453125, Val_Loss: 3954.21875\n",
      "Epoch 6452, Train_Loss: 3822.26708984375, Val_Loss: 3953.877197265625\n",
      "Epoch 6453, Train_Loss: 3822.25634765625, Val_Loss: 3953.7958984375\n",
      "Epoch 6454, Train_Loss: 3822.252685546875, Val_Loss: 3953.906005859375\n",
      "Epoch 6455, Train_Loss: 3822.3388671875, Val_Loss: 3954.206298828125\n",
      "Epoch 6456, Train_Loss: 3822.214599609375, Val_Loss: 3953.837158203125\n",
      "Epoch 6457, Train_Loss: 3822.308349609375, Val_Loss: 3953.739990234375\n",
      "Epoch 6458, Train_Loss: 3822.209228515625, Val_Loss: 3953.868408203125\n",
      "Epoch 6459, Train_Loss: 3822.2802734375, Val_Loss: 3954.162841796875\n",
      "Epoch 6460, Train_Loss: 3822.1826171875, Val_Loss: 3953.808837890625\n",
      "Epoch 6461, Train_Loss: 3822.2490234375, Val_Loss: 3953.69482421875\n",
      "Epoch 6462, Train_Loss: 3822.150634765625, Val_Loss: 3953.818359375\n",
      "Epoch 6463, Train_Loss: 3822.229248046875, Val_Loss: 3954.113525390625\n",
      "Epoch 6464, Train_Loss: 3822.096435546875, Val_Loss: 3953.746337890625\n",
      "Epoch 6465, Train_Loss: 3822.2568359375, Val_Loss: 3953.6083984375\n",
      "Epoch 6466, Train_Loss: 3822.117431640625, Val_Loss: 3953.740478515625\n",
      "Epoch 6467, Train_Loss: 3822.183349609375, Val_Loss: 3954.018798828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6468, Train_Loss: 3822.09130859375, Val_Loss: 3953.738037109375\n",
      "Epoch 6469, Train_Loss: 3822.17529296875, Val_Loss: 3953.614501953125\n",
      "Epoch 6470, Train_Loss: 3822.07763671875, Val_Loss: 3953.748779296875\n",
      "Epoch 6471, Train_Loss: 3822.1787109375, Val_Loss: 3954.0419921875\n",
      "Epoch 6472, Train_Loss: 3822.01806640625, Val_Loss: 3953.685546875\n",
      "Epoch 6473, Train_Loss: 3822.111083984375, Val_Loss: 3953.549560546875\n",
      "Epoch 6474, Train_Loss: 3822.009033203125, Val_Loss: 3953.68798828125\n",
      "Epoch 6475, Train_Loss: 3822.145263671875, Val_Loss: 3953.9541015625\n",
      "Epoch 6476, Train_Loss: 3822.052490234375, Val_Loss: 3953.65234375\n",
      "Epoch 6477, Train_Loss: 3822.0986328125, Val_Loss: 3953.5576171875\n",
      "Epoch 6478, Train_Loss: 3822.0283203125, Val_Loss: 3953.68310546875\n",
      "Epoch 6479, Train_Loss: 3822.0263671875, Val_Loss: 3953.887939453125\n",
      "Epoch 6480, Train_Loss: 3821.942138671875, Val_Loss: 3953.553955078125\n",
      "Epoch 6481, Train_Loss: 3821.942138671875, Val_Loss: 3953.454345703125\n",
      "Epoch 6482, Train_Loss: 3821.93359375, Val_Loss: 3953.58642578125\n",
      "Epoch 6483, Train_Loss: 3822.0537109375, Val_Loss: 3953.936279296875\n",
      "Epoch 6484, Train_Loss: 3821.943115234375, Val_Loss: 3953.55517578125\n",
      "Epoch 6485, Train_Loss: 3821.94287109375, Val_Loss: 3953.43408203125\n",
      "Epoch 6486, Train_Loss: 3821.980224609375, Val_Loss: 3953.5439453125\n",
      "Epoch 6487, Train_Loss: 3821.970458984375, Val_Loss: 3953.85009765625\n",
      "Epoch 6488, Train_Loss: 3821.92431640625, Val_Loss: 3953.519287109375\n",
      "Epoch 6489, Train_Loss: 3821.96240234375, Val_Loss: 3953.403564453125\n",
      "Epoch 6490, Train_Loss: 3821.91748046875, Val_Loss: 3953.552490234375\n",
      "Epoch 6491, Train_Loss: 3821.8935546875, Val_Loss: 3953.813720703125\n",
      "Epoch 6492, Train_Loss: 3821.950927734375, Val_Loss: 3953.44970703125\n",
      "Epoch 6493, Train_Loss: 3821.989501953125, Val_Loss: 3953.364013671875\n",
      "Epoch 6494, Train_Loss: 3821.817138671875, Val_Loss: 3953.4462890625\n",
      "Epoch 6495, Train_Loss: 3821.820068359375, Val_Loss: 3953.74951171875\n",
      "Epoch 6496, Train_Loss: 3821.808349609375, Val_Loss: 3953.363525390625\n",
      "Epoch 6497, Train_Loss: 3821.85888671875, Val_Loss: 3953.251220703125\n",
      "Epoch 6498, Train_Loss: 3821.78466796875, Val_Loss: 3953.430419921875\n",
      "Epoch 6499, Train_Loss: 3821.783447265625, Val_Loss: 3953.72607421875\n",
      "Epoch 6500, Train_Loss: 3821.771728515625, Val_Loss: 3953.377685546875\n",
      "Epoch 6501, Train_Loss: 3821.808349609375, Val_Loss: 3953.2626953125\n",
      "Epoch 6502, Train_Loss: 3821.60107421875, Val_Loss: 3953.37451171875\n",
      "Epoch 6503, Train_Loss: 3821.707763671875, Val_Loss: 3953.651611328125\n",
      "Epoch 6504, Train_Loss: 3821.64306640625, Val_Loss: 3953.31884765625\n",
      "Epoch 6505, Train_Loss: 3821.898681640625, Val_Loss: 3953.23046875\n",
      "Epoch 6506, Train_Loss: 3821.8701171875, Val_Loss: 3953.353271484375\n",
      "Epoch 6507, Train_Loss: 3821.862548828125, Val_Loss: 3953.640869140625\n",
      "Epoch 6508, Train_Loss: 3821.879638671875, Val_Loss: 3953.264404296875\n",
      "Epoch 6509, Train_Loss: 3821.816650390625, Val_Loss: 3953.12353515625\n",
      "Epoch 6510, Train_Loss: 3821.787841796875, Val_Loss: 3953.257080078125\n",
      "Epoch 6511, Train_Loss: 3821.840576171875, Val_Loss: 3953.554443359375\n",
      "Epoch 6512, Train_Loss: 3821.78759765625, Val_Loss: 3953.20751953125\n",
      "Epoch 6513, Train_Loss: 3821.9013671875, Val_Loss: 3953.10205078125\n",
      "Epoch 6514, Train_Loss: 3821.750244140625, Val_Loss: 3953.233642578125\n",
      "Epoch 6515, Train_Loss: 3821.85205078125, Val_Loss: 3953.52685546875\n",
      "Epoch 6516, Train_Loss: 3821.713623046875, Val_Loss: 3953.19921875\n",
      "Epoch 6517, Train_Loss: 3821.72412109375, Val_Loss: 3953.0771484375\n",
      "Epoch 6518, Train_Loss: 3821.720703125, Val_Loss: 3953.21728515625\n",
      "Epoch 6519, Train_Loss: 3821.73291015625, Val_Loss: 3953.490478515625\n",
      "Epoch 6520, Train_Loss: 3821.73876953125, Val_Loss: 3953.15087890625\n",
      "Epoch 6521, Train_Loss: 3821.7236328125, Val_Loss: 3953.029296875\n",
      "Epoch 6522, Train_Loss: 3821.72119140625, Val_Loss: 3953.171630859375\n",
      "Epoch 6523, Train_Loss: 3821.70947265625, Val_Loss: 3953.45751953125\n",
      "Epoch 6524, Train_Loss: 3821.677001953125, Val_Loss: 3953.0458984375\n",
      "Epoch 6525, Train_Loss: 3821.665283203125, Val_Loss: 3952.955078125\n",
      "Epoch 6526, Train_Loss: 3821.63916015625, Val_Loss: 3953.098876953125\n",
      "Epoch 6527, Train_Loss: 3821.732666015625, Val_Loss: 3953.37890625\n",
      "Epoch 6528, Train_Loss: 3821.644775390625, Val_Loss: 3953.06396484375\n",
      "Epoch 6529, Train_Loss: 3821.74658203125, Val_Loss: 3952.952880859375\n",
      "Epoch 6530, Train_Loss: 3821.593017578125, Val_Loss: 3953.111572265625\n",
      "Epoch 6531, Train_Loss: 3821.41552734375, Val_Loss: 3953.346435546875\n",
      "Epoch 6532, Train_Loss: 3821.306396484375, Val_Loss: 3953.029296875\n",
      "Epoch 6533, Train_Loss: 3821.41650390625, Val_Loss: 3952.912841796875\n",
      "Epoch 6534, Train_Loss: 3821.257080078125, Val_Loss: 3953.052001953125\n",
      "Epoch 6535, Train_Loss: 3821.4189453125, Val_Loss: 3953.315185546875\n",
      "Epoch 6536, Train_Loss: 3821.31982421875, Val_Loss: 3952.991943359375\n",
      "Epoch 6537, Train_Loss: 3821.392578125, Val_Loss: 3952.888427734375\n",
      "Epoch 6538, Train_Loss: 3821.277099609375, Val_Loss: 3953.01171875\n",
      "Epoch 6539, Train_Loss: 3821.36279296875, Val_Loss: 3953.246337890625\n",
      "Epoch 6540, Train_Loss: 3821.259765625, Val_Loss: 3952.900390625\n",
      "Epoch 6541, Train_Loss: 3821.33984375, Val_Loss: 3952.78271484375\n",
      "Epoch 6542, Train_Loss: 3821.20703125, Val_Loss: 3952.919921875\n",
      "Epoch 6543, Train_Loss: 3821.31103515625, Val_Loss: 3953.228759765625\n",
      "Epoch 6544, Train_Loss: 3821.350341796875, Val_Loss: 3952.8916015625\n",
      "Epoch 6545, Train_Loss: 3821.29443359375, Val_Loss: 3952.786865234375\n",
      "Epoch 6546, Train_Loss: 3821.252685546875, Val_Loss: 3952.901611328125\n",
      "Epoch 6547, Train_Loss: 3821.178466796875, Val_Loss: 3953.1572265625\n",
      "Epoch 6548, Train_Loss: 3821.28759765625, Val_Loss: 3952.849609375\n",
      "Epoch 6549, Train_Loss: 3821.234375, Val_Loss: 3952.777099609375\n",
      "Epoch 6550, Train_Loss: 3821.254150390625, Val_Loss: 3952.87451171875\n",
      "Epoch 6551, Train_Loss: 3821.168701171875, Val_Loss: 3953.1455078125\n",
      "Epoch 6552, Train_Loss: 3821.28173828125, Val_Loss: 3952.805908203125\n",
      "Epoch 6553, Train_Loss: 3821.213134765625, Val_Loss: 3952.698486328125\n",
      "Epoch 6554, Train_Loss: 3821.181396484375, Val_Loss: 3952.7919921875\n",
      "Epoch 6555, Train_Loss: 3821.187744140625, Val_Loss: 3953.06591796875\n",
      "Epoch 6556, Train_Loss: 3821.236328125, Val_Loss: 3952.71630859375\n",
      "Epoch 6557, Train_Loss: 3821.1201171875, Val_Loss: 3952.647216796875\n",
      "Epoch 6558, Train_Loss: 3821.20654296875, Val_Loss: 3952.775146484375\n",
      "Epoch 6559, Train_Loss: 3821.125732421875, Val_Loss: 3953.08251953125\n",
      "Epoch 6560, Train_Loss: 3821.189697265625, Val_Loss: 3952.72607421875\n",
      "Epoch 6561, Train_Loss: 3820.93115234375, Val_Loss: 3952.60400390625\n",
      "Epoch 6562, Train_Loss: 3821.033935546875, Val_Loss: 3952.719970703125\n",
      "Epoch 6563, Train_Loss: 3820.911376953125, Val_Loss: 3953.00634765625\n",
      "Epoch 6564, Train_Loss: 3821.027099609375, Val_Loss: 3952.67431640625\n",
      "Epoch 6565, Train_Loss: 3820.912109375, Val_Loss: 3952.600341796875\n",
      "Epoch 6566, Train_Loss: 3820.950927734375, Val_Loss: 3952.730712890625\n",
      "Epoch 6567, Train_Loss: 3820.869140625, Val_Loss: 3952.96630859375\n",
      "Epoch 6568, Train_Loss: 3820.903564453125, Val_Loss: 3952.5732421875\n",
      "Epoch 6569, Train_Loss: 3820.848876953125, Val_Loss: 3952.47119140625\n",
      "Epoch 6570, Train_Loss: 3820.8916015625, Val_Loss: 3952.6259765625\n",
      "Epoch 6571, Train_Loss: 3820.94140625, Val_Loss: 3952.8955078125\n",
      "Epoch 6572, Train_Loss: 3820.84130859375, Val_Loss: 3952.563720703125\n",
      "Epoch 6573, Train_Loss: 3820.81494140625, Val_Loss: 3952.4404296875\n",
      "Epoch 6574, Train_Loss: 3820.893310546875, Val_Loss: 3952.611083984375\n",
      "Epoch 6575, Train_Loss: 3820.900146484375, Val_Loss: 3952.92041015625\n",
      "Epoch 6576, Train_Loss: 3820.803466796875, Val_Loss: 3952.55322265625\n",
      "Epoch 6577, Train_Loss: 3820.776611328125, Val_Loss: 3952.415283203125\n",
      "Epoch 6578, Train_Loss: 3820.80224609375, Val_Loss: 3952.567138671875\n",
      "Epoch 6579, Train_Loss: 3820.837890625, Val_Loss: 3952.854736328125\n",
      "Epoch 6580, Train_Loss: 3820.791259765625, Val_Loss: 3952.539306640625\n",
      "Epoch 6581, Train_Loss: 3820.758544921875, Val_Loss: 3952.42041015625\n",
      "Epoch 6582, Train_Loss: 3820.77001953125, Val_Loss: 3952.5419921875\n",
      "Epoch 6583, Train_Loss: 3820.76123046875, Val_Loss: 3952.705078125\n",
      "Epoch 6584, Train_Loss: 3820.746337890625, Val_Loss: 3952.416015625\n",
      "Epoch 6585, Train_Loss: 3820.679931640625, Val_Loss: 3952.355224609375\n",
      "Epoch 6586, Train_Loss: 3820.665283203125, Val_Loss: 3952.47119140625\n",
      "Epoch 6587, Train_Loss: 3820.743896484375, Val_Loss: 3952.72509765625\n",
      "Epoch 6588, Train_Loss: 3820.68896484375, Val_Loss: 3952.4052734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6589, Train_Loss: 3820.643310546875, Val_Loss: 3952.3271484375\n",
      "Epoch 6590, Train_Loss: 3820.71484375, Val_Loss: 3952.469482421875\n",
      "Epoch 6591, Train_Loss: 3820.69384765625, Val_Loss: 3952.716064453125\n",
      "Epoch 6592, Train_Loss: 3820.600830078125, Val_Loss: 3952.367919921875\n",
      "Epoch 6593, Train_Loss: 3820.590576171875, Val_Loss: 3952.253662109375\n",
      "Epoch 6594, Train_Loss: 3820.6904296875, Val_Loss: 3952.40234375\n",
      "Epoch 6595, Train_Loss: 3820.68603515625, Val_Loss: 3952.68115234375\n",
      "Epoch 6596, Train_Loss: 3820.59912109375, Val_Loss: 3952.36083984375\n",
      "Epoch 6597, Train_Loss: 3820.565673828125, Val_Loss: 3952.236083984375\n",
      "Epoch 6598, Train_Loss: 3820.55615234375, Val_Loss: 3952.335205078125\n",
      "Epoch 6599, Train_Loss: 3820.519287109375, Val_Loss: 3952.58642578125\n",
      "Epoch 6600, Train_Loss: 3820.4921875, Val_Loss: 3952.242919921875\n",
      "Epoch 6601, Train_Loss: 3820.451416015625, Val_Loss: 3952.136474609375\n",
      "Epoch 6602, Train_Loss: 3820.521484375, Val_Loss: 3952.345703125\n",
      "Epoch 6603, Train_Loss: 3820.501953125, Val_Loss: 3952.613525390625\n",
      "Epoch 6604, Train_Loss: 3820.473388671875, Val_Loss: 3952.242431640625\n",
      "Epoch 6605, Train_Loss: 3820.3818359375, Val_Loss: 3952.127685546875\n",
      "Epoch 6606, Train_Loss: 3820.47998046875, Val_Loss: 3952.2783203125\n",
      "Epoch 6607, Train_Loss: 3820.4599609375, Val_Loss: 3952.57763671875\n",
      "Epoch 6608, Train_Loss: 3820.42041015625, Val_Loss: 3952.20361328125\n",
      "Epoch 6609, Train_Loss: 3820.349365234375, Val_Loss: 3952.06689453125\n",
      "Epoch 6610, Train_Loss: 3820.45703125, Val_Loss: 3952.218017578125\n",
      "Epoch 6611, Train_Loss: 3820.451904296875, Val_Loss: 3952.527587890625\n",
      "Epoch 6612, Train_Loss: 3820.393310546875, Val_Loss: 3952.216796875\n",
      "Epoch 6613, Train_Loss: 3820.2705078125, Val_Loss: 3951.996337890625\n",
      "Epoch 6614, Train_Loss: 3820.400634765625, Val_Loss: 3952.13037109375\n",
      "Epoch 6615, Train_Loss: 3820.3447265625, Val_Loss: 3952.404296875\n",
      "Epoch 6616, Train_Loss: 3820.285888671875, Val_Loss: 3952.134521484375\n",
      "Epoch 6617, Train_Loss: 3820.248779296875, Val_Loss: 3952.03515625\n",
      "Epoch 6618, Train_Loss: 3820.293701171875, Val_Loss: 3952.195556640625\n",
      "Epoch 6619, Train_Loss: 3820.310302734375, Val_Loss: 3952.410400390625\n",
      "Epoch 6620, Train_Loss: 3820.155029296875, Val_Loss: 3952.0263671875\n",
      "Epoch 6621, Train_Loss: 3820.104736328125, Val_Loss: 3951.965087890625\n",
      "Epoch 6622, Train_Loss: 3820.166015625, Val_Loss: 3952.15869140625\n",
      "Epoch 6623, Train_Loss: 3820.214111328125, Val_Loss: 3952.432861328125\n",
      "Epoch 6624, Train_Loss: 3820.3291015625, Val_Loss: 3952.021484375\n",
      "Epoch 6625, Train_Loss: 3820.32080078125, Val_Loss: 3951.917236328125\n",
      "Epoch 6626, Train_Loss: 3820.320556640625, Val_Loss: 3952.052734375\n",
      "Epoch 6627, Train_Loss: 3820.38232421875, Val_Loss: 3952.3037109375\n",
      "Epoch 6628, Train_Loss: 3820.278076171875, Val_Loss: 3951.9599609375\n",
      "Epoch 6629, Train_Loss: 3820.2275390625, Val_Loss: 3951.857177734375\n",
      "Epoch 6630, Train_Loss: 3820.245361328125, Val_Loss: 3951.970458984375\n",
      "Epoch 6631, Train_Loss: 3820.3154296875, Val_Loss: 3952.263671875\n",
      "Epoch 6632, Train_Loss: 3820.224609375, Val_Loss: 3951.9541015625\n",
      "Epoch 6633, Train_Loss: 3820.216796875, Val_Loss: 3951.856689453125\n",
      "Epoch 6634, Train_Loss: 3820.25439453125, Val_Loss: 3952.010009765625\n",
      "Epoch 6635, Train_Loss: 3820.2646484375, Val_Loss: 3952.25830078125\n",
      "Epoch 6636, Train_Loss: 3820.1748046875, Val_Loss: 3951.894287109375\n",
      "Epoch 6637, Train_Loss: 3820.158447265625, Val_Loss: 3951.785888671875\n",
      "Epoch 6638, Train_Loss: 3820.183837890625, Val_Loss: 3951.95556640625\n",
      "Epoch 6639, Train_Loss: 3820.25439453125, Val_Loss: 3952.232421875\n",
      "Epoch 6640, Train_Loss: 3820.177490234375, Val_Loss: 3951.90673828125\n",
      "Epoch 6641, Train_Loss: 3820.1171875, Val_Loss: 3951.781982421875\n",
      "Epoch 6642, Train_Loss: 3820.1396484375, Val_Loss: 3951.8359375\n",
      "Epoch 6643, Train_Loss: 3820.15576171875, Val_Loss: 3952.100341796875\n",
      "Epoch 6644, Train_Loss: 3820.151123046875, Val_Loss: 3951.799560546875\n",
      "Epoch 6645, Train_Loss: 3820.092529296875, Val_Loss: 3951.740478515625\n",
      "Epoch 6646, Train_Loss: 3820.111328125, Val_Loss: 3951.88916015625\n",
      "Epoch 6647, Train_Loss: 3820.137939453125, Val_Loss: 3952.11328125\n",
      "Epoch 6648, Train_Loss: 3820.08984375, Val_Loss: 3951.76513671875\n",
      "Epoch 6649, Train_Loss: 3819.814453125, Val_Loss: 3951.674072265625\n",
      "Epoch 6650, Train_Loss: 3819.769775390625, Val_Loss: 3951.818115234375\n",
      "Epoch 6651, Train_Loss: 3819.78173828125, Val_Loss: 3952.076904296875\n",
      "Epoch 6652, Train_Loss: 3819.725341796875, Val_Loss: 3951.757080078125\n",
      "Epoch 6653, Train_Loss: 3819.84228515625, Val_Loss: 3951.6201171875\n",
      "Epoch 6654, Train_Loss: 3819.779052734375, Val_Loss: 3951.764404296875\n",
      "Epoch 6655, Train_Loss: 3819.793212890625, Val_Loss: 3952.040771484375\n",
      "Epoch 6656, Train_Loss: 3819.69140625, Val_Loss: 3951.65966796875\n",
      "Epoch 6657, Train_Loss: 3819.788818359375, Val_Loss: 3951.53564453125\n",
      "Epoch 6658, Train_Loss: 3819.70703125, Val_Loss: 3951.682861328125\n",
      "Epoch 6659, Train_Loss: 3819.732666015625, Val_Loss: 3951.980712890625\n",
      "Epoch 6660, Train_Loss: 3819.715576171875, Val_Loss: 3951.664794921875\n",
      "Epoch 6661, Train_Loss: 3819.749267578125, Val_Loss: 3951.536865234375\n",
      "Epoch 6662, Train_Loss: 3819.73486328125, Val_Loss: 3951.68798828125\n",
      "Epoch 6663, Train_Loss: 3819.697509765625, Val_Loss: 3951.96240234375\n",
      "Epoch 6664, Train_Loss: 3819.651123046875, Val_Loss: 3951.615966796875\n",
      "Epoch 6665, Train_Loss: 3819.68310546875, Val_Loss: 3951.49560546875\n",
      "Epoch 6666, Train_Loss: 3819.593994140625, Val_Loss: 3951.656494140625\n",
      "Epoch 6667, Train_Loss: 3819.656005859375, Val_Loss: 3951.941162109375\n",
      "Epoch 6668, Train_Loss: 3819.6357421875, Val_Loss: 3951.581298828125\n",
      "Epoch 6669, Train_Loss: 3819.66552734375, Val_Loss: 3951.46484375\n",
      "Epoch 6670, Train_Loss: 3819.678466796875, Val_Loss: 3951.60888671875\n",
      "Epoch 6671, Train_Loss: 3819.607666015625, Val_Loss: 3951.81689453125\n",
      "Epoch 6672, Train_Loss: 3819.5966796875, Val_Loss: 3951.494384765625\n",
      "Epoch 6673, Train_Loss: 3819.626953125, Val_Loss: 3951.365478515625\n",
      "Epoch 6674, Train_Loss: 3819.61669921875, Val_Loss: 3951.5283203125\n",
      "Epoch 6675, Train_Loss: 3819.56298828125, Val_Loss: 3951.8388671875\n",
      "Epoch 6676, Train_Loss: 3819.552490234375, Val_Loss: 3951.487548828125\n",
      "Epoch 6677, Train_Loss: 3819.603271484375, Val_Loss: 3951.352783203125\n",
      "Epoch 6678, Train_Loss: 3819.45361328125, Val_Loss: 3951.496826171875\n",
      "Epoch 6679, Train_Loss: 3819.384765625, Val_Loss: 3951.80517578125\n",
      "Epoch 6680, Train_Loss: 3819.379150390625, Val_Loss: 3951.449951171875\n",
      "Epoch 6681, Train_Loss: 3819.412353515625, Val_Loss: 3951.324462890625\n",
      "Epoch 6682, Train_Loss: 3819.4326171875, Val_Loss: 3951.46630859375\n",
      "Epoch 6683, Train_Loss: 3819.4248046875, Val_Loss: 3951.71875\n",
      "Epoch 6684, Train_Loss: 3819.358642578125, Val_Loss: 3951.43994140625\n",
      "Epoch 6685, Train_Loss: 3819.35400390625, Val_Loss: 3951.249267578125\n",
      "Epoch 6686, Train_Loss: 3819.34912109375, Val_Loss: 3951.387939453125\n",
      "Epoch 6687, Train_Loss: 3819.332763671875, Val_Loss: 3951.662353515625\n",
      "Epoch 6688, Train_Loss: 3819.2978515625, Val_Loss: 3951.324462890625\n",
      "Epoch 6689, Train_Loss: 3819.323974609375, Val_Loss: 3951.248046875\n",
      "Epoch 6690, Train_Loss: 3819.310302734375, Val_Loss: 3951.41748046875\n",
      "Epoch 6691, Train_Loss: 3819.297607421875, Val_Loss: 3951.689697265625\n",
      "Epoch 6692, Train_Loss: 3819.24658203125, Val_Loss: 3951.32275390625\n",
      "Epoch 6693, Train_Loss: 3819.403564453125, Val_Loss: 3951.169189453125\n",
      "Epoch 6694, Train_Loss: 3819.21533203125, Val_Loss: 3951.33349609375\n",
      "Epoch 6695, Train_Loss: 3819.254638671875, Val_Loss: 3951.61328125\n",
      "Epoch 6696, Train_Loss: 3819.22021484375, Val_Loss: 3951.28515625\n",
      "Epoch 6697, Train_Loss: 3819.327392578125, Val_Loss: 3951.188720703125\n",
      "Epoch 6698, Train_Loss: 3819.210205078125, Val_Loss: 3951.320068359375\n",
      "Epoch 6699, Train_Loss: 3819.23095703125, Val_Loss: 3951.58642578125\n",
      "Epoch 6700, Train_Loss: 3819.1669921875, Val_Loss: 3951.203125\n",
      "Epoch 6701, Train_Loss: 3819.278076171875, Val_Loss: 3951.0947265625\n",
      "Epoch 6702, Train_Loss: 3819.17578125, Val_Loss: 3951.227294921875\n",
      "Epoch 6703, Train_Loss: 3819.188720703125, Val_Loss: 3951.509521484375\n",
      "Epoch 6704, Train_Loss: 3819.258056640625, Val_Loss: 3951.1923828125\n",
      "Epoch 6705, Train_Loss: 3819.289794921875, Val_Loss: 3951.075927734375\n",
      "Epoch 6706, Train_Loss: 3819.14501953125, Val_Loss: 3951.240478515625\n",
      "Epoch 6707, Train_Loss: 3819.095703125, Val_Loss: 3951.505126953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6708, Train_Loss: 3819.18115234375, Val_Loss: 3951.154052734375\n",
      "Epoch 6709, Train_Loss: 3819.22412109375, Val_Loss: 3951.032470703125\n",
      "Epoch 6710, Train_Loss: 3819.160888671875, Val_Loss: 3951.184814453125\n",
      "Epoch 6711, Train_Loss: 3819.14404296875, Val_Loss: 3951.456787109375\n",
      "Epoch 6712, Train_Loss: 3819.180908203125, Val_Loss: 3951.11474609375\n",
      "Epoch 6713, Train_Loss: 3819.220947265625, Val_Loss: 3950.99169921875\n",
      "Epoch 6714, Train_Loss: 3819.06982421875, Val_Loss: 3951.09033203125\n",
      "Epoch 6715, Train_Loss: 3819.009521484375, Val_Loss: 3951.38720703125\n",
      "Epoch 6716, Train_Loss: 3819.05615234375, Val_Loss: 3951.033203125\n",
      "Epoch 6717, Train_Loss: 3819.11279296875, Val_Loss: 3950.91357421875\n",
      "Epoch 6718, Train_Loss: 3819.037353515625, Val_Loss: 3951.12353515625\n",
      "Epoch 6719, Train_Loss: 3818.9599609375, Val_Loss: 3951.399169921875\n",
      "Epoch 6720, Train_Loss: 3819.03515625, Val_Loss: 3951.036865234375\n",
      "Epoch 6721, Train_Loss: 3819.051025390625, Val_Loss: 3950.93408203125\n",
      "Epoch 6722, Train_Loss: 3818.99951171875, Val_Loss: 3951.04443359375\n",
      "Epoch 6723, Train_Loss: 3818.95068359375, Val_Loss: 3951.32568359375\n",
      "Epoch 6724, Train_Loss: 3818.99169921875, Val_Loss: 3950.998046875\n",
      "Epoch 6725, Train_Loss: 3819.009033203125, Val_Loss: 3950.870361328125\n",
      "Epoch 6726, Train_Loss: 3818.990234375, Val_Loss: 3950.983154296875\n",
      "Epoch 6727, Train_Loss: 3818.92626953125, Val_Loss: 3951.29150390625\n",
      "Epoch 6728, Train_Loss: 3818.973388671875, Val_Loss: 3950.971923828125\n",
      "Epoch 6729, Train_Loss: 3818.9716796875, Val_Loss: 3950.81201171875\n",
      "Epoch 6730, Train_Loss: 3818.940673828125, Val_Loss: 3950.912109375\n",
      "Epoch 6731, Train_Loss: 3818.998291015625, Val_Loss: 3951.148681640625\n",
      "Epoch 6732, Train_Loss: 3818.916748046875, Val_Loss: 3950.900390625\n",
      "Epoch 6733, Train_Loss: 3818.939453125, Val_Loss: 3950.78564453125\n",
      "Epoch 6734, Train_Loss: 3818.88134765625, Val_Loss: 3950.934814453125\n",
      "Epoch 6735, Train_Loss: 3818.976806640625, Val_Loss: 3951.211181640625\n",
      "Epoch 6736, Train_Loss: 3818.797607421875, Val_Loss: 3950.8623046875\n",
      "Epoch 6737, Train_Loss: 3818.822265625, Val_Loss: 3950.7119140625\n",
      "Epoch 6738, Train_Loss: 3818.775390625, Val_Loss: 3950.88916015625\n",
      "Epoch 6739, Train_Loss: 3818.830810546875, Val_Loss: 3951.169189453125\n",
      "Epoch 6740, Train_Loss: 3818.939208984375, Val_Loss: 3950.84130859375\n",
      "Epoch 6741, Train_Loss: 3819.0068359375, Val_Loss: 3950.712890625\n",
      "Epoch 6742, Train_Loss: 3818.919921875, Val_Loss: 3950.85205078125\n",
      "Epoch 6743, Train_Loss: 3818.968994140625, Val_Loss: 3951.0556640625\n",
      "Epoch 6744, Train_Loss: 3818.8828125, Val_Loss: 3950.741943359375\n",
      "Epoch 6745, Train_Loss: 3818.93359375, Val_Loss: 3950.649169921875\n",
      "Epoch 6746, Train_Loss: 3818.845947265625, Val_Loss: 3950.777587890625\n",
      "Epoch 6747, Train_Loss: 3818.94091796875, Val_Loss: 3951.082763671875\n",
      "Epoch 6748, Train_Loss: 3818.866943359375, Val_Loss: 3950.736328125\n",
      "Epoch 6749, Train_Loss: 3818.84912109375, Val_Loss: 3950.63916015625\n",
      "Epoch 6750, Train_Loss: 3818.814208984375, Val_Loss: 3950.733154296875\n",
      "Epoch 6751, Train_Loss: 3818.9052734375, Val_Loss: 3951.0400390625\n",
      "Epoch 6752, Train_Loss: 3818.821533203125, Val_Loss: 3950.706787109375\n",
      "Epoch 6753, Train_Loss: 3818.813232421875, Val_Loss: 3950.560302734375\n",
      "Epoch 6754, Train_Loss: 3818.805419921875, Val_Loss: 3950.7060546875\n",
      "Epoch 6755, Train_Loss: 3818.88232421875, Val_Loss: 3950.98828125\n",
      "Epoch 6756, Train_Loss: 3818.796630859375, Val_Loss: 3950.689697265625\n",
      "Epoch 6757, Train_Loss: 3818.856201171875, Val_Loss: 3950.46630859375\n",
      "Epoch 6758, Train_Loss: 3818.759033203125, Val_Loss: 3950.615966796875\n",
      "Epoch 6759, Train_Loss: 3818.839599609375, Val_Loss: 3950.901611328125\n",
      "Epoch 6760, Train_Loss: 3818.740234375, Val_Loss: 3950.618408203125\n",
      "Epoch 6761, Train_Loss: 3818.816162109375, Val_Loss: 3950.5224609375\n",
      "Epoch 6762, Train_Loss: 3818.722412109375, Val_Loss: 3950.64599609375\n",
      "Epoch 6763, Train_Loss: 3818.75390625, Val_Loss: 3950.9111328125\n",
      "Epoch 6764, Train_Loss: 3818.721435546875, Val_Loss: 3950.58203125\n",
      "Epoch 6765, Train_Loss: 3818.443603515625, Val_Loss: 3950.455078125\n",
      "Epoch 6766, Train_Loss: 3818.376708984375, Val_Loss: 3950.60009765625\n",
      "Epoch 6767, Train_Loss: 3818.457275390625, Val_Loss: 3950.890380859375\n",
      "Epoch 6768, Train_Loss: 3818.4482421875, Val_Loss: 3950.55322265625\n",
      "Epoch 6769, Train_Loss: 3818.453369140625, Val_Loss: 3950.42529296875\n",
      "Epoch 6770, Train_Loss: 3818.410888671875, Val_Loss: 3950.57080078125\n",
      "Epoch 6771, Train_Loss: 3818.42919921875, Val_Loss: 3950.864501953125\n",
      "Epoch 6772, Train_Loss: 3818.3916015625, Val_Loss: 3950.472412109375\n",
      "Epoch 6773, Train_Loss: 3818.4521484375, Val_Loss: 3950.353515625\n",
      "Epoch 6774, Train_Loss: 3818.335205078125, Val_Loss: 3950.50439453125\n",
      "Epoch 6775, Train_Loss: 3818.37158203125, Val_Loss: 3950.78759765625\n",
      "Epoch 6776, Train_Loss: 3818.349365234375, Val_Loss: 3950.47509765625\n",
      "Epoch 6777, Train_Loss: 3818.284912109375, Val_Loss: 3950.361083984375\n",
      "Epoch 6778, Train_Loss: 3818.30029296875, Val_Loss: 3950.501708984375\n",
      "Epoch 6779, Train_Loss: 3818.327392578125, Val_Loss: 3950.742919921875\n",
      "Epoch 6780, Train_Loss: 3818.24560546875, Val_Loss: 3950.4111328125\n",
      "Epoch 6781, Train_Loss: 3818.2177734375, Val_Loss: 3950.28564453125\n",
      "Epoch 6782, Train_Loss: 3818.21923828125, Val_Loss: 3950.419677734375\n",
      "Epoch 6783, Train_Loss: 3818.327392578125, Val_Loss: 3950.7451171875\n",
      "Epoch 6784, Train_Loss: 3818.194091796875, Val_Loss: 3950.335693359375\n",
      "Epoch 6785, Train_Loss: 3818.205322265625, Val_Loss: 3950.2119140625\n",
      "Epoch 6786, Train_Loss: 3818.17041015625, Val_Loss: 3950.325927734375\n",
      "Epoch 6787, Train_Loss: 3818.286376953125, Val_Loss: 3950.6396484375\n",
      "Epoch 6788, Train_Loss: 3818.1474609375, Val_Loss: 3950.27392578125\n",
      "Epoch 6789, Train_Loss: 3818.10791015625, Val_Loss: 3950.171142578125\n",
      "Epoch 6790, Train_Loss: 3818.133056640625, Val_Loss: 3950.33203125\n",
      "Epoch 6791, Train_Loss: 3818.251953125, Val_Loss: 3950.6533203125\n",
      "Epoch 6792, Train_Loss: 3818.1259765625, Val_Loss: 3950.293212890625\n",
      "Epoch 6793, Train_Loss: 3817.9208984375, Val_Loss: 3950.133544921875\n",
      "Epoch 6794, Train_Loss: 3817.970947265625, Val_Loss: 3950.281982421875\n",
      "Epoch 6795, Train_Loss: 3818.06787109375, Val_Loss: 3950.6083984375\n",
      "Epoch 6796, Train_Loss: 3817.93701171875, Val_Loss: 3950.281494140625\n",
      "Epoch 6797, Train_Loss: 3817.9326171875, Val_Loss: 3950.151123046875\n",
      "Epoch 6798, Train_Loss: 3817.902099609375, Val_Loss: 3950.294921875\n",
      "Epoch 6799, Train_Loss: 3818.01025390625, Val_Loss: 3950.5439453125\n",
      "Epoch 6800, Train_Loss: 3817.888427734375, Val_Loss: 3950.180419921875\n",
      "Epoch 6801, Train_Loss: 3817.87109375, Val_Loss: 3950.075927734375\n",
      "Epoch 6802, Train_Loss: 3817.8212890625, Val_Loss: 3950.220458984375\n",
      "Epoch 6803, Train_Loss: 3817.987548828125, Val_Loss: 3950.448486328125\n",
      "Epoch 6804, Train_Loss: 3817.833740234375, Val_Loss: 3950.166748046875\n",
      "Epoch 6805, Train_Loss: 3817.865478515625, Val_Loss: 3950.040283203125\n",
      "Epoch 6806, Train_Loss: 3817.7998046875, Val_Loss: 3950.20556640625\n",
      "Epoch 6807, Train_Loss: 3817.953125, Val_Loss: 3950.450439453125\n",
      "Epoch 6808, Train_Loss: 3817.85107421875, Val_Loss: 3950.092041015625\n",
      "Epoch 6809, Train_Loss: 3817.73828125, Val_Loss: 3949.998779296875\n",
      "Epoch 6810, Train_Loss: 3817.781005859375, Val_Loss: 3950.147705078125\n",
      "Epoch 6811, Train_Loss: 3817.8271484375, Val_Loss: 3950.476806640625\n",
      "Epoch 6812, Train_Loss: 3817.82275390625, Val_Loss: 3950.052734375\n",
      "Epoch 6813, Train_Loss: 3817.7919921875, Val_Loss: 3949.916748046875\n",
      "Epoch 6814, Train_Loss: 3817.72607421875, Val_Loss: 3950.020751953125\n",
      "Epoch 6815, Train_Loss: 3817.833984375, Val_Loss: 3950.35205078125\n",
      "Epoch 6816, Train_Loss: 3817.7158203125, Val_Loss: 3949.998291015625\n",
      "Epoch 6817, Train_Loss: 3817.78515625, Val_Loss: 3949.8740234375\n",
      "Epoch 6818, Train_Loss: 3817.708251953125, Val_Loss: 3950.053955078125\n",
      "Epoch 6819, Train_Loss: 3817.773681640625, Val_Loss: 3950.354736328125\n",
      "Epoch 6820, Train_Loss: 3817.76123046875, Val_Loss: 3950.006103515625\n",
      "Epoch 6821, Train_Loss: 3817.676025390625, Val_Loss: 3949.815185546875\n",
      "Epoch 6822, Train_Loss: 3817.630615234375, Val_Loss: 3950.007568359375\n",
      "Epoch 6823, Train_Loss: 3817.725830078125, Val_Loss: 3950.334716796875\n",
      "Epoch 6824, Train_Loss: 3817.6669921875, Val_Loss: 3949.95556640625\n",
      "Epoch 6825, Train_Loss: 3817.708740234375, Val_Loss: 3949.806396484375\n",
      "Epoch 6826, Train_Loss: 3817.68994140625, Val_Loss: 3949.969970703125\n",
      "Epoch 6827, Train_Loss: 3817.712158203125, Val_Loss: 3950.320068359375\n",
      "Epoch 6828, Train_Loss: 3817.58349609375, Val_Loss: 3949.89404296875\n",
      "Epoch 6829, Train_Loss: 3817.62353515625, Val_Loss: 3949.767578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6830, Train_Loss: 3817.57080078125, Val_Loss: 3949.904296875\n",
      "Epoch 6831, Train_Loss: 3817.61181640625, Val_Loss: 3950.17041015625\n",
      "Epoch 6832, Train_Loss: 3817.62890625, Val_Loss: 3949.85791015625\n",
      "Epoch 6833, Train_Loss: 3817.55078125, Val_Loss: 3949.76171875\n",
      "Epoch 6834, Train_Loss: 3817.5390625, Val_Loss: 3949.924072265625\n",
      "Epoch 6835, Train_Loss: 3817.54443359375, Val_Loss: 3950.17529296875\n",
      "Epoch 6836, Train_Loss: 3817.580810546875, Val_Loss: 3949.814697265625\n",
      "Epoch 6837, Train_Loss: 3817.483642578125, Val_Loss: 3949.674072265625\n",
      "Epoch 6838, Train_Loss: 3817.493896484375, Val_Loss: 3949.852783203125\n",
      "Epoch 6839, Train_Loss: 3817.54248046875, Val_Loss: 3950.189697265625\n",
      "Epoch 6840, Train_Loss: 3817.537841796875, Val_Loss: 3949.8056640625\n",
      "Epoch 6841, Train_Loss: 3817.474365234375, Val_Loss: 3949.64404296875\n",
      "Epoch 6842, Train_Loss: 3817.456298828125, Val_Loss: 3949.76318359375\n",
      "Epoch 6843, Train_Loss: 3817.49462890625, Val_Loss: 3950.064453125\n",
      "Epoch 6844, Train_Loss: 3817.493408203125, Val_Loss: 3949.73046875\n",
      "Epoch 6845, Train_Loss: 3817.445068359375, Val_Loss: 3949.569580078125\n",
      "Epoch 6846, Train_Loss: 3817.41796875, Val_Loss: 3949.76708984375\n",
      "Epoch 6847, Train_Loss: 3817.4443359375, Val_Loss: 3950.0615234375\n",
      "Epoch 6848, Train_Loss: 3817.478515625, Val_Loss: 3949.71435546875\n",
      "Epoch 6849, Train_Loss: 3817.313232421875, Val_Loss: 3949.56591796875\n",
      "Epoch 6850, Train_Loss: 3817.2861328125, Val_Loss: 3949.7294921875\n",
      "Epoch 6851, Train_Loss: 3817.327392578125, Val_Loss: 3950.03076171875\n",
      "Epoch 6852, Train_Loss: 3817.35107421875, Val_Loss: 3949.6572265625\n",
      "Epoch 6853, Train_Loss: 3817.494140625, Val_Loss: 3949.527587890625\n",
      "Epoch 6854, Train_Loss: 3817.474853515625, Val_Loss: 3949.724853515625\n",
      "Epoch 6855, Train_Loss: 3817.509033203125, Val_Loss: 3950.018310546875\n",
      "Epoch 6856, Train_Loss: 3817.490234375, Val_Loss: 3949.574462890625\n",
      "Epoch 6857, Train_Loss: 3817.411865234375, Val_Loss: 3949.465087890625\n",
      "Epoch 6858, Train_Loss: 3817.412353515625, Val_Loss: 3949.629150390625\n",
      "Epoch 6859, Train_Loss: 3817.4345703125, Val_Loss: 3949.943603515625\n",
      "Epoch 6860, Train_Loss: 3817.46923828125, Val_Loss: 3949.590087890625\n",
      "Epoch 6861, Train_Loss: 3817.386962890625, Val_Loss: 3949.47607421875\n",
      "Epoch 6862, Train_Loss: 3817.43603515625, Val_Loss: 3949.629638671875\n",
      "Epoch 6863, Train_Loss: 3817.3525390625, Val_Loss: 3949.9462890625\n",
      "Epoch 6864, Train_Loss: 3817.424560546875, Val_Loss: 3949.534423828125\n",
      "Epoch 6865, Train_Loss: 3817.34619140625, Val_Loss: 3949.4423828125\n",
      "Epoch 6866, Train_Loss: 3817.338623046875, Val_Loss: 3949.587158203125\n",
      "Epoch 6867, Train_Loss: 3817.3828125, Val_Loss: 3949.866455078125\n",
      "Epoch 6868, Train_Loss: 3817.3193359375, Val_Loss: 3949.51953125\n",
      "Epoch 6869, Train_Loss: 3817.438232421875, Val_Loss: 3949.379638671875\n",
      "Epoch 6870, Train_Loss: 3817.321533203125, Val_Loss: 3949.553955078125\n",
      "Epoch 6871, Train_Loss: 3817.3408203125, Val_Loss: 3949.80908203125\n",
      "Epoch 6872, Train_Loss: 3817.3525390625, Val_Loss: 3949.44091796875\n",
      "Epoch 6873, Train_Loss: 3817.419189453125, Val_Loss: 3949.302734375\n",
      "Epoch 6874, Train_Loss: 3817.339599609375, Val_Loss: 3949.498779296875\n",
      "Epoch 6875, Train_Loss: 3817.337158203125, Val_Loss: 3949.80810546875\n",
      "Epoch 6876, Train_Loss: 3817.279296875, Val_Loss: 3949.439697265625\n",
      "Epoch 6877, Train_Loss: 3817.388916015625, Val_Loss: 3949.283935546875\n",
      "Epoch 6878, Train_Loss: 3816.978271484375, Val_Loss: 3949.4580078125\n",
      "Epoch 6879, Train_Loss: 3817.003173828125, Val_Loss: 3949.76123046875\n",
      "Epoch 6880, Train_Loss: 3817.0625, Val_Loss: 3949.39208984375\n",
      "Epoch 6881, Train_Loss: 3817.09228515625, Val_Loss: 3949.284423828125\n",
      "Epoch 6882, Train_Loss: 3816.9990234375, Val_Loss: 3949.448486328125\n",
      "Epoch 6883, Train_Loss: 3817.01611328125, Val_Loss: 3949.746826171875\n",
      "Epoch 6884, Train_Loss: 3817.084228515625, Val_Loss: 3949.3369140625\n",
      "Epoch 6885, Train_Loss: 3817.03515625, Val_Loss: 3949.1748046875\n",
      "Epoch 6886, Train_Loss: 3817.07080078125, Val_Loss: 3949.34326171875\n",
      "Epoch 6887, Train_Loss: 3816.96923828125, Val_Loss: 3949.662353515625\n",
      "Epoch 6888, Train_Loss: 3817.019287109375, Val_Loss: 3949.317138671875\n",
      "Epoch 6889, Train_Loss: 3817.00927734375, Val_Loss: 3949.176025390625\n",
      "Epoch 6890, Train_Loss: 3817.0302734375, Val_Loss: 3949.361083984375\n",
      "Epoch 6891, Train_Loss: 3816.934814453125, Val_Loss: 3949.649169921875\n",
      "Epoch 6892, Train_Loss: 3816.96240234375, Val_Loss: 3949.26806640625\n",
      "Epoch 6893, Train_Loss: 3816.9443359375, Val_Loss: 3949.122802734375\n",
      "Epoch 6894, Train_Loss: 3816.9638671875, Val_Loss: 3949.3291015625\n",
      "Epoch 6895, Train_Loss: 3816.885986328125, Val_Loss: 3949.610107421875\n",
      "Epoch 6896, Train_Loss: 3816.945556640625, Val_Loss: 3949.256103515625\n",
      "Epoch 6897, Train_Loss: 3816.929931640625, Val_Loss: 3949.097900390625\n",
      "Epoch 6898, Train_Loss: 3816.95654296875, Val_Loss: 3949.285888671875\n",
      "Epoch 6899, Train_Loss: 3816.823974609375, Val_Loss: 3949.531982421875\n",
      "Epoch 6900, Train_Loss: 3816.90869140625, Val_Loss: 3949.14111328125\n",
      "Epoch 6901, Train_Loss: 3816.839599609375, Val_Loss: 3949.03076171875\n",
      "Epoch 6902, Train_Loss: 3816.90087890625, Val_Loss: 3949.214111328125\n",
      "Epoch 6903, Train_Loss: 3816.787841796875, Val_Loss: 3949.48388671875\n",
      "Epoch 6904, Train_Loss: 3816.87353515625, Val_Loss: 3949.14794921875\n",
      "Epoch 6905, Train_Loss: 3816.79833984375, Val_Loss: 3949.04443359375\n",
      "Epoch 6906, Train_Loss: 3816.7490234375, Val_Loss: 3949.182373046875\n",
      "Epoch 6907, Train_Loss: 3816.747314453125, Val_Loss: 3949.447509765625\n",
      "Epoch 6908, Train_Loss: 3816.649658203125, Val_Loss: 3949.114501953125\n",
      "Epoch 6909, Train_Loss: 3816.64697265625, Val_Loss: 3948.9794921875\n",
      "Epoch 6910, Train_Loss: 3816.7060546875, Val_Loss: 3949.14599609375\n",
      "Epoch 6911, Train_Loss: 3816.701904296875, Val_Loss: 3949.46875\n",
      "Epoch 6912, Train_Loss: 3816.6181640625, Val_Loss: 3949.113525390625\n",
      "Epoch 6913, Train_Loss: 3816.597412109375, Val_Loss: 3948.89794921875\n",
      "Epoch 6914, Train_Loss: 3816.643310546875, Val_Loss: 3949.064697265625\n",
      "Epoch 6915, Train_Loss: 3816.642822265625, Val_Loss: 3949.379638671875\n",
      "Epoch 6916, Train_Loss: 3816.55029296875, Val_Loss: 3949.058837890625\n",
      "Epoch 6917, Train_Loss: 3816.5576171875, Val_Loss: 3948.915283203125\n",
      "Epoch 6918, Train_Loss: 3816.505859375, Val_Loss: 3949.08447265625\n",
      "Epoch 6919, Train_Loss: 3816.605712890625, Val_Loss: 3949.359130859375\n",
      "Epoch 6920, Train_Loss: 3816.495849609375, Val_Loss: 3949.01123046875\n",
      "Epoch 6921, Train_Loss: 3816.4873046875, Val_Loss: 3948.870849609375\n",
      "Epoch 6922, Train_Loss: 3816.45361328125, Val_Loss: 3949.04443359375\n",
      "Epoch 6923, Train_Loss: 3816.56640625, Val_Loss: 3949.3125\n",
      "Epoch 6924, Train_Loss: 3816.49560546875, Val_Loss: 3948.96484375\n",
      "Epoch 6925, Train_Loss: 3816.48046875, Val_Loss: 3948.8427734375\n",
      "Epoch 6926, Train_Loss: 3816.45458984375, Val_Loss: 3948.99169921875\n",
      "Epoch 6927, Train_Loss: 3816.482177734375, Val_Loss: 3949.235595703125\n",
      "Epoch 6928, Train_Loss: 3816.434326171875, Val_Loss: 3948.88330078125\n",
      "Epoch 6929, Train_Loss: 3816.429931640625, Val_Loss: 3948.760009765625\n",
      "Epoch 6930, Train_Loss: 3816.40234375, Val_Loss: 3948.92724609375\n",
      "Epoch 6931, Train_Loss: 3816.45556640625, Val_Loss: 3949.194091796875\n",
      "Epoch 6932, Train_Loss: 3816.40234375, Val_Loss: 3948.87890625\n",
      "Epoch 6933, Train_Loss: 3816.337158203125, Val_Loss: 3948.75634765625\n",
      "Epoch 6934, Train_Loss: 3816.32861328125, Val_Loss: 3948.9072265625\n",
      "Epoch 6935, Train_Loss: 3816.410400390625, Val_Loss: 3949.19287109375\n",
      "Epoch 6936, Train_Loss: 3816.367431640625, Val_Loss: 3948.835693359375\n",
      "Epoch 6937, Train_Loss: 3816.34228515625, Val_Loss: 3948.714111328125\n",
      "Epoch 6938, Train_Loss: 3816.35302734375, Val_Loss: 3948.87158203125\n",
      "Epoch 6939, Train_Loss: 3816.351806640625, Val_Loss: 3949.167236328125\n",
      "Epoch 6940, Train_Loss: 3816.266357421875, Val_Loss: 3948.735595703125\n",
      "Epoch 6941, Train_Loss: 3816.213623046875, Val_Loss: 3948.620361328125\n",
      "Epoch 6942, Train_Loss: 3816.2294921875, Val_Loss: 3948.775146484375\n",
      "Epoch 6943, Train_Loss: 3816.26904296875, Val_Loss: 3949.045654296875\n",
      "Epoch 6944, Train_Loss: 3816.229248046875, Val_Loss: 3948.746826171875\n",
      "Epoch 6945, Train_Loss: 3816.184326171875, Val_Loss: 3948.6484375\n",
      "Epoch 6946, Train_Loss: 3816.2138671875, Val_Loss: 3948.781982421875\n",
      "Epoch 6947, Train_Loss: 3816.224609375, Val_Loss: 3949.02197265625\n",
      "Epoch 6948, Train_Loss: 3816.181396484375, Val_Loss: 3948.715576171875\n",
      "Epoch 6949, Train_Loss: 3816.128662109375, Val_Loss: 3948.573486328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6950, Train_Loss: 3816.150390625, Val_Loss: 3948.75\n",
      "Epoch 6951, Train_Loss: 3816.216064453125, Val_Loss: 3949.02490234375\n",
      "Epoch 6952, Train_Loss: 3816.07421875, Val_Loss: 3948.7099609375\n",
      "Epoch 6953, Train_Loss: 3816.104736328125, Val_Loss: 3948.56591796875\n",
      "Epoch 6954, Train_Loss: 3816.080322265625, Val_Loss: 3948.6591796875\n",
      "Epoch 6955, Train_Loss: 3816.166259765625, Val_Loss: 3948.945556640625\n",
      "Epoch 6956, Train_Loss: 3816.037841796875, Val_Loss: 3948.6259765625\n",
      "Epoch 6957, Train_Loss: 3816.05712890625, Val_Loss: 3948.480712890625\n",
      "Epoch 6958, Train_Loss: 3816.035888671875, Val_Loss: 3948.671630859375\n",
      "Epoch 6959, Train_Loss: 3816.122802734375, Val_Loss: 3948.96923828125\n",
      "Epoch 6960, Train_Loss: 3815.983154296875, Val_Loss: 3948.60595703125\n",
      "Epoch 6961, Train_Loss: 3815.9296875, Val_Loss: 3948.4755859375\n",
      "Epoch 6962, Train_Loss: 3815.918212890625, Val_Loss: 3948.6064453125\n",
      "Epoch 6963, Train_Loss: 3816.002197265625, Val_Loss: 3948.9140625\n",
      "Epoch 6964, Train_Loss: 3815.855224609375, Val_Loss: 3948.580078125\n",
      "Epoch 6965, Train_Loss: 3816.107421875, Val_Loss: 3948.466064453125\n",
      "Epoch 6966, Train_Loss: 3816.080810546875, Val_Loss: 3948.59521484375\n",
      "Epoch 6967, Train_Loss: 3816.189697265625, Val_Loss: 3948.886474609375\n",
      "Epoch 6968, Train_Loss: 3816.0224609375, Val_Loss: 3948.46923828125\n",
      "Epoch 6969, Train_Loss: 3816.06689453125, Val_Loss: 3948.349609375\n",
      "Epoch 6970, Train_Loss: 3816.00732421875, Val_Loss: 3948.489990234375\n",
      "Epoch 6971, Train_Loss: 3816.12744140625, Val_Loss: 3948.808837890625\n",
      "Epoch 6972, Train_Loss: 3815.978271484375, Val_Loss: 3948.496337890625\n",
      "Epoch 6973, Train_Loss: 3816.02197265625, Val_Loss: 3948.3466796875\n",
      "Epoch 6974, Train_Loss: 3815.948974609375, Val_Loss: 3948.501708984375\n",
      "Epoch 6975, Train_Loss: 3816.029296875, Val_Loss: 3948.791259765625\n",
      "Epoch 6976, Train_Loss: 3815.9326171875, Val_Loss: 3948.451904296875\n",
      "Epoch 6977, Train_Loss: 3815.977783203125, Val_Loss: 3948.335693359375\n",
      "Epoch 6978, Train_Loss: 3815.935546875, Val_Loss: 3948.4775390625\n",
      "Epoch 6979, Train_Loss: 3816.008056640625, Val_Loss: 3948.76513671875\n",
      "Epoch 6980, Train_Loss: 3815.94970703125, Val_Loss: 3948.381591796875\n",
      "Epoch 6981, Train_Loss: 3815.93798828125, Val_Loss: 3948.2880859375\n",
      "Epoch 6982, Train_Loss: 3815.881103515625, Val_Loss: 3948.366455078125\n",
      "Epoch 6983, Train_Loss: 3815.996337890625, Val_Loss: 3948.693603515625\n",
      "Epoch 6984, Train_Loss: 3815.886962890625, Val_Loss: 3948.333984375\n",
      "Epoch 6985, Train_Loss: 3816.042236328125, Val_Loss: 3948.16162109375\n",
      "Epoch 6986, Train_Loss: 3815.8427734375, Val_Loss: 3948.36962890625\n",
      "Epoch 6987, Train_Loss: 3815.927001953125, Val_Loss: 3948.676513671875\n",
      "Epoch 6988, Train_Loss: 3815.837158203125, Val_Loss: 3948.350341796875\n",
      "Epoch 6989, Train_Loss: 3815.662353515625, Val_Loss: 3948.193603515625\n",
      "Epoch 6990, Train_Loss: 3815.514404296875, Val_Loss: 3948.355224609375\n",
      "Epoch 6991, Train_Loss: 3815.59375, Val_Loss: 3948.572509765625\n",
      "Epoch 6992, Train_Loss: 3815.63232421875, Val_Loss: 3948.28369140625\n",
      "Epoch 6993, Train_Loss: 3815.661376953125, Val_Loss: 3948.202880859375\n",
      "Epoch 6994, Train_Loss: 3815.585205078125, Val_Loss: 3948.37353515625\n",
      "Epoch 6995, Train_Loss: 3815.5146484375, Val_Loss: 3948.315185546875\n",
      "Epoch 6996, Train_Loss: 3815.5419921875, Val_Loss: 3948.311279296875\n",
      "Epoch 6997, Train_Loss: 3815.6220703125, Val_Loss: 3948.129150390625\n",
      "Epoch 6998, Train_Loss: 3815.586181640625, Val_Loss: 3948.1728515625\n",
      "Epoch 6999, Train_Loss: 3815.529296875, Val_Loss: 3948.44970703125\n",
      "Epoch 7000, Train_Loss: 3815.558349609375, Val_Loss: 3948.209716796875\n",
      "Epoch 7001, Train_Loss: 3815.56103515625, Val_Loss: 3948.176513671875\n",
      "Epoch 7002, Train_Loss: 3815.545166015625, Val_Loss: 3948.30908203125\n",
      "Epoch 7003, Train_Loss: 3815.51318359375, Val_Loss: 3948.156005859375\n",
      "Epoch 7004, Train_Loss: 3815.49072265625, Val_Loss: 3948.189697265625\n",
      "Epoch 7005, Train_Loss: 3815.501708984375, Val_Loss: 3948.381591796875\n",
      "Epoch 7006, Train_Loss: 3815.5234375, Val_Loss: 3948.120849609375\n",
      "Epoch 7007, Train_Loss: 3815.517578125, Val_Loss: 3948.105224609375\n",
      "Epoch 7008, Train_Loss: 3815.519775390625, Val_Loss: 3948.253662109375\n",
      "Epoch 7009, Train_Loss: 3815.457275390625, Val_Loss: 3948.1630859375\n",
      "Epoch 7010, Train_Loss: 3815.44140625, Val_Loss: 3948.11474609375\n",
      "Epoch 7011, Train_Loss: 3815.48291015625, Val_Loss: 3948.01171875\n",
      "Epoch 7012, Train_Loss: 3815.432373046875, Val_Loss: 3948.13232421875\n",
      "Epoch 7013, Train_Loss: 3815.448974609375, Val_Loss: 3948.06396484375\n",
      "Epoch 7014, Train_Loss: 3815.41064453125, Val_Loss: 3948.147705078125\n",
      "Epoch 7015, Train_Loss: 3815.453125, Val_Loss: 3948.017578125\n",
      "Epoch 7016, Train_Loss: 3815.262451171875, Val_Loss: 3948.080322265625\n",
      "Epoch 7017, Train_Loss: 3815.267822265625, Val_Loss: 3948.03369140625\n",
      "Epoch 7018, Train_Loss: 3815.2841796875, Val_Loss: 3948.173583984375\n",
      "Epoch 7019, Train_Loss: 3815.271240234375, Val_Loss: 3947.962890625\n",
      "Epoch 7020, Train_Loss: 3815.2734375, Val_Loss: 3947.9716796875\n",
      "Epoch 7021, Train_Loss: 3815.318359375, Val_Loss: 3948.228759765625\n",
      "Epoch 7022, Train_Loss: 3815.265380859375, Val_Loss: 3947.9736328125\n",
      "Epoch 7023, Train_Loss: 3815.2001953125, Val_Loss: 3947.86962890625\n",
      "Epoch 7024, Train_Loss: 3815.187744140625, Val_Loss: 3948.0703125\n",
      "Epoch 7025, Train_Loss: 3815.134033203125, Val_Loss: 3947.94287109375\n",
      "Epoch 7026, Train_Loss: 3815.12744140625, Val_Loss: 3947.954345703125\n",
      "Epoch 7027, Train_Loss: 3815.1796875, Val_Loss: 3948.129638671875\n",
      "Epoch 7028, Train_Loss: 3815.18115234375, Val_Loss: 3947.888427734375\n",
      "Epoch 7029, Train_Loss: 3815.168701171875, Val_Loss: 3947.895263671875\n",
      "Epoch 7030, Train_Loss: 3815.134765625, Val_Loss: 3948.056884765625\n",
      "Epoch 7031, Train_Loss: 3815.077880859375, Val_Loss: 3947.91845703125\n",
      "Epoch 7032, Train_Loss: 3815.0771484375, Val_Loss: 3947.931884765625\n",
      "Epoch 7033, Train_Loss: 3815.11474609375, Val_Loss: 3948.083984375\n",
      "Epoch 7034, Train_Loss: 3815.2294921875, Val_Loss: 3947.83642578125\n",
      "Epoch 7035, Train_Loss: 3815.2275390625, Val_Loss: 3947.81689453125\n",
      "Epoch 7036, Train_Loss: 3815.076904296875, Val_Loss: 3948.03271484375\n",
      "Epoch 7037, Train_Loss: 3815.146484375, Val_Loss: 3947.8232421875\n",
      "Epoch 7038, Train_Loss: 3815.0283203125, Val_Loss: 3947.839111328125\n",
      "Epoch 7039, Train_Loss: 3815.1328125, Val_Loss: 3947.806884765625\n",
      "Epoch 7040, Train_Loss: 3815.055419921875, Val_Loss: 3947.912353515625\n",
      "Epoch 7041, Train_Loss: 3815.16552734375, Val_Loss: 3947.756103515625\n",
      "Epoch 7042, Train_Loss: 3815.154296875, Val_Loss: 3947.802001953125\n",
      "Epoch 7043, Train_Loss: 3815.056884765625, Val_Loss: 3948.021484375\n",
      "Epoch 7044, Train_Loss: 3815.085693359375, Val_Loss: 3947.764892578125\n",
      "Epoch 7045, Train_Loss: 3815.07470703125, Val_Loss: 3947.73193359375\n",
      "Epoch 7046, Train_Loss: 3815.094970703125, Val_Loss: 3947.87646484375\n",
      "Epoch 7047, Train_Loss: 3815.13330078125, Val_Loss: 3947.762451171875\n",
      "Epoch 7048, Train_Loss: 3815.097412109375, Val_Loss: 3947.782470703125\n",
      "Epoch 7049, Train_Loss: 3815.109375, Val_Loss: 3947.69970703125\n",
      "Epoch 7050, Train_Loss: 3815.0615234375, Val_Loss: 3947.80029296875\n",
      "Epoch 7051, Train_Loss: 3815.017578125, Val_Loss: 3947.62109375\n",
      "Epoch 7052, Train_Loss: 3814.963623046875, Val_Loss: 3947.721923828125\n",
      "Epoch 7053, Train_Loss: 3815.010009765625, Val_Loss: 3947.597900390625\n",
      "Epoch 7054, Train_Loss: 3814.96240234375, Val_Loss: 3947.720458984375\n",
      "Epoch 7055, Train_Loss: 3814.97802734375, Val_Loss: 3947.64599609375\n",
      "Epoch 7056, Train_Loss: 3814.933349609375, Val_Loss: 3947.770751953125\n",
      "Epoch 7057, Train_Loss: 3814.9931640625, Val_Loss: 3947.593994140625\n",
      "Epoch 7058, Train_Loss: 3814.928466796875, Val_Loss: 3947.63720703125\n",
      "Epoch 7059, Train_Loss: 3815.029541015625, Val_Loss: 3947.884033203125\n",
      "Epoch 7060, Train_Loss: 3814.945556640625, Val_Loss: 3947.571533203125\n",
      "Epoch 7061, Train_Loss: 3814.911376953125, Val_Loss: 3947.5126953125\n",
      "Epoch 7062, Train_Loss: 3814.87548828125, Val_Loss: 3947.757080078125\n",
      "Epoch 7063, Train_Loss: 3814.87060546875, Val_Loss: 3947.6630859375\n",
      "Epoch 7064, Train_Loss: 3814.83154296875, Val_Loss: 3947.330810546875\n",
      "Epoch 7065, Train_Loss: 3814.900390625, Val_Loss: 3947.48193359375\n",
      "Epoch 7066, Train_Loss: 3814.916748046875, Val_Loss: 3947.885498046875\n",
      "Epoch 7067, Train_Loss: 3814.846923828125, Val_Loss: 3947.510498046875\n",
      "Epoch 7068, Train_Loss: 3814.802001953125, Val_Loss: 3947.423583984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7069, Train_Loss: 3814.774169921875, Val_Loss: 3947.60595703125\n",
      "Epoch 7070, Train_Loss: 3814.788330078125, Val_Loss: 3947.558349609375\n",
      "Epoch 7071, Train_Loss: 3814.702392578125, Val_Loss: 3947.648681640625\n",
      "Epoch 7072, Train_Loss: 3814.763427734375, Val_Loss: 3947.44921875\n",
      "Epoch 7073, Train_Loss: 3814.70947265625, Val_Loss: 3947.478515625\n",
      "Epoch 7074, Train_Loss: 3814.75048828125, Val_Loss: 3947.704345703125\n",
      "Epoch 7075, Train_Loss: 3814.88037109375, Val_Loss: 3947.452880859375\n",
      "Epoch 7076, Train_Loss: 3814.9345703125, Val_Loss: 3947.428466796875\n",
      "Epoch 7077, Train_Loss: 3814.83642578125, Val_Loss: 3947.644287109375\n",
      "Epoch 7078, Train_Loss: 3814.84375, Val_Loss: 3947.41162109375\n",
      "Epoch 7079, Train_Loss: 3814.82177734375, Val_Loss: 3947.41748046875\n",
      "Epoch 7080, Train_Loss: 3814.829345703125, Val_Loss: 3947.401611328125\n",
      "Epoch 7081, Train_Loss: 3814.782958984375, Val_Loss: 3947.528076171875\n",
      "Epoch 7082, Train_Loss: 3814.839111328125, Val_Loss: 3947.38720703125\n",
      "Epoch 7083, Train_Loss: 3814.793212890625, Val_Loss: 3947.4208984375\n",
      "Epoch 7084, Train_Loss: 3814.822265625, Val_Loss: 3947.62646484375\n",
      "Epoch 7085, Train_Loss: 3814.805419921875, Val_Loss: 3947.34033203125\n",
      "Epoch 7086, Train_Loss: 3814.81494140625, Val_Loss: 3947.341552734375\n",
      "Epoch 7087, Train_Loss: 3814.739990234375, Val_Loss: 3947.502685546875\n",
      "Epoch 7088, Train_Loss: 3814.767578125, Val_Loss: 3947.350341796875\n",
      "Epoch 7089, Train_Loss: 3814.7646484375, Val_Loss: 3947.34765625\n",
      "Epoch 7090, Train_Loss: 3814.783203125, Val_Loss: 3947.518798828125\n",
      "Epoch 7091, Train_Loss: 3814.8076171875, Val_Loss: 3947.28076171875\n",
      "Epoch 7092, Train_Loss: 3814.780517578125, Val_Loss: 3947.222900390625\n",
      "Epoch 7093, Train_Loss: 3814.693359375, Val_Loss: 3947.43994140625\n",
      "Epoch 7094, Train_Loss: 3814.703857421875, Val_Loss: 3947.25390625\n",
      "Epoch 7095, Train_Loss: 3814.705078125, Val_Loss: 3947.280517578125\n",
      "Epoch 7096, Train_Loss: 3814.7041015625, Val_Loss: 3947.454345703125\n",
      "Epoch 7097, Train_Loss: 3814.743896484375, Val_Loss: 3947.206787109375\n",
      "Epoch 7098, Train_Loss: 3814.727783203125, Val_Loss: 3947.22607421875\n",
      "Epoch 7099, Train_Loss: 3814.37890625, Val_Loss: 3947.449951171875\n",
      "Epoch 7100, Train_Loss: 3814.421630859375, Val_Loss: 3947.219970703125\n",
      "Epoch 7101, Train_Loss: 3814.35693359375, Val_Loss: 3947.226806640625\n",
      "Epoch 7102, Train_Loss: 3814.3779296875, Val_Loss: 3947.395263671875\n",
      "Epoch 7103, Train_Loss: 3814.37646484375, Val_Loss: 3947.18115234375\n",
      "Epoch 7104, Train_Loss: 3814.37158203125, Val_Loss: 3947.209228515625\n",
      "Epoch 7105, Train_Loss: 3814.3525390625, Val_Loss: 3947.3603515625\n",
      "Epoch 7106, Train_Loss: 3814.3486328125, Val_Loss: 3947.106689453125\n",
      "Epoch 7107, Train_Loss: 3814.341796875, Val_Loss: 3947.094482421875\n",
      "Epoch 7108, Train_Loss: 3814.32373046875, Val_Loss: 3947.284423828125\n",
      "Epoch 7109, Train_Loss: 3814.327880859375, Val_Loss: 3947.13916015625\n",
      "Epoch 7110, Train_Loss: 3814.316650390625, Val_Loss: 3947.139892578125\n",
      "Epoch 7111, Train_Loss: 3814.3046875, Val_Loss: 3947.287109375\n",
      "Epoch 7112, Train_Loss: 3814.251953125, Val_Loss: 3947.05322265625\n",
      "Epoch 7113, Train_Loss: 3814.27490234375, Val_Loss: 3947.093505859375\n",
      "Epoch 7114, Train_Loss: 3814.260009765625, Val_Loss: 3947.26513671875\n",
      "Epoch 7115, Train_Loss: 3814.26123046875, Val_Loss: 3947.073486328125\n",
      "Epoch 7116, Train_Loss: 3814.27392578125, Val_Loss: 3947.0712890625\n",
      "Epoch 7117, Train_Loss: 3814.2412109375, Val_Loss: 3947.242431640625\n",
      "Epoch 7118, Train_Loss: 3814.251220703125, Val_Loss: 3947.040771484375\n",
      "Epoch 7119, Train_Loss: 3814.22509765625, Val_Loss: 3947.001953125\n",
      "Epoch 7120, Train_Loss: 3814.22216796875, Val_Loss: 3947.17236328125\n",
      "Epoch 7121, Train_Loss: 3814.212646484375, Val_Loss: 3946.963623046875\n",
      "Epoch 7122, Train_Loss: 3814.212158203125, Val_Loss: 3946.986083984375\n",
      "Epoch 7123, Train_Loss: 3814.197021484375, Val_Loss: 3947.17724609375\n",
      "Epoch 7124, Train_Loss: 3814.19921875, Val_Loss: 3946.9892578125\n",
      "Epoch 7125, Train_Loss: 3814.17431640625, Val_Loss: 3947.01171875\n",
      "Epoch 7126, Train_Loss: 3814.039794921875, Val_Loss: 3947.19091796875\n",
      "Epoch 7127, Train_Loss: 3813.9609375, Val_Loss: 3946.955078125\n",
      "Epoch 7128, Train_Loss: 3813.95751953125, Val_Loss: 3946.946044921875\n",
      "Epoch 7129, Train_Loss: 3814.014404296875, Val_Loss: 3947.115478515625\n",
      "Epoch 7130, Train_Loss: 3814.044189453125, Val_Loss: 3946.9443359375\n",
      "Epoch 7131, Train_Loss: 3814.015869140625, Val_Loss: 3946.96484375\n",
      "Epoch 7132, Train_Loss: 3814.0302734375, Val_Loss: 3947.154296875\n",
      "Epoch 7133, Train_Loss: 3813.98291015625, Val_Loss: 3946.810791015625\n",
      "Epoch 7134, Train_Loss: 3813.979736328125, Val_Loss: 3946.813232421875\n",
      "Epoch 7135, Train_Loss: 3813.995849609375, Val_Loss: 3947.03759765625\n",
      "Epoch 7136, Train_Loss: 3813.964599609375, Val_Loss: 3946.881591796875\n",
      "Epoch 7137, Train_Loss: 3813.99755859375, Val_Loss: 3946.877197265625\n",
      "Epoch 7138, Train_Loss: 3813.96630859375, Val_Loss: 3947.053955078125\n",
      "Epoch 7139, Train_Loss: 3813.886474609375, Val_Loss: 3946.784912109375\n",
      "Epoch 7140, Train_Loss: 3813.877685546875, Val_Loss: 3946.79638671875\n",
      "Epoch 7141, Train_Loss: 3813.8759765625, Val_Loss: 3947.034423828125\n",
      "Epoch 7142, Train_Loss: 3813.89599609375, Val_Loss: 3946.83642578125\n",
      "Epoch 7143, Train_Loss: 3813.903076171875, Val_Loss: 3946.823974609375\n",
      "Epoch 7144, Train_Loss: 3813.8857421875, Val_Loss: 3946.987548828125\n",
      "Epoch 7145, Train_Loss: 3813.85986328125, Val_Loss: 3946.74365234375\n",
      "Epoch 7146, Train_Loss: 3813.82568359375, Val_Loss: 3946.70166015625\n",
      "Epoch 7147, Train_Loss: 3813.83447265625, Val_Loss: 3946.919189453125\n",
      "Epoch 7148, Train_Loss: 3813.810791015625, Val_Loss: 3946.711181640625\n",
      "Epoch 7149, Train_Loss: 3813.845458984375, Val_Loss: 3946.763916015625\n",
      "Epoch 7150, Train_Loss: 3813.8515625, Val_Loss: 3946.931640625\n",
      "Epoch 7151, Train_Loss: 3813.8125, Val_Loss: 3946.685302734375\n",
      "Epoch 7152, Train_Loss: 3813.832763671875, Val_Loss: 3946.688720703125\n",
      "Epoch 7153, Train_Loss: 3813.76220703125, Val_Loss: 3946.899658203125\n",
      "Epoch 7154, Train_Loss: 3813.739501953125, Val_Loss: 3946.686767578125\n",
      "Epoch 7155, Train_Loss: 3813.734375, Val_Loss: 3946.7216796875\n",
      "Epoch 7156, Train_Loss: 3813.782958984375, Val_Loss: 3946.89404296875\n",
      "Epoch 7157, Train_Loss: 3813.7666015625, Val_Loss: 3946.66552734375\n",
      "Epoch 7158, Train_Loss: 3813.765380859375, Val_Loss: 3946.649658203125\n",
      "Epoch 7159, Train_Loss: 3813.79052734375, Val_Loss: 3946.83447265625\n",
      "Epoch 7160, Train_Loss: 3813.64697265625, Val_Loss: 3946.585205078125\n",
      "Epoch 7161, Train_Loss: 3813.648193359375, Val_Loss: 3946.629150390625\n",
      "Epoch 7162, Train_Loss: 3813.707763671875, Val_Loss: 3946.8046875\n",
      "Epoch 7163, Train_Loss: 3813.650634765625, Val_Loss: 3946.60009765625\n",
      "Epoch 7164, Train_Loss: 3813.6494140625, Val_Loss: 3946.5791015625\n",
      "Epoch 7165, Train_Loss: 3813.673828125, Val_Loss: 3946.763916015625\n",
      "Epoch 7166, Train_Loss: 3813.60888671875, Val_Loss: 3946.5625\n",
      "Epoch 7167, Train_Loss: 3813.636474609375, Val_Loss: 3946.5966796875\n",
      "Epoch 7168, Train_Loss: 3813.65576171875, Val_Loss: 3946.794921875\n",
      "Epoch 7169, Train_Loss: 3813.650390625, Val_Loss: 3946.5380859375\n",
      "Epoch 7170, Train_Loss: 3813.64599609375, Val_Loss: 3946.50244140625\n",
      "Epoch 7171, Train_Loss: 3813.632080078125, Val_Loss: 3946.716064453125\n",
      "Epoch 7172, Train_Loss: 3813.633544921875, Val_Loss: 3946.50634765625\n",
      "Epoch 7173, Train_Loss: 3813.5966796875, Val_Loss: 3946.465087890625\n",
      "Epoch 7174, Train_Loss: 3813.60546875, Val_Loss: 3946.672119140625\n",
      "Epoch 7175, Train_Loss: 3813.60400390625, Val_Loss: 3946.43310546875\n",
      "Epoch 7176, Train_Loss: 3813.586669921875, Val_Loss: 3946.48486328125\n",
      "Epoch 7177, Train_Loss: 3813.47216796875, Val_Loss: 3946.68408203125\n",
      "Epoch 7178, Train_Loss: 3813.585205078125, Val_Loss: 3946.45361328125\n",
      "Epoch 7179, Train_Loss: 3813.576171875, Val_Loss: 3946.4560546875\n",
      "Epoch 7180, Train_Loss: 3813.37841796875, Val_Loss: 3946.63720703125\n",
      "Epoch 7181, Train_Loss: 3813.3515625, Val_Loss: 3946.421142578125\n",
      "Epoch 7182, Train_Loss: 3813.35546875, Val_Loss: 3946.419189453125\n",
      "Epoch 7183, Train_Loss: 3813.51318359375, Val_Loss: 3946.63720703125\n",
      "Epoch 7184, Train_Loss: 3813.509765625, Val_Loss: 3946.40869140625\n",
      "Epoch 7185, Train_Loss: 3813.572509765625, Val_Loss: 3946.392822265625\n",
      "Epoch 7186, Train_Loss: 3813.573486328125, Val_Loss: 3946.494384765625\n",
      "Epoch 7187, Train_Loss: 3813.439208984375, Val_Loss: 3946.253662109375\n",
      "Epoch 7188, Train_Loss: 3813.36083984375, Val_Loss: 3946.2763671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7189, Train_Loss: 3813.439208984375, Val_Loss: 3946.51513671875\n",
      "Epoch 7190, Train_Loss: 3813.42724609375, Val_Loss: 3946.308837890625\n",
      "Epoch 7191, Train_Loss: 3813.416015625, Val_Loss: 3946.3037109375\n",
      "Epoch 7192, Train_Loss: 3813.487548828125, Val_Loss: 3946.476806640625\n",
      "Epoch 7193, Train_Loss: 3813.37646484375, Val_Loss: 3946.22509765625\n",
      "Epoch 7194, Train_Loss: 3813.336181640625, Val_Loss: 3946.240478515625\n",
      "Epoch 7195, Train_Loss: 3813.364990234375, Val_Loss: 3946.46484375\n",
      "Epoch 7196, Train_Loss: 3813.35595703125, Val_Loss: 3946.25048828125\n",
      "Epoch 7197, Train_Loss: 3813.348388671875, Val_Loss: 3946.250732421875\n",
      "Epoch 7198, Train_Loss: 3813.377685546875, Val_Loss: 3946.434814453125\n",
      "Epoch 7199, Train_Loss: 3813.458251953125, Val_Loss: 3946.199951171875\n",
      "Epoch 7200, Train_Loss: 3813.302490234375, Val_Loss: 3946.18408203125\n",
      "Epoch 7201, Train_Loss: 3813.35693359375, Val_Loss: 3946.387939453125\n",
      "Epoch 7202, Train_Loss: 3813.3291015625, Val_Loss: 3946.13671875\n",
      "Epoch 7203, Train_Loss: 3813.3291015625, Val_Loss: 3946.173095703125\n",
      "Epoch 7204, Train_Loss: 3813.2939453125, Val_Loss: 3946.36962890625\n",
      "Epoch 7205, Train_Loss: 3813.314453125, Val_Loss: 3946.12109375\n",
      "Epoch 7206, Train_Loss: 3812.908935546875, Val_Loss: 3946.1455078125\n",
      "Epoch 7207, Train_Loss: 3812.991943359375, Val_Loss: 3946.3369140625\n",
      "Epoch 7208, Train_Loss: 3812.941650390625, Val_Loss: 3946.113525390625\n",
      "Epoch 7209, Train_Loss: 3812.93896484375, Val_Loss: 3946.111083984375\n",
      "Epoch 7210, Train_Loss: 3812.993896484375, Val_Loss: 3946.325927734375\n",
      "Epoch 7211, Train_Loss: 3813.015625, Val_Loss: 3946.08447265625\n",
      "Epoch 7212, Train_Loss: 3812.988525390625, Val_Loss: 3946.114501953125\n",
      "Epoch 7213, Train_Loss: 3813.0009765625, Val_Loss: 3946.257568359375\n",
      "Epoch 7214, Train_Loss: 3812.964111328125, Val_Loss: 3946.028076171875\n",
      "Epoch 7215, Train_Loss: 3812.963623046875, Val_Loss: 3946.016845703125\n",
      "Epoch 7216, Train_Loss: 3812.933837890625, Val_Loss: 3946.26708984375\n",
      "Epoch 7217, Train_Loss: 3812.887939453125, Val_Loss: 3946.00927734375\n",
      "Epoch 7218, Train_Loss: 3812.863525390625, Val_Loss: 3946.03076171875\n",
      "Epoch 7219, Train_Loss: 3812.904541015625, Val_Loss: 3946.218505859375\n",
      "Epoch 7220, Train_Loss: 3812.894775390625, Val_Loss: 3945.994384765625\n",
      "Epoch 7221, Train_Loss: 3812.877685546875, Val_Loss: 3946.010009765625\n",
      "Epoch 7222, Train_Loss: 3812.844482421875, Val_Loss: 3946.2275390625\n",
      "Epoch 7223, Train_Loss: 3812.89794921875, Val_Loss: 3945.954345703125\n",
      "Epoch 7224, Train_Loss: 3812.88916015625, Val_Loss: 3945.953125\n",
      "Epoch 7225, Train_Loss: 3812.826171875, Val_Loss: 3946.206298828125\n",
      "Epoch 7226, Train_Loss: 3812.852783203125, Val_Loss: 3945.90673828125\n",
      "Epoch 7227, Train_Loss: 3812.817626953125, Val_Loss: 3945.91162109375\n",
      "Epoch 7228, Train_Loss: 3812.917236328125, Val_Loss: 3946.110107421875\n",
      "Epoch 7229, Train_Loss: 3812.84228515625, Val_Loss: 3945.868408203125\n",
      "Epoch 7230, Train_Loss: 3812.826904296875, Val_Loss: 3945.91650390625\n",
      "Epoch 7231, Train_Loss: 3812.906005859375, Val_Loss: 3946.117919921875\n",
      "Epoch 7232, Train_Loss: 3812.831298828125, Val_Loss: 3945.897705078125\n",
      "Epoch 7233, Train_Loss: 3812.671630859375, Val_Loss: 3945.895263671875\n",
      "Epoch 7234, Train_Loss: 3812.7255859375, Val_Loss: 3946.0947265625\n",
      "Epoch 7235, Train_Loss: 3812.670166015625, Val_Loss: 3945.845703125\n",
      "Epoch 7236, Train_Loss: 3812.646484375, Val_Loss: 3945.876708984375\n",
      "Epoch 7237, Train_Loss: 3812.728759765625, Val_Loss: 3946.0751953125\n",
      "Epoch 7238, Train_Loss: 3812.6298828125, Val_Loss: 3945.81884765625\n",
      "Epoch 7239, Train_Loss: 3812.6201171875, Val_Loss: 3945.83203125\n",
      "Epoch 7240, Train_Loss: 3812.662109375, Val_Loss: 3945.997314453125\n",
      "Epoch 7241, Train_Loss: 3812.571044921875, Val_Loss: 3945.76708984375\n",
      "Epoch 7242, Train_Loss: 3812.66943359375, Val_Loss: 3945.76806640625\n",
      "Epoch 7243, Train_Loss: 3812.62841796875, Val_Loss: 3946.030029296875\n",
      "Epoch 7244, Train_Loss: 3812.671875, Val_Loss: 3945.763916015625\n",
      "Epoch 7245, Train_Loss: 3812.646484375, Val_Loss: 3945.77392578125\n",
      "Epoch 7246, Train_Loss: 3812.693603515625, Val_Loss: 3945.9873046875\n",
      "Epoch 7247, Train_Loss: 3812.621826171875, Val_Loss: 3945.73681640625\n",
      "Epoch 7248, Train_Loss: 3812.61279296875, Val_Loss: 3945.747314453125\n",
      "Epoch 7249, Train_Loss: 3812.567138671875, Val_Loss: 3945.99755859375\n",
      "Epoch 7250, Train_Loss: 3812.503662109375, Val_Loss: 3945.7119140625\n",
      "Epoch 7251, Train_Loss: 3812.491943359375, Val_Loss: 3945.728759765625\n",
      "Epoch 7252, Train_Loss: 3812.578857421875, Val_Loss: 3945.943115234375\n",
      "Epoch 7253, Train_Loss: 3812.462158203125, Val_Loss: 3945.654052734375\n",
      "Epoch 7254, Train_Loss: 3812.567138671875, Val_Loss: 3945.65771484375\n",
      "Epoch 7255, Train_Loss: 3812.635986328125, Val_Loss: 3945.8876953125\n",
      "Epoch 7256, Train_Loss: 3812.4970703125, Val_Loss: 3945.6875\n",
      "Epoch 7257, Train_Loss: 3812.489990234375, Val_Loss: 3945.69091796875\n",
      "Epoch 7258, Train_Loss: 3812.6171875, Val_Loss: 3945.91162109375\n",
      "Epoch 7259, Train_Loss: 3812.56884765625, Val_Loss: 3945.647705078125\n",
      "Epoch 7260, Train_Loss: 3812.471435546875, Val_Loss: 3945.654296875\n",
      "Epoch 7261, Train_Loss: 3812.451171875, Val_Loss: 3945.879638671875\n",
      "Epoch 7262, Train_Loss: 3812.38525390625, Val_Loss: 3945.634765625\n",
      "Epoch 7263, Train_Loss: 3812.390869140625, Val_Loss: 3945.63720703125\n",
      "Epoch 7264, Train_Loss: 3812.476806640625, Val_Loss: 3945.83203125\n",
      "Epoch 7265, Train_Loss: 3812.515380859375, Val_Loss: 3945.615966796875\n",
      "Epoch 7266, Train_Loss: 3812.404052734375, Val_Loss: 3945.568115234375\n",
      "Epoch 7267, Train_Loss: 3812.355224609375, Val_Loss: 3945.783935546875\n",
      "Epoch 7268, Train_Loss: 3812.410400390625, Val_Loss: 3945.52197265625\n",
      "Epoch 7269, Train_Loss: 3812.410888671875, Val_Loss: 3945.513671875\n",
      "Epoch 7270, Train_Loss: 3812.36376953125, Val_Loss: 3945.75927734375\n",
      "Epoch 7271, Train_Loss: 3812.381591796875, Val_Loss: 3945.53759765625\n",
      "Epoch 7272, Train_Loss: 3812.392333984375, Val_Loss: 3945.562744140625\n",
      "Epoch 7273, Train_Loss: 3812.319091796875, Val_Loss: 3945.7568359375\n",
      "Epoch 7274, Train_Loss: 3812.356201171875, Val_Loss: 3945.501953125\n",
      "Epoch 7275, Train_Loss: 3812.3486328125, Val_Loss: 3945.516845703125\n",
      "Epoch 7276, Train_Loss: 3812.306396484375, Val_Loss: 3945.716064453125\n",
      "Epoch 7277, Train_Loss: 3812.338134765625, Val_Loss: 3945.47998046875\n",
      "Epoch 7278, Train_Loss: 3812.336669921875, Val_Loss: 3945.498291015625\n",
      "Epoch 7279, Train_Loss: 3812.414794921875, Val_Loss: 3945.70751953125\n",
      "Epoch 7280, Train_Loss: 3812.30126953125, Val_Loss: 3945.409912109375\n",
      "Epoch 7281, Train_Loss: 3812.292724609375, Val_Loss: 3945.4033203125\n",
      "Epoch 7282, Train_Loss: 3812.369140625, Val_Loss: 3945.62646484375\n",
      "Epoch 7283, Train_Loss: 3812.27587890625, Val_Loss: 3945.4130859375\n",
      "Epoch 7284, Train_Loss: 3812.265625, Val_Loss: 3945.44482421875\n",
      "Epoch 7285, Train_Loss: 3812.232177734375, Val_Loss: 3945.66162109375\n",
      "Epoch 7286, Train_Loss: 3812.1630859375, Val_Loss: 3945.403564453125\n",
      "Epoch 7287, Train_Loss: 3812.160400390625, Val_Loss: 3945.388427734375\n",
      "Epoch 7288, Train_Loss: 3812.1806640625, Val_Loss: 3945.61669921875\n",
      "Epoch 7289, Train_Loss: 3812.141845703125, Val_Loss: 3945.3779296875\n",
      "Epoch 7290, Train_Loss: 3812.35888671875, Val_Loss: 3945.364013671875\n",
      "Epoch 7291, Train_Loss: 3812.415283203125, Val_Loss: 3945.60205078125\n",
      "Epoch 7292, Train_Loss: 3812.304443359375, Val_Loss: 3945.370849609375\n",
      "Epoch 7293, Train_Loss: 3812.257568359375, Val_Loss: 3945.301513671875\n",
      "Epoch 7294, Train_Loss: 3812.313720703125, Val_Loss: 3945.51953125\n",
      "Epoch 7295, Train_Loss: 3812.24169921875, Val_Loss: 3945.2958984375\n",
      "Epoch 7296, Train_Loss: 3812.295166015625, Val_Loss: 3945.29931640625\n",
      "Epoch 7297, Train_Loss: 3812.298095703125, Val_Loss: 3945.53955078125\n",
      "Epoch 7298, Train_Loss: 3812.233642578125, Val_Loss: 3945.29833984375\n",
      "Epoch 7299, Train_Loss: 3812.21875, Val_Loss: 3945.295166015625\n",
      "Epoch 7300, Train_Loss: 3812.249755859375, Val_Loss: 3945.496826171875\n",
      "Epoch 7301, Train_Loss: 3812.190185546875, Val_Loss: 3945.284912109375\n",
      "Epoch 7302, Train_Loss: 3812.185546875, Val_Loss: 3945.288818359375\n",
      "Epoch 7303, Train_Loss: 3812.29736328125, Val_Loss: 3945.453125\n",
      "Epoch 7304, Train_Loss: 3812.22265625, Val_Loss: 3945.215576171875\n",
      "Epoch 7305, Train_Loss: 3812.197509765625, Val_Loss: 3945.242431640625\n",
      "Epoch 7306, Train_Loss: 3812.230712890625, Val_Loss: 3945.4296875\n",
      "Epoch 7307, Train_Loss: 3812.18017578125, Val_Loss: 3945.149658203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7308, Train_Loss: 3812.179931640625, Val_Loss: 3945.147705078125\n",
      "Epoch 7309, Train_Loss: 3812.26025390625, Val_Loss: 3945.377197265625\n",
      "Epoch 7310, Train_Loss: 3812.166748046875, Val_Loss: 3945.19091796875\n",
      "Epoch 7311, Train_Loss: 3812.178466796875, Val_Loss: 3945.202392578125\n",
      "Epoch 7312, Train_Loss: 3812.151611328125, Val_Loss: 3945.21728515625\n",
      "Epoch 7313, Train_Loss: 3811.845458984375, Val_Loss: 3945.23388671875\n",
      "Epoch 7314, Train_Loss: 3811.852294921875, Val_Loss: 3945.08447265625\n",
      "Epoch 7315, Train_Loss: 3811.83203125, Val_Loss: 3945.211181640625\n",
      "Epoch 7316, Train_Loss: 3811.87744140625, Val_Loss: 3945.134033203125\n",
      "Epoch 7317, Train_Loss: 3811.87158203125, Val_Loss: 3945.271240234375\n",
      "Epoch 7318, Train_Loss: 3811.911376953125, Val_Loss: 3945.08837890625\n",
      "Epoch 7319, Train_Loss: 3811.864013671875, Val_Loss: 3945.171142578125\n",
      "Epoch 7320, Train_Loss: 3811.8740234375, Val_Loss: 3944.998291015625\n",
      "Epoch 7321, Train_Loss: 3811.814453125, Val_Loss: 3945.194091796875\n",
      "Epoch 7322, Train_Loss: 3811.786865234375, Val_Loss: 3945.048095703125\n",
      "Epoch 7323, Train_Loss: 3811.7958984375, Val_Loss: 3945.15478515625\n",
      "Epoch 7324, Train_Loss: 3811.868896484375, Val_Loss: 3945.014892578125\n",
      "Epoch 7325, Train_Loss: 3811.788818359375, Val_Loss: 3945.146484375\n",
      "Epoch 7326, Train_Loss: 3811.81494140625, Val_Loss: 3945.0283203125\n",
      "Epoch 7327, Train_Loss: 3811.775390625, Val_Loss: 3945.17236328125\n",
      "Epoch 7328, Train_Loss: 3811.78955078125, Val_Loss: 3944.993896484375\n",
      "Epoch 7329, Train_Loss: 3811.74462890625, Val_Loss: 3945.089111328125\n",
      "Epoch 7330, Train_Loss: 3811.799560546875, Val_Loss: 3944.97509765625\n",
      "Epoch 7331, Train_Loss: 3811.760986328125, Val_Loss: 3945.125244140625\n",
      "Epoch 7332, Train_Loss: 3811.78125, Val_Loss: 3944.967529296875\n",
      "Epoch 7333, Train_Loss: 3811.700439453125, Val_Loss: 3945.01318359375\n",
      "Epoch 7334, Train_Loss: 3811.740234375, Val_Loss: 3944.910400390625\n",
      "Epoch 7335, Train_Loss: 3811.631591796875, Val_Loss: 3945.04248046875\n",
      "Epoch 7336, Train_Loss: 3811.67578125, Val_Loss: 3944.88330078125\n",
      "Epoch 7337, Train_Loss: 3811.64404296875, Val_Loss: 3945.060302734375\n",
      "Epoch 7338, Train_Loss: 3811.664794921875, Val_Loss: 3944.912841796875\n",
      "Epoch 7339, Train_Loss: 3811.606201171875, Val_Loss: 3945.030517578125\n",
      "Epoch 7340, Train_Loss: 3811.5400390625, Val_Loss: 3944.9033203125\n",
      "Epoch 7341, Train_Loss: 3811.528564453125, Val_Loss: 3945.01123046875\n",
      "Epoch 7342, Train_Loss: 3811.503173828125, Val_Loss: 3944.8779296875\n",
      "Epoch 7343, Train_Loss: 3811.43798828125, Val_Loss: 3945.00634765625\n",
      "Epoch 7344, Train_Loss: 3811.4794921875, Val_Loss: 3944.856689453125\n",
      "Epoch 7345, Train_Loss: 3811.439208984375, Val_Loss: 3945.0087890625\n",
      "Epoch 7346, Train_Loss: 3811.466552734375, Val_Loss: 3944.856689453125\n",
      "Epoch 7347, Train_Loss: 3811.3798828125, Val_Loss: 3944.90283203125\n",
      "Epoch 7348, Train_Loss: 3811.426025390625, Val_Loss: 3944.80126953125\n",
      "Epoch 7349, Train_Loss: 3811.37841796875, Val_Loss: 3944.91796875\n",
      "Epoch 7350, Train_Loss: 3811.42333984375, Val_Loss: 3944.810302734375\n",
      "Epoch 7351, Train_Loss: 3811.379638671875, Val_Loss: 3944.919677734375\n",
      "Epoch 7352, Train_Loss: 3811.40087890625, Val_Loss: 3944.811279296875\n",
      "Epoch 7353, Train_Loss: 3811.34423828125, Val_Loss: 3944.959716796875\n",
      "Epoch 7354, Train_Loss: 3811.375244140625, Val_Loss: 3944.764404296875\n",
      "Epoch 7355, Train_Loss: 3811.30224609375, Val_Loss: 3944.87109375\n",
      "Epoch 7356, Train_Loss: 3811.353759765625, Val_Loss: 3944.75390625\n",
      "Epoch 7357, Train_Loss: 3811.306396484375, Val_Loss: 3944.888427734375\n",
      "Epoch 7358, Train_Loss: 3811.390380859375, Val_Loss: 3944.764892578125\n",
      "Epoch 7359, Train_Loss: 3811.2998046875, Val_Loss: 3944.876708984375\n",
      "Epoch 7360, Train_Loss: 3811.263671875, Val_Loss: 3944.685302734375\n",
      "Epoch 7361, Train_Loss: 3811.26220703125, Val_Loss: 3944.8125\n",
      "Epoch 7362, Train_Loss: 3811.294189453125, Val_Loss: 3944.684326171875\n",
      "Epoch 7363, Train_Loss: 3811.24560546875, Val_Loss: 3944.830078125\n",
      "Epoch 7364, Train_Loss: 3811.257568359375, Val_Loss: 3944.689697265625\n",
      "Epoch 7365, Train_Loss: 3811.244873046875, Val_Loss: 3944.7958984375\n",
      "Epoch 7366, Train_Loss: 3811.331787109375, Val_Loss: 3944.6796875\n",
      "Epoch 7367, Train_Loss: 3811.195556640625, Val_Loss: 3944.826416015625\n",
      "Epoch 7368, Train_Loss: 3811.24609375, Val_Loss: 3944.65673828125\n",
      "Epoch 7369, Train_Loss: 3811.196044921875, Val_Loss: 3944.76708984375\n",
      "Epoch 7370, Train_Loss: 3811.249267578125, Val_Loss: 3944.64794921875\n",
      "Epoch 7371, Train_Loss: 3811.239501953125, Val_Loss: 3944.781982421875\n",
      "Epoch 7372, Train_Loss: 3811.30615234375, Val_Loss: 3944.64794921875\n",
      "Epoch 7373, Train_Loss: 3811.22705078125, Val_Loss: 3944.760498046875\n",
      "Epoch 7374, Train_Loss: 3811.119873046875, Val_Loss: 3944.5439453125\n",
      "Epoch 7375, Train_Loss: 3811.128173828125, Val_Loss: 3944.682373046875\n",
      "Epoch 7376, Train_Loss: 3811.093505859375, Val_Loss: 3944.571533203125\n",
      "Epoch 7377, Train_Loss: 3811.111083984375, Val_Loss: 3944.7255859375\n",
      "Epoch 7378, Train_Loss: 3811.165283203125, Val_Loss: 3944.558837890625\n",
      "Epoch 7379, Train_Loss: 3811.163818359375, Val_Loss: 3944.6728515625\n",
      "Epoch 7380, Train_Loss: 3811.147705078125, Val_Loss: 3944.590087890625\n",
      "Epoch 7381, Train_Loss: 3811.140625, Val_Loss: 3944.685302734375\n",
      "Epoch 7382, Train_Loss: 3811.13037109375, Val_Loss: 3944.53759765625\n",
      "Epoch 7383, Train_Loss: 3811.0478515625, Val_Loss: 3944.643310546875\n",
      "Epoch 7384, Train_Loss: 3811.114013671875, Val_Loss: 3944.52880859375\n",
      "Epoch 7385, Train_Loss: 3811.056396484375, Val_Loss: 3944.65966796875\n",
      "Epoch 7386, Train_Loss: 3811.12353515625, Val_Loss: 3944.490478515625\n",
      "Epoch 7387, Train_Loss: 3811.08837890625, Val_Loss: 3944.552734375\n",
      "Epoch 7388, Train_Loss: 3811.076416015625, Val_Loss: 3944.4775390625\n",
      "Epoch 7389, Train_Loss: 3811.0771484375, Val_Loss: 3944.585205078125\n",
      "Epoch 7390, Train_Loss: 3811.0830078125, Val_Loss: 3944.414306640625\n",
      "Epoch 7391, Train_Loss: 3811.055419921875, Val_Loss: 3944.55322265625\n",
      "Epoch 7392, Train_Loss: 3811.0576171875, Val_Loss: 3944.47607421875\n",
      "Epoch 7393, Train_Loss: 3810.9775390625, Val_Loss: 3944.60888671875\n",
      "Epoch 7394, Train_Loss: 3810.956787109375, Val_Loss: 3944.427978515625\n",
      "Epoch 7395, Train_Loss: 3810.939697265625, Val_Loss: 3944.527099609375\n",
      "Epoch 7396, Train_Loss: 3810.938720703125, Val_Loss: 3944.421142578125\n",
      "Epoch 7397, Train_Loss: 3810.93310546875, Val_Loss: 3944.572021484375\n",
      "Epoch 7398, Train_Loss: 3811.141845703125, Val_Loss: 3944.399658203125\n",
      "Epoch 7399, Train_Loss: 3811.13232421875, Val_Loss: 3944.533203125\n",
      "Epoch 7400, Train_Loss: 3811.12841796875, Val_Loss: 3944.396728515625\n",
      "Epoch 7401, Train_Loss: 3811.120849609375, Val_Loss: 3944.47509765625\n",
      "Epoch 7402, Train_Loss: 3811.082763671875, Val_Loss: 3944.32568359375\n",
      "Epoch 7403, Train_Loss: 3811.11962890625, Val_Loss: 3944.405517578125\n",
      "Epoch 7404, Train_Loss: 3811.058349609375, Val_Loss: 3944.36083984375\n",
      "Epoch 7405, Train_Loss: 3811.085693359375, Val_Loss: 3944.51708984375\n",
      "Epoch 7406, Train_Loss: 3811.175048828125, Val_Loss: 3944.34716796875\n",
      "Epoch 7407, Train_Loss: 3811.086181640625, Val_Loss: 3944.434326171875\n",
      "Epoch 7408, Train_Loss: 3811.044921875, Val_Loss: 3944.288330078125\n",
      "Epoch 7409, Train_Loss: 3811.041748046875, Val_Loss: 3944.441650390625\n",
      "Epoch 7410, Train_Loss: 3811.02587890625, Val_Loss: 3944.318359375\n",
      "Epoch 7411, Train_Loss: 3811.06689453125, Val_Loss: 3944.416748046875\n",
      "Epoch 7412, Train_Loss: 3811.001953125, Val_Loss: 3944.27587890625\n",
      "Epoch 7413, Train_Loss: 3811.077392578125, Val_Loss: 3944.40478515625\n",
      "Epoch 7414, Train_Loss: 3810.986328125, Val_Loss: 3944.233642578125\n",
      "Epoch 7415, Train_Loss: 3810.974609375, Val_Loss: 3944.3583984375\n",
      "Epoch 7416, Train_Loss: 3811.076416015625, Val_Loss: 3944.189208984375\n",
      "Epoch 7417, Train_Loss: 3811.1494140625, Val_Loss: 3944.32080078125\n",
      "Epoch 7418, Train_Loss: 3811.103515625, Val_Loss: 3944.241943359375\n",
      "Epoch 7419, Train_Loss: 3811.139404296875, Val_Loss: 3944.3408203125\n",
      "Epoch 7420, Train_Loss: 3811.051025390625, Val_Loss: 3944.220703125\n",
      "Epoch 7421, Train_Loss: 3810.810546875, Val_Loss: 3944.326904296875\n",
      "Epoch 7422, Train_Loss: 3810.781982421875, Val_Loss: 3944.1884765625\n",
      "Epoch 7423, Train_Loss: 3810.692626953125, Val_Loss: 3944.323486328125\n",
      "Epoch 7424, Train_Loss: 3810.78271484375, Val_Loss: 3944.187255859375\n",
      "Epoch 7425, Train_Loss: 3810.735107421875, Val_Loss: 3944.31591796875\n",
      "Epoch 7426, Train_Loss: 3810.792724609375, Val_Loss: 3944.1923828125\n",
      "Epoch 7427, Train_Loss: 3810.837646484375, Val_Loss: 3944.296875\n",
      "Epoch 7428, Train_Loss: 3810.717041015625, Val_Loss: 3944.085205078125\n",
      "Epoch 7429, Train_Loss: 3810.7998046875, Val_Loss: 3944.242919921875\n",
      "Epoch 7430, Train_Loss: 3810.751220703125, Val_Loss: 3944.107177734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7431, Train_Loss: 3810.67041015625, Val_Loss: 3944.26123046875\n",
      "Epoch 7432, Train_Loss: 3810.702392578125, Val_Loss: 3944.105712890625\n",
      "Epoch 7433, Train_Loss: 3810.76220703125, Val_Loss: 3944.2275390625\n",
      "Epoch 7434, Train_Loss: 3810.66650390625, Val_Loss: 3944.09521484375\n",
      "Epoch 7435, Train_Loss: 3810.724609375, Val_Loss: 3944.218017578125\n",
      "Epoch 7436, Train_Loss: 3810.650634765625, Val_Loss: 3944.080078125\n",
      "Epoch 7437, Train_Loss: 3810.695068359375, Val_Loss: 3944.196044921875\n",
      "Epoch 7438, Train_Loss: 3810.605224609375, Val_Loss: 3944.0615234375\n",
      "Epoch 7439, Train_Loss: 3810.71435546875, Val_Loss: 3944.195556640625\n",
      "Epoch 7440, Train_Loss: 3810.63037109375, Val_Loss: 3944.056884765625\n",
      "Epoch 7441, Train_Loss: 3810.647705078125, Val_Loss: 3944.144287109375\n",
      "Epoch 7442, Train_Loss: 3810.58642578125, Val_Loss: 3943.974365234375\n",
      "Epoch 7443, Train_Loss: 3810.673828125, Val_Loss: 3944.102294921875\n",
      "Epoch 7444, Train_Loss: 3810.61962890625, Val_Loss: 3943.99169921875\n",
      "Epoch 7445, Train_Loss: 3810.665283203125, Val_Loss: 3944.165283203125\n",
      "Epoch 7446, Train_Loss: 3810.602783203125, Val_Loss: 3943.984375\n",
      "Epoch 7447, Train_Loss: 3810.655517578125, Val_Loss: 3944.12548828125\n",
      "Epoch 7448, Train_Loss: 3810.453125, Val_Loss: 3943.96435546875\n",
      "Epoch 7449, Train_Loss: 3810.50244140625, Val_Loss: 3944.123291015625\n",
      "Epoch 7450, Train_Loss: 3810.3876953125, Val_Loss: 3943.979248046875\n",
      "Epoch 7451, Train_Loss: 3810.475830078125, Val_Loss: 3944.082763671875\n",
      "Epoch 7452, Train_Loss: 3810.36669921875, Val_Loss: 3943.947509765625\n",
      "Epoch 7453, Train_Loss: 3810.4658203125, Val_Loss: 3944.0615234375\n",
      "Epoch 7454, Train_Loss: 3810.400634765625, Val_Loss: 3943.943603515625\n",
      "Epoch 7455, Train_Loss: 3810.424560546875, Val_Loss: 3944.02490234375\n",
      "Epoch 7456, Train_Loss: 3810.36083984375, Val_Loss: 3943.877685546875\n",
      "Epoch 7457, Train_Loss: 3810.40869140625, Val_Loss: 3943.994873046875\n",
      "Epoch 7458, Train_Loss: 3810.318115234375, Val_Loss: 3943.894775390625\n",
      "Epoch 7459, Train_Loss: 3810.291015625, Val_Loss: 3944.034912109375\n",
      "Epoch 7460, Train_Loss: 3810.30224609375, Val_Loss: 3943.89990234375\n",
      "Epoch 7461, Train_Loss: 3810.306396484375, Val_Loss: 3944.01611328125\n",
      "Epoch 7462, Train_Loss: 3810.259765625, Val_Loss: 3943.8779296875\n",
      "Epoch 7463, Train_Loss: 3810.28515625, Val_Loss: 3943.993896484375\n",
      "Epoch 7464, Train_Loss: 3810.274169921875, Val_Loss: 3943.83447265625\n",
      "Epoch 7465, Train_Loss: 3810.250244140625, Val_Loss: 3943.98193359375\n",
      "Epoch 7466, Train_Loss: 3810.2724609375, Val_Loss: 3943.8369140625\n",
      "Epoch 7467, Train_Loss: 3810.327880859375, Val_Loss: 3943.977294921875\n",
      "Epoch 7468, Train_Loss: 3810.24072265625, Val_Loss: 3943.779296875\n",
      "Epoch 7469, Train_Loss: 3810.22119140625, Val_Loss: 3943.88916015625\n",
      "Epoch 7470, Train_Loss: 3810.223388671875, Val_Loss: 3943.76708984375\n",
      "Epoch 7471, Train_Loss: 3810.197998046875, Val_Loss: 3943.930419921875\n",
      "Epoch 7472, Train_Loss: 3810.19873046875, Val_Loss: 3943.81396484375\n",
      "Epoch 7473, Train_Loss: 3810.20947265625, Val_Loss: 3943.9033203125\n",
      "Epoch 7474, Train_Loss: 3810.24658203125, Val_Loss: 3943.76171875\n",
      "Epoch 7475, Train_Loss: 3810.149169921875, Val_Loss: 3943.88330078125\n",
      "Epoch 7476, Train_Loss: 3810.15673828125, Val_Loss: 3943.7763671875\n",
      "Epoch 7477, Train_Loss: 3810.162841796875, Val_Loss: 3943.869140625\n",
      "Epoch 7478, Train_Loss: 3810.2197265625, Val_Loss: 3943.736083984375\n",
      "Epoch 7479, Train_Loss: 3810.185546875, Val_Loss: 3943.867919921875\n",
      "Epoch 7480, Train_Loss: 3810.159912109375, Val_Loss: 3943.758056640625\n",
      "Epoch 7481, Train_Loss: 3810.096923828125, Val_Loss: 3943.785888671875\n",
      "Epoch 7482, Train_Loss: 3810.019775390625, Val_Loss: 3943.6572265625\n",
      "Epoch 7483, Train_Loss: 3810.077880859375, Val_Loss: 3943.79150390625\n",
      "Epoch 7484, Train_Loss: 3810.06982421875, Val_Loss: 3943.658447265625\n",
      "Epoch 7485, Train_Loss: 3810.09130859375, Val_Loss: 3943.801513671875\n",
      "Epoch 7486, Train_Loss: 3810.070556640625, Val_Loss: 3943.640869140625\n",
      "Epoch 7487, Train_Loss: 3810.081787109375, Val_Loss: 3943.79638671875\n",
      "Epoch 7488, Train_Loss: 3809.97900390625, Val_Loss: 3943.677978515625\n",
      "Epoch 7489, Train_Loss: 3809.983642578125, Val_Loss: 3943.783935546875\n",
      "Epoch 7490, Train_Loss: 3809.972412109375, Val_Loss: 3943.63037109375\n",
      "Epoch 7491, Train_Loss: 3810.037353515625, Val_Loss: 3943.764892578125\n",
      "Epoch 7492, Train_Loss: 3809.97021484375, Val_Loss: 3943.63720703125\n",
      "Epoch 7493, Train_Loss: 3810.034423828125, Val_Loss: 3943.7568359375\n",
      "Epoch 7494, Train_Loss: 3809.97900390625, Val_Loss: 3943.59912109375\n",
      "Epoch 7495, Train_Loss: 3810.003173828125, Val_Loss: 3943.68603515625\n",
      "Epoch 7496, Train_Loss: 3809.933837890625, Val_Loss: 3943.555908203125\n",
      "Epoch 7497, Train_Loss: 3809.993408203125, Val_Loss: 3943.675537109375\n",
      "Epoch 7498, Train_Loss: 3809.929443359375, Val_Loss: 3943.54443359375\n",
      "Epoch 7499, Train_Loss: 3809.976318359375, Val_Loss: 3943.704345703125\n",
      "Epoch 7500, Train_Loss: 3809.90380859375, Val_Loss: 3943.544921875\n",
      "Epoch 7501, Train_Loss: 3809.826171875, Val_Loss: 3943.68994140625\n",
      "Epoch 7502, Train_Loss: 3809.8154296875, Val_Loss: 3943.514404296875\n",
      "Epoch 7503, Train_Loss: 3809.78271484375, Val_Loss: 3943.67919921875\n",
      "Epoch 7504, Train_Loss: 3809.791748046875, Val_Loss: 3943.521484375\n",
      "Epoch 7505, Train_Loss: 3809.9990234375, Val_Loss: 3943.65234375\n",
      "Epoch 7506, Train_Loss: 3810.00244140625, Val_Loss: 3943.514892578125\n",
      "Epoch 7507, Train_Loss: 3810.0048828125, Val_Loss: 3943.67041015625\n",
      "Epoch 7508, Train_Loss: 3809.96923828125, Val_Loss: 3943.417236328125\n",
      "Epoch 7509, Train_Loss: 3809.962646484375, Val_Loss: 3943.5712890625\n",
      "Epoch 7510, Train_Loss: 3809.95166015625, Val_Loss: 3943.432373046875\n",
      "Epoch 7511, Train_Loss: 3809.939697265625, Val_Loss: 3943.596435546875\n",
      "Epoch 7512, Train_Loss: 3809.94189453125, Val_Loss: 3943.43359375\n",
      "Epoch 7513, Train_Loss: 3809.9365234375, Val_Loss: 3943.572021484375\n",
      "Epoch 7514, Train_Loss: 3809.96923828125, Val_Loss: 3943.449951171875\n",
      "Epoch 7515, Train_Loss: 3809.899658203125, Val_Loss: 3943.563720703125\n",
      "Epoch 7516, Train_Loss: 3809.890625, Val_Loss: 3943.41552734375\n",
      "Epoch 7517, Train_Loss: 3809.8759765625, Val_Loss: 3943.54248046875\n",
      "Epoch 7518, Train_Loss: 3809.89306640625, Val_Loss: 3943.41552734375\n",
      "Epoch 7519, Train_Loss: 3809.896728515625, Val_Loss: 3943.540771484375\n",
      "Epoch 7520, Train_Loss: 3809.889892578125, Val_Loss: 3943.4111328125\n",
      "Epoch 7521, Train_Loss: 3809.85205078125, Val_Loss: 3943.468017578125\n",
      "Epoch 7522, Train_Loss: 3809.87060546875, Val_Loss: 3943.3037109375\n",
      "Epoch 7523, Train_Loss: 3809.86376953125, Val_Loss: 3943.45556640625\n",
      "Epoch 7524, Train_Loss: 3809.853515625, Val_Loss: 3943.337646484375\n",
      "Epoch 7525, Train_Loss: 3809.846923828125, Val_Loss: 3943.446044921875\n",
      "Epoch 7526, Train_Loss: 3809.83056640625, Val_Loss: 3943.331298828125\n",
      "Epoch 7527, Train_Loss: 3809.854736328125, Val_Loss: 3943.487548828125\n",
      "Epoch 7528, Train_Loss: 3809.52490234375, Val_Loss: 3943.322021484375\n",
      "Epoch 7529, Train_Loss: 3809.521240234375, Val_Loss: 3943.41357421875\n",
      "Epoch 7530, Train_Loss: 3809.50146484375, Val_Loss: 3943.297607421875\n",
      "Epoch 7531, Train_Loss: 3809.5751953125, Val_Loss: 3943.436279296875\n",
      "Epoch 7532, Train_Loss: 3809.58935546875, Val_Loss: 3943.31591796875\n",
      "Epoch 7533, Train_Loss: 3809.528564453125, Val_Loss: 3943.394287109375\n",
      "Epoch 7534, Train_Loss: 3809.5302734375, Val_Loss: 3943.27490234375\n",
      "Epoch 7535, Train_Loss: 3809.52685546875, Val_Loss: 3943.3876953125\n",
      "Epoch 7536, Train_Loss: 3809.52392578125, Val_Loss: 3943.224853515625\n",
      "Epoch 7537, Train_Loss: 3809.49560546875, Val_Loss: 3943.310791015625\n",
      "Epoch 7538, Train_Loss: 3809.55517578125, Val_Loss: 3943.22607421875\n",
      "Epoch 7539, Train_Loss: 3809.537109375, Val_Loss: 3943.408447265625\n",
      "Epoch 7540, Train_Loss: 3809.550537109375, Val_Loss: 3943.25\n",
      "Epoch 7541, Train_Loss: 3809.506591796875, Val_Loss: 3943.344482421875\n",
      "Epoch 7542, Train_Loss: 3809.5517578125, Val_Loss: 3943.1552734375\n",
      "Epoch 7543, Train_Loss: 3809.424560546875, Val_Loss: 3943.361083984375\n",
      "Epoch 7544, Train_Loss: 3809.488525390625, Val_Loss: 3943.23291015625\n",
      "Epoch 7545, Train_Loss: 3809.547119140625, Val_Loss: 3943.14404296875\n",
      "Epoch 7546, Train_Loss: 3809.4638671875, Val_Loss: 3943.412841796875\n",
      "Epoch 7547, Train_Loss: 3809.483642578125, Val_Loss: 3943.284423828125\n",
      "Epoch 7548, Train_Loss: 3809.467041015625, Val_Loss: 3942.935302734375\n",
      "Epoch 7549, Train_Loss: 3809.461669921875, Val_Loss: 3943.213623046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7550, Train_Loss: 3809.50390625, Val_Loss: 3943.291259765625\n",
      "Epoch 7551, Train_Loss: 3809.489501953125, Val_Loss: 3943.0537109375\n",
      "Epoch 7552, Train_Loss: 3809.45458984375, Val_Loss: 3943.205078125\n",
      "Epoch 7553, Train_Loss: 3809.451904296875, Val_Loss: 3943.12548828125\n",
      "Epoch 7554, Train_Loss: 3809.39453125, Val_Loss: 3943.317626953125\n",
      "Epoch 7555, Train_Loss: 3809.32080078125, Val_Loss: 3943.149658203125\n",
      "Epoch 7556, Train_Loss: 3809.279541015625, Val_Loss: 3942.961669921875\n",
      "Epoch 7557, Train_Loss: 3809.27392578125, Val_Loss: 3943.233154296875\n",
      "Epoch 7558, Train_Loss: 3809.262451171875, Val_Loss: 3943.23046875\n",
      "Epoch 7559, Train_Loss: 3809.263427734375, Val_Loss: 3942.983154296875\n",
      "Epoch 7560, Train_Loss: 3809.27099609375, Val_Loss: 3943.162841796875\n",
      "Epoch 7561, Train_Loss: 3809.265869140625, Val_Loss: 3943.142822265625\n",
      "Epoch 7562, Train_Loss: 3809.224853515625, Val_Loss: 3942.901123046875\n",
      "Epoch 7563, Train_Loss: 3809.20361328125, Val_Loss: 3943.17041015625\n",
      "Epoch 7564, Train_Loss: 3809.189453125, Val_Loss: 3943.116455078125\n",
      "Epoch 7565, Train_Loss: 3809.32080078125, Val_Loss: 3942.899169921875\n",
      "Epoch 7566, Train_Loss: 3809.19580078125, Val_Loss: 3943.095947265625\n",
      "Epoch 7567, Train_Loss: 3809.187744140625, Val_Loss: 3943.138427734375\n",
      "Epoch 7568, Train_Loss: 3809.145263671875, Val_Loss: 3942.9462890625\n",
      "Epoch 7569, Train_Loss: 3809.156005859375, Val_Loss: 3943.120849609375\n",
      "Epoch 7570, Train_Loss: 3809.152099609375, Val_Loss: 3943.049560546875\n",
      "Epoch 7571, Train_Loss: 3809.252685546875, Val_Loss: 3942.87548828125\n",
      "Epoch 7572, Train_Loss: 3809.15576171875, Val_Loss: 3943.10400390625\n",
      "Epoch 7573, Train_Loss: 3809.13818359375, Val_Loss: 3943.0703125\n",
      "Epoch 7574, Train_Loss: 3809.2529296875, Val_Loss: 3942.846435546875\n",
      "Epoch 7575, Train_Loss: 3809.126220703125, Val_Loss: 3942.980712890625\n",
      "Epoch 7576, Train_Loss: 3809.109375, Val_Loss: 3942.992919921875\n",
      "Epoch 7577, Train_Loss: 3809.2421875, Val_Loss: 3942.793212890625\n",
      "Epoch 7578, Train_Loss: 3809.096923828125, Val_Loss: 3943.05810546875\n",
      "Epoch 7579, Train_Loss: 3809.117431640625, Val_Loss: 3942.993896484375\n",
      "Epoch 7580, Train_Loss: 3809.198974609375, Val_Loss: 3942.768310546875\n",
      "Epoch 7581, Train_Loss: 3809.142333984375, Val_Loss: 3942.996826171875\n",
      "Epoch 7582, Train_Loss: 3809.05908203125, Val_Loss: 3942.981201171875\n",
      "Epoch 7583, Train_Loss: 3809.17822265625, Val_Loss: 3942.76953125\n",
      "Epoch 7584, Train_Loss: 3809.14404296875, Val_Loss: 3942.9931640625\n",
      "Epoch 7585, Train_Loss: 3809.10888671875, Val_Loss: 3942.96435546875\n",
      "Epoch 7586, Train_Loss: 3809.194091796875, Val_Loss: 3942.744384765625\n",
      "Epoch 7587, Train_Loss: 3809.05517578125, Val_Loss: 3942.966796875\n",
      "Epoch 7588, Train_Loss: 3809.174072265625, Val_Loss: 3942.948486328125\n",
      "Epoch 7589, Train_Loss: 3809.084228515625, Val_Loss: 3942.66748046875\n",
      "Epoch 7590, Train_Loss: 3808.93603515625, Val_Loss: 3942.925537109375\n",
      "Epoch 7591, Train_Loss: 3808.98583984375, Val_Loss: 3942.894775390625\n",
      "Epoch 7592, Train_Loss: 3809.08203125, Val_Loss: 3942.68408203125\n",
      "Epoch 7593, Train_Loss: 3809.074951171875, Val_Loss: 3942.8935546875\n",
      "Epoch 7594, Train_Loss: 3809.07080078125, Val_Loss: 3942.88916015625\n",
      "Epoch 7595, Train_Loss: 3809.04443359375, Val_Loss: 3942.66162109375\n",
      "Epoch 7596, Train_Loss: 3809.022216796875, Val_Loss: 3942.921630859375\n",
      "Epoch 7597, Train_Loss: 3809.05322265625, Val_Loss: 3942.877197265625\n",
      "Epoch 7598, Train_Loss: 3808.98486328125, Val_Loss: 3942.649658203125\n",
      "Epoch 7599, Train_Loss: 3809.0419921875, Val_Loss: 3942.86279296875\n",
      "Epoch 7600, Train_Loss: 3809.03759765625, Val_Loss: 3942.868408203125\n",
      "Epoch 7601, Train_Loss: 3808.96875, Val_Loss: 3942.662353515625\n",
      "Epoch 7602, Train_Loss: 3808.997314453125, Val_Loss: 3942.82275390625\n",
      "Epoch 7603, Train_Loss: 3809.0205078125, Val_Loss: 3942.7587890625\n",
      "Epoch 7604, Train_Loss: 3808.94482421875, Val_Loss: 3942.536376953125\n",
      "Epoch 7605, Train_Loss: 3809.00390625, Val_Loss: 3942.823974609375\n",
      "Epoch 7606, Train_Loss: 3808.98486328125, Val_Loss: 3942.835693359375\n",
      "Epoch 7607, Train_Loss: 3808.91259765625, Val_Loss: 3942.600830078125\n",
      "Epoch 7608, Train_Loss: 3808.98388671875, Val_Loss: 3942.796875\n",
      "Epoch 7609, Train_Loss: 3808.892822265625, Val_Loss: 3942.741943359375\n",
      "Epoch 7610, Train_Loss: 3808.85107421875, Val_Loss: 3942.538330078125\n",
      "Epoch 7611, Train_Loss: 3808.87255859375, Val_Loss: 3942.795166015625\n",
      "Epoch 7612, Train_Loss: 3809.0751953125, Val_Loss: 3942.788330078125\n",
      "Epoch 7613, Train_Loss: 3809.01611328125, Val_Loss: 3942.556884765625\n",
      "Epoch 7614, Train_Loss: 3809.0654296875, Val_Loss: 3942.7724609375\n",
      "Epoch 7615, Train_Loss: 3808.99169921875, Val_Loss: 3942.69677734375\n",
      "Epoch 7616, Train_Loss: 3808.984130859375, Val_Loss: 3942.478515625\n",
      "Epoch 7617, Train_Loss: 3808.97998046875, Val_Loss: 3942.715087890625\n",
      "Epoch 7618, Train_Loss: 3808.991943359375, Val_Loss: 3942.660888671875\n",
      "Epoch 7619, Train_Loss: 3808.98828125, Val_Loss: 3942.49755859375\n",
      "Epoch 7620, Train_Loss: 3808.96630859375, Val_Loss: 3942.733154296875\n",
      "Epoch 7621, Train_Loss: 3809.009033203125, Val_Loss: 3942.71044921875\n",
      "Epoch 7622, Train_Loss: 3808.9345703125, Val_Loss: 3942.460693359375\n",
      "Epoch 7623, Train_Loss: 3808.947265625, Val_Loss: 3942.674072265625\n",
      "Epoch 7624, Train_Loss: 3808.944580078125, Val_Loss: 3942.669189453125\n",
      "Epoch 7625, Train_Loss: 3808.934814453125, Val_Loss: 3942.4580078125\n",
      "Epoch 7626, Train_Loss: 3808.945556640625, Val_Loss: 3942.680908203125\n",
      "Epoch 7627, Train_Loss: 3808.929931640625, Val_Loss: 3942.640380859375\n",
      "Epoch 7628, Train_Loss: 3808.93017578125, Val_Loss: 3942.409912109375\n",
      "Epoch 7629, Train_Loss: 3808.913818359375, Val_Loss: 3942.6044921875\n",
      "Epoch 7630, Train_Loss: 3808.903564453125, Val_Loss: 3942.5908203125\n",
      "Epoch 7631, Train_Loss: 3808.91943359375, Val_Loss: 3942.364501953125\n",
      "Epoch 7632, Train_Loss: 3808.890869140625, Val_Loss: 3942.605224609375\n",
      "Epoch 7633, Train_Loss: 3808.900390625, Val_Loss: 3942.595703125\n",
      "Epoch 7634, Train_Loss: 3808.860595703125, Val_Loss: 3942.3955078125\n",
      "Epoch 7635, Train_Loss: 3808.62255859375, Val_Loss: 3942.6064453125\n",
      "Epoch 7636, Train_Loss: 3808.578857421875, Val_Loss: 3942.576416015625\n",
      "Epoch 7637, Train_Loss: 3808.59228515625, Val_Loss: 3942.322021484375\n",
      "Epoch 7638, Train_Loss: 3808.568115234375, Val_Loss: 3942.564697265625\n",
      "Epoch 7639, Train_Loss: 3808.606689453125, Val_Loss: 3942.580810546875\n",
      "Epoch 7640, Train_Loss: 3808.569580078125, Val_Loss: 3942.364013671875\n",
      "Epoch 7641, Train_Loss: 3808.627197265625, Val_Loss: 3942.564697265625\n",
      "Epoch 7642, Train_Loss: 3808.621826171875, Val_Loss: 3942.468505859375\n",
      "Epoch 7643, Train_Loss: 3808.577880859375, Val_Loss: 3942.248291015625\n",
      "Epoch 7644, Train_Loss: 3808.564208984375, Val_Loss: 3942.502685546875\n",
      "Epoch 7645, Train_Loss: 3808.552978515625, Val_Loss: 3942.487548828125\n",
      "Epoch 7646, Train_Loss: 3808.532470703125, Val_Loss: 3942.29150390625\n",
      "Epoch 7647, Train_Loss: 3808.5380859375, Val_Loss: 3942.514892578125\n",
      "Epoch 7648, Train_Loss: 3808.59326171875, Val_Loss: 3942.46875\n",
      "Epoch 7649, Train_Loss: 3808.443603515625, Val_Loss: 3942.228759765625\n",
      "Epoch 7650, Train_Loss: 3808.51708984375, Val_Loss: 3942.476318359375\n",
      "Epoch 7651, Train_Loss: 3808.48583984375, Val_Loss: 3942.461669921875\n",
      "Epoch 7652, Train_Loss: 3808.43896484375, Val_Loss: 3942.224365234375\n",
      "Epoch 7653, Train_Loss: 3808.494140625, Val_Loss: 3942.44921875\n",
      "Epoch 7654, Train_Loss: 3808.534423828125, Val_Loss: 3942.4423828125\n",
      "Epoch 7655, Train_Loss: 3808.427734375, Val_Loss: 3942.23876953125\n",
      "Epoch 7656, Train_Loss: 3808.407470703125, Val_Loss: 3942.4111328125\n",
      "Epoch 7657, Train_Loss: 3808.44775390625, Val_Loss: 3942.397216796875\n",
      "Epoch 7658, Train_Loss: 3808.392333984375, Val_Loss: 3942.160888671875\n",
      "Epoch 7659, Train_Loss: 3808.495361328125, Val_Loss: 3942.40869140625\n",
      "Epoch 7660, Train_Loss: 3808.44091796875, Val_Loss: 3942.396484375\n",
      "Epoch 7661, Train_Loss: 3808.376953125, Val_Loss: 3942.175537109375\n",
      "Epoch 7662, Train_Loss: 3808.234130859375, Val_Loss: 3942.399169921875\n",
      "Epoch 7663, Train_Loss: 3808.269775390625, Val_Loss: 3942.370361328125\n",
      "Epoch 7664, Train_Loss: 3808.234130859375, Val_Loss: 3942.136474609375\n",
      "Epoch 7665, Train_Loss: 3808.280517578125, Val_Loss: 3942.38330078125\n",
      "Epoch 7666, Train_Loss: 3808.26025390625, Val_Loss: 3942.3701171875\n",
      "Epoch 7667, Train_Loss: 3808.19287109375, Val_Loss: 3942.15869140625\n",
      "Epoch 7668, Train_Loss: 3808.249267578125, Val_Loss: 3942.370361328125\n",
      "Epoch 7669, Train_Loss: 3808.228271484375, Val_Loss: 3942.28564453125\n",
      "Epoch 7670, Train_Loss: 3808.1416015625, Val_Loss: 3942.072021484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7671, Train_Loss: 3808.17578125, Val_Loss: 3942.302001953125\n",
      "Epoch 7672, Train_Loss: 3808.173583984375, Val_Loss: 3942.320068359375\n",
      "Epoch 7673, Train_Loss: 3808.12353515625, Val_Loss: 3942.114013671875\n",
      "Epoch 7674, Train_Loss: 3808.174072265625, Val_Loss: 3942.304443359375\n",
      "Epoch 7675, Train_Loss: 3808.168212890625, Val_Loss: 3942.26611328125\n",
      "Epoch 7676, Train_Loss: 3808.110595703125, Val_Loss: 3942.044921875\n",
      "Epoch 7677, Train_Loss: 3808.084228515625, Val_Loss: 3942.308349609375\n",
      "Epoch 7678, Train_Loss: 3808.128173828125, Val_Loss: 3942.275634765625\n",
      "Epoch 7679, Train_Loss: 3808.114990234375, Val_Loss: 3942.023193359375\n",
      "Epoch 7680, Train_Loss: 3808.135498046875, Val_Loss: 3942.266357421875\n",
      "Epoch 7681, Train_Loss: 3808.1259765625, Val_Loss: 3942.264404296875\n",
      "Epoch 7682, Train_Loss: 3808.067626953125, Val_Loss: 3941.9892578125\n",
      "Epoch 7683, Train_Loss: 3808.103515625, Val_Loss: 3942.20166015625\n",
      "Epoch 7684, Train_Loss: 3808.108154296875, Val_Loss: 3942.132080078125\n",
      "Epoch 7685, Train_Loss: 3808.13330078125, Val_Loss: 3941.947509765625\n",
      "Epoch 7686, Train_Loss: 3808.061279296875, Val_Loss: 3942.243896484375\n",
      "Epoch 7687, Train_Loss: 3808.095703125, Val_Loss: 3942.200439453125\n",
      "Epoch 7688, Train_Loss: 3808.12744140625, Val_Loss: 3941.97607421875\n",
      "Epoch 7689, Train_Loss: 3808.045654296875, Val_Loss: 3942.194091796875\n",
      "Epoch 7690, Train_Loss: 3808.041259765625, Val_Loss: 3942.1904296875\n",
      "Epoch 7691, Train_Loss: 3807.994873046875, Val_Loss: 3941.985595703125\n",
      "Epoch 7692, Train_Loss: 3808.100830078125, Val_Loss: 3942.165283203125\n",
      "Epoch 7693, Train_Loss: 3808.0478515625, Val_Loss: 3942.1484375\n",
      "Epoch 7694, Train_Loss: 3808.0927734375, Val_Loss: 3941.950439453125\n",
      "Epoch 7695, Train_Loss: 3808.033935546875, Val_Loss: 3942.201171875\n",
      "Epoch 7696, Train_Loss: 3807.9375, Val_Loss: 3942.076904296875\n",
      "Epoch 7697, Train_Loss: 3808.008056640625, Val_Loss: 3941.860107421875\n",
      "Epoch 7698, Train_Loss: 3807.974365234375, Val_Loss: 3942.098388671875\n",
      "Epoch 7699, Train_Loss: 3807.96875, Val_Loss: 3942.114501953125\n",
      "Epoch 7700, Train_Loss: 3808.015869140625, Val_Loss: 3941.889892578125\n",
      "Epoch 7701, Train_Loss: 3807.9287109375, Val_Loss: 3942.105224609375\n",
      "Epoch 7702, Train_Loss: 3807.909912109375, Val_Loss: 3942.073974609375\n",
      "Epoch 7703, Train_Loss: 3807.9814453125, Val_Loss: 3941.8740234375\n",
      "Epoch 7704, Train_Loss: 3807.96240234375, Val_Loss: 3942.0927734375\n",
      "Epoch 7705, Train_Loss: 3807.896240234375, Val_Loss: 3942.065673828125\n",
      "Epoch 7706, Train_Loss: 3807.97509765625, Val_Loss: 3941.85205078125\n",
      "Epoch 7707, Train_Loss: 3807.89794921875, Val_Loss: 3942.08203125\n",
      "Epoch 7708, Train_Loss: 3807.970947265625, Val_Loss: 3942.047119140625\n",
      "Epoch 7709, Train_Loss: 3807.941162109375, Val_Loss: 3941.777587890625\n",
      "Epoch 7710, Train_Loss: 3807.95068359375, Val_Loss: 3941.978759765625\n",
      "Epoch 7711, Train_Loss: 3807.884033203125, Val_Loss: 3941.96044921875\n",
      "Epoch 7712, Train_Loss: 3807.927978515625, Val_Loss: 3941.77197265625\n",
      "Epoch 7713, Train_Loss: 3807.917724609375, Val_Loss: 3942.028076171875\n",
      "Epoch 7714, Train_Loss: 3807.8876953125, Val_Loss: 3942.00390625\n",
      "Epoch 7715, Train_Loss: 3807.910888671875, Val_Loss: 3941.792724609375\n",
      "Epoch 7716, Train_Loss: 3807.830810546875, Val_Loss: 3941.9931640625\n",
      "Epoch 7717, Train_Loss: 3807.763916015625, Val_Loss: 3941.974365234375\n",
      "Epoch 7718, Train_Loss: 3807.77490234375, Val_Loss: 3941.75634765625\n",
      "Epoch 7719, Train_Loss: 3808.043212890625, Val_Loss: 3942.0068359375\n",
      "Epoch 7720, Train_Loss: 3807.983642578125, Val_Loss: 3941.964111328125\n",
      "Epoch 7721, Train_Loss: 3808.001220703125, Val_Loss: 3941.75830078125\n",
      "Epoch 7722, Train_Loss: 3808.01611328125, Val_Loss: 3941.9228515625\n",
      "Epoch 7723, Train_Loss: 3807.940185546875, Val_Loss: 3941.900390625\n",
      "Epoch 7724, Train_Loss: 3807.9482421875, Val_Loss: 3941.6630859375\n",
      "Epoch 7725, Train_Loss: 3807.93798828125, Val_Loss: 3941.88330078125\n",
      "Epoch 7726, Train_Loss: 3807.93603515625, Val_Loss: 3941.909912109375\n",
      "Epoch 7727, Train_Loss: 3807.912841796875, Val_Loss: 3941.71484375\n",
      "Epoch 7728, Train_Loss: 3808.005126953125, Val_Loss: 3941.934814453125\n",
      "Epoch 7729, Train_Loss: 3807.892333984375, Val_Loss: 3941.87353515625\n",
      "Epoch 7730, Train_Loss: 3807.9013671875, Val_Loss: 3941.662353515625\n",
      "Epoch 7731, Train_Loss: 3807.91162109375, Val_Loss: 3941.91650390625\n",
      "Epoch 7732, Train_Loss: 3807.894287109375, Val_Loss: 3941.851318359375\n",
      "Epoch 7733, Train_Loss: 3807.989990234375, Val_Loss: 3941.634765625\n",
      "Epoch 7734, Train_Loss: 3807.8896484375, Val_Loss: 3941.87158203125\n",
      "Epoch 7735, Train_Loss: 3807.885009765625, Val_Loss: 3941.847900390625\n",
      "Epoch 7736, Train_Loss: 3807.9892578125, Val_Loss: 3941.576416015625\n",
      "Epoch 7737, Train_Loss: 3807.857177734375, Val_Loss: 3941.81396484375\n",
      "Epoch 7738, Train_Loss: 3807.870361328125, Val_Loss: 3941.767578125\n",
      "Epoch 7739, Train_Loss: 3807.9765625, Val_Loss: 3941.6044921875\n",
      "Epoch 7740, Train_Loss: 3807.85888671875, Val_Loss: 3941.851318359375\n",
      "Epoch 7741, Train_Loss: 3807.8369140625, Val_Loss: 3941.825927734375\n",
      "Epoch 7742, Train_Loss: 3807.657470703125, Val_Loss: 3941.593994140625\n",
      "Epoch 7743, Train_Loss: 3807.530029296875, Val_Loss: 3941.8037109375\n",
      "Epoch 7744, Train_Loss: 3807.514404296875, Val_Loss: 3941.79248046875\n",
      "Epoch 7745, Train_Loss: 3807.63623046875, Val_Loss: 3941.58203125\n",
      "Epoch 7746, Train_Loss: 3807.565185546875, Val_Loss: 3941.7919921875\n",
      "Epoch 7747, Train_Loss: 3807.55859375, Val_Loss: 3941.77392578125\n",
      "Epoch 7748, Train_Loss: 3807.663330078125, Val_Loss: 3941.538330078125\n",
      "Epoch 7749, Train_Loss: 3807.643310546875, Val_Loss: 3941.720458984375\n",
      "Epoch 7750, Train_Loss: 3807.642333984375, Val_Loss: 3941.704833984375\n",
      "Epoch 7751, Train_Loss: 3807.619384765625, Val_Loss: 3941.498046875\n",
      "Epoch 7752, Train_Loss: 3807.535888671875, Val_Loss: 3941.74365234375\n",
      "Epoch 7753, Train_Loss: 3807.622802734375, Val_Loss: 3941.709228515625\n",
      "Epoch 7754, Train_Loss: 3807.6142578125, Val_Loss: 3941.50634765625\n",
      "Epoch 7755, Train_Loss: 3807.638671875, Val_Loss: 3941.70361328125\n",
      "Epoch 7756, Train_Loss: 3807.579345703125, Val_Loss: 3941.700439453125\n",
      "Epoch 7757, Train_Loss: 3807.56005859375, Val_Loss: 3941.481201171875\n",
      "Epoch 7758, Train_Loss: 3807.58251953125, Val_Loss: 3941.720458984375\n",
      "Epoch 7759, Train_Loss: 3807.590576171875, Val_Loss: 3941.706298828125\n",
      "Epoch 7760, Train_Loss: 3807.527587890625, Val_Loss: 3941.4775390625\n",
      "Epoch 7761, Train_Loss: 3807.59326171875, Val_Loss: 3941.691162109375\n",
      "Epoch 7762, Train_Loss: 3807.56103515625, Val_Loss: 3941.6298828125\n",
      "Epoch 7763, Train_Loss: 3807.592041015625, Val_Loss: 3941.383544921875\n",
      "Epoch 7764, Train_Loss: 3807.553466796875, Val_Loss: 3941.63671875\n",
      "Epoch 7765, Train_Loss: 3807.55322265625, Val_Loss: 3941.61669921875\n",
      "Epoch 7766, Train_Loss: 3807.544677734375, Val_Loss: 3941.40283203125\n",
      "Epoch 7767, Train_Loss: 3807.54296875, Val_Loss: 3941.643310546875\n",
      "Epoch 7768, Train_Loss: 3807.528564453125, Val_Loss: 3941.6103515625\n",
      "Epoch 7769, Train_Loss: 3807.366455078125, Val_Loss: 3941.37890625\n",
      "Epoch 7770, Train_Loss: 3807.377197265625, Val_Loss: 3941.63232421875\n",
      "Epoch 7771, Train_Loss: 3807.3896484375, Val_Loss: 3941.6298828125\n",
      "Epoch 7772, Train_Loss: 3807.37646484375, Val_Loss: 3941.365966796875\n",
      "Epoch 7773, Train_Loss: 3807.353759765625, Val_Loss: 3941.55517578125\n",
      "Epoch 7774, Train_Loss: 3807.303955078125, Val_Loss: 3941.553955078125\n",
      "Epoch 7775, Train_Loss: 3807.3583984375, Val_Loss: 3941.365234375\n",
      "Epoch 7776, Train_Loss: 3807.2841796875, Val_Loss: 3941.572021484375\n",
      "Epoch 7777, Train_Loss: 3807.267578125, Val_Loss: 3941.572021484375\n",
      "Epoch 7778, Train_Loss: 3807.297119140625, Val_Loss: 3941.2958984375\n",
      "Epoch 7779, Train_Loss: 3807.210205078125, Val_Loss: 3941.529296875\n",
      "Epoch 7780, Train_Loss: 3807.20166015625, Val_Loss: 3941.496337890625\n",
      "Epoch 7781, Train_Loss: 3807.29931640625, Val_Loss: 3941.321533203125\n",
      "Epoch 7782, Train_Loss: 3807.271728515625, Val_Loss: 3941.552001953125\n",
      "Epoch 7783, Train_Loss: 3807.2158203125, Val_Loss: 3941.562744140625\n",
      "Epoch 7784, Train_Loss: 3807.252685546875, Val_Loss: 3941.310791015625\n",
      "Epoch 7785, Train_Loss: 3807.166748046875, Val_Loss: 3941.506103515625\n",
      "Epoch 7786, Train_Loss: 3807.150146484375, Val_Loss: 3941.454833984375\n",
      "Epoch 7787, Train_Loss: 3807.24267578125, Val_Loss: 3941.279296875\n",
      "Epoch 7788, Train_Loss: 3807.208984375, Val_Loss: 3941.5576171875\n",
      "Epoch 7789, Train_Loss: 3807.195068359375, Val_Loss: 3941.432861328125\n",
      "Epoch 7790, Train_Loss: 3807.212890625, Val_Loss: 3941.184814453125\n",
      "Epoch 7791, Train_Loss: 3807.13330078125, Val_Loss: 3941.423095703125\n",
      "Epoch 7792, Train_Loss: 3807.171142578125, Val_Loss: 3941.4443359375\n",
      "Epoch 7793, Train_Loss: 3807.21142578125, Val_Loss: 3941.23486328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7794, Train_Loss: 3807.17822265625, Val_Loss: 3941.45166015625\n",
      "Epoch 7795, Train_Loss: 3807.0966796875, Val_Loss: 3941.410400390625\n",
      "Epoch 7796, Train_Loss: 3807.162841796875, Val_Loss: 3941.20556640625\n",
      "Epoch 7797, Train_Loss: 3807.1201171875, Val_Loss: 3941.428466796875\n",
      "Epoch 7798, Train_Loss: 3807.115966796875, Val_Loss: 3941.4248046875\n",
      "Epoch 7799, Train_Loss: 3807.191650390625, Val_Loss: 3941.20166015625\n",
      "Epoch 7800, Train_Loss: 3807.135986328125, Val_Loss: 3941.41357421875\n",
      "Epoch 7801, Train_Loss: 3807.11279296875, Val_Loss: 3941.392822265625\n",
      "Epoch 7802, Train_Loss: 3807.08935546875, Val_Loss: 3941.1201171875\n",
      "Epoch 7803, Train_Loss: 3807.048828125, Val_Loss: 3941.34912109375\n",
      "Epoch 7804, Train_Loss: 3807.046142578125, Val_Loss: 3941.34033203125\n",
      "Epoch 7805, Train_Loss: 3807.023193359375, Val_Loss: 3941.1708984375\n",
      "Epoch 7806, Train_Loss: 3807.02099609375, Val_Loss: 3941.32568359375\n",
      "Epoch 7807, Train_Loss: 3807.039794921875, Val_Loss: 3941.3056640625\n",
      "Epoch 7808, Train_Loss: 3807.03759765625, Val_Loss: 3941.12890625\n",
      "Epoch 7809, Train_Loss: 3807.027099609375, Val_Loss: 3941.36083984375\n",
      "Epoch 7810, Train_Loss: 3807.008056640625, Val_Loss: 3941.317138671875\n",
      "Epoch 7811, Train_Loss: 3806.990966796875, Val_Loss: 3941.0908203125\n",
      "Epoch 7812, Train_Loss: 3807.011474609375, Val_Loss: 3941.30322265625\n",
      "Epoch 7813, Train_Loss: 3807.007080078125, Val_Loss: 3941.297607421875\n",
      "Epoch 7814, Train_Loss: 3807.001953125, Val_Loss: 3941.1123046875\n",
      "Epoch 7815, Train_Loss: 3806.9794921875, Val_Loss: 3941.21630859375\n",
      "Epoch 7816, Train_Loss: 3806.89013671875, Val_Loss: 3941.19970703125\n",
      "Epoch 7817, Train_Loss: 3806.89404296875, Val_Loss: 3941.018798828125\n",
      "Epoch 7818, Train_Loss: 3807.026123046875, Val_Loss: 3941.255126953125\n",
      "Epoch 7819, Train_Loss: 3806.96240234375, Val_Loss: 3941.208740234375\n",
      "Epoch 7820, Train_Loss: 3806.916748046875, Val_Loss: 3940.99755859375\n",
      "Epoch 7821, Train_Loss: 3806.96142578125, Val_Loss: 3941.257568359375\n",
      "Epoch 7822, Train_Loss: 3806.828125, Val_Loss: 3941.2939453125\n",
      "Epoch 7823, Train_Loss: 3806.8046875, Val_Loss: 3940.976806640625\n",
      "Epoch 7824, Train_Loss: 3806.74267578125, Val_Loss: 3941.184814453125\n",
      "Epoch 7825, Train_Loss: 3806.977294921875, Val_Loss: 3941.199951171875\n",
      "Epoch 7826, Train_Loss: 3807.001220703125, Val_Loss: 3941.05322265625\n",
      "Epoch 7827, Train_Loss: 3806.9951171875, Val_Loss: 3941.082763671875\n",
      "Epoch 7828, Train_Loss: 3806.982421875, Val_Loss: 3941.063232421875\n",
      "Epoch 7829, Train_Loss: 3806.984375, Val_Loss: 3941.248291015625\n",
      "Epoch 7830, Train_Loss: 3806.97412109375, Val_Loss: 3940.98828125\n",
      "Epoch 7831, Train_Loss: 3806.952392578125, Val_Loss: 3940.783203125\n",
      "Epoch 7832, Train_Loss: 3806.968017578125, Val_Loss: 3941.251953125\n",
      "Epoch 7833, Train_Loss: 3806.987548828125, Val_Loss: 3941.28125\n",
      "Epoch 7834, Train_Loss: 3806.957763671875, Val_Loss: 3940.857666015625\n",
      "Epoch 7835, Train_Loss: 3806.93408203125, Val_Loss: 3940.9755859375\n",
      "Epoch 7836, Train_Loss: 3806.87451171875, Val_Loss: 3941.115478515625\n",
      "Epoch 7837, Train_Loss: 3806.90478515625, Val_Loss: 3941.040283203125\n",
      "Epoch 7838, Train_Loss: 3806.907958984375, Val_Loss: 3940.887939453125\n",
      "Epoch 7839, Train_Loss: 3806.891845703125, Val_Loss: 3941.184814453125\n",
      "Epoch 7840, Train_Loss: 3806.876953125, Val_Loss: 3941.077880859375\n",
      "Epoch 7841, Train_Loss: 3806.89111328125, Val_Loss: 3940.855224609375\n",
      "Epoch 7842, Train_Loss: 3806.87646484375, Val_Loss: 3941.060302734375\n",
      "Epoch 7843, Train_Loss: 3806.86767578125, Val_Loss: 3941.08154296875\n",
      "Epoch 7844, Train_Loss: 3806.881103515625, Val_Loss: 3940.79638671875\n",
      "Epoch 7845, Train_Loss: 3806.86328125, Val_Loss: 3941.0107421875\n",
      "Epoch 7846, Train_Loss: 3806.84912109375, Val_Loss: 3941.061279296875\n",
      "Epoch 7847, Train_Loss: 3806.916015625, Val_Loss: 3940.912353515625\n",
      "Epoch 7848, Train_Loss: 3806.909423828125, Val_Loss: 3940.755615234375\n",
      "Epoch 7849, Train_Loss: 3806.54296875, Val_Loss: 3941.09326171875\n",
      "Epoch 7850, Train_Loss: 3806.55224609375, Val_Loss: 3941.077880859375\n",
      "Epoch 7851, Train_Loss: 3806.60302734375, Val_Loss: 3940.7783203125\n",
      "Epoch 7852, Train_Loss: 3806.568115234375, Val_Loss: 3940.97998046875\n",
      "Epoch 7853, Train_Loss: 3806.58203125, Val_Loss: 3941.018798828125\n",
      "Epoch 7854, Train_Loss: 3806.652099609375, Val_Loss: 3940.845947265625\n",
      "Epoch 7855, Train_Loss: 3806.55712890625, Val_Loss: 3940.873291015625\n",
      "Epoch 7856, Train_Loss: 3806.61962890625, Val_Loss: 3940.8232421875\n",
      "Epoch 7857, Train_Loss: 3806.634521484375, Val_Loss: 3940.704345703125\n",
      "Epoch 7858, Train_Loss: 3806.585693359375, Val_Loss: 3941.103271484375\n",
      "Epoch 7859, Train_Loss: 3806.50244140625, Val_Loss: 3940.892822265625\n",
      "Epoch 7860, Train_Loss: 3806.62353515625, Val_Loss: 3940.6220703125\n",
      "Epoch 7861, Train_Loss: 3806.470458984375, Val_Loss: 3940.988037109375\n",
      "Epoch 7862, Train_Loss: 3806.504150390625, Val_Loss: 3941.08203125\n",
      "Epoch 7863, Train_Loss: 3806.5078125, Val_Loss: 3940.651611328125\n",
      "Epoch 7864, Train_Loss: 3806.58251953125, Val_Loss: 3940.785888671875\n",
      "Epoch 7865, Train_Loss: 3806.5302734375, Val_Loss: 3940.847900390625\n",
      "Epoch 7866, Train_Loss: 3806.57177734375, Val_Loss: 3940.790771484375\n",
      "Epoch 7867, Train_Loss: 3806.535400390625, Val_Loss: 3940.691650390625\n",
      "Epoch 7868, Train_Loss: 3806.46484375, Val_Loss: 3941.033935546875\n",
      "Epoch 7869, Train_Loss: 3806.514892578125, Val_Loss: 3940.787109375\n",
      "Epoch 7870, Train_Loss: 3806.489501953125, Val_Loss: 3940.5595703125\n",
      "Epoch 7871, Train_Loss: 3806.49560546875, Val_Loss: 3940.828369140625\n",
      "Epoch 7872, Train_Loss: 3806.4873046875, Val_Loss: 3940.97998046875\n",
      "Epoch 7873, Train_Loss: 3806.4736328125, Val_Loss: 3940.651123046875\n",
      "Epoch 7874, Train_Loss: 3806.514892578125, Val_Loss: 3940.786865234375\n",
      "Epoch 7875, Train_Loss: 3806.462158203125, Val_Loss: 3940.81396484375\n",
      "Epoch 7876, Train_Loss: 3806.3427734375, Val_Loss: 3940.70556640625\n",
      "Epoch 7877, Train_Loss: 3806.322509765625, Val_Loss: 3940.780029296875\n",
      "Epoch 7878, Train_Loss: 3806.3837890625, Val_Loss: 3940.71728515625\n",
      "Epoch 7879, Train_Loss: 3806.3056640625, Val_Loss: 3940.613525390625\n",
      "Epoch 7880, Train_Loss: 3806.305908203125, Val_Loss: 3940.95361328125\n",
      "Epoch 7881, Train_Loss: 3806.29296875, Val_Loss: 3940.78955078125\n",
      "Epoch 7882, Train_Loss: 3806.348388671875, Val_Loss: 3940.461181640625\n",
      "Epoch 7883, Train_Loss: 3806.2685546875, Val_Loss: 3940.770751953125\n",
      "Epoch 7884, Train_Loss: 3806.26953125, Val_Loss: 3940.857177734375\n",
      "Epoch 7885, Train_Loss: 3806.245849609375, Val_Loss: 3940.580810546875\n",
      "Epoch 7886, Train_Loss: 3806.306396484375, Val_Loss: 3940.695556640625\n",
      "Epoch 7887, Train_Loss: 3806.2568359375, Val_Loss: 3940.726318359375\n",
      "Epoch 7888, Train_Loss: 3806.257080078125, Val_Loss: 3940.615966796875\n",
      "Epoch 7889, Train_Loss: 3806.206298828125, Val_Loss: 3940.561279296875\n",
      "Epoch 7890, Train_Loss: 3806.220458984375, Val_Loss: 3940.86669921875\n",
      "Epoch 7891, Train_Loss: 3806.25830078125, Val_Loss: 3940.649658203125\n",
      "Epoch 7892, Train_Loss: 3806.355712890625, Val_Loss: 3940.412841796875\n",
      "Epoch 7893, Train_Loss: 3806.185302734375, Val_Loss: 3940.808837890625\n",
      "Epoch 7894, Train_Loss: 3806.22314453125, Val_Loss: 3940.8515625\n",
      "Epoch 7895, Train_Loss: 3806.2822265625, Val_Loss: 3940.4716796875\n",
      "Epoch 7896, Train_Loss: 3806.233642578125, Val_Loss: 3940.554443359375\n",
      "Epoch 7897, Train_Loss: 3806.152587890625, Val_Loss: 3940.65283203125\n",
      "Epoch 7898, Train_Loss: 3806.214599609375, Val_Loss: 3940.552490234375\n",
      "Epoch 7899, Train_Loss: 3806.29638671875, Val_Loss: 3940.463134765625\n",
      "Epoch 7900, Train_Loss: 3806.197265625, Val_Loss: 3940.777099609375\n",
      "Epoch 7901, Train_Loss: 3806.22900390625, Val_Loss: 3940.6640625\n",
      "Epoch 7902, Train_Loss: 3806.260986328125, Val_Loss: 3940.345947265625\n",
      "Epoch 7903, Train_Loss: 3806.17333984375, Val_Loss: 3940.60791015625\n",
      "Epoch 7904, Train_Loss: 3806.1201171875, Val_Loss: 3940.731689453125\n",
      "Epoch 7905, Train_Loss: 3806.1865234375, Val_Loss: 3940.483642578125\n",
      "Epoch 7906, Train_Loss: 3806.323974609375, Val_Loss: 3940.550048828125\n",
      "Epoch 7907, Train_Loss: 3806.3203125, Val_Loss: 3940.54833984375\n",
      "Epoch 7908, Train_Loss: 3806.2568359375, Val_Loss: 3940.472900390625\n",
      "Epoch 7909, Train_Loss: 3806.048095703125, Val_Loss: 3940.6591796875\n",
      "Epoch 7910, Train_Loss: 3806.210205078125, Val_Loss: 3940.498779296875\n",
      "Epoch 7911, Train_Loss: 3806.177001953125, Val_Loss: 3940.291259765625\n",
      "Epoch 7912, Train_Loss: 3806.1845703125, Val_Loss: 3940.62646484375\n",
      "Epoch 7913, Train_Loss: 3806.149658203125, Val_Loss: 3940.643310546875\n",
      "Epoch 7914, Train_Loss: 3806.1591796875, Val_Loss: 3940.323486328125\n",
      "Epoch 7915, Train_Loss: 3806.216796875, Val_Loss: 3940.53369140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7916, Train_Loss: 3806.16015625, Val_Loss: 3940.5712890625\n",
      "Epoch 7917, Train_Loss: 3806.09130859375, Val_Loss: 3940.384033203125\n",
      "Epoch 7918, Train_Loss: 3806.1708984375, Val_Loss: 3940.5419921875\n",
      "Epoch 7919, Train_Loss: 3806.175537109375, Val_Loss: 3940.5\n",
      "Epoch 7920, Train_Loss: 3806.0927734375, Val_Loss: 3940.348388671875\n",
      "Epoch 7921, Train_Loss: 3806.13720703125, Val_Loss: 3940.564453125\n",
      "Epoch 7922, Train_Loss: 3806.14208984375, Val_Loss: 3940.43115234375\n",
      "Epoch 7923, Train_Loss: 3806.05029296875, Val_Loss: 3940.25244140625\n",
      "Epoch 7924, Train_Loss: 3806.09228515625, Val_Loss: 3940.5380859375\n",
      "Epoch 7925, Train_Loss: 3806.158935546875, Val_Loss: 3940.452880859375\n",
      "Epoch 7926, Train_Loss: 3806.050048828125, Val_Loss: 3940.242431640625\n",
      "Epoch 7927, Train_Loss: 3806.1455078125, Val_Loss: 3940.507080078125\n",
      "Epoch 7928, Train_Loss: 3806.11279296875, Val_Loss: 3940.539306640625\n",
      "Epoch 7929, Train_Loss: 3805.937255859375, Val_Loss: 3940.24755859375\n",
      "Epoch 7930, Train_Loss: 3806.0263671875, Val_Loss: 3940.423583984375\n",
      "Epoch 7931, Train_Loss: 3806.03515625, Val_Loss: 3940.44921875\n",
      "Epoch 7932, Train_Loss: 3806.13671875, Val_Loss: 3940.26318359375\n",
      "Epoch 7933, Train_Loss: 3806.193359375, Val_Loss: 3940.463134765625\n",
      "Epoch 7934, Train_Loss: 3806.122802734375, Val_Loss: 3940.416748046875\n",
      "Epoch 7935, Train_Loss: 3806.085205078125, Val_Loss: 3940.268310546875\n",
      "Epoch 7936, Train_Loss: 3806.1533203125, Val_Loss: 3940.4453125\n",
      "Epoch 7937, Train_Loss: 3806.072998046875, Val_Loss: 3940.343994140625\n",
      "Epoch 7938, Train_Loss: 3806.1240234375, Val_Loss: 3940.1552734375\n",
      "Epoch 7939, Train_Loss: 3806.13427734375, Val_Loss: 3940.461181640625\n",
      "Epoch 7940, Train_Loss: 3806.144287109375, Val_Loss: 3940.42529296875\n",
      "Epoch 7941, Train_Loss: 3806.139892578125, Val_Loss: 3940.165283203125\n",
      "Epoch 7942, Train_Loss: 3806.12939453125, Val_Loss: 3940.339599609375\n",
      "Epoch 7943, Train_Loss: 3806.140380859375, Val_Loss: 3940.385498046875\n",
      "Epoch 7944, Train_Loss: 3806.058349609375, Val_Loss: 3940.182373046875\n",
      "Epoch 7945, Train_Loss: 3806.09765625, Val_Loss: 3940.35205078125\n",
      "Epoch 7946, Train_Loss: 3806.053955078125, Val_Loss: 3940.30078125\n",
      "Epoch 7947, Train_Loss: 3806.037109375, Val_Loss: 3940.194091796875\n",
      "Epoch 7948, Train_Loss: 3806.064208984375, Val_Loss: 3940.439208984375\n",
      "Epoch 7949, Train_Loss: 3806.030517578125, Val_Loss: 3940.244384765625\n",
      "Epoch 7950, Train_Loss: 3806.02587890625, Val_Loss: 3940.0771484375\n",
      "Epoch 7951, Train_Loss: 3806.105712890625, Val_Loss: 3940.318115234375\n",
      "Epoch 7952, Train_Loss: 3806.0908203125, Val_Loss: 3940.3095703125\n",
      "Epoch 7953, Train_Loss: 3806.087890625, Val_Loss: 3940.076416015625\n",
      "Epoch 7954, Train_Loss: 3806.101806640625, Val_Loss: 3940.272705078125\n",
      "Epoch 7955, Train_Loss: 3806.087890625, Val_Loss: 3940.279541015625\n",
      "Epoch 7956, Train_Loss: 3805.67724609375, Val_Loss: 3940.0947265625\n",
      "Epoch 7957, Train_Loss: 3805.79296875, Val_Loss: 3940.275146484375\n",
      "Epoch 7958, Train_Loss: 3805.721435546875, Val_Loss: 3940.27587890625\n",
      "Epoch 7959, Train_Loss: 3805.79736328125, Val_Loss: 3940.047119140625\n",
      "Epoch 7960, Train_Loss: 3805.811279296875, Val_Loss: 3940.31396484375\n",
      "Epoch 7961, Train_Loss: 3805.8291015625, Val_Loss: 3940.27587890625\n",
      "Epoch 7962, Train_Loss: 3805.725830078125, Val_Loss: 3940.05810546875\n",
      "Epoch 7963, Train_Loss: 3805.794677734375, Val_Loss: 3940.206787109375\n",
      "Epoch 7964, Train_Loss: 3805.794677734375, Val_Loss: 3940.214111328125\n",
      "Epoch 7965, Train_Loss: 3805.677978515625, Val_Loss: 3940.0068359375\n",
      "Epoch 7966, Train_Loss: 3805.711669921875, Val_Loss: 3940.22509765625\n",
      "Epoch 7967, Train_Loss: 3805.6630859375, Val_Loss: 3940.19189453125\n",
      "Epoch 7968, Train_Loss: 3805.6123046875, Val_Loss: 3940.048095703125\n",
      "Epoch 7969, Train_Loss: 3805.667236328125, Val_Loss: 3940.262451171875\n",
      "Epoch 7970, Train_Loss: 3805.667236328125, Val_Loss: 3940.1591796875\n",
      "Epoch 7971, Train_Loss: 3805.646240234375, Val_Loss: 3939.958740234375\n",
      "Epoch 7972, Train_Loss: 3805.715087890625, Val_Loss: 3940.242431640625\n",
      "Epoch 7973, Train_Loss: 3805.73486328125, Val_Loss: 3940.181640625\n",
      "Epoch 7974, Train_Loss: 3805.622314453125, Val_Loss: 3939.927978515625\n",
      "Epoch 7975, Train_Loss: 3805.682861328125, Val_Loss: 3940.17529296875\n",
      "Epoch 7976, Train_Loss: 3805.66796875, Val_Loss: 3940.10400390625\n",
      "Epoch 7977, Train_Loss: 3805.59619140625, Val_Loss: 3939.906494140625\n",
      "Epoch 7978, Train_Loss: 3805.589599609375, Val_Loss: 3940.094482421875\n",
      "Epoch 7979, Train_Loss: 3805.571533203125, Val_Loss: 3940.08349609375\n",
      "Epoch 7980, Train_Loss: 3805.568115234375, Val_Loss: 3939.932861328125\n",
      "Epoch 7981, Train_Loss: 3805.63330078125, Val_Loss: 3940.142333984375\n",
      "Epoch 7982, Train_Loss: 3805.59765625, Val_Loss: 3940.134521484375\n",
      "Epoch 7983, Train_Loss: 3805.429443359375, Val_Loss: 3939.8828125\n",
      "Epoch 7984, Train_Loss: 3805.4375, Val_Loss: 3940.137939453125\n",
      "Epoch 7985, Train_Loss: 3805.4443359375, Val_Loss: 3940.129150390625\n",
      "Epoch 7986, Train_Loss: 3805.405029296875, Val_Loss: 3939.9033203125\n",
      "Epoch 7987, Train_Loss: 3805.4248046875, Val_Loss: 3940.0927734375\n",
      "Epoch 7988, Train_Loss: 3805.424072265625, Val_Loss: 3940.08251953125\n",
      "Epoch 7989, Train_Loss: 3805.382080078125, Val_Loss: 3939.896728515625\n",
      "Epoch 7990, Train_Loss: 3805.363525390625, Val_Loss: 3940.04931640625\n",
      "Epoch 7991, Train_Loss: 3805.370849609375, Val_Loss: 3940.0\n",
      "Epoch 7992, Train_Loss: 3805.358642578125, Val_Loss: 3939.78564453125\n",
      "Epoch 7993, Train_Loss: 3805.34765625, Val_Loss: 3940.067138671875\n",
      "Epoch 7994, Train_Loss: 3805.3662109375, Val_Loss: 3940.04150390625\n",
      "Epoch 7995, Train_Loss: 3805.32861328125, Val_Loss: 3939.830078125\n",
      "Epoch 7996, Train_Loss: 3805.328857421875, Val_Loss: 3940.00830078125\n",
      "Epoch 7997, Train_Loss: 3805.324951171875, Val_Loss: 3940.007080078125\n",
      "Epoch 7998, Train_Loss: 3805.26953125, Val_Loss: 3939.818359375\n",
      "Epoch 7999, Train_Loss: 3805.302490234375, Val_Loss: 3940.0615234375\n",
      "Epoch 8000, Train_Loss: 3805.338134765625, Val_Loss: 3940.012451171875\n",
      "Epoch 8001, Train_Loss: 3805.27783203125, Val_Loss: 3939.782470703125\n",
      "Epoch 8002, Train_Loss: 3805.316650390625, Val_Loss: 3940.021484375\n",
      "Epoch 8003, Train_Loss: 3805.309326171875, Val_Loss: 3939.947998046875\n",
      "Epoch 8004, Train_Loss: 3805.25634765625, Val_Loss: 3939.726806640625\n",
      "Epoch 8005, Train_Loss: 3805.299560546875, Val_Loss: 3939.950439453125\n",
      "Epoch 8006, Train_Loss: 3805.30224609375, Val_Loss: 3939.90234375\n",
      "Epoch 8007, Train_Loss: 3805.30029296875, Val_Loss: 3939.756103515625\n",
      "Epoch 8008, Train_Loss: 3805.284912109375, Val_Loss: 3940.009521484375\n",
      "Epoch 8009, Train_Loss: 3805.3212890625, Val_Loss: 3939.939697265625\n",
      "Epoch 8010, Train_Loss: 3805.31005859375, Val_Loss: 3939.70751953125\n",
      "Epoch 8011, Train_Loss: 3805.244140625, Val_Loss: 3939.959228515625\n",
      "Epoch 8012, Train_Loss: 3805.2421875, Val_Loss: 3939.9580078125\n",
      "Epoch 8013, Train_Loss: 3805.304931640625, Val_Loss: 3939.706787109375\n",
      "Epoch 8014, Train_Loss: 3805.281982421875, Val_Loss: 3939.934326171875\n",
      "Epoch 8015, Train_Loss: 3805.2705078125, Val_Loss: 3939.912353515625\n",
      "Epoch 8016, Train_Loss: 3805.27880859375, Val_Loss: 3939.72412109375\n",
      "Epoch 8017, Train_Loss: 3805.194580078125, Val_Loss: 3939.88037109375\n",
      "Epoch 8018, Train_Loss: 3805.12255859375, Val_Loss: 3939.810791015625\n",
      "Epoch 8019, Train_Loss: 3805.25927734375, Val_Loss: 3939.6044921875\n",
      "Epoch 8020, Train_Loss: 3805.169921875, Val_Loss: 3939.912353515625\n",
      "Epoch 8021, Train_Loss: 3805.22412109375, Val_Loss: 3939.9072265625\n",
      "Epoch 8022, Train_Loss: 3805.239013671875, Val_Loss: 3939.65478515625\n",
      "Epoch 8023, Train_Loss: 3805.170166015625, Val_Loss: 3939.84765625\n",
      "Epoch 8024, Train_Loss: 3805.213134765625, Val_Loss: 3939.823974609375\n",
      "Epoch 8025, Train_Loss: 3805.177490234375, Val_Loss: 3939.66796875\n",
      "Epoch 8026, Train_Loss: 3805.111572265625, Val_Loss: 3939.91748046875\n",
      "Epoch 8027, Train_Loss: 3805.1513671875, Val_Loss: 3939.794921875\n",
      "Epoch 8028, Train_Loss: 3805.18310546875, Val_Loss: 3939.546875\n",
      "Epoch 8029, Train_Loss: 3805.12060546875, Val_Loss: 3939.8779296875\n",
      "Epoch 8030, Train_Loss: 3805.13427734375, Val_Loss: 3939.863525390625\n",
      "Epoch 8031, Train_Loss: 3805.203125, Val_Loss: 3939.53955078125\n",
      "Epoch 8032, Train_Loss: 3805.119384765625, Val_Loss: 3939.67236328125\n",
      "Epoch 8033, Train_Loss: 3805.12939453125, Val_Loss: 3939.713623046875\n",
      "Epoch 8034, Train_Loss: 3805.12060546875, Val_Loss: 3939.6533203125\n",
      "Epoch 8035, Train_Loss: 3805.131103515625, Val_Loss: 3939.62353515625\n",
      "Epoch 8036, Train_Loss: 3805.12890625, Val_Loss: 3940.015625\n",
      "Epoch 8037, Train_Loss: 3805.007568359375, Val_Loss: 3939.603271484375\n",
      "Epoch 8038, Train_Loss: 3805.07568359375, Val_Loss: 3939.552001953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8039, Train_Loss: 3805.100341796875, Val_Loss: 3939.9287109375\n",
      "Epoch 8040, Train_Loss: 3805.1943359375, Val_Loss: 3939.685546875\n",
      "Epoch 8041, Train_Loss: 3805.23583984375, Val_Loss: 3939.4091796875\n",
      "Epoch 8042, Train_Loss: 3805.252685546875, Val_Loss: 3939.810791015625\n",
      "Epoch 8043, Train_Loss: 3805.29638671875, Val_Loss: 3939.8935546875\n",
      "Epoch 8044, Train_Loss: 3805.190673828125, Val_Loss: 3939.37353515625\n",
      "Epoch 8045, Train_Loss: 3805.191650390625, Val_Loss: 3939.51611328125\n",
      "Epoch 8046, Train_Loss: 3805.169189453125, Val_Loss: 3939.71435546875\n",
      "Epoch 8047, Train_Loss: 3805.180908203125, Val_Loss: 3939.67529296875\n",
      "Epoch 8048, Train_Loss: 3805.160400390625, Val_Loss: 3939.47412109375\n",
      "Epoch 8049, Train_Loss: 3805.1953125, Val_Loss: 3939.746337890625\n",
      "Epoch 8050, Train_Loss: 3805.159423828125, Val_Loss: 3939.713623046875\n",
      "Epoch 8051, Train_Loss: 3805.1826171875, Val_Loss: 3939.496337890625\n",
      "Epoch 8052, Train_Loss: 3805.173583984375, Val_Loss: 3939.722412109375\n",
      "Epoch 8053, Train_Loss: 3805.14794921875, Val_Loss: 3939.63232421875\n",
      "Epoch 8054, Train_Loss: 3805.271240234375, Val_Loss: 3939.41748046875\n",
      "Epoch 8055, Train_Loss: 3805.1640625, Val_Loss: 3939.706787109375\n",
      "Epoch 8056, Train_Loss: 3805.138916015625, Val_Loss: 3939.664306640625\n",
      "Epoch 8057, Train_Loss: 3805.2802734375, Val_Loss: 3939.39208984375\n",
      "Epoch 8058, Train_Loss: 3805.1123046875, Val_Loss: 3939.6396484375\n",
      "Epoch 8059, Train_Loss: 3805.11865234375, Val_Loss: 3939.547119140625\n",
      "Epoch 8060, Train_Loss: 3805.236572265625, Val_Loss: 3939.36962890625\n",
      "Epoch 8061, Train_Loss: 3805.120361328125, Val_Loss: 3939.6611328125\n",
      "Epoch 8062, Train_Loss: 3805.0986328125, Val_Loss: 3939.63330078125\n",
      "Epoch 8063, Train_Loss: 3805.262451171875, Val_Loss: 3939.4130859375\n",
      "Epoch 8064, Train_Loss: 3804.7880859375, Val_Loss: 3939.63916015625\n",
      "Epoch 8065, Train_Loss: 3804.785400390625, Val_Loss: 3939.556396484375\n",
      "Epoch 8066, Train_Loss: 3804.941162109375, Val_Loss: 3939.3447265625\n",
      "Epoch 8067, Train_Loss: 3804.7587890625, Val_Loss: 3939.638427734375\n",
      "Epoch 8068, Train_Loss: 3804.8505859375, Val_Loss: 3939.600341796875\n",
      "Epoch 8069, Train_Loss: 3804.96337890625, Val_Loss: 3939.3359375\n",
      "Epoch 8070, Train_Loss: 3804.93017578125, Val_Loss: 3939.5556640625\n",
      "Epoch 8071, Train_Loss: 3804.821044921875, Val_Loss: 3939.533203125\n",
      "Epoch 8072, Train_Loss: 3804.955078125, Val_Loss: 3939.3564453125\n",
      "Epoch 8073, Train_Loss: 3804.774658203125, Val_Loss: 3939.5703125\n",
      "Epoch 8074, Train_Loss: 3804.895751953125, Val_Loss: 3939.4560546875\n",
      "Epoch 8075, Train_Loss: 3804.9443359375, Val_Loss: 3939.246337890625\n",
      "Epoch 8076, Train_Loss: 3804.873046875, Val_Loss: 3939.58154296875\n",
      "Epoch 8077, Train_Loss: 3804.8828125, Val_Loss: 3939.56396484375\n",
      "Epoch 8078, Train_Loss: 3804.845458984375, Val_Loss: 3939.275146484375\n",
      "Epoch 8079, Train_Loss: 3804.85400390625, Val_Loss: 3939.49072265625\n",
      "Epoch 8080, Train_Loss: 3804.822265625, Val_Loss: 3939.56640625\n",
      "Epoch 8081, Train_Loss: 3804.90869140625, Val_Loss: 3939.294921875\n",
      "Epoch 8082, Train_Loss: 3804.860107421875, Val_Loss: 3939.512451171875\n",
      "Epoch 8083, Train_Loss: 3804.844482421875, Val_Loss: 3939.4619140625\n",
      "Epoch 8084, Train_Loss: 3804.8447265625, Val_Loss: 3939.2744140625\n",
      "Epoch 8085, Train_Loss: 3804.830810546875, Val_Loss: 3939.465576171875\n",
      "Epoch 8086, Train_Loss: 3804.830078125, Val_Loss: 3939.42724609375\n",
      "Epoch 8087, Train_Loss: 3804.815673828125, Val_Loss: 3939.23193359375\n",
      "Epoch 8088, Train_Loss: 3804.80908203125, Val_Loss: 3939.532470703125\n",
      "Epoch 8089, Train_Loss: 3804.826171875, Val_Loss: 3939.399169921875\n",
      "Epoch 8090, Train_Loss: 3804.822509765625, Val_Loss: 3939.166748046875\n",
      "Epoch 8091, Train_Loss: 3804.669921875, Val_Loss: 3939.47998046875\n",
      "Epoch 8092, Train_Loss: 3804.679443359375, Val_Loss: 3939.452392578125\n",
      "Epoch 8093, Train_Loss: 3804.692138671875, Val_Loss: 3939.166748046875\n",
      "Epoch 8094, Train_Loss: 3804.662109375, Val_Loss: 3939.39208984375\n",
      "Epoch 8095, Train_Loss: 3804.646240234375, Val_Loss: 3939.44970703125\n",
      "Epoch 8096, Train_Loss: 3804.6103515625, Val_Loss: 3939.228759765625\n",
      "Epoch 8097, Train_Loss: 3804.661376953125, Val_Loss: 3939.43310546875\n",
      "Epoch 8098, Train_Loss: 3804.63134765625, Val_Loss: 3939.31201171875\n",
      "Epoch 8099, Train_Loss: 3804.57763671875, Val_Loss: 3939.13232421875\n",
      "Epoch 8100, Train_Loss: 3804.5439453125, Val_Loss: 3939.44287109375\n",
      "Epoch 8101, Train_Loss: 3804.569580078125, Val_Loss: 3939.244873046875\n",
      "Epoch 8102, Train_Loss: 3804.600830078125, Val_Loss: 3939.045166015625\n",
      "Epoch 8103, Train_Loss: 3804.5888671875, Val_Loss: 3939.441162109375\n",
      "Epoch 8104, Train_Loss: 3804.603759765625, Val_Loss: 3939.410888671875\n",
      "Epoch 8105, Train_Loss: 3804.52978515625, Val_Loss: 3939.0703125\n",
      "Epoch 8106, Train_Loss: 3804.55419921875, Val_Loss: 3939.29443359375\n",
      "Epoch 8107, Train_Loss: 3804.55322265625, Val_Loss: 3939.38720703125\n",
      "Epoch 8108, Train_Loss: 3804.512939453125, Val_Loss: 3939.184326171875\n",
      "Epoch 8109, Train_Loss: 3804.544677734375, Val_Loss: 3939.367919921875\n",
      "Epoch 8110, Train_Loss: 3804.53759765625, Val_Loss: 3939.266845703125\n",
      "Epoch 8111, Train_Loss: 3804.5595703125, Val_Loss: 3939.058837890625\n",
      "Epoch 8112, Train_Loss: 3804.50048828125, Val_Loss: 3939.345947265625\n",
      "Epoch 8113, Train_Loss: 3804.550537109375, Val_Loss: 3939.230712890625\n",
      "Epoch 8114, Train_Loss: 3804.557373046875, Val_Loss: 3939.0087890625\n",
      "Epoch 8115, Train_Loss: 3804.510986328125, Val_Loss: 3939.328369140625\n",
      "Epoch 8116, Train_Loss: 3804.5380859375, Val_Loss: 3939.27392578125\n",
      "Epoch 8117, Train_Loss: 3804.46826171875, Val_Loss: 3939.09326171875\n",
      "Epoch 8118, Train_Loss: 3804.539306640625, Val_Loss: 3939.35205078125\n",
      "Epoch 8119, Train_Loss: 3804.446533203125, Val_Loss: 3939.203125\n",
      "Epoch 8120, Train_Loss: 3804.460693359375, Val_Loss: 3938.966796875\n",
      "Epoch 8121, Train_Loss: 3804.482421875, Val_Loss: 3939.318115234375\n",
      "Epoch 8122, Train_Loss: 3804.54296875, Val_Loss: 3939.25634765625\n",
      "Epoch 8123, Train_Loss: 3804.5380859375, Val_Loss: 3939.0048828125\n",
      "Epoch 8124, Train_Loss: 3804.497314453125, Val_Loss: 3939.283203125\n",
      "Epoch 8125, Train_Loss: 3804.491455078125, Val_Loss: 3939.27880859375\n",
      "Epoch 8126, Train_Loss: 3804.344482421875, Val_Loss: 3938.971923828125\n",
      "Epoch 8127, Train_Loss: 3804.41552734375, Val_Loss: 3939.202392578125\n",
      "Epoch 8128, Train_Loss: 3804.439697265625, Val_Loss: 3939.1630859375\n",
      "Epoch 8129, Train_Loss: 3804.36474609375, Val_Loss: 3938.99755859375\n",
      "Epoch 8130, Train_Loss: 3804.403076171875, Val_Loss: 3939.290283203125\n",
      "Epoch 8131, Train_Loss: 3804.435791015625, Val_Loss: 3939.102783203125\n",
      "Epoch 8132, Train_Loss: 3804.327392578125, Val_Loss: 3938.850830078125\n",
      "Epoch 8133, Train_Loss: 3804.38330078125, Val_Loss: 3939.268798828125\n",
      "Epoch 8134, Train_Loss: 3804.397705078125, Val_Loss: 3939.212890625\n",
      "Epoch 8135, Train_Loss: 3804.31298828125, Val_Loss: 3938.896484375\n",
      "Epoch 8136, Train_Loss: 3804.42578125, Val_Loss: 3939.149658203125\n",
      "Epoch 8137, Train_Loss: 3804.3837890625, Val_Loss: 3939.24755859375\n",
      "Epoch 8138, Train_Loss: 3804.33154296875, Val_Loss: 3938.903564453125\n",
      "Epoch 8139, Train_Loss: 3804.351806640625, Val_Loss: 3939.0380859375\n",
      "Epoch 8140, Train_Loss: 3804.3818359375, Val_Loss: 3939.10888671875\n",
      "Epoch 8141, Train_Loss: 3804.3037109375, Val_Loss: 3938.934326171875\n",
      "Epoch 8142, Train_Loss: 3804.283935546875, Val_Loss: 3938.8447265625\n",
      "Epoch 8143, Train_Loss: 3804.365966796875, Val_Loss: 3939.29638671875\n",
      "Epoch 8144, Train_Loss: 3804.326416015625, Val_Loss: 3939.09228515625\n",
      "Epoch 8145, Train_Loss: 3804.260986328125, Val_Loss: 3938.84130859375\n",
      "Epoch 8146, Train_Loss: 3804.214111328125, Val_Loss: 3939.120849609375\n",
      "Epoch 8147, Train_Loss: 3804.24169921875, Val_Loss: 3939.169189453125\n",
      "Epoch 8148, Train_Loss: 3804.16259765625, Val_Loss: 3938.84912109375\n",
      "Epoch 8149, Train_Loss: 3804.212158203125, Val_Loss: 3939.03955078125\n",
      "Epoch 8150, Train_Loss: 3804.41845703125, Val_Loss: 3939.0908203125\n",
      "Epoch 8151, Train_Loss: 3804.390869140625, Val_Loss: 3938.9462890625\n",
      "Epoch 8152, Train_Loss: 3804.35498046875, Val_Loss: 3938.79638671875\n",
      "Epoch 8153, Train_Loss: 3804.43115234375, Val_Loss: 3939.1796875\n",
      "Epoch 8154, Train_Loss: 3804.34326171875, Val_Loss: 3938.9931640625\n",
      "Epoch 8155, Train_Loss: 3804.34326171875, Val_Loss: 3938.7119140625\n",
      "Epoch 8156, Train_Loss: 3804.3896484375, Val_Loss: 3939.067626953125\n",
      "Epoch 8157, Train_Loss: 3804.349853515625, Val_Loss: 3939.123291015625\n",
      "Epoch 8158, Train_Loss: 3804.2998046875, Val_Loss: 3938.763671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8159, Train_Loss: 3804.361083984375, Val_Loss: 3938.981689453125\n",
      "Epoch 8160, Train_Loss: 3804.316162109375, Val_Loss: 3939.058837890625\n",
      "Epoch 8161, Train_Loss: 3804.24072265625, Val_Loss: 3938.837646484375\n",
      "Epoch 8162, Train_Loss: 3804.320556640625, Val_Loss: 3939.0576171875\n",
      "Epoch 8163, Train_Loss: 3804.290771484375, Val_Loss: 3938.916748046875\n",
      "Epoch 8164, Train_Loss: 3804.281982421875, Val_Loss: 3938.710693359375\n",
      "Epoch 8165, Train_Loss: 3804.316162109375, Val_Loss: 3939.085205078125\n",
      "Epoch 8166, Train_Loss: 3804.309326171875, Val_Loss: 3939.046875\n",
      "Epoch 8167, Train_Loss: 3804.31884765625, Val_Loss: 3938.63037109375\n",
      "Epoch 8168, Train_Loss: 3804.3173828125, Val_Loss: 3938.8916015625\n",
      "Epoch 8169, Train_Loss: 3804.306884765625, Val_Loss: 3939.08447265625\n",
      "Epoch 8170, Train_Loss: 3804.2421875, Val_Loss: 3938.728759765625\n",
      "Epoch 8171, Train_Loss: 3804.31298828125, Val_Loss: 3938.8720703125\n",
      "Epoch 8172, Train_Loss: 3804.31396484375, Val_Loss: 3938.922119140625\n",
      "Epoch 8173, Train_Loss: 3804.274169921875, Val_Loss: 3938.8447265625\n",
      "Epoch 8174, Train_Loss: 3804.016357421875, Val_Loss: 3938.68359375\n",
      "Epoch 8175, Train_Loss: 3803.989990234375, Val_Loss: 3939.07080078125\n",
      "Epoch 8176, Train_Loss: 3804.021728515625, Val_Loss: 3938.910888671875\n",
      "Epoch 8177, Train_Loss: 3804.062255859375, Val_Loss: 3938.647216796875\n",
      "Epoch 8178, Train_Loss: 3804.001220703125, Val_Loss: 3938.93994140625\n",
      "Epoch 8179, Train_Loss: 3803.99951171875, Val_Loss: 3938.94921875\n",
      "Epoch 8180, Train_Loss: 3804.024169921875, Val_Loss: 3938.602294921875\n",
      "Epoch 8181, Train_Loss: 3804.000244140625, Val_Loss: 3938.8515625\n",
      "Epoch 8182, Train_Loss: 3803.97021484375, Val_Loss: 3938.890380859375\n",
      "Epoch 8183, Train_Loss: 3804.003173828125, Val_Loss: 3938.615234375\n",
      "Epoch 8184, Train_Loss: 3803.998291015625, Val_Loss: 3938.8828125\n",
      "Epoch 8185, Train_Loss: 3803.978515625, Val_Loss: 3938.900390625\n",
      "Epoch 8186, Train_Loss: 3803.95703125, Val_Loss: 3938.659912109375\n",
      "Epoch 8187, Train_Loss: 3803.9296875, Val_Loss: 3938.917236328125\n",
      "Epoch 8188, Train_Loss: 3803.9423828125, Val_Loss: 3938.793701171875\n",
      "Epoch 8189, Train_Loss: 3803.977783203125, Val_Loss: 3938.595947265625\n",
      "Epoch 8190, Train_Loss: 3803.96240234375, Val_Loss: 3938.919189453125\n",
      "Epoch 8191, Train_Loss: 3803.99560546875, Val_Loss: 3938.8408203125\n",
      "Epoch 8192, Train_Loss: 3803.947998046875, Val_Loss: 3938.627197265625\n",
      "Epoch 8193, Train_Loss: 3803.977783203125, Val_Loss: 3938.90087890625\n",
      "Epoch 8194, Train_Loss: 3803.928955078125, Val_Loss: 3938.73193359375\n",
      "Epoch 8195, Train_Loss: 3803.955078125, Val_Loss: 3938.52685546875\n",
      "Epoch 8196, Train_Loss: 3803.873779296875, Val_Loss: 3938.888916015625\n",
      "Epoch 8197, Train_Loss: 3803.990478515625, Val_Loss: 3938.77001953125\n",
      "Epoch 8198, Train_Loss: 3803.942138671875, Val_Loss: 3938.55029296875\n",
      "Epoch 8199, Train_Loss: 3803.921875, Val_Loss: 3938.86962890625\n",
      "Epoch 8200, Train_Loss: 3803.97216796875, Val_Loss: 3938.748779296875\n",
      "Epoch 8201, Train_Loss: 3803.75830078125, Val_Loss: 3938.56005859375\n",
      "Epoch 8202, Train_Loss: 3803.7939453125, Val_Loss: 3938.88916015625\n",
      "Epoch 8203, Train_Loss: 3803.775390625, Val_Loss: 3938.757080078125\n",
      "Epoch 8204, Train_Loss: 3803.78955078125, Val_Loss: 3938.511962890625\n",
      "Epoch 8205, Train_Loss: 3803.7763671875, Val_Loss: 3938.830322265625\n",
      "Epoch 8206, Train_Loss: 3803.818115234375, Val_Loss: 3938.75439453125\n",
      "Epoch 8207, Train_Loss: 3803.73779296875, Val_Loss: 3938.54931640625\n",
      "Epoch 8208, Train_Loss: 3803.75341796875, Val_Loss: 3938.782470703125\n",
      "Epoch 8209, Train_Loss: 3803.722900390625, Val_Loss: 3938.66748046875\n",
      "Epoch 8210, Train_Loss: 3803.71142578125, Val_Loss: 3938.478515625\n",
      "Epoch 8211, Train_Loss: 3803.74560546875, Val_Loss: 3938.818359375\n",
      "Epoch 8212, Train_Loss: 3803.70458984375, Val_Loss: 3938.70068359375\n",
      "Epoch 8213, Train_Loss: 3803.82958984375, Val_Loss: 3938.471923828125\n",
      "Epoch 8214, Train_Loss: 3803.693115234375, Val_Loss: 3938.8046875\n",
      "Epoch 8215, Train_Loss: 3803.69189453125, Val_Loss: 3938.6630859375\n",
      "Epoch 8216, Train_Loss: 3803.66357421875, Val_Loss: 3938.48193359375\n",
      "Epoch 8217, Train_Loss: 3803.682861328125, Val_Loss: 3938.790771484375\n",
      "Epoch 8218, Train_Loss: 3803.69189453125, Val_Loss: 3938.646484375\n",
      "Epoch 8219, Train_Loss: 3803.812744140625, Val_Loss: 3938.43359375\n",
      "Epoch 8220, Train_Loss: 3803.689208984375, Val_Loss: 3938.767578125\n",
      "Epoch 8221, Train_Loss: 3803.67724609375, Val_Loss: 3938.66748046875\n",
      "Epoch 8222, Train_Loss: 3803.805908203125, Val_Loss: 3938.34326171875\n",
      "Epoch 8223, Train_Loss: 3803.72216796875, Val_Loss: 3938.651123046875\n",
      "Epoch 8224, Train_Loss: 3803.660888671875, Val_Loss: 3938.605224609375\n",
      "Epoch 8225, Train_Loss: 3803.7978515625, Val_Loss: 3938.388916015625\n",
      "Epoch 8226, Train_Loss: 3803.72021484375, Val_Loss: 3938.69189453125\n",
      "Epoch 8227, Train_Loss: 3803.69677734375, Val_Loss: 3938.62548828125\n",
      "Epoch 8228, Train_Loss: 3803.72216796875, Val_Loss: 3938.406005859375\n",
      "Epoch 8229, Train_Loss: 3803.607177734375, Val_Loss: 3938.728515625\n",
      "Epoch 8230, Train_Loss: 3803.6181640625, Val_Loss: 3938.6015625\n",
      "Epoch 8231, Train_Loss: 3803.7431640625, Val_Loss: 3938.370361328125\n",
      "Epoch 8232, Train_Loss: 3803.703369140625, Val_Loss: 3938.677490234375\n",
      "Epoch 8233, Train_Loss: 3803.638916015625, Val_Loss: 3938.6171875\n",
      "Epoch 8234, Train_Loss: 3803.83203125, Val_Loss: 3938.337158203125\n",
      "Epoch 8235, Train_Loss: 3803.56103515625, Val_Loss: 3938.56201171875\n",
      "Epoch 8236, Train_Loss: 3803.615478515625, Val_Loss: 3938.56396484375\n",
      "Epoch 8237, Train_Loss: 3803.678955078125, Val_Loss: 3938.302490234375\n",
      "Epoch 8238, Train_Loss: 3803.67919921875, Val_Loss: 3938.554443359375\n",
      "Epoch 8239, Train_Loss: 3803.680908203125, Val_Loss: 3938.555908203125\n",
      "Epoch 8240, Train_Loss: 3803.670166015625, Val_Loss: 3938.322509765625\n",
      "Epoch 8241, Train_Loss: 3803.56640625, Val_Loss: 3938.6875\n",
      "Epoch 8242, Train_Loss: 3803.64208984375, Val_Loss: 3938.513671875\n",
      "Epoch 8243, Train_Loss: 3803.720458984375, Val_Loss: 3938.277099609375\n",
      "Epoch 8244, Train_Loss: 3803.693603515625, Val_Loss: 3938.572021484375\n",
      "Epoch 8245, Train_Loss: 3803.64990234375, Val_Loss: 3938.5546875\n",
      "Epoch 8246, Train_Loss: 3803.7138671875, Val_Loss: 3938.264892578125\n",
      "Epoch 8247, Train_Loss: 3803.64697265625, Val_Loss: 3938.5419921875\n",
      "Epoch 8248, Train_Loss: 3803.65576171875, Val_Loss: 3938.541259765625\n",
      "Epoch 8249, Train_Loss: 3803.639892578125, Val_Loss: 3938.221923828125\n",
      "Epoch 8250, Train_Loss: 3803.608154296875, Val_Loss: 3938.50830078125\n",
      "Epoch 8251, Train_Loss: 3803.629150390625, Val_Loss: 3938.446044921875\n",
      "Epoch 8252, Train_Loss: 3803.62451171875, Val_Loss: 3938.255615234375\n",
      "Epoch 8253, Train_Loss: 3803.648193359375, Val_Loss: 3938.58154296875\n",
      "Epoch 8254, Train_Loss: 3803.628662109375, Val_Loss: 3938.4755859375\n",
      "Epoch 8255, Train_Loss: 3803.627685546875, Val_Loss: 3938.238037109375\n",
      "Epoch 8256, Train_Loss: 3803.563720703125, Val_Loss: 3938.52685546875\n",
      "Epoch 8257, Train_Loss: 3803.525390625, Val_Loss: 3938.4619140625\n",
      "Epoch 8258, Train_Loss: 3803.546875, Val_Loss: 3938.18115234375\n",
      "Epoch 8259, Train_Loss: 3803.72607421875, Val_Loss: 3938.467529296875\n",
      "Epoch 8260, Train_Loss: 3803.7431640625, Val_Loss: 3938.457275390625\n",
      "Epoch 8261, Train_Loss: 3803.7421875, Val_Loss: 3938.200439453125\n",
      "Epoch 8262, Train_Loss: 3803.7236328125, Val_Loss: 3938.46044921875\n",
      "Epoch 8263, Train_Loss: 3803.697265625, Val_Loss: 3938.372802734375\n",
      "Epoch 8264, Train_Loss: 3803.706787109375, Val_Loss: 3938.1416015625\n",
      "Epoch 8265, Train_Loss: 3803.677734375, Val_Loss: 3938.4462890625\n",
      "Epoch 8266, Train_Loss: 3803.702880859375, Val_Loss: 3938.41650390625\n",
      "Epoch 8267, Train_Loss: 3803.72216796875, Val_Loss: 3938.12841796875\n",
      "Epoch 8268, Train_Loss: 3803.677734375, Val_Loss: 3938.4404296875\n",
      "Epoch 8269, Train_Loss: 3803.690185546875, Val_Loss: 3938.4111328125\n",
      "Epoch 8270, Train_Loss: 3803.692138671875, Val_Loss: 3938.130859375\n",
      "Epoch 8271, Train_Loss: 3803.599609375, Val_Loss: 3938.42724609375\n",
      "Epoch 8272, Train_Loss: 3803.616455078125, Val_Loss: 3938.4140625\n",
      "Epoch 8273, Train_Loss: 3803.693115234375, Val_Loss: 3938.12646484375\n",
      "Epoch 8274, Train_Loss: 3803.621826171875, Val_Loss: 3938.4033203125\n",
      "Epoch 8275, Train_Loss: 3803.632568359375, Val_Loss: 3938.381591796875\n",
      "Epoch 8276, Train_Loss: 3803.6630859375, Val_Loss: 3938.15234375\n",
      "Epoch 8277, Train_Loss: 3803.578369140625, Val_Loss: 3938.403564453125\n",
      "Epoch 8278, Train_Loss: 3803.600341796875, Val_Loss: 3938.2880859375\n",
      "Epoch 8279, Train_Loss: 3803.616455078125, Val_Loss: 3938.02001953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8280, Train_Loss: 3803.591552734375, Val_Loss: 3938.384521484375\n",
      "Epoch 8281, Train_Loss: 3803.6064453125, Val_Loss: 3938.355712890625\n",
      "Epoch 8282, Train_Loss: 3803.60498046875, Val_Loss: 3938.064453125\n",
      "Epoch 8283, Train_Loss: 3803.62451171875, Val_Loss: 3938.33447265625\n",
      "Epoch 8284, Train_Loss: 3803.314453125, Val_Loss: 3938.306884765625\n",
      "Epoch 8285, Train_Loss: 3803.337158203125, Val_Loss: 3938.061279296875\n",
      "Epoch 8286, Train_Loss: 3803.299072265625, Val_Loss: 3938.345947265625\n",
      "Epoch 8287, Train_Loss: 3803.35595703125, Val_Loss: 3938.3095703125\n",
      "Epoch 8288, Train_Loss: 3803.296630859375, Val_Loss: 3938.0380859375\n",
      "Epoch 8289, Train_Loss: 3803.31884765625, Val_Loss: 3938.349609375\n",
      "Epoch 8290, Train_Loss: 3803.31201171875, Val_Loss: 3938.26611328125\n",
      "Epoch 8291, Train_Loss: 3803.29638671875, Val_Loss: 3937.992431640625\n",
      "Epoch 8292, Train_Loss: 3803.296142578125, Val_Loss: 3938.27392578125\n",
      "Epoch 8293, Train_Loss: 3803.263671875, Val_Loss: 3938.2412109375\n",
      "Epoch 8294, Train_Loss: 3803.23095703125, Val_Loss: 3938.033203125\n",
      "Epoch 8295, Train_Loss: 3803.288330078125, Val_Loss: 3938.31396484375\n",
      "Epoch 8296, Train_Loss: 3803.245361328125, Val_Loss: 3938.273193359375\n",
      "Epoch 8297, Train_Loss: 3803.191162109375, Val_Loss: 3937.954833984375\n",
      "Epoch 8298, Train_Loss: 3803.24609375, Val_Loss: 3938.267578125\n",
      "Epoch 8299, Train_Loss: 3803.250244140625, Val_Loss: 3938.2587890625\n",
      "Epoch 8300, Train_Loss: 3803.169921875, Val_Loss: 3938.009521484375\n",
      "Epoch 8301, Train_Loss: 3803.24853515625, Val_Loss: 3938.250732421875\n",
      "Epoch 8302, Train_Loss: 3803.226806640625, Val_Loss: 3938.219482421875\n",
      "Epoch 8303, Train_Loss: 3803.189208984375, Val_Loss: 3937.982421875\n",
      "Epoch 8304, Train_Loss: 3803.2392578125, Val_Loss: 3938.24072265625\n",
      "Epoch 8305, Train_Loss: 3803.210693359375, Val_Loss: 3938.167236328125\n",
      "Epoch 8306, Train_Loss: 3803.187744140625, Val_Loss: 3937.882080078125\n",
      "Epoch 8307, Train_Loss: 3803.181884765625, Val_Loss: 3938.193115234375\n",
      "Epoch 8308, Train_Loss: 3803.12158203125, Val_Loss: 3938.244384765625\n",
      "Epoch 8309, Train_Loss: 3803.156982421875, Val_Loss: 3937.925537109375\n",
      "Epoch 8310, Train_Loss: 3803.224365234375, Val_Loss: 3938.213623046875\n",
      "Epoch 8311, Train_Loss: 3803.04833984375, Val_Loss: 3938.1796875\n",
      "Epoch 8312, Train_Loss: 3803.01171875, Val_Loss: 3937.91748046875\n",
      "Epoch 8313, Train_Loss: 3803.015625, Val_Loss: 3938.2255859375\n",
      "Epoch 8314, Train_Loss: 3802.972412109375, Val_Loss: 3938.207275390625\n",
      "Epoch 8315, Train_Loss: 3803.00341796875, Val_Loss: 3937.90771484375\n",
      "Epoch 8316, Train_Loss: 3802.96533203125, Val_Loss: 3938.2080078125\n",
      "Epoch 8317, Train_Loss: 3802.969482421875, Val_Loss: 3938.194091796875\n",
      "Epoch 8318, Train_Loss: 3802.95458984375, Val_Loss: 3937.85205078125\n",
      "Epoch 8319, Train_Loss: 3802.931396484375, Val_Loss: 3938.15234375\n",
      "Epoch 8320, Train_Loss: 3802.91650390625, Val_Loss: 3938.0966796875\n",
      "Epoch 8321, Train_Loss: 3802.9423828125, Val_Loss: 3937.84033203125\n",
      "Epoch 8322, Train_Loss: 3802.9365234375, Val_Loss: 3938.16552734375\n",
      "Epoch 8323, Train_Loss: 3802.939208984375, Val_Loss: 3938.129150390625\n",
      "Epoch 8324, Train_Loss: 3802.953125, Val_Loss: 3937.85205078125\n",
      "Epoch 8325, Train_Loss: 3802.90185546875, Val_Loss: 3938.140869140625\n",
      "Epoch 8326, Train_Loss: 3802.882080078125, Val_Loss: 3938.098388671875\n",
      "Epoch 8327, Train_Loss: 3802.9169921875, Val_Loss: 3937.83349609375\n",
      "Epoch 8328, Train_Loss: 3802.877685546875, Val_Loss: 3938.138427734375\n",
      "Epoch 8329, Train_Loss: 3802.88037109375, Val_Loss: 3938.09228515625\n",
      "Epoch 8330, Train_Loss: 3802.91455078125, Val_Loss: 3937.831298828125\n",
      "Epoch 8331, Train_Loss: 3802.865478515625, Val_Loss: 3938.11083984375\n",
      "Epoch 8332, Train_Loss: 3802.8583984375, Val_Loss: 3938.030029296875\n",
      "Epoch 8333, Train_Loss: 3802.876953125, Val_Loss: 3937.789306640625\n",
      "Epoch 8334, Train_Loss: 3802.854736328125, Val_Loss: 3938.033203125\n",
      "Epoch 8335, Train_Loss: 3802.8623046875, Val_Loss: 3938.011962890625\n",
      "Epoch 8336, Train_Loss: 3802.885498046875, Val_Loss: 3937.81884765625\n",
      "Epoch 8337, Train_Loss: 3802.86669921875, Val_Loss: 3938.0966796875\n",
      "Epoch 8338, Train_Loss: 3802.884765625, Val_Loss: 3938.0751953125\n",
      "Epoch 8339, Train_Loss: 3802.90576171875, Val_Loss: 3937.74951171875\n",
      "Epoch 8340, Train_Loss: 3802.809814453125, Val_Loss: 3938.0595703125\n",
      "Epoch 8341, Train_Loss: 3802.8115234375, Val_Loss: 3938.056884765625\n",
      "Epoch 8342, Train_Loss: 3802.903564453125, Val_Loss: 3937.773193359375\n",
      "Epoch 8343, Train_Loss: 3802.870849609375, Val_Loss: 3938.055908203125\n",
      "Epoch 8344, Train_Loss: 3802.855224609375, Val_Loss: 3938.042724609375\n",
      "Epoch 8345, Train_Loss: 3802.9384765625, Val_Loss: 3937.755615234375\n",
      "Epoch 8346, Train_Loss: 3802.768310546875, Val_Loss: 3937.9951171875\n",
      "Epoch 8347, Train_Loss: 3802.76416015625, Val_Loss: 3937.972412109375\n",
      "Epoch 8348, Train_Loss: 3802.847900390625, Val_Loss: 3937.69921875\n",
      "Epoch 8349, Train_Loss: 3802.7626953125, Val_Loss: 3937.98388671875\n",
      "Epoch 8350, Train_Loss: 3802.769287109375, Val_Loss: 3937.97607421875\n",
      "Epoch 8351, Train_Loss: 3802.857177734375, Val_Loss: 3937.718017578125\n",
      "Epoch 8352, Train_Loss: 3802.7421875, Val_Loss: 3938.000732421875\n",
      "Epoch 8353, Train_Loss: 3802.730224609375, Val_Loss: 3937.957275390625\n",
      "Epoch 8354, Train_Loss: 3802.843994140625, Val_Loss: 3937.711181640625\n",
      "Epoch 8355, Train_Loss: 3802.716796875, Val_Loss: 3937.999267578125\n",
      "Epoch 8356, Train_Loss: 3802.71142578125, Val_Loss: 3937.960693359375\n",
      "Epoch 8357, Train_Loss: 3802.840576171875, Val_Loss: 3937.6923828125\n",
      "Epoch 8358, Train_Loss: 3802.722412109375, Val_Loss: 3937.9892578125\n",
      "Epoch 8359, Train_Loss: 3802.71826171875, Val_Loss: 3937.959716796875\n",
      "Epoch 8360, Train_Loss: 3802.783203125, Val_Loss: 3937.640380859375\n",
      "Epoch 8361, Train_Loss: 3802.770263671875, Val_Loss: 3937.87841796875\n",
      "Epoch 8362, Train_Loss: 3802.751708984375, Val_Loss: 3937.889892578125\n",
      "Epoch 8363, Train_Loss: 3802.80126953125, Val_Loss: 3937.63671875\n",
      "Epoch 8364, Train_Loss: 3802.75341796875, Val_Loss: 3937.952880859375\n",
      "Epoch 8365, Train_Loss: 3802.747802734375, Val_Loss: 3937.91357421875\n",
      "Epoch 8366, Train_Loss: 3802.774658203125, Val_Loss: 3937.632080078125\n",
      "Epoch 8367, Train_Loss: 3802.65234375, Val_Loss: 3937.908447265625\n",
      "Epoch 8368, Train_Loss: 3802.65673828125, Val_Loss: 3937.892822265625\n",
      "Epoch 8369, Train_Loss: 3802.674072265625, Val_Loss: 3937.642822265625\n",
      "Epoch 8370, Train_Loss: 3802.65185546875, Val_Loss: 3937.884765625\n",
      "Epoch 8371, Train_Loss: 3802.871337890625, Val_Loss: 3937.89404296875\n",
      "Epoch 8372, Train_Loss: 3802.86962890625, Val_Loss: 3937.622802734375\n",
      "Epoch 8373, Train_Loss: 3802.869384765625, Val_Loss: 3937.899169921875\n",
      "Epoch 8374, Train_Loss: 3802.82421875, Val_Loss: 3937.835205078125\n",
      "Epoch 8375, Train_Loss: 3802.83984375, Val_Loss: 3937.567138671875\n",
      "Epoch 8376, Train_Loss: 3802.82421875, Val_Loss: 3937.820068359375\n",
      "Epoch 8377, Train_Loss: 3802.843994140625, Val_Loss: 3937.79931640625\n",
      "Epoch 8378, Train_Loss: 3802.822509765625, Val_Loss: 3937.6044921875\n",
      "Epoch 8379, Train_Loss: 3802.8232421875, Val_Loss: 3937.864501953125\n",
      "Epoch 8380, Train_Loss: 3802.815673828125, Val_Loss: 3937.866455078125\n",
      "Epoch 8381, Train_Loss: 3802.826904296875, Val_Loss: 3937.55810546875\n",
      "Epoch 8382, Train_Loss: 3802.804931640625, Val_Loss: 3937.826416015625\n",
      "Epoch 8383, Train_Loss: 3802.819580078125, Val_Loss: 3937.81640625\n",
      "Epoch 8384, Train_Loss: 3802.804931640625, Val_Loss: 3937.57275390625\n",
      "Epoch 8385, Train_Loss: 3802.80322265625, Val_Loss: 3937.847900390625\n",
      "Epoch 8386, Train_Loss: 3802.8095703125, Val_Loss: 3937.82958984375\n",
      "Epoch 8387, Train_Loss: 3802.9365234375, Val_Loss: 3937.532470703125\n",
      "Epoch 8388, Train_Loss: 3802.801513671875, Val_Loss: 3937.74609375\n",
      "Epoch 8389, Train_Loss: 3802.797607421875, Val_Loss: 3937.755126953125\n",
      "Epoch 8390, Train_Loss: 3802.9013671875, Val_Loss: 3937.49609375\n",
      "Epoch 8391, Train_Loss: 3802.79296875, Val_Loss: 3937.766357421875\n",
      "Epoch 8392, Train_Loss: 3802.80810546875, Val_Loss: 3937.775634765625\n",
      "Epoch 8393, Train_Loss: 3802.93115234375, Val_Loss: 3937.50634765625\n",
      "Epoch 8394, Train_Loss: 3802.798828125, Val_Loss: 3937.771484375\n",
      "Epoch 8395, Train_Loss: 3802.49658203125, Val_Loss: 3937.743896484375\n",
      "Epoch 8396, Train_Loss: 3802.607421875, Val_Loss: 3937.47998046875\n",
      "Epoch 8397, Train_Loss: 3802.477294921875, Val_Loss: 3937.78271484375\n",
      "Epoch 8398, Train_Loss: 3802.468994140625, Val_Loss: 3937.770751953125\n",
      "Epoch 8399, Train_Loss: 3802.639404296875, Val_Loss: 3937.484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8400, Train_Loss: 3802.511962890625, Val_Loss: 3937.775634765625\n",
      "Epoch 8401, Train_Loss: 3802.525146484375, Val_Loss: 3937.760009765625\n",
      "Epoch 8402, Train_Loss: 3802.596923828125, Val_Loss: 3937.42724609375\n",
      "Epoch 8403, Train_Loss: 3802.4794921875, Val_Loss: 3937.7099609375\n",
      "Epoch 8404, Train_Loss: 3802.48046875, Val_Loss: 3937.6875\n",
      "Epoch 8405, Train_Loss: 3802.573486328125, Val_Loss: 3937.427978515625\n",
      "Epoch 8406, Train_Loss: 3802.476806640625, Val_Loss: 3937.71630859375\n",
      "Epoch 8407, Train_Loss: 3802.498046875, Val_Loss: 3937.706298828125\n",
      "Epoch 8408, Train_Loss: 3802.58447265625, Val_Loss: 3937.445556640625\n",
      "Epoch 8409, Train_Loss: 3802.43603515625, Val_Loss: 3937.730712890625\n",
      "Epoch 8410, Train_Loss: 3802.5625, Val_Loss: 3937.680419921875\n",
      "Epoch 8411, Train_Loss: 3802.52294921875, Val_Loss: 3937.412353515625\n",
      "Epoch 8412, Train_Loss: 3802.55224609375, Val_Loss: 3937.6943359375\n",
      "Epoch 8413, Train_Loss: 3802.5751953125, Val_Loss: 3937.67529296875\n",
      "Epoch 8414, Train_Loss: 3802.5283203125, Val_Loss: 3937.4208984375\n",
      "Epoch 8415, Train_Loss: 3802.563232421875, Val_Loss: 3937.69921875\n",
      "Epoch 8416, Train_Loss: 3802.539794921875, Val_Loss: 3937.595947265625\n",
      "Epoch 8417, Train_Loss: 3802.510986328125, Val_Loss: 3937.330078125\n",
      "Epoch 8418, Train_Loss: 3802.533447265625, Val_Loss: 3937.643310546875\n",
      "Epoch 8419, Train_Loss: 3802.54443359375, Val_Loss: 3937.61328125\n",
      "Epoch 8420, Train_Loss: 3802.5087890625, Val_Loss: 3937.374755859375\n",
      "Epoch 8421, Train_Loss: 3802.5283203125, Val_Loss: 3937.67041015625\n",
      "Epoch 8422, Train_Loss: 3802.5302734375, Val_Loss: 3937.66357421875\n",
      "Epoch 8423, Train_Loss: 3802.346923828125, Val_Loss: 3937.38671875\n",
      "Epoch 8424, Train_Loss: 3802.37744140625, Val_Loss: 3937.65869140625\n",
      "Epoch 8425, Train_Loss: 3802.398681640625, Val_Loss: 3937.603515625\n",
      "Epoch 8426, Train_Loss: 3802.336669921875, Val_Loss: 3937.333984375\n",
      "Epoch 8427, Train_Loss: 3802.3681640625, Val_Loss: 3937.649658203125\n",
      "Epoch 8428, Train_Loss: 3802.377197265625, Val_Loss: 3937.642333984375\n",
      "Epoch 8429, Train_Loss: 3802.323974609375, Val_Loss: 3937.35595703125\n",
      "Epoch 8430, Train_Loss: 3802.332275390625, Val_Loss: 3937.575927734375\n",
      "Epoch 8431, Train_Loss: 3802.34130859375, Val_Loss: 3937.52685546875\n",
      "Epoch 8432, Train_Loss: 3802.302734375, Val_Loss: 3937.26318359375\n",
      "Epoch 8433, Train_Loss: 3802.32763671875, Val_Loss: 3937.58447265625\n",
      "Epoch 8434, Train_Loss: 3802.3203125, Val_Loss: 3937.563232421875\n",
      "Epoch 8435, Train_Loss: 3802.29150390625, Val_Loss: 3937.2939453125\n",
      "Epoch 8436, Train_Loss: 3802.326416015625, Val_Loss: 3937.5888671875\n",
      "Epoch 8437, Train_Loss: 3802.31005859375, Val_Loss: 3937.539306640625\n",
      "Epoch 8438, Train_Loss: 3802.259765625, Val_Loss: 3937.29931640625\n",
      "Epoch 8439, Train_Loss: 3802.302490234375, Val_Loss: 3937.587158203125\n",
      "Epoch 8440, Train_Loss: 3802.2958984375, Val_Loss: 3937.531494140625\n",
      "Epoch 8441, Train_Loss: 3802.288330078125, Val_Loss: 3937.268798828125\n",
      "Epoch 8442, Train_Loss: 3802.31298828125, Val_Loss: 3937.563720703125\n",
      "Epoch 8443, Train_Loss: 3802.2880859375, Val_Loss: 3937.554443359375\n",
      "Epoch 8444, Train_Loss: 3802.25537109375, Val_Loss: 3937.23193359375\n",
      "Epoch 8445, Train_Loss: 3802.288330078125, Val_Loss: 3937.4892578125\n",
      "Epoch 8446, Train_Loss: 3802.22509765625, Val_Loss: 3937.484375\n",
      "Epoch 8447, Train_Loss: 3802.249267578125, Val_Loss: 3937.226806640625\n",
      "Epoch 8448, Train_Loss: 3802.249755859375, Val_Loss: 3937.552001953125\n",
      "Epoch 8449, Train_Loss: 3802.28759765625, Val_Loss: 3937.4794921875\n",
      "Epoch 8450, Train_Loss: 3802.301513671875, Val_Loss: 3937.23046875\n",
      "Epoch 8451, Train_Loss: 3802.20751953125, Val_Loss: 3937.50927734375\n",
      "Epoch 8452, Train_Loss: 3802.170166015625, Val_Loss: 3937.503662109375\n",
      "Epoch 8453, Train_Loss: 3802.221435546875, Val_Loss: 3937.219970703125\n",
      "Epoch 8454, Train_Loss: 3802.20849609375, Val_Loss: 3937.5087890625\n",
      "Epoch 8455, Train_Loss: 3802.310791015625, Val_Loss: 3937.447998046875\n",
      "Epoch 8456, Train_Loss: 3802.260009765625, Val_Loss: 3937.2041015625\n",
      "Epoch 8457, Train_Loss: 3802.2353515625, Val_Loss: 3937.5205078125\n",
      "Epoch 8458, Train_Loss: 3802.208740234375, Val_Loss: 3937.3896484375\n",
      "Epoch 8459, Train_Loss: 3802.175048828125, Val_Loss: 3937.129638671875\n",
      "Epoch 8460, Train_Loss: 3802.145263671875, Val_Loss: 3937.432861328125\n",
      "Epoch 8461, Train_Loss: 3802.188232421875, Val_Loss: 3937.4091796875\n",
      "Epoch 8462, Train_Loss: 3802.173095703125, Val_Loss: 3937.175537109375\n",
      "Epoch 8463, Train_Loss: 3802.154296875, Val_Loss: 3937.46240234375\n",
      "Epoch 8464, Train_Loss: 3802.20849609375, Val_Loss: 3937.421630859375\n",
      "Epoch 8465, Train_Loss: 3802.11865234375, Val_Loss: 3937.18115234375\n",
      "Epoch 8466, Train_Loss: 3802.11865234375, Val_Loss: 3937.47509765625\n",
      "Epoch 8467, Train_Loss: 3802.164306640625, Val_Loss: 3937.419189453125\n",
      "Epoch 8468, Train_Loss: 3802.109619140625, Val_Loss: 3937.1552734375\n",
      "Epoch 8469, Train_Loss: 3802.169921875, Val_Loss: 3937.43115234375\n",
      "Epoch 8470, Train_Loss: 3802.178466796875, Val_Loss: 3937.4208984375\n",
      "Epoch 8471, Train_Loss: 3802.094970703125, Val_Loss: 3937.15771484375\n",
      "Epoch 8472, Train_Loss: 3802.122802734375, Val_Loss: 3937.4599609375\n",
      "Epoch 8473, Train_Loss: 3802.185791015625, Val_Loss: 3937.33837890625\n",
      "Epoch 8474, Train_Loss: 3802.0927734375, Val_Loss: 3937.0791015625\n",
      "Epoch 8475, Train_Loss: 3802.09521484375, Val_Loss: 3937.3984375\n",
      "Epoch 8476, Train_Loss: 3802.150390625, Val_Loss: 3937.388427734375\n",
      "Epoch 8477, Train_Loss: 3802.05859375, Val_Loss: 3937.087158203125\n",
      "Epoch 8478, Train_Loss: 3802.14697265625, Val_Loss: 3937.37451171875\n",
      "Epoch 8479, Train_Loss: 3802.148681640625, Val_Loss: 3937.37646484375\n",
      "Epoch 8480, Train_Loss: 3801.939208984375, Val_Loss: 3937.10888671875\n",
      "Epoch 8481, Train_Loss: 3802.041748046875, Val_Loss: 3937.37451171875\n",
      "Epoch 8482, Train_Loss: 3802.061279296875, Val_Loss: 3937.34033203125\n",
      "Epoch 8483, Train_Loss: 3802.1650390625, Val_Loss: 3937.087646484375\n",
      "Epoch 8484, Train_Loss: 3802.199951171875, Val_Loss: 3937.3916015625\n",
      "Epoch 8485, Train_Loss: 3802.2568359375, Val_Loss: 3937.34716796875\n",
      "Epoch 8486, Train_Loss: 3802.142333984375, Val_Loss: 3937.0732421875\n",
      "Epoch 8487, Train_Loss: 3802.230712890625, Val_Loss: 3937.290283203125\n",
      "Epoch 8488, Train_Loss: 3802.2373046875, Val_Loss: 3937.313232421875\n",
      "Epoch 8489, Train_Loss: 3802.11669921875, Val_Loss: 3937.035888671875\n",
      "Epoch 8490, Train_Loss: 3802.197021484375, Val_Loss: 3937.35205078125\n",
      "Epoch 8491, Train_Loss: 3802.17333984375, Val_Loss: 3937.279296875\n",
      "Epoch 8492, Train_Loss: 3802.11181640625, Val_Loss: 3937.064697265625\n",
      "Epoch 8493, Train_Loss: 3802.162353515625, Val_Loss: 3937.377197265625\n",
      "Epoch 8494, Train_Loss: 3802.15087890625, Val_Loss: 3937.30029296875\n",
      "Epoch 8495, Train_Loss: 3802.094970703125, Val_Loss: 3936.994873046875\n",
      "Epoch 8496, Train_Loss: 3802.168701171875, Val_Loss: 3937.28759765625\n",
      "Epoch 8497, Train_Loss: 3802.161865234375, Val_Loss: 3937.323974609375\n",
      "Epoch 8498, Train_Loss: 3802.103759765625, Val_Loss: 3937.031982421875\n",
      "Epoch 8499, Train_Loss: 3802.15234375, Val_Loss: 3937.2939453125\n",
      "Epoch 8500, Train_Loss: 3802.096923828125, Val_Loss: 3937.247314453125\n",
      "Epoch 8501, Train_Loss: 3802.067626953125, Val_Loss: 3936.947998046875\n",
      "Epoch 8502, Train_Loss: 3802.087646484375, Val_Loss: 3937.30029296875\n",
      "Epoch 8503, Train_Loss: 3802.125244140625, Val_Loss: 3937.219970703125\n",
      "Epoch 8504, Train_Loss: 3802.12548828125, Val_Loss: 3936.90478515625\n",
      "Epoch 8505, Train_Loss: 3802.07666015625, Val_Loss: 3937.224365234375\n",
      "Epoch 8506, Train_Loss: 3802.118408203125, Val_Loss: 3937.277587890625\n",
      "Epoch 8507, Train_Loss: 3802.051513671875, Val_Loss: 3937.025634765625\n",
      "Epoch 8508, Train_Loss: 3801.8369140625, Val_Loss: 3937.22509765625\n",
      "Epoch 8509, Train_Loss: 3801.781005859375, Val_Loss: 3937.191162109375\n",
      "Epoch 8510, Train_Loss: 3801.75830078125, Val_Loss: 3936.977294921875\n",
      "Epoch 8511, Train_Loss: 3801.80322265625, Val_Loss: 3937.29150390625\n",
      "Epoch 8512, Train_Loss: 3801.7939453125, Val_Loss: 3937.228515625\n",
      "Epoch 8513, Train_Loss: 3801.89453125, Val_Loss: 3936.93115234375\n",
      "Epoch 8514, Train_Loss: 3801.8515625, Val_Loss: 3937.242431640625\n",
      "Epoch 8515, Train_Loss: 3801.8310546875, Val_Loss: 3937.174072265625\n",
      "Epoch 8516, Train_Loss: 3801.802734375, Val_Loss: 3936.8984375\n",
      "Epoch 8517, Train_Loss: 3801.825927734375, Val_Loss: 3937.1728515625\n",
      "Epoch 8518, Train_Loss: 3801.75048828125, Val_Loss: 3937.13037109375\n",
      "Epoch 8519, Train_Loss: 3801.817626953125, Val_Loss: 3936.92919921875\n",
      "Epoch 8520, Train_Loss: 3801.803955078125, Val_Loss: 3937.24072265625\n",
      "Epoch 8521, Train_Loss: 3801.755859375, Val_Loss: 3937.193603515625\n",
      "Epoch 8522, Train_Loss: 3801.847412109375, Val_Loss: 3936.867919921875\n",
      "Epoch 8523, Train_Loss: 3801.787109375, Val_Loss: 3937.181640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8524, Train_Loss: 3801.71435546875, Val_Loss: 3937.17431640625\n",
      "Epoch 8525, Train_Loss: 3801.81494140625, Val_Loss: 3936.8896484375\n",
      "Epoch 8526, Train_Loss: 3801.790771484375, Val_Loss: 3937.162353515625\n",
      "Epoch 8527, Train_Loss: 3801.742919921875, Val_Loss: 3937.146484375\n",
      "Epoch 8528, Train_Loss: 3801.826171875, Val_Loss: 3936.88720703125\n",
      "Epoch 8529, Train_Loss: 3801.787841796875, Val_Loss: 3937.18603515625\n",
      "Epoch 8530, Train_Loss: 3801.72607421875, Val_Loss: 3937.077880859375\n",
      "Epoch 8531, Train_Loss: 3801.798583984375, Val_Loss: 3936.8359375\n",
      "Epoch 8532, Train_Loss: 3801.756103515625, Val_Loss: 3937.129150390625\n",
      "Epoch 8533, Train_Loss: 3801.693603515625, Val_Loss: 3937.140380859375\n",
      "Epoch 8534, Train_Loss: 3801.79443359375, Val_Loss: 3936.84326171875\n",
      "Epoch 8535, Train_Loss: 3801.76025390625, Val_Loss: 3937.15283203125\n",
      "Epoch 8536, Train_Loss: 3801.69482421875, Val_Loss: 3937.152099609375\n",
      "Epoch 8537, Train_Loss: 3801.650634765625, Val_Loss: 3936.830810546875\n",
      "Epoch 8538, Train_Loss: 3801.608154296875, Val_Loss: 3937.139892578125\n",
      "Epoch 8539, Train_Loss: 3801.61962890625, Val_Loss: 3937.10400390625\n",
      "Epoch 8540, Train_Loss: 3801.647705078125, Val_Loss: 3936.837890625\n",
      "Epoch 8541, Train_Loss: 3801.533203125, Val_Loss: 3937.14599609375\n",
      "Epoch 8542, Train_Loss: 3801.61474609375, Val_Loss: 3937.0927734375\n",
      "Epoch 8543, Train_Loss: 3801.63427734375, Val_Loss: 3936.8203125\n",
      "Epoch 8544, Train_Loss: 3801.55810546875, Val_Loss: 3937.0791015625\n",
      "Epoch 8545, Train_Loss: 3801.598876953125, Val_Loss: 3937.018798828125\n",
      "Epoch 8546, Train_Loss: 3801.580810546875, Val_Loss: 3936.777099609375\n",
      "Epoch 8547, Train_Loss: 3801.57568359375, Val_Loss: 3937.107177734375\n",
      "Epoch 8548, Train_Loss: 3801.579833984375, Val_Loss: 3937.06005859375\n",
      "Epoch 8549, Train_Loss: 3801.595458984375, Val_Loss: 3936.797607421875\n",
      "Epoch 8550, Train_Loss: 3801.572021484375, Val_Loss: 3937.073486328125\n",
      "Epoch 8551, Train_Loss: 3801.54638671875, Val_Loss: 3937.03076171875\n",
      "Epoch 8552, Train_Loss: 3801.576904296875, Val_Loss: 3936.77490234375\n",
      "Epoch 8553, Train_Loss: 3801.5439453125, Val_Loss: 3937.073486328125\n",
      "Epoch 8554, Train_Loss: 3801.56103515625, Val_Loss: 3937.031494140625\n",
      "Epoch 8555, Train_Loss: 3801.563720703125, Val_Loss: 3936.7587890625\n",
      "Epoch 8556, Train_Loss: 3801.543212890625, Val_Loss: 3937.058837890625\n",
      "Epoch 8557, Train_Loss: 3801.554443359375, Val_Loss: 3937.052001953125\n",
      "Epoch 8558, Train_Loss: 3801.542724609375, Val_Loss: 3936.706787109375\n",
      "Epoch 8559, Train_Loss: 3801.529296875, Val_Loss: 3936.994384765625\n",
      "Epoch 8560, Train_Loss: 3801.53125, Val_Loss: 3936.9619140625\n",
      "Epoch 8561, Train_Loss: 3801.657470703125, Val_Loss: 3936.694091796875\n",
      "Epoch 8562, Train_Loss: 3801.5283203125, Val_Loss: 3937.0380859375\n",
      "Epoch 8563, Train_Loss: 3801.53369140625, Val_Loss: 3937.007568359375\n",
      "Epoch 8564, Train_Loss: 3801.572998046875, Val_Loss: 3936.7216796875\n",
      "Epoch 8565, Train_Loss: 3801.4833984375, Val_Loss: 3937.010009765625\n",
      "Epoch 8566, Train_Loss: 3801.4873046875, Val_Loss: 3936.996826171875\n",
      "Epoch 8567, Train_Loss: 3801.62939453125, Val_Loss: 3936.69482421875\n",
      "Epoch 8568, Train_Loss: 3801.4931640625, Val_Loss: 3937.001220703125\n",
      "Epoch 8569, Train_Loss: 3801.54345703125, Val_Loss: 3936.983642578125\n",
      "Epoch 8570, Train_Loss: 3801.6552734375, Val_Loss: 3936.70849609375\n",
      "Epoch 8571, Train_Loss: 3801.516845703125, Val_Loss: 3936.997314453125\n",
      "Epoch 8572, Train_Loss: 3801.44970703125, Val_Loss: 3936.901611328125\n",
      "Epoch 8573, Train_Loss: 3801.57666015625, Val_Loss: 3936.631103515625\n",
      "Epoch 8574, Train_Loss: 3801.438720703125, Val_Loss: 3936.9228515625\n",
      "Epoch 8575, Train_Loss: 3801.442626953125, Val_Loss: 3936.91162109375\n",
      "Epoch 8576, Train_Loss: 3801.584228515625, Val_Loss: 3936.673583984375\n",
      "Epoch 8577, Train_Loss: 3801.4345703125, Val_Loss: 3936.962890625\n",
      "Epoch 8578, Train_Loss: 3801.428955078125, Val_Loss: 3936.936767578125\n",
      "Epoch 8579, Train_Loss: 3801.55908203125, Val_Loss: 3936.656005859375\n",
      "Epoch 8580, Train_Loss: 3801.417236328125, Val_Loss: 3936.952392578125\n",
      "Epoch 8581, Train_Loss: 3801.412353515625, Val_Loss: 3936.930419921875\n",
      "Epoch 8582, Train_Loss: 3801.5, Val_Loss: 3936.63720703125\n",
      "Epoch 8583, Train_Loss: 3801.415283203125, Val_Loss: 3936.94921875\n",
      "Epoch 8584, Train_Loss: 3801.41748046875, Val_Loss: 3936.91357421875\n",
      "Epoch 8585, Train_Loss: 3801.51025390625, Val_Loss: 3936.63720703125\n",
      "Epoch 8586, Train_Loss: 3801.40869140625, Val_Loss: 3936.96484375\n",
      "Epoch 8587, Train_Loss: 3801.401611328125, Val_Loss: 3936.8544921875\n",
      "Epoch 8588, Train_Loss: 3801.480712890625, Val_Loss: 3936.56689453125\n",
      "Epoch 8589, Train_Loss: 3801.388671875, Val_Loss: 3936.89208984375\n",
      "Epoch 8590, Train_Loss: 3801.394775390625, Val_Loss: 3936.879150390625\n",
      "Epoch 8591, Train_Loss: 3801.47509765625, Val_Loss: 3936.59716796875\n",
      "Epoch 8592, Train_Loss: 3801.4853515625, Val_Loss: 3936.9052734375\n",
      "Epoch 8593, Train_Loss: 3801.50927734375, Val_Loss: 3936.880859375\n",
      "Epoch 8594, Train_Loss: 3801.341796875, Val_Loss: 3936.60009765625\n",
      "Epoch 8595, Train_Loss: 3801.39794921875, Val_Loss: 3936.877685546875\n",
      "Epoch 8596, Train_Loss: 3801.362060546875, Val_Loss: 3936.86328125\n",
      "Epoch 8597, Train_Loss: 3801.589111328125, Val_Loss: 3936.59521484375\n",
      "Epoch 8598, Train_Loss: 3801.625, Val_Loss: 3936.8701171875\n",
      "Epoch 8599, Train_Loss: 3801.5888671875, Val_Loss: 3936.852294921875\n",
      "Epoch 8600, Train_Loss: 3801.555908203125, Val_Loss: 3936.607177734375\n",
      "Epoch 8601, Train_Loss: 3801.576416015625, Val_Loss: 3936.82958984375\n",
      "Epoch 8602, Train_Loss: 3801.582275390625, Val_Loss: 3936.813232421875\n",
      "Epoch 8603, Train_Loss: 3801.515380859375, Val_Loss: 3936.52587890625\n",
      "Epoch 8604, Train_Loss: 3801.54541015625, Val_Loss: 3936.80908203125\n",
      "Epoch 8605, Train_Loss: 3801.537841796875, Val_Loss: 3936.83203125\n",
      "Epoch 8606, Train_Loss: 3801.51171875, Val_Loss: 3936.552734375\n",
      "Epoch 8607, Train_Loss: 3801.58154296875, Val_Loss: 3936.821533203125\n",
      "Epoch 8608, Train_Loss: 3801.5166015625, Val_Loss: 3936.8037109375\n",
      "Epoch 8609, Train_Loss: 3801.481201171875, Val_Loss: 3936.538818359375\n",
      "Epoch 8610, Train_Loss: 3801.546142578125, Val_Loss: 3936.8388671875\n",
      "Epoch 8611, Train_Loss: 3801.514404296875, Val_Loss: 3936.82568359375\n",
      "Epoch 8612, Train_Loss: 3801.51806640625, Val_Loss: 3936.505615234375\n",
      "Epoch 8613, Train_Loss: 3801.52783203125, Val_Loss: 3936.811279296875\n",
      "Epoch 8614, Train_Loss: 3801.52392578125, Val_Loss: 3936.759521484375\n",
      "Epoch 8615, Train_Loss: 3801.491943359375, Val_Loss: 3936.46044921875\n",
      "Epoch 8616, Train_Loss: 3801.509033203125, Val_Loss: 3936.74072265625\n",
      "Epoch 8617, Train_Loss: 3801.507080078125, Val_Loss: 3936.719970703125\n",
      "Epoch 8618, Train_Loss: 3801.5068359375, Val_Loss: 3936.447265625\n",
      "Epoch 8619, Train_Loss: 3801.508544921875, Val_Loss: 3936.779296875\n",
      "Epoch 8620, Train_Loss: 3801.51611328125, Val_Loss: 3936.762451171875\n",
      "Epoch 8621, Train_Loss: 3801.496337890625, Val_Loss: 3936.49072265625\n",
      "Epoch 8622, Train_Loss: 3801.46484375, Val_Loss: 3936.755615234375\n",
      "Epoch 8623, Train_Loss: 3801.160888671875, Val_Loss: 3936.72607421875\n",
      "Epoch 8624, Train_Loss: 3801.179443359375, Val_Loss: 3936.492919921875\n",
      "Epoch 8625, Train_Loss: 3801.22119140625, Val_Loss: 3936.770751953125\n",
      "Epoch 8626, Train_Loss: 3801.195068359375, Val_Loss: 3936.736083984375\n",
      "Epoch 8627, Train_Loss: 3801.234619140625, Val_Loss: 3936.45556640625\n",
      "Epoch 8628, Train_Loss: 3801.18310546875, Val_Loss: 3936.751220703125\n",
      "Epoch 8629, Train_Loss: 3801.17724609375, Val_Loss: 3936.7900390625\n",
      "Epoch 8630, Train_Loss: 3801.193359375, Val_Loss: 3936.41796875\n",
      "Epoch 8631, Train_Loss: 3801.158935546875, Val_Loss: 3936.68115234375\n",
      "Epoch 8632, Train_Loss: 3801.16357421875, Val_Loss: 3936.63525390625\n",
      "Epoch 8633, Train_Loss: 3801.192138671875, Val_Loss: 3936.430908203125\n",
      "Epoch 8634, Train_Loss: 3801.220947265625, Val_Loss: 3936.763671875\n",
      "Epoch 8635, Train_Loss: 3801.180908203125, Val_Loss: 3936.70068359375\n",
      "Epoch 8636, Train_Loss: 3801.207763671875, Val_Loss: 3936.389892578125\n",
      "Epoch 8637, Train_Loss: 3801.128662109375, Val_Loss: 3936.68359375\n",
      "Epoch 8638, Train_Loss: 3801.129150390625, Val_Loss: 3936.691650390625\n",
      "Epoch 8639, Train_Loss: 3801.1513671875, Val_Loss: 3936.430419921875\n",
      "Epoch 8640, Train_Loss: 3801.150146484375, Val_Loss: 3936.5732421875\n",
      "Epoch 8641, Train_Loss: 3801.169677734375, Val_Loss: 3936.5859375\n",
      "Epoch 8642, Train_Loss: 3801.170654296875, Val_Loss: 3936.4384765625\n",
      "Epoch 8643, Train_Loss: 3801.16455078125, Val_Loss: 3936.42919921875\n",
      "Epoch 8644, Train_Loss: 3801.1748046875, Val_Loss: 3936.89208984375\n",
      "Epoch 8645, Train_Loss: 3801.1396484375, Val_Loss: 3936.5908203125\n",
      "Epoch 8646, Train_Loss: 3801.14013671875, Val_Loss: 3936.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8647, Train_Loss: 3801.153076171875, Val_Loss: 3936.554443359375\n",
      "Epoch 8648, Train_Loss: 3801.148681640625, Val_Loss: 3936.803955078125\n",
      "Epoch 8649, Train_Loss: 3801.19873046875, Val_Loss: 3936.43603515625\n",
      "Epoch 8650, Train_Loss: 3801.0947265625, Val_Loss: 3936.268310546875\n",
      "Epoch 8651, Train_Loss: 3801.16845703125, Val_Loss: 3936.786376953125\n",
      "Epoch 8652, Train_Loss: 3801.0205078125, Val_Loss: 3936.767578125\n",
      "Epoch 8653, Train_Loss: 3800.94287109375, Val_Loss: 3936.28271484375\n",
      "Epoch 8654, Train_Loss: 3800.968994140625, Val_Loss: 3936.503662109375\n",
      "Epoch 8655, Train_Loss: 3800.970458984375, Val_Loss: 3936.605224609375\n",
      "Epoch 8656, Train_Loss: 3801.034912109375, Val_Loss: 3936.460693359375\n",
      "Epoch 8657, Train_Loss: 3800.898681640625, Val_Loss: 3936.3251953125\n",
      "Epoch 8658, Train_Loss: 3801.00537109375, Val_Loss: 3936.8037109375\n",
      "Epoch 8659, Train_Loss: 3800.96630859375, Val_Loss: 3936.61083984375\n",
      "Epoch 8660, Train_Loss: 3800.898193359375, Val_Loss: 3936.216064453125\n",
      "Epoch 8661, Train_Loss: 3800.952880859375, Val_Loss: 3936.497314453125\n",
      "Epoch 8662, Train_Loss: 3800.959716796875, Val_Loss: 3936.593505859375\n",
      "Epoch 8663, Train_Loss: 3800.95361328125, Val_Loss: 3936.439697265625\n",
      "Epoch 8664, Train_Loss: 3800.8857421875, Val_Loss: 3936.258056640625\n",
      "Epoch 8665, Train_Loss: 3800.974365234375, Val_Loss: 3936.746826171875\n",
      "Epoch 8666, Train_Loss: 3800.960205078125, Val_Loss: 3936.713134765625\n",
      "Epoch 8667, Train_Loss: 3800.85107421875, Val_Loss: 3936.222900390625\n",
      "Epoch 8668, Train_Loss: 3800.84716796875, Val_Loss: 3936.4736328125\n",
      "Epoch 8669, Train_Loss: 3800.888916015625, Val_Loss: 3936.575927734375\n",
      "Epoch 8670, Train_Loss: 3800.921875, Val_Loss: 3936.422119140625\n",
      "Epoch 8671, Train_Loss: 3800.8583984375, Val_Loss: 3936.209716796875\n",
      "Epoch 8672, Train_Loss: 3800.96337890625, Val_Loss: 3936.691650390625\n",
      "Epoch 8673, Train_Loss: 3800.927001953125, Val_Loss: 3936.6533203125\n",
      "Epoch 8674, Train_Loss: 3800.835693359375, Val_Loss: 3936.187255859375\n",
      "Epoch 8675, Train_Loss: 3800.841064453125, Val_Loss: 3936.376708984375\n",
      "Epoch 8676, Train_Loss: 3800.861328125, Val_Loss: 3936.466064453125\n",
      "Epoch 8677, Train_Loss: 3800.822509765625, Val_Loss: 3936.343505859375\n",
      "Epoch 8678, Train_Loss: 3800.8330078125, Val_Loss: 3936.272705078125\n",
      "Epoch 8679, Train_Loss: 3800.957275390625, Val_Loss: 3936.747314453125\n",
      "Epoch 8680, Train_Loss: 3800.866943359375, Val_Loss: 3936.54150390625\n",
      "Epoch 8681, Train_Loss: 3800.809326171875, Val_Loss: 3936.129638671875\n",
      "Epoch 8682, Train_Loss: 3800.789794921875, Val_Loss: 3936.46875\n",
      "Epoch 8683, Train_Loss: 3800.82666015625, Val_Loss: 3936.637939453125\n",
      "Epoch 8684, Train_Loss: 3800.792236328125, Val_Loss: 3936.3095703125\n",
      "Epoch 8685, Train_Loss: 3800.89794921875, Val_Loss: 3936.166015625\n",
      "Epoch 8686, Train_Loss: 3800.8779296875, Val_Loss: 3936.701904296875\n",
      "Epoch 8687, Train_Loss: 3800.872314453125, Val_Loss: 3936.693115234375\n",
      "Epoch 8688, Train_Loss: 3800.722900390625, Val_Loss: 3936.12548828125\n",
      "Epoch 8689, Train_Loss: 3800.799072265625, Val_Loss: 3936.272705078125\n",
      "Epoch 8690, Train_Loss: 3800.759033203125, Val_Loss: 3936.41796875\n",
      "Epoch 8691, Train_Loss: 3800.7900390625, Val_Loss: 3936.32080078125\n",
      "Epoch 8692, Train_Loss: 3800.794189453125, Val_Loss: 3936.181640625\n",
      "Epoch 8693, Train_Loss: 3800.75830078125, Val_Loss: 3936.5537109375\n",
      "Epoch 8694, Train_Loss: 3800.76025390625, Val_Loss: 3936.51513671875\n",
      "Epoch 8695, Train_Loss: 3800.707763671875, Val_Loss: 3936.15673828125\n",
      "Epoch 8696, Train_Loss: 3800.739501953125, Val_Loss: 3936.384521484375\n",
      "Epoch 8697, Train_Loss: 3800.738525390625, Val_Loss: 3936.441650390625\n",
      "Epoch 8698, Train_Loss: 3800.783203125, Val_Loss: 3936.269287109375\n",
      "Epoch 8699, Train_Loss: 3800.78759765625, Val_Loss: 3936.156494140625\n",
      "Epoch 8700, Train_Loss: 3800.783447265625, Val_Loss: 3936.664794921875\n",
      "Epoch 8701, Train_Loss: 3800.73291015625, Val_Loss: 3936.485107421875\n",
      "Epoch 8702, Train_Loss: 3800.792236328125, Val_Loss: 3936.07080078125\n",
      "Epoch 8703, Train_Loss: 3800.73291015625, Val_Loss: 3936.299560546875\n",
      "Epoch 8704, Train_Loss: 3800.713623046875, Val_Loss: 3936.487548828125\n",
      "Epoch 8705, Train_Loss: 3800.709716796875, Val_Loss: 3936.21728515625\n",
      "Epoch 8706, Train_Loss: 3800.807373046875, Val_Loss: 3936.087646484375\n",
      "Epoch 8707, Train_Loss: 3800.7470703125, Val_Loss: 3936.575927734375\n",
      "Epoch 8708, Train_Loss: 3800.739501953125, Val_Loss: 3936.580322265625\n",
      "Epoch 8709, Train_Loss: 3800.759521484375, Val_Loss: 3936.0927734375\n",
      "Epoch 8710, Train_Loss: 3800.6611328125, Val_Loss: 3936.243896484375\n",
      "Epoch 8711, Train_Loss: 3800.596923828125, Val_Loss: 3936.37841796875\n",
      "Epoch 8712, Train_Loss: 3800.712646484375, Val_Loss: 3936.28955078125\n",
      "Epoch 8713, Train_Loss: 3800.641845703125, Val_Loss: 3936.101318359375\n",
      "Epoch 8714, Train_Loss: 3800.839599609375, Val_Loss: 3936.435302734375\n",
      "Epoch 8715, Train_Loss: 3800.80517578125, Val_Loss: 3936.410400390625\n",
      "Epoch 8716, Train_Loss: 3800.854736328125, Val_Loss: 3936.10009765625\n",
      "Epoch 8717, Train_Loss: 3800.833251953125, Val_Loss: 3936.28759765625\n",
      "Epoch 8718, Train_Loss: 3800.771240234375, Val_Loss: 3936.34326171875\n",
      "Epoch 8719, Train_Loss: 3800.85205078125, Val_Loss: 3936.11962890625\n",
      "Epoch 8720, Train_Loss: 3800.819580078125, Val_Loss: 3936.048095703125\n",
      "Epoch 8721, Train_Loss: 3800.859619140625, Val_Loss: 3936.580810546875\n",
      "Epoch 8722, Train_Loss: 3800.762939453125, Val_Loss: 3936.4111328125\n",
      "Epoch 8723, Train_Loss: 3800.851318359375, Val_Loss: 3935.983642578125\n",
      "Epoch 8724, Train_Loss: 3800.834228515625, Val_Loss: 3936.30322265625\n",
      "Epoch 8725, Train_Loss: 3800.751708984375, Val_Loss: 3936.454345703125\n",
      "Epoch 8726, Train_Loss: 3800.824462890625, Val_Loss: 3936.1611328125\n",
      "Epoch 8727, Train_Loss: 3800.84814453125, Val_Loss: 3935.98876953125\n",
      "Epoch 8728, Train_Loss: 3800.7626953125, Val_Loss: 3936.466064453125\n",
      "Epoch 8729, Train_Loss: 3800.747802734375, Val_Loss: 3936.478515625\n",
      "Epoch 8730, Train_Loss: 3800.808349609375, Val_Loss: 3936.02392578125\n",
      "Epoch 8731, Train_Loss: 3800.826904296875, Val_Loss: 3936.181640625\n",
      "Epoch 8732, Train_Loss: 3800.833251953125, Val_Loss: 3936.23291015625\n",
      "Epoch 8733, Train_Loss: 3800.8115234375, Val_Loss: 3936.11083984375\n",
      "Epoch 8734, Train_Loss: 3800.77294921875, Val_Loss: 3935.982421875\n",
      "Epoch 8735, Train_Loss: 3800.744873046875, Val_Loss: 3936.445556640625\n",
      "Epoch 8736, Train_Loss: 3800.822265625, Val_Loss: 3936.374755859375\n",
      "Epoch 8737, Train_Loss: 3800.841796875, Val_Loss: 3935.943603515625\n",
      "Epoch 8738, Train_Loss: 3800.878662109375, Val_Loss: 3936.247314453125\n",
      "Epoch 8739, Train_Loss: 3800.8251953125, Val_Loss: 3936.353515625\n",
      "Epoch 8740, Train_Loss: 3800.477294921875, Val_Loss: 3936.069580078125\n",
      "Epoch 8741, Train_Loss: 3800.514404296875, Val_Loss: 3935.961181640625\n",
      "Epoch 8742, Train_Loss: 3800.513671875, Val_Loss: 3936.42529296875\n",
      "Epoch 8743, Train_Loss: 3800.56103515625, Val_Loss: 3936.3876953125\n",
      "Epoch 8744, Train_Loss: 3800.67041015625, Val_Loss: 3935.951904296875\n",
      "Epoch 8745, Train_Loss: 3800.53369140625, Val_Loss: 3936.129638671875\n",
      "Epoch 8746, Train_Loss: 3800.551025390625, Val_Loss: 3936.258056640625\n",
      "Epoch 8747, Train_Loss: 3800.508544921875, Val_Loss: 3936.06396484375\n",
      "Epoch 8748, Train_Loss: 3800.51171875, Val_Loss: 3935.899169921875\n",
      "Epoch 8749, Train_Loss: 3800.53271484375, Val_Loss: 3936.322021484375\n",
      "Epoch 8750, Train_Loss: 3800.5029296875, Val_Loss: 3936.31591796875\n",
      "Epoch 8751, Train_Loss: 3800.620849609375, Val_Loss: 3935.925537109375\n",
      "Epoch 8752, Train_Loss: 3800.513427734375, Val_Loss: 3936.134033203125\n",
      "Epoch 8753, Train_Loss: 3800.514404296875, Val_Loss: 3936.217529296875\n",
      "Epoch 8754, Train_Loss: 3800.510986328125, Val_Loss: 3936.069580078125\n",
      "Epoch 8755, Train_Loss: 3800.544189453125, Val_Loss: 3935.941162109375\n",
      "Epoch 8756, Train_Loss: 3800.487060546875, Val_Loss: 3936.426513671875\n",
      "Epoch 8757, Train_Loss: 3800.476318359375, Val_Loss: 3936.218505859375\n",
      "Epoch 8758, Train_Loss: 3800.64697265625, Val_Loss: 3935.830322265625\n",
      "Epoch 8759, Train_Loss: 3800.552001953125, Val_Loss: 3936.18115234375\n",
      "Epoch 8760, Train_Loss: 3800.486572265625, Val_Loss: 3936.340087890625\n",
      "Epoch 8761, Train_Loss: 3800.572509765625, Val_Loss: 3936.022705078125\n",
      "Epoch 8762, Train_Loss: 3800.64697265625, Val_Loss: 3935.804443359375\n",
      "Epoch 8763, Train_Loss: 3800.465087890625, Val_Loss: 3936.292724609375\n",
      "Epoch 8764, Train_Loss: 3800.4833984375, Val_Loss: 3936.295166015625\n",
      "Epoch 8765, Train_Loss: 3800.556640625, Val_Loss: 3935.865478515625\n",
      "Epoch 8766, Train_Loss: 3800.473388671875, Val_Loss: 3936.07568359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8767, Train_Loss: 3800.508056640625, Val_Loss: 3936.15234375\n",
      "Epoch 8768, Train_Loss: 3800.55322265625, Val_Loss: 3935.996826171875\n",
      "Epoch 8769, Train_Loss: 3800.552978515625, Val_Loss: 3935.94287109375\n",
      "Epoch 8770, Train_Loss: 3800.421630859375, Val_Loss: 3936.0224609375\n",
      "Epoch 8771, Train_Loss: 3800.436767578125, Val_Loss: 3936.064453125\n",
      "Epoch 8772, Train_Loss: 3800.40185546875, Val_Loss: 3935.99072265625\n",
      "Epoch 8773, Train_Loss: 3800.38134765625, Val_Loss: 3935.96728515625\n",
      "Epoch 8774, Train_Loss: 3800.41259765625, Val_Loss: 3936.04443359375\n",
      "Epoch 8775, Train_Loss: 3800.421142578125, Val_Loss: 3936.014404296875\n",
      "Epoch 8776, Train_Loss: 3800.3857421875, Val_Loss: 3935.961181640625\n",
      "Epoch 8777, Train_Loss: 3800.347412109375, Val_Loss: 3935.9384765625\n",
      "Epoch 8778, Train_Loss: 3800.38525390625, Val_Loss: 3935.99560546875\n",
      "Epoch 8779, Train_Loss: 3800.326904296875, Val_Loss: 3935.94482421875\n",
      "Epoch 8780, Train_Loss: 3800.349853515625, Val_Loss: 3935.894775390625\n",
      "Epoch 8781, Train_Loss: 3800.384765625, Val_Loss: 3935.98193359375\n",
      "Epoch 8782, Train_Loss: 3800.376953125, Val_Loss: 3935.99072265625\n",
      "Epoch 8783, Train_Loss: 3800.348876953125, Val_Loss: 3935.943115234375\n",
      "Epoch 8784, Train_Loss: 3800.343505859375, Val_Loss: 3935.95068359375\n",
      "Epoch 8785, Train_Loss: 3800.34033203125, Val_Loss: 3935.97509765625\n",
      "Epoch 8786, Train_Loss: 3800.306640625, Val_Loss: 3935.95361328125\n",
      "Epoch 8787, Train_Loss: 3800.32568359375, Val_Loss: 3935.906494140625\n",
      "Epoch 8788, Train_Loss: 3800.335205078125, Val_Loss: 3935.95849609375\n",
      "Epoch 8789, Train_Loss: 3800.331298828125, Val_Loss: 3935.93798828125\n",
      "Epoch 8790, Train_Loss: 3800.33251953125, Val_Loss: 3935.899169921875\n",
      "Epoch 8791, Train_Loss: 3800.335205078125, Val_Loss: 3935.949951171875\n",
      "Epoch 8792, Train_Loss: 3800.316162109375, Val_Loss: 3935.893310546875\n",
      "Epoch 8793, Train_Loss: 3800.32470703125, Val_Loss: 3935.85205078125\n",
      "Epoch 8794, Train_Loss: 3800.297119140625, Val_Loss: 3935.8671875\n",
      "Epoch 8795, Train_Loss: 3800.316650390625, Val_Loss: 3935.885986328125\n",
      "Epoch 8796, Train_Loss: 3800.275146484375, Val_Loss: 3935.9072265625\n",
      "Epoch 8797, Train_Loss: 3800.264892578125, Val_Loss: 3935.90283203125\n",
      "Epoch 8798, Train_Loss: 3800.31298828125, Val_Loss: 3935.9267578125\n",
      "Epoch 8799, Train_Loss: 3800.29638671875, Val_Loss: 3935.901123046875\n",
      "Epoch 8800, Train_Loss: 3800.2265625, Val_Loss: 3935.8828125\n",
      "Epoch 8801, Train_Loss: 3800.222412109375, Val_Loss: 3935.928466796875\n",
      "Epoch 8802, Train_Loss: 3800.241455078125, Val_Loss: 3935.899658203125\n",
      "Epoch 8803, Train_Loss: 3800.23974609375, Val_Loss: 3935.89404296875\n",
      "Epoch 8804, Train_Loss: 3800.275146484375, Val_Loss: 3935.883544921875\n",
      "Epoch 8805, Train_Loss: 3800.2724609375, Val_Loss: 3935.883544921875\n",
      "Epoch 8806, Train_Loss: 3800.255615234375, Val_Loss: 3935.886474609375\n",
      "Epoch 8807, Train_Loss: 3800.175048828125, Val_Loss: 3935.826904296875\n",
      "Epoch 8808, Train_Loss: 3800.18994140625, Val_Loss: 3935.822509765625\n",
      "Epoch 8809, Train_Loss: 3800.197998046875, Val_Loss: 3935.83203125\n",
      "Epoch 8810, Train_Loss: 3800.18505859375, Val_Loss: 3935.822509765625\n",
      "Epoch 8811, Train_Loss: 3800.2109375, Val_Loss: 3935.854736328125\n",
      "Epoch 8812, Train_Loss: 3800.203125, Val_Loss: 3935.870361328125\n",
      "Epoch 8813, Train_Loss: 3800.2216796875, Val_Loss: 3935.837158203125\n",
      "Epoch 8814, Train_Loss: 3800.214111328125, Val_Loss: 3935.84912109375\n",
      "Epoch 8815, Train_Loss: 3800.197021484375, Val_Loss: 3935.813232421875\n",
      "Epoch 8816, Train_Loss: 3800.19970703125, Val_Loss: 3935.80810546875\n",
      "Epoch 8817, Train_Loss: 3800.194580078125, Val_Loss: 3935.8271484375\n",
      "Epoch 8818, Train_Loss: 3800.18994140625, Val_Loss: 3935.8291015625\n",
      "Epoch 8819, Train_Loss: 3800.213134765625, Val_Loss: 3935.7958984375\n",
      "Epoch 8820, Train_Loss: 3800.20166015625, Val_Loss: 3935.83837890625\n",
      "Epoch 8821, Train_Loss: 3800.19287109375, Val_Loss: 3935.84130859375\n",
      "Epoch 8822, Train_Loss: 3800.206787109375, Val_Loss: 3935.818359375\n",
      "Epoch 8823, Train_Loss: 3800.1865234375, Val_Loss: 3935.787109375\n",
      "Epoch 8824, Train_Loss: 3800.186767578125, Val_Loss: 3935.790283203125\n",
      "Epoch 8825, Train_Loss: 3800.1845703125, Val_Loss: 3935.74609375\n",
      "Epoch 8826, Train_Loss: 3800.1767578125, Val_Loss: 3935.8125\n",
      "Epoch 8827, Train_Loss: 3800.1748046875, Val_Loss: 3935.83349609375\n",
      "Epoch 8828, Train_Loss: 3800.177734375, Val_Loss: 3935.78515625\n",
      "Epoch 8829, Train_Loss: 3800.1796875, Val_Loss: 3935.77880859375\n",
      "Epoch 8830, Train_Loss: 3800.076904296875, Val_Loss: 3935.79833984375\n",
      "Epoch 8831, Train_Loss: 3800.06982421875, Val_Loss: 3935.7919921875\n",
      "Epoch 8832, Train_Loss: 3800.08349609375, Val_Loss: 3935.75244140625\n",
      "Epoch 8833, Train_Loss: 3800.085693359375, Val_Loss: 3935.753662109375\n",
      "Epoch 8834, Train_Loss: 3800.292236328125, Val_Loss: 3935.792724609375\n",
      "Epoch 8835, Train_Loss: 3800.2998046875, Val_Loss: 3935.74951171875\n",
      "Epoch 8836, Train_Loss: 3800.2939453125, Val_Loss: 3935.750732421875\n",
      "Epoch 8837, Train_Loss: 3800.299560546875, Val_Loss: 3935.751953125\n",
      "Epoch 8838, Train_Loss: 3800.261962890625, Val_Loss: 3935.735595703125\n",
      "Epoch 8839, Train_Loss: 3800.26025390625, Val_Loss: 3935.68310546875\n",
      "Epoch 8840, Train_Loss: 3800.259033203125, Val_Loss: 3935.682861328125\n",
      "Epoch 8841, Train_Loss: 3800.251953125, Val_Loss: 3935.677490234375\n",
      "Epoch 8842, Train_Loss: 3800.26708984375, Val_Loss: 3935.738037109375\n",
      "Epoch 8843, Train_Loss: 3800.245361328125, Val_Loss: 3935.71435546875\n",
      "Epoch 8844, Train_Loss: 3800.23583984375, Val_Loss: 3935.734375\n",
      "Epoch 8845, Train_Loss: 3800.169677734375, Val_Loss: 3935.709228515625\n",
      "Epoch 8846, Train_Loss: 3800.23388671875, Val_Loss: 3935.728759765625\n",
      "Epoch 8847, Train_Loss: 3800.23388671875, Val_Loss: 3935.709228515625\n",
      "Epoch 8848, Train_Loss: 3800.157470703125, Val_Loss: 3935.7255859375\n",
      "Epoch 8849, Train_Loss: 3800.24755859375, Val_Loss: 3935.702880859375\n",
      "Epoch 8850, Train_Loss: 3800.23486328125, Val_Loss: 3935.7275390625\n",
      "Epoch 8851, Train_Loss: 3800.187255859375, Val_Loss: 3935.71630859375\n",
      "Epoch 8852, Train_Loss: 3800.186279296875, Val_Loss: 3935.722900390625\n",
      "Epoch 8853, Train_Loss: 3800.1708984375, Val_Loss: 3935.6611328125\n",
      "Epoch 8854, Train_Loss: 3800.170166015625, Val_Loss: 3935.66845703125\n",
      "Epoch 8855, Train_Loss: 3800.1611328125, Val_Loss: 3935.6591796875\n",
      "Epoch 8856, Train_Loss: 3800.182861328125, Val_Loss: 3935.649658203125\n",
      "Epoch 8857, Train_Loss: 3800.18505859375, Val_Loss: 3935.695556640625\n",
      "Epoch 8858, Train_Loss: 3800.182373046875, Val_Loss: 3935.68994140625\n",
      "Epoch 8859, Train_Loss: 3800.182373046875, Val_Loss: 3935.685302734375\n",
      "Epoch 8860, Train_Loss: 3800.18310546875, Val_Loss: 3935.681640625\n",
      "Epoch 8861, Train_Loss: 3799.880859375, Val_Loss: 3935.676025390625\n",
      "Epoch 8862, Train_Loss: 3799.874267578125, Val_Loss: 3935.681640625\n",
      "Epoch 8863, Train_Loss: 3799.8564453125, Val_Loss: 3935.65771484375\n",
      "Epoch 8864, Train_Loss: 3799.83203125, Val_Loss: 3935.677490234375\n",
      "Epoch 8865, Train_Loss: 3799.884033203125, Val_Loss: 3935.6611328125\n",
      "Epoch 8866, Train_Loss: 3799.89697265625, Val_Loss: 3935.665283203125\n",
      "Epoch 8867, Train_Loss: 3799.89013671875, Val_Loss: 3935.6640625\n",
      "Epoch 8868, Train_Loss: 3799.872802734375, Val_Loss: 3935.667236328125\n",
      "Epoch 8869, Train_Loss: 3799.876953125, Val_Loss: 3935.598388671875\n",
      "Epoch 8870, Train_Loss: 3799.8740234375, Val_Loss: 3935.610107421875\n",
      "Epoch 8871, Train_Loss: 3799.861328125, Val_Loss: 3935.60791015625\n",
      "Epoch 8872, Train_Loss: 3799.864013671875, Val_Loss: 3935.648681640625\n",
      "Epoch 8873, Train_Loss: 3799.879638671875, Val_Loss: 3935.631103515625\n",
      "Epoch 8874, Train_Loss: 3799.85791015625, Val_Loss: 3935.64111328125\n",
      "Epoch 8875, Train_Loss: 3799.843505859375, Val_Loss: 3935.648681640625\n",
      "Epoch 8876, Train_Loss: 3799.840087890625, Val_Loss: 3935.607177734375\n",
      "Epoch 8877, Train_Loss: 3799.8447265625, Val_Loss: 3935.62109375\n",
      "Epoch 8878, Train_Loss: 3799.80908203125, Val_Loss: 3935.651611328125\n",
      "Epoch 8879, Train_Loss: 3799.840576171875, Val_Loss: 3935.618896484375\n",
      "Epoch 8880, Train_Loss: 3799.85595703125, Val_Loss: 3935.61474609375\n",
      "Epoch 8881, Train_Loss: 3799.83203125, Val_Loss: 3935.650390625\n",
      "Epoch 8882, Train_Loss: 3799.862060546875, Val_Loss: 3935.6044921875\n",
      "Epoch 8883, Train_Loss: 3799.866943359375, Val_Loss: 3935.609619140625\n",
      "Epoch 8884, Train_Loss: 3799.8349609375, Val_Loss: 3935.587158203125\n",
      "Epoch 8885, Train_Loss: 3799.8466796875, Val_Loss: 3935.553955078125\n",
      "Epoch 8886, Train_Loss: 3799.836669921875, Val_Loss: 3935.551513671875\n",
      "Epoch 8887, Train_Loss: 3799.83203125, Val_Loss: 3935.57470703125\n",
      "Epoch 8888, Train_Loss: 3799.84716796875, Val_Loss: 3935.582763671875\n",
      "Epoch 8889, Train_Loss: 3799.843505859375, Val_Loss: 3935.5703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8890, Train_Loss: 3799.810546875, Val_Loss: 3935.611083984375\n",
      "Epoch 8891, Train_Loss: 3799.834716796875, Val_Loss: 3935.576416015625\n",
      "Epoch 8892, Train_Loss: 3799.70703125, Val_Loss: 3935.54443359375\n",
      "Epoch 8893, Train_Loss: 3799.699462890625, Val_Loss: 3935.580078125\n",
      "Epoch 8894, Train_Loss: 3799.67529296875, Val_Loss: 3935.558349609375\n",
      "Epoch 8895, Train_Loss: 3799.679443359375, Val_Loss: 3935.556396484375\n",
      "Epoch 8896, Train_Loss: 3799.687744140625, Val_Loss: 3935.589111328125\n",
      "Epoch 8897, Train_Loss: 3799.68310546875, Val_Loss: 3935.55810546875\n",
      "Epoch 8898, Train_Loss: 3799.680908203125, Val_Loss: 3935.555908203125\n",
      "Epoch 8899, Train_Loss: 3799.6630859375, Val_Loss: 3935.580322265625\n",
      "Epoch 8900, Train_Loss: 3799.63720703125, Val_Loss: 3935.51123046875\n",
      "Epoch 8901, Train_Loss: 3799.632568359375, Val_Loss: 3935.518310546875\n",
      "Epoch 8902, Train_Loss: 3799.629150390625, Val_Loss: 3935.539306640625\n",
      "Epoch 8903, Train_Loss: 3799.6259765625, Val_Loss: 3935.505615234375\n",
      "Epoch 8904, Train_Loss: 3799.63720703125, Val_Loss: 3935.5400390625\n",
      "Epoch 8905, Train_Loss: 3799.63427734375, Val_Loss: 3935.563232421875\n",
      "Epoch 8906, Train_Loss: 3799.637451171875, Val_Loss: 3935.545654296875\n",
      "Epoch 8907, Train_Loss: 3799.63623046875, Val_Loss: 3935.529541015625\n",
      "Epoch 8908, Train_Loss: 3799.609130859375, Val_Loss: 3935.5419921875\n",
      "Epoch 8909, Train_Loss: 3799.6708984375, Val_Loss: 3935.51318359375\n",
      "Epoch 8910, Train_Loss: 3799.67236328125, Val_Loss: 3935.53125\n",
      "Epoch 8911, Train_Loss: 3799.610595703125, Val_Loss: 3935.548828125\n",
      "Epoch 8912, Train_Loss: 3799.6826171875, Val_Loss: 3935.497314453125\n",
      "Epoch 8913, Train_Loss: 3799.695068359375, Val_Loss: 3935.501953125\n",
      "Epoch 8914, Train_Loss: 3799.675537109375, Val_Loss: 3935.538330078125\n",
      "Epoch 8915, Train_Loss: 3799.6875, Val_Loss: 3935.483154296875\n",
      "Epoch 8916, Train_Loss: 3799.685546875, Val_Loss: 3935.4208984375\n",
      "Epoch 8917, Train_Loss: 3799.66259765625, Val_Loss: 3935.47998046875\n",
      "Epoch 8918, Train_Loss: 3799.684814453125, Val_Loss: 3935.40478515625\n",
      "Epoch 8919, Train_Loss: 3799.6826171875, Val_Loss: 3935.403564453125\n",
      "Epoch 8920, Train_Loss: 3799.682861328125, Val_Loss: 3935.494384765625\n",
      "Epoch 8921, Train_Loss: 3799.68505859375, Val_Loss: 3935.47412109375\n",
      "Epoch 8922, Train_Loss: 3799.702880859375, Val_Loss: 3935.46923828125\n",
      "Epoch 8923, Train_Loss: 3799.71728515625, Val_Loss: 3935.5\n",
      "Epoch 8924, Train_Loss: 3799.650634765625, Val_Loss: 3935.45361328125\n",
      "Epoch 8925, Train_Loss: 3799.641845703125, Val_Loss: 3935.466064453125\n",
      "Epoch 8926, Train_Loss: 3799.67333984375, Val_Loss: 3935.505615234375\n",
      "Epoch 8927, Train_Loss: 3799.6728515625, Val_Loss: 3935.4580078125\n",
      "Epoch 8928, Train_Loss: 3799.71337890625, Val_Loss: 3935.460693359375\n",
      "Epoch 8929, Train_Loss: 3799.7099609375, Val_Loss: 3935.492431640625\n",
      "Epoch 8930, Train_Loss: 3799.68359375, Val_Loss: 3935.456298828125\n",
      "Epoch 8931, Train_Loss: 3799.696533203125, Val_Loss: 3935.46728515625\n",
      "Epoch 8932, Train_Loss: 3799.629150390625, Val_Loss: 3935.437255859375\n",
      "Epoch 8933, Train_Loss: 3799.68798828125, Val_Loss: 3935.343994140625\n",
      "Epoch 8934, Train_Loss: 3799.6064453125, Val_Loss: 3935.393310546875\n",
      "Epoch 8935, Train_Loss: 3799.598388671875, Val_Loss: 3935.486083984375\n",
      "Epoch 8936, Train_Loss: 3799.61865234375, Val_Loss: 3935.4228515625\n",
      "Epoch 8937, Train_Loss: 3799.690185546875, Val_Loss: 3935.392333984375\n",
      "Epoch 8938, Train_Loss: 3799.63818359375, Val_Loss: 3935.434326171875\n",
      "Epoch 8939, Train_Loss: 3799.64111328125, Val_Loss: 3935.479248046875\n",
      "Epoch 8940, Train_Loss: 3799.651611328125, Val_Loss: 3935.38232421875\n",
      "Epoch 8941, Train_Loss: 3799.655517578125, Val_Loss: 3935.36279296875\n",
      "Epoch 8942, Train_Loss: 3799.6103515625, Val_Loss: 3935.458740234375\n",
      "Epoch 8943, Train_Loss: 3799.6005859375, Val_Loss: 3935.394287109375\n",
      "Epoch 8944, Train_Loss: 3799.638671875, Val_Loss: 3935.40087890625\n",
      "Epoch 8945, Train_Loss: 3799.631103515625, Val_Loss: 3935.39990234375\n",
      "Epoch 8946, Train_Loss: 3799.6015625, Val_Loss: 3935.393310546875\n",
      "Epoch 8947, Train_Loss: 3799.639892578125, Val_Loss: 3935.4052734375\n",
      "Epoch 8948, Train_Loss: 3799.658203125, Val_Loss: 3935.3076171875\n",
      "Epoch 8949, Train_Loss: 3799.58154296875, Val_Loss: 3935.349609375\n",
      "Epoch 8950, Train_Loss: 3799.61083984375, Val_Loss: 3935.406494140625\n",
      "Epoch 8951, Train_Loss: 3799.636962890625, Val_Loss: 3935.308837890625\n",
      "Epoch 8952, Train_Loss: 3799.63427734375, Val_Loss: 3935.3427734375\n",
      "Epoch 8953, Train_Loss: 3799.593017578125, Val_Loss: 3935.40283203125\n",
      "Epoch 8954, Train_Loss: 3799.569091796875, Val_Loss: 3935.381591796875\n",
      "Epoch 8955, Train_Loss: 3799.56787109375, Val_Loss: 3935.396484375\n",
      "Epoch 8956, Train_Loss: 3799.467041015625, Val_Loss: 3935.3779296875\n",
      "Epoch 8957, Train_Loss: 3799.518798828125, Val_Loss: 3935.347900390625\n",
      "Epoch 8958, Train_Loss: 3799.494384765625, Val_Loss: 3935.3876953125\n",
      "Epoch 8959, Train_Loss: 3799.53173828125, Val_Loss: 3935.343994140625\n",
      "Epoch 8960, Train_Loss: 3799.679443359375, Val_Loss: 3935.381591796875\n",
      "Epoch 8961, Train_Loss: 3799.726806640625, Val_Loss: 3935.41796875\n",
      "Epoch 8962, Train_Loss: 3799.744873046875, Val_Loss: 3935.321533203125\n",
      "Epoch 8963, Train_Loss: 3799.741455078125, Val_Loss: 3935.33642578125\n",
      "Epoch 8964, Train_Loss: 3799.6748046875, Val_Loss: 3935.369140625\n",
      "Epoch 8965, Train_Loss: 3799.702392578125, Val_Loss: 3935.28759765625\n",
      "Epoch 8966, Train_Loss: 3799.8173828125, Val_Loss: 3935.273193359375\n",
      "Epoch 8967, Train_Loss: 3799.681884765625, Val_Loss: 3935.3466796875\n",
      "Epoch 8968, Train_Loss: 3799.826416015625, Val_Loss: 3935.27392578125\n",
      "Epoch 8969, Train_Loss: 3799.83056640625, Val_Loss: 3935.298095703125\n",
      "Epoch 8970, Train_Loss: 3799.67236328125, Val_Loss: 3935.405517578125\n",
      "Epoch 8971, Train_Loss: 3799.835693359375, Val_Loss: 3935.306396484375\n",
      "Epoch 8972, Train_Loss: 3799.810302734375, Val_Loss: 3935.287109375\n",
      "Epoch 8973, Train_Loss: 3799.76904296875, Val_Loss: 3935.37109375\n",
      "Epoch 8974, Train_Loss: 3799.809814453125, Val_Loss: 3935.292724609375\n",
      "Epoch 8975, Train_Loss: 3799.79833984375, Val_Loss: 3935.29833984375\n",
      "Epoch 8976, Train_Loss: 3799.66259765625, Val_Loss: 3935.38671875\n",
      "Epoch 8977, Train_Loss: 3799.8115234375, Val_Loss: 3935.293212890625\n",
      "Epoch 8978, Train_Loss: 3799.81982421875, Val_Loss: 3935.2939453125\n",
      "Epoch 8979, Train_Loss: 3799.77880859375, Val_Loss: 3935.357666015625\n",
      "Epoch 8980, Train_Loss: 3799.797607421875, Val_Loss: 3935.24169921875\n",
      "Epoch 8981, Train_Loss: 3799.78955078125, Val_Loss: 3935.2548828125\n",
      "Epoch 8982, Train_Loss: 3799.76123046875, Val_Loss: 3935.308349609375\n",
      "Epoch 8983, Train_Loss: 3799.806640625, Val_Loss: 3935.252685546875\n",
      "Epoch 8984, Train_Loss: 3799.808349609375, Val_Loss: 3935.283203125\n",
      "Epoch 8985, Train_Loss: 3799.768310546875, Val_Loss: 3935.33642578125\n",
      "Epoch 8986, Train_Loss: 3799.8046875, Val_Loss: 3935.283203125\n",
      "Epoch 8987, Train_Loss: 3799.80029296875, Val_Loss: 3935.283203125\n",
      "Epoch 8988, Train_Loss: 3799.46240234375, Val_Loss: 3935.3291015625\n",
      "Epoch 8989, Train_Loss: 3799.49267578125, Val_Loss: 3935.239990234375\n",
      "Epoch 8990, Train_Loss: 3799.5087890625, Val_Loss: 3935.26611328125\n",
      "Epoch 8991, Train_Loss: 3799.452880859375, Val_Loss: 3935.34130859375\n",
      "Epoch 8992, Train_Loss: 3799.53857421875, Val_Loss: 3935.2431640625\n",
      "Epoch 8993, Train_Loss: 3799.539306640625, Val_Loss: 3935.257568359375\n",
      "Epoch 8994, Train_Loss: 3799.546142578125, Val_Loss: 3935.29150390625\n",
      "Epoch 8995, Train_Loss: 3799.465576171875, Val_Loss: 3935.227294921875\n",
      "Epoch 8996, Train_Loss: 3799.50146484375, Val_Loss: 3935.213623046875\n",
      "Epoch 8997, Train_Loss: 3799.506591796875, Val_Loss: 3935.182373046875\n",
      "Epoch 8998, Train_Loss: 3799.521484375, Val_Loss: 3935.23388671875\n",
      "Epoch 8999, Train_Loss: 3799.449951171875, Val_Loss: 3935.16357421875\n",
      "Epoch 9000, Train_Loss: 3799.519775390625, Val_Loss: 3935.23046875\n",
      "Epoch 9001, Train_Loss: 3799.5107421875, Val_Loss: 3935.199951171875\n",
      "Epoch 9002, Train_Loss: 3799.466796875, Val_Loss: 3935.268310546875\n",
      "Epoch 9003, Train_Loss: 3799.397705078125, Val_Loss: 3935.209716796875\n",
      "Epoch 9004, Train_Loss: 3799.476806640625, Val_Loss: 3935.19921875\n",
      "Epoch 9005, Train_Loss: 3799.493896484375, Val_Loss: 3935.24755859375\n",
      "Epoch 9006, Train_Loss: 3799.392333984375, Val_Loss: 3935.184814453125\n",
      "Epoch 9007, Train_Loss: 3799.435546875, Val_Loss: 3935.218017578125\n",
      "Epoch 9008, Train_Loss: 3799.437255859375, Val_Loss: 3935.25\n",
      "Epoch 9009, Train_Loss: 3799.40087890625, Val_Loss: 3935.171630859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9010, Train_Loss: 3799.442626953125, Val_Loss: 3935.212890625\n",
      "Epoch 9011, Train_Loss: 3799.4580078125, Val_Loss: 3935.2705078125\n",
      "Epoch 9012, Train_Loss: 3799.395751953125, Val_Loss: 3935.205078125\n",
      "Epoch 9013, Train_Loss: 3799.4287109375, Val_Loss: 3935.149169921875\n",
      "Epoch 9014, Train_Loss: 3799.419189453125, Val_Loss: 3935.22607421875\n",
      "Epoch 9015, Train_Loss: 3799.370361328125, Val_Loss: 3935.15869140625\n",
      "Epoch 9016, Train_Loss: 3799.375244140625, Val_Loss: 3935.146484375\n",
      "Epoch 9017, Train_Loss: 3799.419921875, Val_Loss: 3935.26611328125\n",
      "Epoch 9018, Train_Loss: 3799.367431640625, Val_Loss: 3935.174072265625\n",
      "Epoch 9019, Train_Loss: 3799.42041015625, Val_Loss: 3935.1787109375\n",
      "Epoch 9020, Train_Loss: 3799.42041015625, Val_Loss: 3935.196044921875\n",
      "Epoch 9021, Train_Loss: 3799.253173828125, Val_Loss: 3935.133544921875\n",
      "Epoch 9022, Train_Loss: 3799.294921875, Val_Loss: 3935.229248046875\n",
      "Epoch 9023, Train_Loss: 3799.246337890625, Val_Loss: 3935.172119140625\n",
      "Epoch 9024, Train_Loss: 3799.241455078125, Val_Loss: 3935.162109375\n",
      "Epoch 9025, Train_Loss: 3799.295166015625, Val_Loss: 3935.209716796875\n",
      "Epoch 9026, Train_Loss: 3799.25634765625, Val_Loss: 3935.111083984375\n",
      "Epoch 9027, Train_Loss: 3799.302490234375, Val_Loss: 3935.191162109375\n",
      "Epoch 9028, Train_Loss: 3799.232421875, Val_Loss: 3935.1611328125\n",
      "Epoch 9029, Train_Loss: 3799.1982421875, Val_Loss: 3935.104736328125\n",
      "Epoch 9030, Train_Loss: 3799.20166015625, Val_Loss: 3935.097900390625\n",
      "Epoch 9031, Train_Loss: 3799.198974609375, Val_Loss: 3935.1103515625\n",
      "Epoch 9032, Train_Loss: 3799.196533203125, Val_Loss: 3935.10595703125\n",
      "Epoch 9033, Train_Loss: 3799.191650390625, Val_Loss: 3935.13720703125\n",
      "Epoch 9034, Train_Loss: 3799.20458984375, Val_Loss: 3935.132080078125\n",
      "Epoch 9035, Train_Loss: 3799.189697265625, Val_Loss: 3935.13330078125\n",
      "Epoch 9036, Train_Loss: 3799.218994140625, Val_Loss: 3935.11328125\n",
      "Epoch 9037, Train_Loss: 3799.241455078125, Val_Loss: 3935.15771484375\n",
      "Epoch 9038, Train_Loss: 3799.196044921875, Val_Loss: 3935.0966796875\n",
      "Epoch 9039, Train_Loss: 3799.242431640625, Val_Loss: 3935.140869140625\n",
      "Epoch 9040, Train_Loss: 3799.193115234375, Val_Loss: 3935.103515625\n",
      "Epoch 9041, Train_Loss: 3799.192138671875, Val_Loss: 3935.135498046875\n",
      "Epoch 9042, Train_Loss: 3799.19677734375, Val_Loss: 3935.099609375\n",
      "Epoch 9043, Train_Loss: 3799.25048828125, Val_Loss: 3935.15087890625\n",
      "Epoch 9044, Train_Loss: 3799.21923828125, Val_Loss: 3935.094482421875\n",
      "Epoch 9045, Train_Loss: 3799.237060546875, Val_Loss: 3935.0908203125\n",
      "Epoch 9046, Train_Loss: 3799.1923828125, Val_Loss: 3935.043701171875\n",
      "Epoch 9047, Train_Loss: 3799.193603515625, Val_Loss: 3935.041259765625\n",
      "Epoch 9048, Train_Loss: 3799.23095703125, Val_Loss: 3935.093505859375\n",
      "Epoch 9049, Train_Loss: 3799.206787109375, Val_Loss: 3935.05908203125\n",
      "Epoch 9050, Train_Loss: 3799.179443359375, Val_Loss: 3935.1064453125\n",
      "Epoch 9051, Train_Loss: 3799.1806640625, Val_Loss: 3935.10205078125\n",
      "Epoch 9052, Train_Loss: 3799.234375, Val_Loss: 3935.04931640625\n",
      "Epoch 9053, Train_Loss: 3799.16064453125, Val_Loss: 3935.103271484375\n",
      "Epoch 9054, Train_Loss: 3799.140380859375, Val_Loss: 3935.061279296875\n",
      "Epoch 9055, Train_Loss: 3799.159423828125, Val_Loss: 3935.052001953125\n",
      "Epoch 9056, Train_Loss: 3799.187744140625, Val_Loss: 3935.102294921875\n",
      "Epoch 9057, Train_Loss: 3799.2236328125, Val_Loss: 3935.04443359375\n",
      "Epoch 9058, Train_Loss: 3799.211181640625, Val_Loss: 3935.085205078125\n",
      "Epoch 9059, Train_Loss: 3799.194091796875, Val_Loss: 3935.0537109375\n",
      "Epoch 9060, Train_Loss: 3799.17529296875, Val_Loss: 3935.068115234375\n",
      "Epoch 9061, Train_Loss: 3799.1025390625, Val_Loss: 3935.0048828125\n",
      "Epoch 9062, Train_Loss: 3799.11669921875, Val_Loss: 3934.99072265625\n",
      "Epoch 9063, Train_Loss: 3799.11669921875, Val_Loss: 3935.043212890625\n",
      "Epoch 9064, Train_Loss: 3799.11328125, Val_Loss: 3934.994384765625\n",
      "Epoch 9065, Train_Loss: 3799.06103515625, Val_Loss: 3935.0419921875\n",
      "Epoch 9066, Train_Loss: 3799.126708984375, Val_Loss: 3935.071533203125\n",
      "Epoch 9067, Train_Loss: 3799.07958984375, Val_Loss: 3935.02392578125\n",
      "Epoch 9068, Train_Loss: 3799.087890625, Val_Loss: 3935.07763671875\n",
      "Epoch 9069, Train_Loss: 3799.093994140625, Val_Loss: 3935.032470703125\n",
      "Epoch 9070, Train_Loss: 3799.04931640625, Val_Loss: 3935.025146484375\n",
      "Epoch 9071, Train_Loss: 3799.07568359375, Val_Loss: 3935.046875\n",
      "Epoch 9072, Train_Loss: 3799.07666015625, Val_Loss: 3934.998779296875\n",
      "Epoch 9073, Train_Loss: 3799.057373046875, Val_Loss: 3935.072021484375\n",
      "Epoch 9074, Train_Loss: 3799.07421875, Val_Loss: 3935.0224609375\n",
      "Epoch 9075, Train_Loss: 3799.084228515625, Val_Loss: 3934.986328125\n",
      "Epoch 9076, Train_Loss: 3799.0595703125, Val_Loss: 3935.078857421875\n",
      "Epoch 9077, Train_Loss: 3799.07373046875, Val_Loss: 3934.981689453125\n",
      "Epoch 9078, Train_Loss: 3799.05615234375, Val_Loss: 3934.95849609375\n",
      "Epoch 9079, Train_Loss: 3799.072998046875, Val_Loss: 3934.97509765625\n",
      "Epoch 9080, Train_Loss: 3799.049072265625, Val_Loss: 3934.91845703125\n",
      "Epoch 9081, Train_Loss: 3799.07421875, Val_Loss: 3934.987548828125\n",
      "Epoch 9082, Train_Loss: 3799.021484375, Val_Loss: 3934.97607421875\n",
      "Epoch 9083, Train_Loss: 3799.02685546875, Val_Loss: 3934.9423828125\n",
      "Epoch 9084, Train_Loss: 3799.022705078125, Val_Loss: 3935.034423828125\n",
      "Epoch 9085, Train_Loss: 3799.031005859375, Val_Loss: 3934.956787109375\n",
      "Epoch 9086, Train_Loss: 3798.93017578125, Val_Loss: 3934.939697265625\n",
      "Epoch 9087, Train_Loss: 3798.947998046875, Val_Loss: 3934.985595703125\n",
      "Epoch 9088, Train_Loss: 3798.924560546875, Val_Loss: 3934.9560546875\n",
      "Epoch 9089, Train_Loss: 3798.923095703125, Val_Loss: 3934.954833984375\n",
      "Epoch 9090, Train_Loss: 3799.16357421875, Val_Loss: 3934.989501953125\n",
      "Epoch 9091, Train_Loss: 3799.1455078125, Val_Loss: 3934.956298828125\n",
      "Epoch 9092, Train_Loss: 3799.141357421875, Val_Loss: 3934.963134765625\n",
      "Epoch 9093, Train_Loss: 3799.14599609375, Val_Loss: 3934.956787109375\n",
      "Epoch 9094, Train_Loss: 3799.11083984375, Val_Loss: 3934.90234375\n",
      "Epoch 9095, Train_Loss: 3799.10888671875, Val_Loss: 3934.909912109375\n",
      "Epoch 9096, Train_Loss: 3799.133056640625, Val_Loss: 3934.935546875\n",
      "Epoch 9097, Train_Loss: 3799.107177734375, Val_Loss: 3934.90673828125\n",
      "Epoch 9098, Train_Loss: 3799.13720703125, Val_Loss: 3934.958740234375\n",
      "Epoch 9099, Train_Loss: 3799.102783203125, Val_Loss: 3934.93310546875\n",
      "Epoch 9100, Train_Loss: 3799.10791015625, Val_Loss: 3934.9423828125\n",
      "Epoch 9101, Train_Loss: 3799.10400390625, Val_Loss: 3934.931640625\n",
      "Epoch 9102, Train_Loss: 3799.109130859375, Val_Loss: 3934.941162109375\n",
      "Epoch 9103, Train_Loss: 3799.08984375, Val_Loss: 3934.916748046875\n",
      "Epoch 9104, Train_Loss: 3799.092529296875, Val_Loss: 3934.916748046875\n",
      "Epoch 9105, Train_Loss: 3799.097412109375, Val_Loss: 3934.921630859375\n",
      "Epoch 9106, Train_Loss: 3799.054443359375, Val_Loss: 3934.93310546875\n",
      "Epoch 9107, Train_Loss: 3799.119873046875, Val_Loss: 3934.9443359375\n",
      "Epoch 9108, Train_Loss: 3799.098876953125, Val_Loss: 3934.919921875\n",
      "Epoch 9109, Train_Loss: 3799.082763671875, Val_Loss: 3934.8740234375\n",
      "Epoch 9110, Train_Loss: 3799.135009765625, Val_Loss: 3934.9736328125\n",
      "Epoch 9111, Train_Loss: 3799.05322265625, Val_Loss: 3934.84912109375\n",
      "Epoch 9112, Train_Loss: 3799.0625, Val_Loss: 3934.82275390625\n",
      "Epoch 9113, Train_Loss: 3799.119384765625, Val_Loss: 3934.90869140625\n",
      "Epoch 9114, Train_Loss: 3799.0546875, Val_Loss: 3934.840087890625\n",
      "Epoch 9115, Train_Loss: 3799.047607421875, Val_Loss: 3934.896728515625\n",
      "Epoch 9116, Train_Loss: 3799.11279296875, Val_Loss: 3934.927978515625\n",
      "Epoch 9117, Train_Loss: 3799.060791015625, Val_Loss: 3934.855224609375\n",
      "Epoch 9118, Train_Loss: 3799.025146484375, Val_Loss: 3934.906494140625\n",
      "Epoch 9119, Train_Loss: 3798.78662109375, Val_Loss: 3934.887939453125\n",
      "Epoch 9120, Train_Loss: 3798.733154296875, Val_Loss: 3934.872314453125\n",
      "Epoch 9121, Train_Loss: 3798.737060546875, Val_Loss: 3934.8583984375\n",
      "Epoch 9122, Train_Loss: 3798.77001953125, Val_Loss: 3934.92236328125\n",
      "Epoch 9123, Train_Loss: 3798.836181640625, Val_Loss: 3934.873291015625\n",
      "Epoch 9124, Train_Loss: 3798.763427734375, Val_Loss: 3934.884521484375\n",
      "Epoch 9125, Train_Loss: 3798.806396484375, Val_Loss: 3934.896484375\n",
      "Epoch 9126, Train_Loss: 3798.757568359375, Val_Loss: 3934.899658203125\n",
      "Epoch 9127, Train_Loss: 3798.741455078125, Val_Loss: 3934.8046875\n",
      "Epoch 9128, Train_Loss: 3798.78271484375, Val_Loss: 3934.85009765625\n",
      "Epoch 9129, Train_Loss: 3798.823486328125, Val_Loss: 3934.833984375\n",
      "Epoch 9130, Train_Loss: 3798.8134765625, Val_Loss: 3934.780029296875\n",
      "Epoch 9131, Train_Loss: 3798.7744140625, Val_Loss: 3934.8408203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9132, Train_Loss: 3798.7822265625, Val_Loss: 3934.877685546875\n",
      "Epoch 9133, Train_Loss: 3798.82421875, Val_Loss: 3934.8115234375\n",
      "Epoch 9134, Train_Loss: 3798.8359375, Val_Loss: 3934.86962890625\n",
      "Epoch 9135, Train_Loss: 3798.8447265625, Val_Loss: 3934.87353515625\n",
      "Epoch 9136, Train_Loss: 3798.80810546875, Val_Loss: 3934.76953125\n",
      "Epoch 9137, Train_Loss: 3798.76611328125, Val_Loss: 3934.859619140625\n",
      "Epoch 9138, Train_Loss: 3798.818115234375, Val_Loss: 3934.88232421875\n",
      "Epoch 9139, Train_Loss: 3798.803955078125, Val_Loss: 3934.7705078125\n",
      "Epoch 9140, Train_Loss: 3798.806396484375, Val_Loss: 3934.8359375\n",
      "Epoch 9141, Train_Loss: 3798.843994140625, Val_Loss: 3934.86962890625\n",
      "Epoch 9142, Train_Loss: 3798.811767578125, Val_Loss: 3934.7939453125\n",
      "Epoch 9143, Train_Loss: 3798.806396484375, Val_Loss: 3934.80078125\n",
      "Epoch 9144, Train_Loss: 3798.830810546875, Val_Loss: 3934.7744140625\n",
      "Epoch 9145, Train_Loss: 3798.801513671875, Val_Loss: 3934.744384765625\n",
      "Epoch 9146, Train_Loss: 3798.809326171875, Val_Loss: 3934.74951171875\n",
      "Epoch 9147, Train_Loss: 3798.8486328125, Val_Loss: 3934.786865234375\n",
      "Epoch 9148, Train_Loss: 3798.802734375, Val_Loss: 3934.798095703125\n",
      "Epoch 9149, Train_Loss: 3798.796630859375, Val_Loss: 3934.7626953125\n",
      "Epoch 9150, Train_Loss: 3798.84912109375, Val_Loss: 3934.83447265625\n",
      "Epoch 9151, Train_Loss: 3798.802001953125, Val_Loss: 3934.822021484375\n",
      "Epoch 9152, Train_Loss: 3798.655517578125, Val_Loss: 3934.74169921875\n",
      "Epoch 9153, Train_Loss: 3798.649658203125, Val_Loss: 3934.7958984375\n",
      "Epoch 9154, Train_Loss: 3798.650634765625, Val_Loss: 3934.811279296875\n",
      "Epoch 9155, Train_Loss: 3798.665771484375, Val_Loss: 3934.771484375\n",
      "Epoch 9156, Train_Loss: 3798.638427734375, Val_Loss: 3934.817138671875\n",
      "Epoch 9157, Train_Loss: 3798.65576171875, Val_Loss: 3934.793701171875\n",
      "Epoch 9158, Train_Loss: 3798.654052734375, Val_Loss: 3934.77587890625\n",
      "Epoch 9159, Train_Loss: 3798.6474609375, Val_Loss: 3934.7880859375\n",
      "Epoch 9160, Train_Loss: 3798.640380859375, Val_Loss: 3934.80517578125\n",
      "Epoch 9161, Train_Loss: 3798.628173828125, Val_Loss: 3934.722412109375\n",
      "Epoch 9162, Train_Loss: 3798.627197265625, Val_Loss: 3934.724365234375\n",
      "Epoch 9163, Train_Loss: 3798.619384765625, Val_Loss: 3934.73046875\n",
      "Epoch 9164, Train_Loss: 3798.630126953125, Val_Loss: 3934.72314453125\n",
      "Epoch 9165, Train_Loss: 3798.626708984375, Val_Loss: 3934.75439453125\n",
      "Epoch 9166, Train_Loss: 3798.6240234375, Val_Loss: 3934.757080078125\n",
      "Epoch 9167, Train_Loss: 3798.628662109375, Val_Loss: 3934.74755859375\n",
      "Epoch 9168, Train_Loss: 3798.6337890625, Val_Loss: 3934.741943359375\n",
      "Epoch 9169, Train_Loss: 3798.607666015625, Val_Loss: 3934.73046875\n",
      "Epoch 9170, Train_Loss: 3798.59326171875, Val_Loss: 3934.729248046875\n",
      "Epoch 9171, Train_Loss: 3798.5908203125, Val_Loss: 3934.731201171875\n",
      "Epoch 9172, Train_Loss: 3798.607666015625, Val_Loss: 3934.735107421875\n",
      "Epoch 9173, Train_Loss: 3798.616455078125, Val_Loss: 3934.72314453125\n",
      "Epoch 9174, Train_Loss: 3798.626708984375, Val_Loss: 3934.731689453125\n",
      "Epoch 9175, Train_Loss: 3798.611083984375, Val_Loss: 3934.73193359375\n",
      "Epoch 9176, Train_Loss: 3798.61572265625, Val_Loss: 3934.733642578125\n",
      "Epoch 9177, Train_Loss: 3798.62158203125, Val_Loss: 3934.743896484375\n",
      "Epoch 9178, Train_Loss: 3798.596923828125, Val_Loss: 3934.66357421875\n",
      "Epoch 9179, Train_Loss: 3798.6064453125, Val_Loss: 3934.68115234375\n",
      "Epoch 9180, Train_Loss: 3798.605224609375, Val_Loss: 3934.681640625\n",
      "Epoch 9181, Train_Loss: 3798.574462890625, Val_Loss: 3934.66162109375\n",
      "Epoch 9182, Train_Loss: 3798.6044921875, Val_Loss: 3934.712890625\n",
      "Epoch 9183, Train_Loss: 3798.58251953125, Val_Loss: 3934.701904296875\n",
      "Epoch 9184, Train_Loss: 3798.584228515625, Val_Loss: 3934.7294921875\n",
      "Epoch 9185, Train_Loss: 3798.612548828125, Val_Loss: 3934.698486328125\n",
      "Epoch 9186, Train_Loss: 3798.550537109375, Val_Loss: 3934.7099609375\n",
      "Epoch 9187, Train_Loss: 3798.544921875, Val_Loss: 3934.71728515625\n",
      "Epoch 9188, Train_Loss: 3798.537109375, Val_Loss: 3934.681640625\n",
      "Epoch 9189, Train_Loss: 3798.5654296875, Val_Loss: 3934.71728515625\n",
      "Epoch 9190, Train_Loss: 3798.561279296875, Val_Loss: 3934.72314453125\n",
      "Epoch 9191, Train_Loss: 3798.566162109375, Val_Loss: 3934.680419921875\n",
      "Epoch 9192, Train_Loss: 3798.55126953125, Val_Loss: 3934.689208984375\n",
      "Epoch 9193, Train_Loss: 3798.568115234375, Val_Loss: 3934.7392578125\n",
      "Epoch 9194, Train_Loss: 3798.6083984375, Val_Loss: 3934.7080078125\n",
      "Epoch 9195, Train_Loss: 3798.601318359375, Val_Loss: 3934.666748046875\n",
      "Epoch 9196, Train_Loss: 3798.61865234375, Val_Loss: 3934.7236328125\n",
      "Epoch 9197, Train_Loss: 3798.603515625, Val_Loss: 3934.71875\n",
      "Epoch 9198, Train_Loss: 3798.60693359375, Val_Loss: 3934.654296875\n",
      "Epoch 9199, Train_Loss: 3798.62255859375, Val_Loss: 3934.7041015625\n",
      "Epoch 9200, Train_Loss: 3798.614990234375, Val_Loss: 3934.726806640625\n",
      "Epoch 9201, Train_Loss: 3798.589599609375, Val_Loss: 3934.702392578125\n",
      "Epoch 9202, Train_Loss: 3798.546630859375, Val_Loss: 3934.673095703125\n",
      "Epoch 9203, Train_Loss: 3798.540771484375, Val_Loss: 3934.711669921875\n",
      "Epoch 9204, Train_Loss: 3798.548828125, Val_Loss: 3934.69921875\n",
      "Epoch 9205, Train_Loss: 3798.53515625, Val_Loss: 3934.69091796875\n",
      "Epoch 9206, Train_Loss: 3798.552978515625, Val_Loss: 3934.68994140625\n",
      "Epoch 9207, Train_Loss: 3798.534912109375, Val_Loss: 3934.66650390625\n",
      "Epoch 9208, Train_Loss: 3798.559814453125, Val_Loss: 3934.686279296875\n",
      "Epoch 9209, Train_Loss: 3798.54931640625, Val_Loss: 3934.6796875\n",
      "Epoch 9210, Train_Loss: 3798.53369140625, Val_Loss: 3934.640869140625\n",
      "Epoch 9211, Train_Loss: 3798.51611328125, Val_Loss: 3934.684814453125\n",
      "Epoch 9212, Train_Loss: 3798.537353515625, Val_Loss: 3934.6787109375\n",
      "Epoch 9213, Train_Loss: 3798.532470703125, Val_Loss: 3934.66162109375\n",
      "Epoch 9214, Train_Loss: 3798.52734375, Val_Loss: 3934.644287109375\n",
      "Epoch 9215, Train_Loss: 3798.52783203125, Val_Loss: 3934.656494140625\n",
      "Epoch 9216, Train_Loss: 3798.533935546875, Val_Loss: 3934.654296875\n",
      "Epoch 9217, Train_Loss: 3798.533935546875, Val_Loss: 3934.651123046875\n",
      "Epoch 9218, Train_Loss: 3798.52197265625, Val_Loss: 3934.64404296875\n",
      "Epoch 9219, Train_Loss: 3798.525146484375, Val_Loss: 3934.644287109375\n",
      "Epoch 9220, Train_Loss: 3798.528564453125, Val_Loss: 3934.643310546875\n",
      "Epoch 9221, Train_Loss: 3798.519287109375, Val_Loss: 3934.649658203125\n",
      "Epoch 9222, Train_Loss: 3798.52001953125, Val_Loss: 3934.638427734375\n",
      "Epoch 9223, Train_Loss: 3798.52392578125, Val_Loss: 3934.62841796875\n",
      "Epoch 9224, Train_Loss: 3798.516845703125, Val_Loss: 3934.6416015625\n",
      "Epoch 9225, Train_Loss: 3798.529541015625, Val_Loss: 3934.626708984375\n",
      "Epoch 9226, Train_Loss: 3798.526123046875, Val_Loss: 3934.626708984375\n",
      "Epoch 9227, Train_Loss: 3798.51708984375, Val_Loss: 3934.636474609375\n",
      "Epoch 9228, Train_Loss: 3798.528076171875, Val_Loss: 3934.6240234375\n",
      "Epoch 9229, Train_Loss: 3798.52734375, Val_Loss: 3934.63232421875\n",
      "Epoch 9230, Train_Loss: 3798.512451171875, Val_Loss: 3934.63037109375\n",
      "Epoch 9231, Train_Loss: 3798.517333984375, Val_Loss: 3934.6220703125\n",
      "Epoch 9232, Train_Loss: 3798.523193359375, Val_Loss: 3934.6259765625\n",
      "Epoch 9233, Train_Loss: 3798.50634765625, Val_Loss: 3934.634033203125\n",
      "Epoch 9234, Train_Loss: 3798.512451171875, Val_Loss: 3934.633544921875\n",
      "Epoch 9235, Train_Loss: 3798.5126953125, Val_Loss: 3934.61669921875\n",
      "Epoch 9236, Train_Loss: 3798.5, Val_Loss: 3934.62646484375\n",
      "Epoch 9237, Train_Loss: 3798.511474609375, Val_Loss: 3934.614501953125\n",
      "Epoch 9238, Train_Loss: 3798.510498046875, Val_Loss: 3934.616455078125\n",
      "Epoch 9239, Train_Loss: 3798.49755859375, Val_Loss: 3934.635498046875\n",
      "Epoch 9240, Train_Loss: 3798.51513671875, Val_Loss: 3934.614013671875\n",
      "Epoch 9241, Train_Loss: 3798.51123046875, Val_Loss: 3934.614013671875\n",
      "Epoch 9242, Train_Loss: 3798.498291015625, Val_Loss: 3934.636474609375\n",
      "Epoch 9243, Train_Loss: 3798.4990234375, Val_Loss: 3934.62890625\n",
      "Epoch 9244, Train_Loss: 3798.507080078125, Val_Loss: 3934.622802734375\n",
      "Epoch 9245, Train_Loss: 3798.505615234375, Val_Loss: 3934.615478515625\n",
      "Epoch 9246, Train_Loss: 3798.490234375, Val_Loss: 3934.627685546875\n",
      "Epoch 9247, Train_Loss: 3798.5029296875, Val_Loss: 3934.60400390625\n",
      "Epoch 9248, Train_Loss: 3798.503662109375, Val_Loss: 3934.62109375\n",
      "Epoch 9249, Train_Loss: 3798.490966796875, Val_Loss: 3934.6259765625\n",
      "Epoch 9250, Train_Loss: 3798.4970703125, Val_Loss: 3934.6171875\n",
      "Epoch 9251, Train_Loss: 3798.494384765625, Val_Loss: 3934.613525390625\n",
      "Epoch 9252, Train_Loss: 3798.494140625, Val_Loss: 3934.618896484375\n",
      "Epoch 9253, Train_Loss: 3798.482177734375, Val_Loss: 3934.620361328125\n",
      "Epoch 9254, Train_Loss: 3798.489013671875, Val_Loss: 3934.597900390625\n",
      "Epoch 9255, Train_Loss: 3798.486572265625, Val_Loss: 3934.603271484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9256, Train_Loss: 3798.46923828125, Val_Loss: 3934.62451171875\n",
      "Epoch 9257, Train_Loss: 3798.482177734375, Val_Loss: 3934.604736328125\n",
      "Epoch 9258, Train_Loss: 3798.489013671875, Val_Loss: 3934.607666015625\n",
      "Epoch 9259, Train_Loss: 3798.47998046875, Val_Loss: 3934.61962890625\n",
      "Epoch 9260, Train_Loss: 3798.47607421875, Val_Loss: 3934.623291015625\n",
      "Epoch 9261, Train_Loss: 3798.48486328125, Val_Loss: 3934.597900390625\n",
      "Epoch 9262, Train_Loss: 3798.477783203125, Val_Loss: 3934.59716796875\n",
      "Epoch 9263, Train_Loss: 3798.47021484375, Val_Loss: 3934.615966796875\n",
      "Epoch 9264, Train_Loss: 3798.47802734375, Val_Loss: 3934.6103515625\n",
      "Epoch 9265, Train_Loss: 3798.470947265625, Val_Loss: 3934.589111328125\n",
      "Epoch 9266, Train_Loss: 3798.4697265625, Val_Loss: 3934.5888671875\n",
      "Epoch 9267, Train_Loss: 3798.4755859375, Val_Loss: 3934.598388671875\n",
      "Epoch 9268, Train_Loss: 3798.46630859375, Val_Loss: 3934.61083984375\n",
      "Epoch 9269, Train_Loss: 3798.466796875, Val_Loss: 3934.5966796875\n",
      "Epoch 9270, Train_Loss: 3798.466064453125, Val_Loss: 3934.58642578125\n",
      "Epoch 9271, Train_Loss: 3798.464111328125, Val_Loss: 3934.598388671875\n",
      "Epoch 9272, Train_Loss: 3798.469970703125, Val_Loss: 3934.59521484375\n",
      "Epoch 9273, Train_Loss: 3798.478515625, Val_Loss: 3934.598388671875\n",
      "Epoch 9274, Train_Loss: 3798.46484375, Val_Loss: 3934.58642578125\n",
      "Epoch 9275, Train_Loss: 3798.457763671875, Val_Loss: 3934.58447265625\n",
      "Epoch 9276, Train_Loss: 3798.46337890625, Val_Loss: 3934.584716796875\n",
      "Epoch 9277, Train_Loss: 3798.46728515625, Val_Loss: 3934.5712890625\n",
      "Epoch 9278, Train_Loss: 3798.454833984375, Val_Loss: 3934.58349609375\n",
      "Epoch 9279, Train_Loss: 3798.459228515625, Val_Loss: 3934.567626953125\n",
      "Epoch 9280, Train_Loss: 3798.447509765625, Val_Loss: 3934.590087890625\n",
      "Epoch 9281, Train_Loss: 3798.468505859375, Val_Loss: 3934.567626953125\n",
      "Epoch 9282, Train_Loss: 3798.453857421875, Val_Loss: 3934.56396484375\n",
      "Epoch 9283, Train_Loss: 3798.452392578125, Val_Loss: 3934.565185546875\n",
      "Epoch 9284, Train_Loss: 3798.447509765625, Val_Loss: 3934.56640625\n",
      "Epoch 9285, Train_Loss: 3798.46240234375, Val_Loss: 3934.56689453125\n",
      "Epoch 9286, Train_Loss: 3798.443359375, Val_Loss: 3934.565673828125\n",
      "Epoch 9287, Train_Loss: 3798.446533203125, Val_Loss: 3934.560791015625\n",
      "Epoch 9288, Train_Loss: 3798.4443359375, Val_Loss: 3934.555908203125\n",
      "Epoch 9289, Train_Loss: 3798.442626953125, Val_Loss: 3934.564453125\n",
      "Epoch 9290, Train_Loss: 3798.4404296875, Val_Loss: 3934.56396484375\n",
      "Epoch 9291, Train_Loss: 3798.44287109375, Val_Loss: 3934.560791015625\n",
      "Epoch 9292, Train_Loss: 3798.44189453125, Val_Loss: 3934.558349609375\n",
      "Epoch 9293, Train_Loss: 3798.434326171875, Val_Loss: 3934.552490234375\n",
      "Epoch 9294, Train_Loss: 3798.435546875, Val_Loss: 3934.5537109375\n",
      "Epoch 9295, Train_Loss: 3798.437255859375, Val_Loss: 3934.551513671875\n",
      "Epoch 9296, Train_Loss: 3798.43408203125, Val_Loss: 3934.5595703125\n",
      "Epoch 9297, Train_Loss: 3798.424560546875, Val_Loss: 3934.547119140625\n",
      "Epoch 9298, Train_Loss: 3798.428466796875, Val_Loss: 3934.538818359375\n",
      "Epoch 9299, Train_Loss: 3798.4228515625, Val_Loss: 3934.534423828125\n",
      "Epoch 9300, Train_Loss: 3798.427734375, Val_Loss: 3934.554443359375\n",
      "Epoch 9301, Train_Loss: 3798.423583984375, Val_Loss: 3934.533935546875\n",
      "Epoch 9302, Train_Loss: 3798.423828125, Val_Loss: 3934.534423828125\n",
      "Epoch 9303, Train_Loss: 3798.42333984375, Val_Loss: 3934.531982421875\n",
      "Epoch 9304, Train_Loss: 3798.42822265625, Val_Loss: 3934.540771484375\n",
      "Epoch 9305, Train_Loss: 3798.421875, Val_Loss: 3934.5380859375\n",
      "Epoch 9306, Train_Loss: 3798.423095703125, Val_Loss: 3934.52587890625\n",
      "Epoch 9307, Train_Loss: 3798.42138671875, Val_Loss: 3934.527099609375\n",
      "Epoch 9308, Train_Loss: 3798.41943359375, Val_Loss: 3934.530029296875\n",
      "Epoch 9309, Train_Loss: 3798.421142578125, Val_Loss: 3934.531494140625\n",
      "Epoch 9310, Train_Loss: 3798.419677734375, Val_Loss: 3934.5263671875\n",
      "Epoch 9311, Train_Loss: 3798.42236328125, Val_Loss: 3934.530517578125\n",
      "Epoch 9312, Train_Loss: 3798.419677734375, Val_Loss: 3934.53759765625\n",
      "Epoch 9313, Train_Loss: 3798.420654296875, Val_Loss: 3934.5205078125\n",
      "Epoch 9314, Train_Loss: 3798.4189453125, Val_Loss: 3934.533935546875\n",
      "Epoch 9315, Train_Loss: 3798.416748046875, Val_Loss: 3934.528076171875\n",
      "Epoch 9316, Train_Loss: 3798.41455078125, Val_Loss: 3934.52880859375\n",
      "Epoch 9317, Train_Loss: 3798.396240234375, Val_Loss: 3934.543212890625\n",
      "Epoch 9318, Train_Loss: 3798.41259765625, Val_Loss: 3934.5283203125\n",
      "Epoch 9319, Train_Loss: 3798.39208984375, Val_Loss: 3934.541259765625\n",
      "Epoch 9320, Train_Loss: 3798.388916015625, Val_Loss: 3934.54150390625\n",
      "Epoch 9321, Train_Loss: 3798.39013671875, Val_Loss: 3934.531494140625\n",
      "Epoch 9322, Train_Loss: 3798.3896484375, Val_Loss: 3934.53515625\n",
      "Epoch 9323, Train_Loss: 3798.388916015625, Val_Loss: 3934.53369140625\n",
      "Epoch 9324, Train_Loss: 3798.38623046875, Val_Loss: 3934.5283203125\n",
      "Epoch 9325, Train_Loss: 3798.384521484375, Val_Loss: 3934.5244140625\n",
      "Epoch 9326, Train_Loss: 3798.385009765625, Val_Loss: 3934.522705078125\n",
      "Epoch 9327, Train_Loss: 3798.38232421875, Val_Loss: 3934.52392578125\n",
      "Epoch 9328, Train_Loss: 3798.379150390625, Val_Loss: 3934.527099609375\n",
      "Epoch 9329, Train_Loss: 3798.37939453125, Val_Loss: 3934.52392578125\n",
      "Epoch 9330, Train_Loss: 3798.3798828125, Val_Loss: 3934.519287109375\n",
      "Epoch 9331, Train_Loss: 3798.376953125, Val_Loss: 3934.522705078125\n",
      "Epoch 9332, Train_Loss: 3798.373291015625, Val_Loss: 3934.5244140625\n",
      "Epoch 9333, Train_Loss: 3798.374755859375, Val_Loss: 3934.52197265625\n",
      "Epoch 9334, Train_Loss: 3798.372314453125, Val_Loss: 3934.51611328125\n",
      "Epoch 9335, Train_Loss: 3798.368896484375, Val_Loss: 3934.512451171875\n",
      "Epoch 9336, Train_Loss: 3798.302978515625, Val_Loss: 3934.514892578125\n",
      "Epoch 9337, Train_Loss: 3798.30517578125, Val_Loss: 3934.515625\n",
      "Epoch 9338, Train_Loss: 3798.302978515625, Val_Loss: 3934.511962890625\n",
      "Epoch 9339, Train_Loss: 3798.3642578125, Val_Loss: 3934.510009765625\n",
      "Epoch 9340, Train_Loss: 3798.300048828125, Val_Loss: 3934.513671875\n",
      "Epoch 9341, Train_Loss: 3798.298828125, Val_Loss: 3934.51513671875\n",
      "Epoch 9342, Train_Loss: 3798.36181640625, Val_Loss: 3934.507568359375\n",
      "Epoch 9343, Train_Loss: 3798.29638671875, Val_Loss: 3934.5048828125\n",
      "Epoch 9344, Train_Loss: 3798.29541015625, Val_Loss: 3934.503173828125\n",
      "Epoch 9345, Train_Loss: 3798.2978515625, Val_Loss: 3934.501220703125\n",
      "Epoch 9346, Train_Loss: 3798.294921875, Val_Loss: 3934.501953125\n",
      "Epoch 9347, Train_Loss: 3798.292236328125, Val_Loss: 3934.503662109375\n",
      "Epoch 9348, Train_Loss: 3798.288330078125, Val_Loss: 3934.49951171875\n",
      "Epoch 9349, Train_Loss: 3798.285400390625, Val_Loss: 3934.501220703125\n",
      "Epoch 9350, Train_Loss: 3798.28466796875, Val_Loss: 3934.498779296875\n",
      "Epoch 9351, Train_Loss: 3798.283935546875, Val_Loss: 3934.49755859375\n",
      "Epoch 9352, Train_Loss: 3798.279052734375, Val_Loss: 3934.498046875\n",
      "Epoch 9353, Train_Loss: 3798.280517578125, Val_Loss: 3934.492431640625\n",
      "Epoch 9354, Train_Loss: 3798.2802734375, Val_Loss: 3934.48876953125\n",
      "Epoch 9355, Train_Loss: 3798.331298828125, Val_Loss: 3934.48388671875\n",
      "Epoch 9356, Train_Loss: 3798.283447265625, Val_Loss: 3934.47998046875\n",
      "Epoch 9357, Train_Loss: 3798.299072265625, Val_Loss: 3934.466064453125\n",
      "Epoch 9358, Train_Loss: 3798.33544921875, Val_Loss: 3934.49169921875\n",
      "Epoch 9359, Train_Loss: 3798.3359375, Val_Loss: 3934.478759765625\n",
      "Epoch 9360, Train_Loss: 3798.298095703125, Val_Loss: 3934.443603515625\n",
      "Epoch 9361, Train_Loss: 3798.327880859375, Val_Loss: 3934.501220703125\n",
      "Epoch 9362, Train_Loss: 3798.280517578125, Val_Loss: 3934.470458984375\n",
      "Epoch 9363, Train_Loss: 3798.2841796875, Val_Loss: 3934.436767578125\n",
      "Epoch 9364, Train_Loss: 3798.353515625, Val_Loss: 3934.520751953125\n",
      "Epoch 9365, Train_Loss: 3798.27685546875, Val_Loss: 3934.46875\n",
      "Epoch 9366, Train_Loss: 3798.294189453125, Val_Loss: 3934.419189453125\n",
      "Epoch 9367, Train_Loss: 3798.347900390625, Val_Loss: 3934.501220703125\n",
      "Epoch 9368, Train_Loss: 3798.34228515625, Val_Loss: 3934.48046875\n",
      "Epoch 9369, Train_Loss: 3798.28173828125, Val_Loss: 3934.42041015625\n",
      "Epoch 9370, Train_Loss: 3798.3427734375, Val_Loss: 3934.49609375\n",
      "Epoch 9371, Train_Loss: 3798.275146484375, Val_Loss: 3934.457275390625\n",
      "Epoch 9372, Train_Loss: 3798.27392578125, Val_Loss: 3934.417236328125\n",
      "Epoch 9373, Train_Loss: 3798.360107421875, Val_Loss: 3934.492431640625\n",
      "Epoch 9374, Train_Loss: 3798.270751953125, Val_Loss: 3934.45556640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9375, Train_Loss: 3798.291748046875, Val_Loss: 3934.404296875\n",
      "Epoch 9376, Train_Loss: 3798.341796875, Val_Loss: 3934.5263671875\n",
      "Epoch 9377, Train_Loss: 3798.343017578125, Val_Loss: 3934.446044921875\n",
      "Epoch 9378, Train_Loss: 3798.292236328125, Val_Loss: 3934.38720703125\n",
      "Epoch 9379, Train_Loss: 3798.33447265625, Val_Loss: 3934.4775390625\n",
      "Epoch 9380, Train_Loss: 3798.345703125, Val_Loss: 3934.500732421875\n",
      "Epoch 9381, Train_Loss: 3798.296630859375, Val_Loss: 3934.384765625\n",
      "Epoch 9382, Train_Loss: 3798.306396484375, Val_Loss: 3934.426513671875\n",
      "Epoch 9383, Train_Loss: 3798.346923828125, Val_Loss: 3934.4931640625\n",
      "Epoch 9384, Train_Loss: 3798.271728515625, Val_Loss: 3934.399169921875\n",
      "Epoch 9385, Train_Loss: 3798.334228515625, Val_Loss: 3934.4443359375\n",
      "Epoch 9386, Train_Loss: 3798.3310546875, Val_Loss: 3934.472412109375\n",
      "Epoch 9387, Train_Loss: 3798.2705078125, Val_Loss: 3934.430419921875\n",
      "Epoch 9388, Train_Loss: 3798.328857421875, Val_Loss: 3934.44921875\n",
      "Epoch 9389, Train_Loss: 3798.34130859375, Val_Loss: 3934.431884765625\n",
      "Epoch 9390, Train_Loss: 3798.27880859375, Val_Loss: 3934.414306640625\n",
      "Epoch 9391, Train_Loss: 3798.323486328125, Val_Loss: 3934.478515625\n",
      "Epoch 9392, Train_Loss: 3798.314697265625, Val_Loss: 3934.419677734375\n",
      "Epoch 9393, Train_Loss: 3798.287109375, Val_Loss: 3934.37646484375\n",
      "Epoch 9394, Train_Loss: 3798.31298828125, Val_Loss: 3934.50244140625\n",
      "Epoch 9395, Train_Loss: 3798.2998046875, Val_Loss: 3934.42236328125\n",
      "Epoch 9396, Train_Loss: 3798.279296875, Val_Loss: 3934.357177734375\n",
      "Epoch 9397, Train_Loss: 3798.321044921875, Val_Loss: 3934.488037109375\n",
      "Epoch 9398, Train_Loss: 3798.31201171875, Val_Loss: 3934.449951171875\n",
      "Epoch 9399, Train_Loss: 3798.268798828125, Val_Loss: 3934.361572265625\n",
      "Epoch 9400, Train_Loss: 3798.307861328125, Val_Loss: 3934.466064453125\n",
      "Epoch 9401, Train_Loss: 3798.30517578125, Val_Loss: 3934.467529296875\n",
      "Epoch 9402, Train_Loss: 3798.26513671875, Val_Loss: 3934.367919921875\n",
      "Epoch 9403, Train_Loss: 3798.303955078125, Val_Loss: 3934.4580078125\n",
      "Epoch 9404, Train_Loss: 3798.310791015625, Val_Loss: 3934.4462890625\n",
      "Epoch 9405, Train_Loss: 3798.27294921875, Val_Loss: 3934.372802734375\n",
      "Epoch 9406, Train_Loss: 3798.30419921875, Val_Loss: 3934.464111328125\n",
      "Epoch 9407, Train_Loss: 3798.28759765625, Val_Loss: 3934.40869140625\n",
      "Epoch 9408, Train_Loss: 3798.254150390625, Val_Loss: 3934.399658203125\n",
      "Epoch 9409, Train_Loss: 3798.308837890625, Val_Loss: 3934.459228515625\n",
      "Epoch 9410, Train_Loss: 3798.25439453125, Val_Loss: 3934.401123046875\n",
      "Epoch 9411, Train_Loss: 3798.2900390625, Val_Loss: 3934.412109375\n",
      "Epoch 9412, Train_Loss: 3798.31689453125, Val_Loss: 3934.421630859375\n",
      "Epoch 9413, Train_Loss: 3798.28662109375, Val_Loss: 3934.39794921875\n",
      "Epoch 9414, Train_Loss: 3798.306396484375, Val_Loss: 3934.43603515625\n",
      "Epoch 9415, Train_Loss: 3798.291748046875, Val_Loss: 3934.401611328125\n",
      "Epoch 9416, Train_Loss: 3798.270263671875, Val_Loss: 3934.3720703125\n",
      "Epoch 9417, Train_Loss: 3798.294677734375, Val_Loss: 3934.463134765625\n",
      "Epoch 9418, Train_Loss: 3798.284423828125, Val_Loss: 3934.396728515625\n",
      "Epoch 9419, Train_Loss: 3798.261474609375, Val_Loss: 3934.3583984375\n",
      "Epoch 9420, Train_Loss: 3798.286376953125, Val_Loss: 3934.469482421875\n",
      "Epoch 9421, Train_Loss: 3798.2763671875, Val_Loss: 3934.399658203125\n",
      "Epoch 9422, Train_Loss: 3798.25537109375, Val_Loss: 3934.325927734375\n",
      "Epoch 9423, Train_Loss: 3798.3369140625, Val_Loss: 3934.45849609375\n",
      "Epoch 9424, Train_Loss: 3798.29931640625, Val_Loss: 3934.412353515625\n",
      "Epoch 9425, Train_Loss: 3798.264404296875, Val_Loss: 3934.33154296875\n",
      "Epoch 9426, Train_Loss: 3798.2919921875, Val_Loss: 3934.432861328125\n",
      "Epoch 9427, Train_Loss: 3798.285888671875, Val_Loss: 3934.40869140625\n",
      "Epoch 9428, Train_Loss: 3798.247314453125, Val_Loss: 3934.335205078125\n",
      "Epoch 9429, Train_Loss: 3798.292236328125, Val_Loss: 3934.414794921875\n",
      "Epoch 9430, Train_Loss: 3798.2900390625, Val_Loss: 3934.4072265625\n",
      "Epoch 9431, Train_Loss: 3798.24609375, Val_Loss: 3934.35595703125\n",
      "Epoch 9432, Train_Loss: 3798.294677734375, Val_Loss: 3934.398681640625\n",
      "Epoch 9433, Train_Loss: 3798.291748046875, Val_Loss: 3934.39794921875\n",
      "Epoch 9434, Train_Loss: 3798.247314453125, Val_Loss: 3934.35205078125\n",
      "Epoch 9435, Train_Loss: 3798.28515625, Val_Loss: 3934.405517578125\n",
      "Epoch 9436, Train_Loss: 3798.288330078125, Val_Loss: 3934.392822265625\n",
      "Epoch 9437, Train_Loss: 3798.24462890625, Val_Loss: 3934.344482421875\n",
      "Epoch 9438, Train_Loss: 3798.28173828125, Val_Loss: 3934.41748046875\n",
      "Epoch 9439, Train_Loss: 3798.27587890625, Val_Loss: 3934.376708984375\n",
      "Epoch 9440, Train_Loss: 3798.234619140625, Val_Loss: 3934.33837890625\n",
      "Epoch 9441, Train_Loss: 3798.282470703125, Val_Loss: 3934.38037109375\n",
      "Epoch 9442, Train_Loss: 3798.2783203125, Val_Loss: 3934.380859375\n",
      "Epoch 9443, Train_Loss: 3798.24462890625, Val_Loss: 3934.327880859375\n",
      "Epoch 9444, Train_Loss: 3798.27685546875, Val_Loss: 3934.383544921875\n",
      "Epoch 9445, Train_Loss: 3798.2841796875, Val_Loss: 3934.3740234375\n",
      "Epoch 9446, Train_Loss: 3798.237060546875, Val_Loss: 3934.323486328125\n",
      "Epoch 9447, Train_Loss: 3798.271484375, Val_Loss: 3934.383544921875\n",
      "Epoch 9448, Train_Loss: 3798.28173828125, Val_Loss: 3934.36669921875\n",
      "Epoch 9449, Train_Loss: 3798.237060546875, Val_Loss: 3934.3115234375\n",
      "Epoch 9450, Train_Loss: 3798.260498046875, Val_Loss: 3934.397216796875\n",
      "Epoch 9451, Train_Loss: 3798.2724609375, Val_Loss: 3934.360107421875\n",
      "Epoch 9452, Train_Loss: 3798.228271484375, Val_Loss: 3934.30517578125\n",
      "Epoch 9453, Train_Loss: 3798.284912109375, Val_Loss: 3934.423583984375\n",
      "Epoch 9454, Train_Loss: 3798.252685546875, Val_Loss: 3934.333251953125\n",
      "Epoch 9455, Train_Loss: 3798.225341796875, Val_Loss: 3934.30126953125\n",
      "Epoch 9456, Train_Loss: 3798.328857421875, Val_Loss: 3934.416748046875\n",
      "Epoch 9457, Train_Loss: 3798.2421875, Val_Loss: 3934.343505859375\n",
      "Epoch 9458, Train_Loss: 3798.2451171875, Val_Loss: 3934.295654296875\n",
      "Epoch 9459, Train_Loss: 3798.323486328125, Val_Loss: 3934.443115234375\n",
      "Epoch 9460, Train_Loss: 3798.242919921875, Val_Loss: 3934.30517578125\n",
      "Epoch 9461, Train_Loss: 3798.244140625, Val_Loss: 3934.333984375\n",
      "Epoch 9462, Train_Loss: 3798.275146484375, Val_Loss: 3934.385986328125\n",
      "Epoch 9463, Train_Loss: 3798.257080078125, Val_Loss: 3934.2958984375\n",
      "Epoch 9464, Train_Loss: 3798.281494140625, Val_Loss: 3934.353515625\n",
      "Epoch 9465, Train_Loss: 3798.237060546875, Val_Loss: 3934.32958984375\n",
      "Epoch 9466, Train_Loss: 3798.265380859375, Val_Loss: 3934.330322265625\n",
      "Epoch 9467, Train_Loss: 3798.265625, Val_Loss: 3934.381103515625\n",
      "Epoch 9468, Train_Loss: 3798.259521484375, Val_Loss: 3934.33154296875\n",
      "Epoch 9469, Train_Loss: 3798.255615234375, Val_Loss: 3934.345947265625\n",
      "Epoch 9470, Train_Loss: 3798.25537109375, Val_Loss: 3934.33349609375\n",
      "Epoch 9471, Train_Loss: 3798.256103515625, Val_Loss: 3934.33349609375\n",
      "Epoch 9472, Train_Loss: 3798.25732421875, Val_Loss: 3934.3251953125\n",
      "Epoch 9473, Train_Loss: 3798.26025390625, Val_Loss: 3934.31591796875\n",
      "Epoch 9474, Train_Loss: 3798.24365234375, Val_Loss: 3934.37109375\n",
      "Epoch 9475, Train_Loss: 3798.2548828125, Val_Loss: 3934.3212890625\n",
      "Epoch 9476, Train_Loss: 3798.24658203125, Val_Loss: 3934.3291015625\n",
      "Epoch 9477, Train_Loss: 3798.263427734375, Val_Loss: 3934.341552734375\n",
      "Epoch 9478, Train_Loss: 3798.236572265625, Val_Loss: 3934.278076171875\n",
      "Epoch 9479, Train_Loss: 3798.32373046875, Val_Loss: 3934.379150390625\n",
      "Epoch 9480, Train_Loss: 3798.25927734375, Val_Loss: 3934.286376953125\n",
      "Epoch 9481, Train_Loss: 3798.250732421875, Val_Loss: 3934.34130859375\n",
      "Epoch 9482, Train_Loss: 3798.227294921875, Val_Loss: 3934.32275390625\n",
      "Epoch 9483, Train_Loss: 3798.2255859375, Val_Loss: 3934.31640625\n",
      "Epoch 9484, Train_Loss: 3798.25830078125, Val_Loss: 3934.3369140625\n",
      "Epoch 9485, Train_Loss: 3798.2490234375, Val_Loss: 3934.29638671875\n",
      "Epoch 9486, Train_Loss: 3798.338623046875, Val_Loss: 3934.3515625\n",
      "Epoch 9487, Train_Loss: 3798.263427734375, Val_Loss: 3934.27197265625\n",
      "Epoch 9488, Train_Loss: 3798.24169921875, Val_Loss: 3934.32958984375\n",
      "Epoch 9489, Train_Loss: 3798.21826171875, Val_Loss: 3934.311279296875\n",
      "Epoch 9490, Train_Loss: 3798.225341796875, Val_Loss: 3934.3056640625\n",
      "Epoch 9491, Train_Loss: 3798.2763671875, Val_Loss: 3934.34033203125\n",
      "Epoch 9492, Train_Loss: 3798.23291015625, Val_Loss: 3934.295654296875\n",
      "Epoch 9493, Train_Loss: 3798.223388671875, Val_Loss: 3934.342041015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9494, Train_Loss: 3798.22216796875, Val_Loss: 3934.29638671875\n",
      "Epoch 9495, Train_Loss: 3798.261474609375, Val_Loss: 3934.28955078125\n",
      "Epoch 9496, Train_Loss: 3798.214111328125, Val_Loss: 3934.306884765625\n",
      "Epoch 9497, Train_Loss: 3798.212890625, Val_Loss: 3934.295166015625\n",
      "Epoch 9498, Train_Loss: 3798.275390625, Val_Loss: 3934.328857421875\n",
      "Epoch 9499, Train_Loss: 3798.232421875, Val_Loss: 3934.283935546875\n",
      "Epoch 9500, Train_Loss: 3798.32177734375, Val_Loss: 3934.335205078125\n",
      "Epoch 9501, Train_Loss: 3798.228515625, Val_Loss: 3934.279296875\n",
      "Epoch 9502, Train_Loss: 3798.269287109375, Val_Loss: 3934.3212890625\n",
      "Epoch 9503, Train_Loss: 3798.258544921875, Val_Loss: 3934.27392578125\n",
      "Epoch 9504, Train_Loss: 3798.258544921875, Val_Loss: 3934.279296875\n",
      "Epoch 9505, Train_Loss: 3798.24267578125, Val_Loss: 3934.277099609375\n",
      "Epoch 9506, Train_Loss: 3798.24072265625, Val_Loss: 3934.2744140625\n",
      "Epoch 9507, Train_Loss: 3798.25, Val_Loss: 3934.279296875\n",
      "Epoch 9508, Train_Loss: 3798.2470703125, Val_Loss: 3934.268798828125\n",
      "Epoch 9509, Train_Loss: 3798.2705078125, Val_Loss: 3934.29248046875\n",
      "Epoch 9510, Train_Loss: 3798.2724609375, Val_Loss: 3934.246337890625\n",
      "Epoch 9511, Train_Loss: 3798.30517578125, Val_Loss: 3934.3271484375\n",
      "Epoch 9512, Train_Loss: 3798.2548828125, Val_Loss: 3934.25439453125\n",
      "Epoch 9513, Train_Loss: 3798.24951171875, Val_Loss: 3934.268798828125\n",
      "Epoch 9514, Train_Loss: 3798.185546875, Val_Loss: 3934.27685546875\n",
      "Epoch 9515, Train_Loss: 3798.23779296875, Val_Loss: 3934.2705078125\n",
      "Epoch 9516, Train_Loss: 3798.25, Val_Loss: 3934.248046875\n",
      "Epoch 9517, Train_Loss: 3798.302490234375, Val_Loss: 3934.294921875\n",
      "Epoch 9518, Train_Loss: 3798.261962890625, Val_Loss: 3934.23046875\n",
      "Epoch 9519, Train_Loss: 3798.2646484375, Val_Loss: 3934.29248046875\n",
      "Epoch 9520, Train_Loss: 3798.24755859375, Val_Loss: 3934.2451171875\n",
      "Epoch 9521, Train_Loss: 3798.26513671875, Val_Loss: 3934.279541015625\n",
      "Epoch 9522, Train_Loss: 3798.248291015625, Val_Loss: 3934.2392578125\n",
      "Epoch 9523, Train_Loss: 3798.294921875, Val_Loss: 3934.324462890625\n",
      "Epoch 9524, Train_Loss: 3798.253662109375, Val_Loss: 3934.2275390625\n",
      "Epoch 9525, Train_Loss: 3798.26171875, Val_Loss: 3934.26806640625\n",
      "Epoch 9526, Train_Loss: 3798.236572265625, Val_Loss: 3934.239990234375\n",
      "Epoch 9527, Train_Loss: 3798.2353515625, Val_Loss: 3934.25634765625\n",
      "Epoch 9528, Train_Loss: 3798.23828125, Val_Loss: 3934.23291015625\n",
      "Epoch 9529, Train_Loss: 3798.286865234375, Val_Loss: 3934.28271484375\n",
      "Epoch 9530, Train_Loss: 3798.24755859375, Val_Loss: 3934.22119140625\n",
      "Epoch 9531, Train_Loss: 3798.281005859375, Val_Loss: 3934.281982421875\n",
      "Epoch 9532, Train_Loss: 3798.23291015625, Val_Loss: 3934.22119140625\n",
      "Epoch 9533, Train_Loss: 3798.243408203125, Val_Loss: 3934.289306640625\n",
      "Epoch 9534, Train_Loss: 3798.253173828125, Val_Loss: 3934.201171875\n",
      "Epoch 9535, Train_Loss: 3798.27685546875, Val_Loss: 3934.31591796875\n",
      "Epoch 9536, Train_Loss: 3798.24609375, Val_Loss: 3934.203125\n",
      "Epoch 9537, Train_Loss: 3798.238525390625, Val_Loss: 3934.273193359375\n",
      "Epoch 9538, Train_Loss: 3798.23291015625, Val_Loss: 3934.218017578125\n",
      "Epoch 9539, Train_Loss: 3798.234619140625, Val_Loss: 3934.280029296875\n",
      "Epoch 9540, Train_Loss: 3798.24169921875, Val_Loss: 3934.1943359375\n",
      "Epoch 9541, Train_Loss: 3798.27392578125, Val_Loss: 3934.3271484375\n",
      "Epoch 9542, Train_Loss: 3798.23095703125, Val_Loss: 3934.213623046875\n",
      "Epoch 9543, Train_Loss: 3798.2060546875, Val_Loss: 3934.228515625\n",
      "Epoch 9544, Train_Loss: 3798.272705078125, Val_Loss: 3934.26611328125\n",
      "Epoch 9545, Train_Loss: 3798.236328125, Val_Loss: 3934.185546875\n",
      "Epoch 9546, Train_Loss: 3798.27197265625, Val_Loss: 3934.30029296875\n",
      "Epoch 9547, Train_Loss: 3798.222900390625, Val_Loss: 3934.20849609375\n",
      "Epoch 9548, Train_Loss: 3798.19775390625, Val_Loss: 3934.242431640625\n",
      "Epoch 9549, Train_Loss: 3798.208251953125, Val_Loss: 3934.20751953125\n",
      "Epoch 9550, Train_Loss: 3798.268310546875, Val_Loss: 3934.248046875\n",
      "Epoch 9551, Train_Loss: 3798.224853515625, Val_Loss: 3934.20068359375\n",
      "Epoch 9552, Train_Loss: 3798.2646484375, Val_Loss: 3934.298095703125\n",
      "Epoch 9553, Train_Loss: 3798.223876953125, Val_Loss: 3934.191162109375\n",
      "Epoch 9554, Train_Loss: 3798.198486328125, Val_Loss: 3934.216064453125\n",
      "Epoch 9555, Train_Loss: 3798.2607421875, Val_Loss: 3934.273681640625\n",
      "Epoch 9556, Train_Loss: 3798.2197265625, Val_Loss: 3934.17236328125\n",
      "Epoch 9557, Train_Loss: 3798.254150390625, Val_Loss: 3934.244873046875\n",
      "Epoch 9558, Train_Loss: 3798.1845703125, Val_Loss: 3934.2255859375\n",
      "Epoch 9559, Train_Loss: 3798.1982421875, Val_Loss: 3934.19970703125\n",
      "Epoch 9560, Train_Loss: 3798.266845703125, Val_Loss: 3934.26953125\n",
      "Epoch 9561, Train_Loss: 3798.2060546875, Val_Loss: 3934.1787109375\n",
      "Epoch 9562, Train_Loss: 3798.251708984375, Val_Loss: 3934.224365234375\n",
      "Epoch 9563, Train_Loss: 3798.193603515625, Val_Loss: 3934.202392578125\n",
      "Epoch 9564, Train_Loss: 3798.25, Val_Loss: 3934.219970703125\n",
      "Epoch 9565, Train_Loss: 3798.20068359375, Val_Loss: 3934.178466796875\n",
      "Epoch 9566, Train_Loss: 3798.25537109375, Val_Loss: 3934.265625\n",
      "Epoch 9567, Train_Loss: 3798.191650390625, Val_Loss: 3934.184326171875\n",
      "Epoch 9568, Train_Loss: 3798.1796875, Val_Loss: 3934.209716796875\n",
      "Epoch 9569, Train_Loss: 3798.18408203125, Val_Loss: 3934.18310546875\n",
      "Epoch 9570, Train_Loss: 3798.251708984375, Val_Loss: 3934.2392578125\n",
      "Epoch 9571, Train_Loss: 3798.186767578125, Val_Loss: 3934.15478515625\n",
      "Epoch 9572, Train_Loss: 3798.23681640625, Val_Loss: 3934.229248046875\n",
      "Epoch 9573, Train_Loss: 3798.16748046875, Val_Loss: 3934.19970703125\n",
      "Epoch 9574, Train_Loss: 3798.243408203125, Val_Loss: 3934.191162109375\n",
      "Epoch 9575, Train_Loss: 3798.1884765625, Val_Loss: 3934.150390625\n",
      "Epoch 9576, Train_Loss: 3798.161865234375, Val_Loss: 3934.284912109375\n",
      "Epoch 9577, Train_Loss: 3798.1943359375, Val_Loss: 3934.140380859375\n",
      "Epoch 9578, Train_Loss: 3798.240478515625, Val_Loss: 3934.173583984375\n",
      "Epoch 9579, Train_Loss: 3798.16015625, Val_Loss: 3934.17919921875\n",
      "Epoch 9580, Train_Loss: 3798.1650390625, Val_Loss: 3934.17919921875\n",
      "Epoch 9581, Train_Loss: 3798.225341796875, Val_Loss: 3934.20068359375\n",
      "Epoch 9582, Train_Loss: 3798.189208984375, Val_Loss: 3934.1279296875\n",
      "Epoch 9583, Train_Loss: 3798.235595703125, Val_Loss: 3934.224365234375\n",
      "Epoch 9584, Train_Loss: 3798.166748046875, Val_Loss: 3934.155517578125\n",
      "Epoch 9585, Train_Loss: 3798.213623046875, Val_Loss: 3934.15673828125\n",
      "Epoch 9586, Train_Loss: 3798.160888671875, Val_Loss: 3934.1455078125\n",
      "Epoch 9587, Train_Loss: 3798.2080078125, Val_Loss: 3934.231201171875\n",
      "Epoch 9588, Train_Loss: 3798.177001953125, Val_Loss: 3934.129150390625\n",
      "Epoch 9589, Train_Loss: 3798.222900390625, Val_Loss: 3934.204345703125\n",
      "Epoch 9590, Train_Loss: 3798.1630859375, Val_Loss: 3934.12841796875\n",
      "Epoch 9591, Train_Loss: 3798.19482421875, Val_Loss: 3934.224365234375\n",
      "Epoch 9592, Train_Loss: 3798.15771484375, Val_Loss: 3934.140869140625\n",
      "Epoch 9593, Train_Loss: 3798.204833984375, Val_Loss: 3934.14990234375\n",
      "Epoch 9594, Train_Loss: 3798.159423828125, Val_Loss: 3934.125244140625\n",
      "Epoch 9595, Train_Loss: 3798.145263671875, Val_Loss: 3934.249267578125\n",
      "Epoch 9596, Train_Loss: 3798.163330078125, Val_Loss: 3934.09033203125\n",
      "Epoch 9597, Train_Loss: 3798.20068359375, Val_Loss: 3934.1708984375\n",
      "Epoch 9598, Train_Loss: 3798.184814453125, Val_Loss: 3934.14794921875\n",
      "Epoch 9599, Train_Loss: 3798.152587890625, Val_Loss: 3934.1416015625\n",
      "Epoch 9600, Train_Loss: 3798.1953125, Val_Loss: 3934.21044921875\n",
      "Epoch 9601, Train_Loss: 3798.159423828125, Val_Loss: 3934.0869140625\n",
      "Epoch 9602, Train_Loss: 3798.18994140625, Val_Loss: 3934.18359375\n",
      "Epoch 9603, Train_Loss: 3798.117919921875, Val_Loss: 3934.15478515625\n",
      "Epoch 9604, Train_Loss: 3798.171875, Val_Loss: 3934.154296875\n",
      "Epoch 9605, Train_Loss: 3798.158203125, Val_Loss: 3934.09716796875\n",
      "Epoch 9606, Train_Loss: 3798.140869140625, Val_Loss: 3934.242919921875\n",
      "Epoch 9607, Train_Loss: 3798.1337890625, Val_Loss: 3934.09912109375\n",
      "Epoch 9608, Train_Loss: 3798.1611328125, Val_Loss: 3934.147705078125\n",
      "Epoch 9609, Train_Loss: 3798.17529296875, Val_Loss: 3934.135986328125\n",
      "Epoch 9610, Train_Loss: 3798.163330078125, Val_Loss: 3934.144287109375\n",
      "Epoch 9611, Train_Loss: 3798.116943359375, Val_Loss: 3934.12890625\n",
      "Epoch 9612, Train_Loss: 3798.180908203125, Val_Loss: 3934.23828125\n",
      "Epoch 9613, Train_Loss: 3798.127197265625, Val_Loss: 3934.091552734375\n",
      "Epoch 9614, Train_Loss: 3798.158935546875, Val_Loss: 3934.135498046875\n",
      "Epoch 9615, Train_Loss: 3798.180419921875, Val_Loss: 3934.15869140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9616, Train_Loss: 3798.1220703125, Val_Loss: 3934.075927734375\n",
      "Epoch 9617, Train_Loss: 3798.1220703125, Val_Loss: 3934.21044921875\n",
      "Epoch 9618, Train_Loss: 3798.159423828125, Val_Loss: 3934.1171875\n",
      "Epoch 9619, Train_Loss: 3798.116455078125, Val_Loss: 3934.06640625\n",
      "Epoch 9620, Train_Loss: 3798.1572265625, Val_Loss: 3934.193603515625\n",
      "Epoch 9621, Train_Loss: 3798.146484375, Val_Loss: 3934.134521484375\n",
      "Epoch 9622, Train_Loss: 3798.1298828125, Val_Loss: 3934.02685546875\n",
      "Epoch 9623, Train_Loss: 3798.162841796875, Val_Loss: 3934.232421875\n",
      "Epoch 9624, Train_Loss: 3798.163330078125, Val_Loss: 3934.162109375\n",
      "Epoch 9625, Train_Loss: 3798.09521484375, Val_Loss: 3933.96435546875\n",
      "Epoch 9626, Train_Loss: 3798.191650390625, Val_Loss: 3934.890869140625\n",
      "Epoch 9627, Train_Loss: 3798.1220703125, Val_Loss: 3934.0205078125\n",
      "Epoch 9628, Train_Loss: 3798.19873046875, Val_Loss: 3933.69677734375\n",
      "Epoch 9629, Train_Loss: 3798.25732421875, Val_Loss: 3935.064453125\n",
      "Epoch 9630, Train_Loss: 3798.14453125, Val_Loss: 3934.2783203125\n",
      "Epoch 9631, Train_Loss: 3798.243408203125, Val_Loss: 3933.565673828125\n",
      "Epoch 9632, Train_Loss: 3798.2314453125, Val_Loss: 3935.0439453125\n",
      "Epoch 9633, Train_Loss: 3798.112548828125, Val_Loss: 3934.66552734375\n",
      "Epoch 9634, Train_Loss: 3798.286376953125, Val_Loss: 3933.517578125\n",
      "Epoch 9635, Train_Loss: 3798.132080078125, Val_Loss: 3934.5400390625\n",
      "Epoch 9636, Train_Loss: 3798.215576171875, Val_Loss: 3934.90087890625\n",
      "Epoch 9637, Train_Loss: 3798.20654296875, Val_Loss: 3933.718017578125\n",
      "Epoch 9638, Train_Loss: 3798.167724609375, Val_Loss: 3934.106689453125\n",
      "Epoch 9639, Train_Loss: 3798.134521484375, Val_Loss: 3934.50634765625\n",
      "Epoch 9640, Train_Loss: 3798.1201171875, Val_Loss: 3933.97802734375\n",
      "Epoch 9641, Train_Loss: 3798.140869140625, Val_Loss: 3933.862060546875\n",
      "Epoch 9642, Train_Loss: 3798.140380859375, Val_Loss: 3934.790771484375\n",
      "Epoch 9643, Train_Loss: 3798.140869140625, Val_Loss: 3934.2119140625\n",
      "Epoch 9644, Train_Loss: 3798.157958984375, Val_Loss: 3933.640380859375\n",
      "Epoch 9645, Train_Loss: 3798.119873046875, Val_Loss: 3934.612060546875\n",
      "Epoch 9646, Train_Loss: 3798.07470703125, Val_Loss: 3934.574462890625\n",
      "Epoch 9647, Train_Loss: 3798.193115234375, Val_Loss: 3933.666015625\n",
      "Epoch 9648, Train_Loss: 3798.12353515625, Val_Loss: 3934.2373046875\n",
      "Epoch 9649, Train_Loss: 3798.091552734375, Val_Loss: 3934.61669921875\n",
      "Epoch 9650, Train_Loss: 3798.082275390625, Val_Loss: 3933.89990234375\n",
      "Epoch 9651, Train_Loss: 3798.140380859375, Val_Loss: 3934.20068359375\n",
      "Epoch 9652, Train_Loss: 3798.129638671875, Val_Loss: 3934.25\n",
      "Epoch 9653, Train_Loss: 3798.134521484375, Val_Loss: 3933.99560546875\n",
      "Epoch 9654, Train_Loss: 3798.135009765625, Val_Loss: 3933.99169921875\n",
      "Epoch 9655, Train_Loss: 3798.114990234375, Val_Loss: 3934.231201171875\n",
      "Epoch 9656, Train_Loss: 3798.138671875, Val_Loss: 3934.15869140625\n",
      "Epoch 9657, Train_Loss: 3798.058349609375, Val_Loss: 3933.92431640625\n",
      "Epoch 9658, Train_Loss: 3798.109619140625, Val_Loss: 3934.5966796875\n",
      "Epoch 9659, Train_Loss: 3798.1181640625, Val_Loss: 3934.1376953125\n",
      "Epoch 9660, Train_Loss: 3798.15234375, Val_Loss: 3933.711181640625\n",
      "Epoch 9661, Train_Loss: 3798.09521484375, Val_Loss: 3934.56689453125\n",
      "Epoch 9662, Train_Loss: 3798.028076171875, Val_Loss: 3934.436279296875\n",
      "Epoch 9663, Train_Loss: 3798.17822265625, Val_Loss: 3933.6630859375\n",
      "Epoch 9664, Train_Loss: 3798.148193359375, Val_Loss: 3934.252685546875\n",
      "Epoch 9665, Train_Loss: 3798.06591796875, Val_Loss: 3934.643310546875\n",
      "Epoch 9666, Train_Loss: 3798.05029296875, Val_Loss: 3933.90087890625\n",
      "Epoch 9667, Train_Loss: 3798.110595703125, Val_Loss: 3934.117919921875\n",
      "Epoch 9668, Train_Loss: 3798.12353515625, Val_Loss: 3934.176513671875\n",
      "Epoch 9669, Train_Loss: 3798.097412109375, Val_Loss: 3934.001708984375\n",
      "Epoch 9670, Train_Loss: 3798.114501953125, Val_Loss: 3933.97119140625\n",
      "Epoch 9671, Train_Loss: 3798.1201171875, Val_Loss: 3934.12353515625\n",
      "Epoch 9672, Train_Loss: 3798.091064453125, Val_Loss: 3934.114013671875\n",
      "Epoch 9673, Train_Loss: 3798.119140625, Val_Loss: 3933.946044921875\n",
      "Epoch 9674, Train_Loss: 3798.0986328125, Val_Loss: 3934.102783203125\n",
      "Epoch 9675, Train_Loss: 3798.08740234375, Val_Loss: 3934.103271484375\n",
      "Epoch 9676, Train_Loss: 3798.109619140625, Val_Loss: 3933.965576171875\n",
      "Epoch 9677, Train_Loss: 3798.14892578125, Val_Loss: 3934.046875\n",
      "Epoch 9678, Train_Loss: 3798.09814453125, Val_Loss: 3934.175537109375\n",
      "Epoch 9679, Train_Loss: 3798.12890625, Val_Loss: 3934.05810546875\n",
      "Epoch 9680, Train_Loss: 3798.053955078125, Val_Loss: 3933.91357421875\n",
      "Epoch 9681, Train_Loss: 3798.0771484375, Val_Loss: 3934.49609375\n",
      "Epoch 9682, Train_Loss: 3798.0947265625, Val_Loss: 3934.1103515625\n",
      "Epoch 9683, Train_Loss: 3798.1240234375, Val_Loss: 3933.691650390625\n",
      "Epoch 9684, Train_Loss: 3798.0341796875, Val_Loss: 3934.464111328125\n",
      "Epoch 9685, Train_Loss: 3798.08544921875, Val_Loss: 3934.409912109375\n",
      "Epoch 9686, Train_Loss: 3798.125, Val_Loss: 3933.676025390625\n",
      "Epoch 9687, Train_Loss: 3798.086669921875, Val_Loss: 3934.169189453125\n",
      "Epoch 9688, Train_Loss: 3798.03955078125, Val_Loss: 3934.45166015625\n",
      "Epoch 9689, Train_Loss: 3798.05224609375, Val_Loss: 3933.86962890625\n",
      "Epoch 9690, Train_Loss: 3798.075439453125, Val_Loss: 3934.171630859375\n",
      "Epoch 9691, Train_Loss: 3797.97705078125, Val_Loss: 3934.1875\n",
      "Epoch 9692, Train_Loss: 3798.099609375, Val_Loss: 3933.895263671875\n",
      "Epoch 9693, Train_Loss: 3798.12548828125, Val_Loss: 3934.018310546875\n",
      "Epoch 9694, Train_Loss: 3797.97998046875, Val_Loss: 3934.211181640625\n",
      "Epoch 9695, Train_Loss: 3798.08349609375, Val_Loss: 3934.070068359375\n",
      "Epoch 9696, Train_Loss: 3798.0673828125, Val_Loss: 3933.832763671875\n",
      "Epoch 9697, Train_Loss: 3798.044921875, Val_Loss: 3934.4736328125\n",
      "Epoch 9698, Train_Loss: 3797.94580078125, Val_Loss: 3934.143310546875\n",
      "Epoch 9699, Train_Loss: 3798.135986328125, Val_Loss: 3933.697998046875\n",
      "Epoch 9700, Train_Loss: 3798.078857421875, Val_Loss: 3934.349609375\n",
      "Epoch 9701, Train_Loss: 3797.982177734375, Val_Loss: 3934.301513671875\n",
      "Epoch 9702, Train_Loss: 3798.048828125, Val_Loss: 3933.735595703125\n",
      "Epoch 9703, Train_Loss: 3797.971435546875, Val_Loss: 3934.20556640625\n",
      "Epoch 9704, Train_Loss: 3798.064697265625, Val_Loss: 3934.3896484375\n",
      "Epoch 9705, Train_Loss: 3798.06689453125, Val_Loss: 3933.773681640625\n",
      "Epoch 9706, Train_Loss: 3797.967529296875, Val_Loss: 3934.1044921875\n",
      "Epoch 9707, Train_Loss: 3797.9658203125, Val_Loss: 3934.189208984375\n",
      "Epoch 9708, Train_Loss: 3798.111572265625, Val_Loss: 3933.951171875\n",
      "Epoch 9709, Train_Loss: 3798.060546875, Val_Loss: 3933.865966796875\n",
      "Epoch 9710, Train_Loss: 3798.013427734375, Val_Loss: 3934.423095703125\n",
      "Epoch 9711, Train_Loss: 3798.074951171875, Val_Loss: 3934.080322265625\n",
      "Epoch 9712, Train_Loss: 3798.1240234375, Val_Loss: 3933.669921875\n",
      "Epoch 9713, Train_Loss: 3798.052490234375, Val_Loss: 3934.372802734375\n",
      "Epoch 9714, Train_Loss: 3797.968017578125, Val_Loss: 3934.2900390625\n",
      "Epoch 9715, Train_Loss: 3798.12939453125, Val_Loss: 3933.666015625\n",
      "Epoch 9716, Train_Loss: 3797.928955078125, Val_Loss: 3934.1328125\n",
      "Epoch 9717, Train_Loss: 3797.9931640625, Val_Loss: 3934.40869140625\n",
      "Epoch 9718, Train_Loss: 3798.041015625, Val_Loss: 3933.852783203125\n",
      "Epoch 9719, Train_Loss: 3797.95751953125, Val_Loss: 3934.068359375\n",
      "Epoch 9720, Train_Loss: 3797.908203125, Val_Loss: 3934.116455078125\n",
      "Epoch 9721, Train_Loss: 3798.05126953125, Val_Loss: 3933.906494140625\n",
      "Epoch 9722, Train_Loss: 3798.049072265625, Val_Loss: 3933.903564453125\n",
      "Epoch 9723, Train_Loss: 3797.918212890625, Val_Loss: 3934.0966796875\n",
      "Epoch 9724, Train_Loss: 3798.07177734375, Val_Loss: 3934.021484375\n",
      "Epoch 9725, Train_Loss: 3798.03466796875, Val_Loss: 3933.865234375\n",
      "Epoch 9726, Train_Loss: 3797.9287109375, Val_Loss: 3934.134765625\n",
      "Epoch 9727, Train_Loss: 3797.90771484375, Val_Loss: 3934.10791015625\n",
      "Epoch 9728, Train_Loss: 3798.041259765625, Val_Loss: 3933.842041015625\n",
      "Epoch 9729, Train_Loss: 3797.930419921875, Val_Loss: 3934.184326171875\n",
      "Epoch 9730, Train_Loss: 3797.932861328125, Val_Loss: 3934.134765625\n",
      "Epoch 9731, Train_Loss: 3798.03662109375, Val_Loss: 3933.8369140625\n",
      "Epoch 9732, Train_Loss: 3798.033935546875, Val_Loss: 3934.33447265625\n",
      "Epoch 9733, Train_Loss: 3797.920166015625, Val_Loss: 3934.048095703125\n",
      "Epoch 9734, Train_Loss: 3798.066650390625, Val_Loss: 3933.689697265625\n",
      "Epoch 9735, Train_Loss: 3798.03173828125, Val_Loss: 3934.330322265625\n",
      "Epoch 9736, Train_Loss: 3797.93603515625, Val_Loss: 3934.1884765625\n",
      "Epoch 9737, Train_Loss: 3798.028076171875, Val_Loss: 3933.737548828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9738, Train_Loss: 3797.94091796875, Val_Loss: 3934.15234375\n",
      "Epoch 9739, Train_Loss: 3797.9189453125, Val_Loss: 3934.264404296875\n",
      "Epoch 9740, Train_Loss: 3798.003662109375, Val_Loss: 3933.7958984375\n",
      "Epoch 9741, Train_Loss: 3797.9296875, Val_Loss: 3934.191162109375\n",
      "Epoch 9742, Train_Loss: 3797.9228515625, Val_Loss: 3934.160888671875\n",
      "Epoch 9743, Train_Loss: 3798.03466796875, Val_Loss: 3933.800048828125\n",
      "Epoch 9744, Train_Loss: 3798.013427734375, Val_Loss: 3934.268310546875\n",
      "Epoch 9745, Train_Loss: 3797.929443359375, Val_Loss: 3933.99951171875\n",
      "Epoch 9746, Train_Loss: 3798.044677734375, Val_Loss: 3933.681640625\n",
      "Epoch 9747, Train_Loss: 3798.008544921875, Val_Loss: 3934.269287109375\n",
      "Epoch 9748, Train_Loss: 3797.922607421875, Val_Loss: 3934.134521484375\n",
      "Epoch 9749, Train_Loss: 3798.019287109375, Val_Loss: 3933.742431640625\n",
      "Epoch 9750, Train_Loss: 3797.906982421875, Val_Loss: 3934.185546875\n",
      "Epoch 9751, Train_Loss: 3797.9111328125, Val_Loss: 3934.20068359375\n",
      "Epoch 9752, Train_Loss: 3798.00390625, Val_Loss: 3933.749267578125\n",
      "Epoch 9753, Train_Loss: 3797.9384765625, Val_Loss: 3934.14111328125\n",
      "Epoch 9754, Train_Loss: 3797.909423828125, Val_Loss: 3934.200439453125\n",
      "Epoch 9755, Train_Loss: 3798.00390625, Val_Loss: 3933.761962890625\n",
      "Epoch 9756, Train_Loss: 3797.900390625, Val_Loss: 3934.16552734375\n",
      "Epoch 9757, Train_Loss: 3797.900390625, Val_Loss: 3934.197265625\n",
      "Epoch 9758, Train_Loss: 3797.995361328125, Val_Loss: 3933.737548828125\n",
      "Epoch 9759, Train_Loss: 3797.921875, Val_Loss: 3934.1484375\n",
      "Epoch 9760, Train_Loss: 3797.890625, Val_Loss: 3934.193603515625\n",
      "Epoch 9761, Train_Loss: 3798.00146484375, Val_Loss: 3933.749267578125\n",
      "Epoch 9762, Train_Loss: 3797.9013671875, Val_Loss: 3934.1748046875\n",
      "Epoch 9763, Train_Loss: 3797.898193359375, Val_Loss: 3934.06689453125\n",
      "Epoch 9764, Train_Loss: 3797.99560546875, Val_Loss: 3933.720703125\n",
      "Epoch 9765, Train_Loss: 3797.896728515625, Val_Loss: 3934.187255859375\n",
      "Epoch 9766, Train_Loss: 3797.912841796875, Val_Loss: 3934.13232421875\n",
      "Epoch 9767, Train_Loss: 3798.012939453125, Val_Loss: 3933.74560546875\n",
      "Epoch 9768, Train_Loss: 3797.88671875, Val_Loss: 3934.2099609375\n",
      "Epoch 9769, Train_Loss: 3797.86328125, Val_Loss: 3934.098388671875\n",
      "Epoch 9770, Train_Loss: 3797.988037109375, Val_Loss: 3933.70556640625\n",
      "Epoch 9771, Train_Loss: 3797.892822265625, Val_Loss: 3934.15869140625\n",
      "Epoch 9772, Train_Loss: 3797.917724609375, Val_Loss: 3934.12353515625\n",
      "Epoch 9773, Train_Loss: 3797.877197265625, Val_Loss: 3933.742919921875\n",
      "Epoch 9774, Train_Loss: 3797.886474609375, Val_Loss: 3934.197509765625\n",
      "Epoch 9775, Train_Loss: 3797.866943359375, Val_Loss: 3934.080322265625\n",
      "Epoch 9776, Train_Loss: 3797.989501953125, Val_Loss: 3933.685302734375\n",
      "Epoch 9777, Train_Loss: 3797.865966796875, Val_Loss: 3934.132080078125\n",
      "Epoch 9778, Train_Loss: 3797.960205078125, Val_Loss: 3934.19482421875\n",
      "Epoch 9779, Train_Loss: 3797.97412109375, Val_Loss: 3933.68408203125\n",
      "Epoch 9780, Train_Loss: 3797.870361328125, Val_Loss: 3934.031982421875\n",
      "Epoch 9781, Train_Loss: 3797.894287109375, Val_Loss: 3934.103271484375\n",
      "Epoch 9782, Train_Loss: 3797.930908203125, Val_Loss: 3933.844482421875\n",
      "Epoch 9783, Train_Loss: 3797.844482421875, Val_Loss: 3933.73876953125\n",
      "Epoch 9784, Train_Loss: 3797.84130859375, Val_Loss: 3934.40771484375\n",
      "Epoch 9785, Train_Loss: 3797.90234375, Val_Loss: 3934.087890625\n",
      "Epoch 9786, Train_Loss: 3798.019775390625, Val_Loss: 3933.580810546875\n",
      "Epoch 9787, Train_Loss: 3797.8623046875, Val_Loss: 3934.117919921875\n",
      "Epoch 9788, Train_Loss: 3797.848876953125, Val_Loss: 3934.312744140625\n",
      "Epoch 9789, Train_Loss: 3797.855712890625, Val_Loss: 3933.727294921875\n",
      "Epoch 9790, Train_Loss: 3797.8408203125, Val_Loss: 3933.97412109375\n",
      "Epoch 9791, Train_Loss: 3797.84375, Val_Loss: 3934.0205078125\n",
      "Epoch 9792, Train_Loss: 3797.923583984375, Val_Loss: 3933.81689453125\n",
      "Epoch 9793, Train_Loss: 3797.84912109375, Val_Loss: 3933.740478515625\n",
      "Epoch 9794, Train_Loss: 3797.830078125, Val_Loss: 3934.2548828125\n",
      "Epoch 9795, Train_Loss: 3797.84326171875, Val_Loss: 3933.9794921875\n",
      "Epoch 9796, Train_Loss: 3797.978515625, Val_Loss: 3933.58251953125\n",
      "Epoch 9797, Train_Loss: 3797.914794921875, Val_Loss: 3934.173095703125\n",
      "Epoch 9798, Train_Loss: 3797.871337890625, Val_Loss: 3934.19921875\n",
      "Epoch 9799, Train_Loss: 3797.84228515625, Val_Loss: 3933.620849609375\n",
      "Epoch 9800, Train_Loss: 3797.8525390625, Val_Loss: 3933.916015625\n",
      "Epoch 9801, Train_Loss: 3797.8369140625, Val_Loss: 3934.10400390625\n",
      "Epoch 9802, Train_Loss: 3797.84765625, Val_Loss: 3933.906005859375\n",
      "Epoch 9803, Train_Loss: 3797.8173828125, Val_Loss: 3933.6591796875\n",
      "Epoch 9804, Train_Loss: 3797.86865234375, Val_Loss: 3934.196044921875\n",
      "Epoch 9805, Train_Loss: 3797.814453125, Val_Loss: 3934.03076171875\n",
      "Epoch 9806, Train_Loss: 3797.8466796875, Val_Loss: 3933.649169921875\n",
      "Epoch 9807, Train_Loss: 3797.829345703125, Val_Loss: 3934.039306640625\n",
      "Epoch 9808, Train_Loss: 3797.839599609375, Val_Loss: 3934.1484375\n",
      "Epoch 9809, Train_Loss: 3797.847900390625, Val_Loss: 3933.701171875\n",
      "Epoch 9810, Train_Loss: 3797.84619140625, Val_Loss: 3934.02001953125\n",
      "Epoch 9811, Train_Loss: 3797.827880859375, Val_Loss: 3934.0263671875\n",
      "Epoch 9812, Train_Loss: 3797.822509765625, Val_Loss: 3933.71923828125\n",
      "Epoch 9813, Train_Loss: 3797.84423828125, Val_Loss: 3933.87109375\n",
      "Epoch 9814, Train_Loss: 3797.801025390625, Val_Loss: 3933.9140625\n",
      "Epoch 9815, Train_Loss: 3797.887939453125, Val_Loss: 3933.79833984375\n",
      "Epoch 9816, Train_Loss: 3797.891845703125, Val_Loss: 3933.78369140625\n",
      "Epoch 9817, Train_Loss: 3797.794921875, Val_Loss: 3933.909912109375\n",
      "Epoch 9818, Train_Loss: 3797.835693359375, Val_Loss: 3933.8740234375\n",
      "Epoch 9819, Train_Loss: 3797.90185546875, Val_Loss: 3933.743896484375\n",
      "Epoch 9820, Train_Loss: 3797.823974609375, Val_Loss: 3933.8427734375\n",
      "Epoch 9821, Train_Loss: 3797.81884765625, Val_Loss: 3933.927490234375\n",
      "Epoch 9822, Train_Loss: 3797.815673828125, Val_Loss: 3933.846435546875\n",
      "Epoch 9823, Train_Loss: 3797.882080078125, Val_Loss: 3933.7587890625\n",
      "Epoch 9824, Train_Loss: 3797.821044921875, Val_Loss: 3933.853515625\n",
      "Epoch 9825, Train_Loss: 3797.821044921875, Val_Loss: 3933.85595703125\n",
      "Epoch 9826, Train_Loss: 3797.883056640625, Val_Loss: 3933.7705078125\n",
      "Epoch 9827, Train_Loss: 3797.817138671875, Val_Loss: 3933.8115234375\n",
      "Epoch 9828, Train_Loss: 3797.79833984375, Val_Loss: 3933.93359375\n",
      "Epoch 9829, Train_Loss: 3797.8046875, Val_Loss: 3933.8388671875\n",
      "Epoch 9830, Train_Loss: 3797.871337890625, Val_Loss: 3933.75830078125\n",
      "Epoch 9831, Train_Loss: 3797.807373046875, Val_Loss: 3933.827880859375\n",
      "Epoch 9832, Train_Loss: 3797.782470703125, Val_Loss: 3933.8779296875\n",
      "Epoch 9833, Train_Loss: 3797.8154296875, Val_Loss: 3933.77685546875\n",
      "Epoch 9834, Train_Loss: 3797.862060546875, Val_Loss: 3933.75634765625\n",
      "Epoch 9835, Train_Loss: 3797.763671875, Val_Loss: 3933.890869140625\n",
      "Epoch 9836, Train_Loss: 3797.801513671875, Val_Loss: 3933.8359375\n",
      "Epoch 9837, Train_Loss: 3797.883544921875, Val_Loss: 3933.713623046875\n",
      "Epoch 9838, Train_Loss: 3797.80029296875, Val_Loss: 3933.818359375\n",
      "Epoch 9839, Train_Loss: 3797.782470703125, Val_Loss: 3933.89794921875\n",
      "Epoch 9840, Train_Loss: 3797.792236328125, Val_Loss: 3933.77587890625\n",
      "Epoch 9841, Train_Loss: 3797.86083984375, Val_Loss: 3933.736083984375\n",
      "Epoch 9842, Train_Loss: 3797.797607421875, Val_Loss: 3933.8056640625\n",
      "Epoch 9843, Train_Loss: 3797.74853515625, Val_Loss: 3933.863525390625\n",
      "Epoch 9844, Train_Loss: 3797.80029296875, Val_Loss: 3933.761962890625\n",
      "Epoch 9845, Train_Loss: 3797.84521484375, Val_Loss: 3933.74169921875\n",
      "Epoch 9846, Train_Loss: 3797.767333984375, Val_Loss: 3933.823486328125\n",
      "Epoch 9847, Train_Loss: 3797.7568359375, Val_Loss: 3933.855712890625\n",
      "Epoch 9848, Train_Loss: 3797.802490234375, Val_Loss: 3933.73681640625\n",
      "Epoch 9849, Train_Loss: 3797.846435546875, Val_Loss: 3933.71630859375\n",
      "Epoch 9850, Train_Loss: 3797.747314453125, Val_Loss: 3933.853515625\n",
      "Epoch 9851, Train_Loss: 3797.76708984375, Val_Loss: 3933.80712890625\n",
      "Epoch 9852, Train_Loss: 3797.784912109375, Val_Loss: 3933.73876953125\n",
      "Epoch 9853, Train_Loss: 3797.77734375, Val_Loss: 3933.739501953125\n",
      "Epoch 9854, Train_Loss: 3797.675537109375, Val_Loss: 3933.847900390625\n",
      "Epoch 9855, Train_Loss: 3797.74853515625, Val_Loss: 3933.784912109375\n",
      "Epoch 9856, Train_Loss: 3797.740234375, Val_Loss: 3933.760498046875\n",
      "Epoch 9857, Train_Loss: 3797.75634765625, Val_Loss: 3933.8037109375\n",
      "Epoch 9858, Train_Loss: 3797.76171875, Val_Loss: 3933.797119140625\n",
      "Epoch 9859, Train_Loss: 3797.754638671875, Val_Loss: 3933.73046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9860, Train_Loss: 3797.745361328125, Val_Loss: 3933.782470703125\n",
      "Epoch 9861, Train_Loss: 3797.6611328125, Val_Loss: 3933.84033203125\n",
      "Epoch 9862, Train_Loss: 3797.7578125, Val_Loss: 3933.73193359375\n",
      "Epoch 9863, Train_Loss: 3797.739501953125, Val_Loss: 3933.7412109375\n",
      "Epoch 9864, Train_Loss: 3797.6806640625, Val_Loss: 3933.79443359375\n",
      "Epoch 9865, Train_Loss: 3797.660888671875, Val_Loss: 3933.81884765625\n",
      "Epoch 9866, Train_Loss: 3797.738037109375, Val_Loss: 3933.728515625\n",
      "Epoch 9867, Train_Loss: 3797.72998046875, Val_Loss: 3933.73876953125\n",
      "Epoch 9868, Train_Loss: 3797.652587890625, Val_Loss: 3933.83154296875\n",
      "Epoch 9869, Train_Loss: 3797.660400390625, Val_Loss: 3933.74560546875\n",
      "Epoch 9870, Train_Loss: 3797.659912109375, Val_Loss: 3933.74072265625\n",
      "Epoch 9871, Train_Loss: 3797.675048828125, Val_Loss: 3933.797119140625\n",
      "Epoch 9872, Train_Loss: 3797.6591796875, Val_Loss: 3933.763916015625\n",
      "Epoch 9873, Train_Loss: 3797.670654296875, Val_Loss: 3933.753173828125\n",
      "Epoch 9874, Train_Loss: 3797.6591796875, Val_Loss: 3933.788818359375\n",
      "Epoch 9875, Train_Loss: 3797.65625, Val_Loss: 3933.7705078125\n",
      "Epoch 9876, Train_Loss: 3797.66357421875, Val_Loss: 3933.749267578125\n",
      "Epoch 9877, Train_Loss: 3797.659912109375, Val_Loss: 3933.75927734375\n",
      "Epoch 9878, Train_Loss: 3797.652587890625, Val_Loss: 3933.769287109375\n",
      "Epoch 9879, Train_Loss: 3797.65673828125, Val_Loss: 3933.74609375\n",
      "Epoch 9880, Train_Loss: 3797.645263671875, Val_Loss: 3933.76953125\n",
      "Epoch 9881, Train_Loss: 3797.6572265625, Val_Loss: 3933.748779296875\n",
      "Epoch 9882, Train_Loss: 3797.65625, Val_Loss: 3933.753173828125\n",
      "Epoch 9883, Train_Loss: 3797.652587890625, Val_Loss: 3933.75\n",
      "Epoch 9884, Train_Loss: 3797.668212890625, Val_Loss: 3933.73193359375\n",
      "Epoch 9885, Train_Loss: 3797.656005859375, Val_Loss: 3933.7373046875\n",
      "Epoch 9886, Train_Loss: 3797.654052734375, Val_Loss: 3933.746826171875\n",
      "Epoch 9887, Train_Loss: 3797.666015625, Val_Loss: 3933.722412109375\n",
      "Epoch 9888, Train_Loss: 3797.654296875, Val_Loss: 3933.744873046875\n",
      "Epoch 9889, Train_Loss: 3797.649658203125, Val_Loss: 3933.733154296875\n",
      "Epoch 9890, Train_Loss: 3797.644287109375, Val_Loss: 3933.742431640625\n",
      "Epoch 9891, Train_Loss: 3797.66455078125, Val_Loss: 3933.726318359375\n",
      "Epoch 9892, Train_Loss: 3797.6591796875, Val_Loss: 3933.727294921875\n",
      "Epoch 9893, Train_Loss: 3797.65576171875, Val_Loss: 3933.718017578125\n",
      "Epoch 9894, Train_Loss: 3797.656982421875, Val_Loss: 3933.720458984375\n",
      "Epoch 9895, Train_Loss: 3797.655517578125, Val_Loss: 3933.72119140625\n",
      "Epoch 9896, Train_Loss: 3797.655517578125, Val_Loss: 3933.719970703125\n",
      "Epoch 9897, Train_Loss: 3797.659423828125, Val_Loss: 3933.707275390625\n",
      "Epoch 9898, Train_Loss: 3797.652099609375, Val_Loss: 3933.70361328125\n",
      "Epoch 9899, Train_Loss: 3797.650146484375, Val_Loss: 3933.711181640625\n",
      "Epoch 9900, Train_Loss: 3797.657470703125, Val_Loss: 3933.70556640625\n",
      "Epoch 9901, Train_Loss: 3797.64697265625, Val_Loss: 3933.7099609375\n",
      "Epoch 9902, Train_Loss: 3797.65087890625, Val_Loss: 3933.697265625\n",
      "Epoch 9903, Train_Loss: 3797.65087890625, Val_Loss: 3933.691650390625\n",
      "Epoch 9904, Train_Loss: 3797.642822265625, Val_Loss: 3933.701171875\n",
      "Epoch 9905, Train_Loss: 3797.64697265625, Val_Loss: 3933.69287109375\n",
      "Epoch 9906, Train_Loss: 3797.638916015625, Val_Loss: 3933.697265625\n",
      "Epoch 9907, Train_Loss: 3797.635498046875, Val_Loss: 3933.705078125\n",
      "Epoch 9908, Train_Loss: 3797.643310546875, Val_Loss: 3933.704345703125\n",
      "Epoch 9909, Train_Loss: 3797.626220703125, Val_Loss: 3933.71484375\n",
      "Epoch 9910, Train_Loss: 3797.63916015625, Val_Loss: 3933.702392578125\n",
      "Epoch 9911, Train_Loss: 3797.629150390625, Val_Loss: 3933.698486328125\n",
      "Epoch 9912, Train_Loss: 3797.630615234375, Val_Loss: 3933.6943359375\n",
      "Epoch 9913, Train_Loss: 3797.637451171875, Val_Loss: 3933.680419921875\n",
      "Epoch 9914, Train_Loss: 3797.61962890625, Val_Loss: 3933.70068359375\n",
      "Epoch 9915, Train_Loss: 3797.633056640625, Val_Loss: 3933.691650390625\n",
      "Epoch 9916, Train_Loss: 3797.62548828125, Val_Loss: 3933.694091796875\n",
      "Epoch 9917, Train_Loss: 3797.61865234375, Val_Loss: 3933.697998046875\n",
      "Epoch 9918, Train_Loss: 3797.618896484375, Val_Loss: 3933.696044921875\n",
      "Epoch 9919, Train_Loss: 3797.617919921875, Val_Loss: 3933.691650390625\n",
      "Epoch 9920, Train_Loss: 3797.615478515625, Val_Loss: 3933.6923828125\n",
      "Epoch 9921, Train_Loss: 3797.614990234375, Val_Loss: 3933.693603515625\n",
      "Epoch 9922, Train_Loss: 3797.6123046875, Val_Loss: 3933.6923828125\n",
      "Epoch 9923, Train_Loss: 3797.612548828125, Val_Loss: 3933.693603515625\n",
      "Epoch 9924, Train_Loss: 3797.60986328125, Val_Loss: 3933.6923828125\n",
      "Epoch 9925, Train_Loss: 3797.60546875, Val_Loss: 3933.69189453125\n",
      "Epoch 9926, Train_Loss: 3797.606201171875, Val_Loss: 3933.6904296875\n",
      "Epoch 9927, Train_Loss: 3797.60205078125, Val_Loss: 3933.68994140625\n",
      "Epoch 9928, Train_Loss: 3797.605712890625, Val_Loss: 3933.688720703125\n",
      "Epoch 9929, Train_Loss: 3797.6005859375, Val_Loss: 3933.689208984375\n",
      "Epoch 9930, Train_Loss: 3797.6015625, Val_Loss: 3933.681640625\n",
      "Epoch 9931, Train_Loss: 3797.59912109375, Val_Loss: 3933.681884765625\n",
      "Epoch 9932, Train_Loss: 3797.59716796875, Val_Loss: 3933.6787109375\n",
      "Epoch 9933, Train_Loss: 3797.599365234375, Val_Loss: 3933.67431640625\n",
      "Epoch 9934, Train_Loss: 3797.591064453125, Val_Loss: 3933.675537109375\n",
      "Epoch 9935, Train_Loss: 3797.594970703125, Val_Loss: 3933.671630859375\n",
      "Epoch 9936, Train_Loss: 3797.59033203125, Val_Loss: 3933.669921875\n",
      "Epoch 9937, Train_Loss: 3797.591796875, Val_Loss: 3933.65771484375\n",
      "Epoch 9938, Train_Loss: 3797.59228515625, Val_Loss: 3933.654052734375\n",
      "Epoch 9939, Train_Loss: 3797.590087890625, Val_Loss: 3933.653564453125\n",
      "Epoch 9940, Train_Loss: 3797.575439453125, Val_Loss: 3933.65087890625\n",
      "Epoch 9941, Train_Loss: 3797.5712890625, Val_Loss: 3933.6611328125\n",
      "Epoch 9942, Train_Loss: 3797.573974609375, Val_Loss: 3933.6455078125\n",
      "Epoch 9943, Train_Loss: 3797.567138671875, Val_Loss: 3933.6572265625\n",
      "Epoch 9944, Train_Loss: 3797.5712890625, Val_Loss: 3933.64208984375\n",
      "Epoch 9945, Train_Loss: 3797.56982421875, Val_Loss: 3933.642333984375\n",
      "Epoch 9946, Train_Loss: 3797.56689453125, Val_Loss: 3933.6376953125\n",
      "Epoch 9947, Train_Loss: 3797.56640625, Val_Loss: 3933.638427734375\n",
      "Epoch 9948, Train_Loss: 3797.56396484375, Val_Loss: 3933.63671875\n",
      "Epoch 9949, Train_Loss: 3797.561279296875, Val_Loss: 3933.634521484375\n",
      "Epoch 9950, Train_Loss: 3797.560302734375, Val_Loss: 3933.634521484375\n",
      "Epoch 9951, Train_Loss: 3797.55859375, Val_Loss: 3933.63330078125\n",
      "Epoch 9952, Train_Loss: 3797.55810546875, Val_Loss: 3933.622802734375\n",
      "Epoch 9953, Train_Loss: 3797.554443359375, Val_Loss: 3933.62158203125\n",
      "Epoch 9954, Train_Loss: 3797.55126953125, Val_Loss: 3933.616455078125\n",
      "Epoch 9955, Train_Loss: 3797.551025390625, Val_Loss: 3933.61474609375\n",
      "Epoch 9956, Train_Loss: 3797.547607421875, Val_Loss: 3933.612060546875\n",
      "Epoch 9957, Train_Loss: 3797.548095703125, Val_Loss: 3933.6083984375\n",
      "Epoch 9958, Train_Loss: 3797.546630859375, Val_Loss: 3933.620849609375\n",
      "Epoch 9959, Train_Loss: 3797.548828125, Val_Loss: 3933.6083984375\n",
      "Epoch 9960, Train_Loss: 3797.546630859375, Val_Loss: 3933.60400390625\n",
      "Epoch 9961, Train_Loss: 3797.54248046875, Val_Loss: 3933.60595703125\n",
      "Epoch 9962, Train_Loss: 3797.539306640625, Val_Loss: 3933.606689453125\n",
      "Epoch 9963, Train_Loss: 3797.54052734375, Val_Loss: 3933.605712890625\n",
      "Epoch 9964, Train_Loss: 3797.539306640625, Val_Loss: 3933.603515625\n",
      "Epoch 9965, Train_Loss: 3797.546630859375, Val_Loss: 3933.593505859375\n",
      "Epoch 9966, Train_Loss: 3797.545654296875, Val_Loss: 3933.592041015625\n",
      "Epoch 9967, Train_Loss: 3797.571044921875, Val_Loss: 3933.609130859375\n",
      "Epoch 9968, Train_Loss: 3797.568115234375, Val_Loss: 3933.604736328125\n",
      "Epoch 9969, Train_Loss: 3797.566162109375, Val_Loss: 3933.605224609375\n",
      "Epoch 9970, Train_Loss: 3797.56640625, Val_Loss: 3933.60009765625\n",
      "Epoch 9971, Train_Loss: 3797.539306640625, Val_Loss: 3933.584716796875\n",
      "Epoch 9972, Train_Loss: 3797.564453125, Val_Loss: 3933.59716796875\n",
      "Epoch 9973, Train_Loss: 3797.5390625, Val_Loss: 3933.582763671875\n",
      "Epoch 9974, Train_Loss: 3797.558349609375, Val_Loss: 3933.59765625\n",
      "Epoch 9975, Train_Loss: 3797.555419921875, Val_Loss: 3933.597900390625\n",
      "Epoch 9976, Train_Loss: 3797.554931640625, Val_Loss: 3933.592041015625\n",
      "Epoch 9977, Train_Loss: 3797.55615234375, Val_Loss: 3933.587646484375\n",
      "Epoch 9978, Train_Loss: 3797.552001953125, Val_Loss: 3933.58447265625\n",
      "Epoch 9979, Train_Loss: 3797.5517578125, Val_Loss: 3933.583984375\n",
      "Epoch 9980, Train_Loss: 3797.548583984375, Val_Loss: 3933.578369140625\n",
      "Epoch 9981, Train_Loss: 3797.55029296875, Val_Loss: 3933.578857421875\n",
      "Epoch 9982, Train_Loss: 3797.5341796875, Val_Loss: 3933.578857421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9983, Train_Loss: 3797.533447265625, Val_Loss: 3933.580078125\n",
      "Epoch 9984, Train_Loss: 3797.528564453125, Val_Loss: 3933.580078125\n",
      "Epoch 9985, Train_Loss: 3797.529541015625, Val_Loss: 3933.578369140625\n",
      "Epoch 9986, Train_Loss: 3797.54150390625, Val_Loss: 3933.57763671875\n",
      "Epoch 9987, Train_Loss: 3797.528076171875, Val_Loss: 3933.576416015625\n",
      "Epoch 9988, Train_Loss: 3797.5400390625, Val_Loss: 3933.576904296875\n",
      "Epoch 9989, Train_Loss: 3797.523681640625, Val_Loss: 3933.57958984375\n",
      "Epoch 9990, Train_Loss: 3797.5234375, Val_Loss: 3933.577880859375\n",
      "Epoch 9991, Train_Loss: 3797.519287109375, Val_Loss: 3933.5791015625\n",
      "Epoch 9992, Train_Loss: 3797.524169921875, Val_Loss: 3933.573974609375\n",
      "Epoch 9993, Train_Loss: 3797.52392578125, Val_Loss: 3933.5732421875\n",
      "Epoch 9994, Train_Loss: 3797.52099609375, Val_Loss: 3933.575927734375\n",
      "Epoch 9995, Train_Loss: 3797.54736328125, Val_Loss: 3933.572509765625\n",
      "Epoch 9996, Train_Loss: 3797.519775390625, Val_Loss: 3933.57275390625\n",
      "Epoch 9997, Train_Loss: 3797.5693359375, Val_Loss: 3933.5595703125\n",
      "Epoch 9998, Train_Loss: 3797.56982421875, Val_Loss: 3933.5595703125\n",
      "Epoch 9999, Train_Loss: 3797.5693359375, Val_Loss: 3933.55712890625\n",
      "Test Loss: 3727.65087890625\n"
     ]
    }
   ],
   "source": [
    "# Train linear regression - ONLY APPROVED CLAIMS\n",
    "assessor_config =  config['assessor']\n",
    "assessor_config['linear_regression']['epochs'] = 10000\n",
    "assessor = HomeAssessor(assessor_config)\n",
    "\n",
    "train_calculate_test_loss(assessor, \n",
    "                          train_test_val_dataset_nz['train_x'], \n",
    "                          train_test_val_dataset_nz['train_y'],\n",
    "                          train_test_val_dataset_nz['test_x'],\n",
    "                          train_test_val_dataset_nz['test_y'],\n",
    "                          train_test_val_dataset_nz['val_x'],\n",
    "                          train_test_val_dataset_nz['val_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train nn module -- ALL CLAIMS\n",
    "assessor_config =  config['assessor']\n",
    "assessor_config['simple_nn']['epochs'] = 10\n",
    "assessor_config['model'] = 'simple_nn'\n",
    "assessor = HomeAssessor(assessor_config)\n",
    "\n",
    "train_calculate_test_loss(assessor, \n",
    "                          train_test_val_dataset['train_x'], \n",
    "                          train_test_val_dataset['train_y'],\n",
    "                          train_test_val_dataset['test_x'],\n",
    "                          train_test_val_dataset['test_y'],\n",
    "                          train_test_val_dataset['val_x'],\n",
    "                          train_test_val_dataset['val_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train_Loss: 5810.2001953125, Val_Loss: 6186.708984375\n",
      "Epoch 1, Train_Loss: 5809.87939453125, Val_Loss: 6186.34228515625\n",
      "Epoch 2, Train_Loss: 5809.5517578125, Val_Loss: 6185.9658203125\n",
      "Epoch 3, Train_Loss: 5809.203125, Val_Loss: 6185.56689453125\n",
      "Epoch 4, Train_Loss: 5808.87255859375, Val_Loss: 6185.22021484375\n",
      "Epoch 5, Train_Loss: 5808.48291015625, Val_Loss: 6184.8447265625\n",
      "Epoch 6, Train_Loss: 5808.1611328125, Val_Loss: 6184.48779296875\n",
      "Epoch 7, Train_Loss: 5807.81689453125, Val_Loss: 6184.1279296875\n",
      "Epoch 8, Train_Loss: 5807.48681640625, Val_Loss: 6183.7490234375\n",
      "Epoch 9, Train_Loss: 5807.169921875, Val_Loss: 6183.3857421875\n",
      "Epoch 10, Train_Loss: 5806.86376953125, Val_Loss: 6183.0283203125\n",
      "Epoch 11, Train_Loss: 5806.54736328125, Val_Loss: 6182.66064453125\n",
      "Epoch 12, Train_Loss: 5806.18994140625, Val_Loss: 6182.29541015625\n",
      "Epoch 13, Train_Loss: 5805.8603515625, Val_Loss: 6181.93310546875\n",
      "Epoch 14, Train_Loss: 5805.53076171875, Val_Loss: 6181.548828125\n",
      "Epoch 15, Train_Loss: 5805.21875, Val_Loss: 6181.18505859375\n",
      "Epoch 16, Train_Loss: 5804.8984375, Val_Loss: 6180.8447265625\n",
      "Epoch 17, Train_Loss: 5804.52392578125, Val_Loss: 6180.4658203125\n",
      "Epoch 18, Train_Loss: 5804.20263671875, Val_Loss: 6180.11376953125\n",
      "Epoch 19, Train_Loss: 5803.87939453125, Val_Loss: 6179.75537109375\n",
      "Epoch 20, Train_Loss: 5803.3828125, Val_Loss: 6179.369140625\n",
      "Epoch 21, Train_Loss: 5803.06689453125, Val_Loss: 6179.01123046875\n",
      "Epoch 22, Train_Loss: 5802.740234375, Val_Loss: 6178.6611328125\n",
      "Epoch 23, Train_Loss: 5802.40966796875, Val_Loss: 6178.29541015625\n",
      "Epoch 24, Train_Loss: 5802.0869140625, Val_Loss: 6177.94384765625\n",
      "Epoch 25, Train_Loss: 5801.76708984375, Val_Loss: 6177.58447265625\n",
      "Epoch 26, Train_Loss: 5801.4296875, Val_Loss: 6177.19677734375\n",
      "Epoch 27, Train_Loss: 5801.107421875, Val_Loss: 6176.830078125\n",
      "Epoch 28, Train_Loss: 5800.765625, Val_Loss: 6176.4775390625\n",
      "Epoch 29, Train_Loss: 5800.375, Val_Loss: 6176.09130859375\n",
      "Epoch 30, Train_Loss: 5800.05517578125, Val_Loss: 6175.732421875\n",
      "Epoch 31, Train_Loss: 5799.7216796875, Val_Loss: 6175.388671875\n",
      "Epoch 32, Train_Loss: 5799.396484375, Val_Loss: 6175.00244140625\n",
      "Epoch 33, Train_Loss: 5799.08740234375, Val_Loss: 6174.63525390625\n",
      "Epoch 34, Train_Loss: 5798.76953125, Val_Loss: 6174.2880859375\n",
      "Epoch 35, Train_Loss: 5798.41748046875, Val_Loss: 6173.9189453125\n",
      "Epoch 36, Train_Loss: 5798.09716796875, Val_Loss: 6173.5576171875\n",
      "Epoch 37, Train_Loss: 5797.76708984375, Val_Loss: 6173.1748046875\n",
      "Epoch 38, Train_Loss: 5797.4443359375, Val_Loss: 6172.78662109375\n",
      "Epoch 39, Train_Loss: 5797.107421875, Val_Loss: 6172.423828125\n",
      "Epoch 40, Train_Loss: 5796.7744140625, Val_Loss: 6172.0634765625\n",
      "Epoch 41, Train_Loss: 5796.36279296875, Val_Loss: 6171.70361328125\n",
      "Epoch 42, Train_Loss: 5796.03076171875, Val_Loss: 6171.33837890625\n",
      "Epoch 43, Train_Loss: 5795.71533203125, Val_Loss: 6170.9833984375\n",
      "Epoch 44, Train_Loss: 5795.3681640625, Val_Loss: 6170.619140625\n",
      "Epoch 45, Train_Loss: 5795.04345703125, Val_Loss: 6170.24609375\n",
      "Epoch 46, Train_Loss: 5794.70361328125, Val_Loss: 6169.87939453125\n",
      "Epoch 47, Train_Loss: 5794.3837890625, Val_Loss: 6169.5146484375\n",
      "Epoch 48, Train_Loss: 5794.08154296875, Val_Loss: 6169.1630859375\n",
      "Epoch 49, Train_Loss: 5793.73974609375, Val_Loss: 6168.8056640625\n",
      "Epoch 50, Train_Loss: 5793.43212890625, Val_Loss: 6168.423828125\n",
      "Epoch 51, Train_Loss: 5793.10302734375, Val_Loss: 6168.08251953125\n",
      "Epoch 52, Train_Loss: 5792.7666015625, Val_Loss: 6167.70849609375\n",
      "Epoch 53, Train_Loss: 5792.4150390625, Val_Loss: 6167.33935546875\n",
      "Epoch 54, Train_Loss: 5792.07861328125, Val_Loss: 6166.97265625\n",
      "Epoch 55, Train_Loss: 5791.76513671875, Val_Loss: 6166.6123046875\n",
      "Epoch 56, Train_Loss: 5791.443359375, Val_Loss: 6166.2421875\n",
      "Epoch 57, Train_Loss: 5791.11962890625, Val_Loss: 6165.8955078125\n",
      "Epoch 58, Train_Loss: 5790.79296875, Val_Loss: 6165.52880859375\n",
      "Epoch 59, Train_Loss: 5790.45361328125, Val_Loss: 6165.1494140625\n",
      "Epoch 60, Train_Loss: 5790.13720703125, Val_Loss: 6164.79150390625\n",
      "Epoch 61, Train_Loss: 5789.818359375, Val_Loss: 6164.42724609375\n",
      "Epoch 62, Train_Loss: 5789.4912109375, Val_Loss: 6164.0849609375\n",
      "Epoch 63, Train_Loss: 5789.16845703125, Val_Loss: 6163.6796875\n",
      "Epoch 64, Train_Loss: 5788.8486328125, Val_Loss: 6163.32861328125\n",
      "Epoch 65, Train_Loss: 5788.52294921875, Val_Loss: 6162.984375\n",
      "Epoch 66, Train_Loss: 5788.1142578125, Val_Loss: 6162.615234375\n",
      "Epoch 67, Train_Loss: 5787.77978515625, Val_Loss: 6162.27001953125\n",
      "Epoch 68, Train_Loss: 5787.46435546875, Val_Loss: 6161.90185546875\n",
      "Epoch 69, Train_Loss: 5787.13525390625, Val_Loss: 6161.5078125\n",
      "Epoch 70, Train_Loss: 5786.818359375, Val_Loss: 6161.15478515625\n",
      "Epoch 71, Train_Loss: 5786.501953125, Val_Loss: 6160.80126953125\n",
      "Epoch 72, Train_Loss: 5786.14599609375, Val_Loss: 6160.43115234375\n",
      "Epoch 73, Train_Loss: 5785.81982421875, Val_Loss: 6160.0546875\n",
      "Epoch 74, Train_Loss: 5785.515625, Val_Loss: 6159.69482421875\n",
      "Epoch 75, Train_Loss: 5785.19384765625, Val_Loss: 6159.314453125\n",
      "Epoch 76, Train_Loss: 5784.84228515625, Val_Loss: 6158.95947265625\n",
      "Epoch 77, Train_Loss: 5784.53515625, Val_Loss: 6158.6044921875\n",
      "Epoch 78, Train_Loss: 5784.16650390625, Val_Loss: 6158.248046875\n",
      "Epoch 79, Train_Loss: 5783.8564453125, Val_Loss: 6157.896484375\n",
      "Epoch 80, Train_Loss: 5783.4423828125, Val_Loss: 6157.54052734375\n",
      "Epoch 81, Train_Loss: 5783.09814453125, Val_Loss: 6157.1533203125\n",
      "Epoch 82, Train_Loss: 5782.78759765625, Val_Loss: 6156.79345703125\n",
      "Epoch 83, Train_Loss: 5782.46630859375, Val_Loss: 6156.44775390625\n",
      "Epoch 84, Train_Loss: 5782.1494140625, Val_Loss: 6156.06689453125\n",
      "Epoch 85, Train_Loss: 5781.81005859375, Val_Loss: 6155.71337890625\n",
      "Epoch 86, Train_Loss: 5781.4892578125, Val_Loss: 6155.3642578125\n",
      "Epoch 87, Train_Loss: 5781.177734375, Val_Loss: 6154.99072265625\n",
      "Epoch 88, Train_Loss: 5780.86474609375, Val_Loss: 6154.6455078125\n",
      "Epoch 89, Train_Loss: 5780.51171875, Val_Loss: 6154.2626953125\n",
      "Epoch 90, Train_Loss: 5780.13671875, Val_Loss: 6153.90576171875\n",
      "Epoch 91, Train_Loss: 5779.80908203125, Val_Loss: 6153.54248046875\n",
      "Epoch 92, Train_Loss: 5779.4921875, Val_Loss: 6153.1982421875\n",
      "Epoch 93, Train_Loss: 5779.14990234375, Val_Loss: 6152.81103515625\n",
      "Epoch 94, Train_Loss: 5778.830078125, Val_Loss: 6152.4609375\n",
      "Epoch 95, Train_Loss: 5778.4921875, Val_Loss: 6152.09423828125\n",
      "Epoch 96, Train_Loss: 5778.1728515625, Val_Loss: 6151.73681640625\n",
      "Epoch 97, Train_Loss: 5777.84033203125, Val_Loss: 6151.359375\n",
      "Epoch 98, Train_Loss: 5777.529296875, Val_Loss: 6151.00146484375\n",
      "Epoch 99, Train_Loss: 5777.19775390625, Val_Loss: 6150.62109375\n",
      "Epoch 100, Train_Loss: 5776.88037109375, Val_Loss: 6150.27001953125\n",
      "Epoch 101, Train_Loss: 5776.541015625, Val_Loss: 6149.91650390625\n",
      "Epoch 102, Train_Loss: 5776.1318359375, Val_Loss: 6149.55126953125\n",
      "Epoch 103, Train_Loss: 5775.8271484375, Val_Loss: 6149.197265625\n",
      "Epoch 104, Train_Loss: 5775.51220703125, Val_Loss: 6148.83349609375\n",
      "Epoch 105, Train_Loss: 5775.1904296875, Val_Loss: 6148.46484375\n",
      "Epoch 106, Train_Loss: 5774.8544921875, Val_Loss: 6148.0849609375\n",
      "Epoch 107, Train_Loss: 5774.533203125, Val_Loss: 6147.7119140625\n",
      "Epoch 108, Train_Loss: 5774.18505859375, Val_Loss: 6147.337890625\n",
      "Epoch 109, Train_Loss: 5773.88427734375, Val_Loss: 6147.0107421875\n",
      "Epoch 110, Train_Loss: 5773.564453125, Val_Loss: 6146.64013671875\n",
      "Epoch 111, Train_Loss: 5773.263671875, Val_Loss: 6146.275390625\n",
      "Epoch 112, Train_Loss: 5772.9296875, Val_Loss: 6145.908203125\n",
      "Epoch 113, Train_Loss: 5772.599609375, Val_Loss: 6145.56201171875\n",
      "Epoch 114, Train_Loss: 5772.228515625, Val_Loss: 6145.2158203125\n",
      "Epoch 115, Train_Loss: 5771.91455078125, Val_Loss: 6144.841796875\n",
      "Epoch 116, Train_Loss: 5771.60205078125, Val_Loss: 6144.47802734375\n",
      "Epoch 117, Train_Loss: 5771.28369140625, Val_Loss: 6144.10107421875\n",
      "Epoch 118, Train_Loss: 5770.94140625, Val_Loss: 6143.751953125\n",
      "Epoch 119, Train_Loss: 5770.615234375, Val_Loss: 6143.4013671875\n",
      "Epoch 120, Train_Loss: 5770.2900390625, Val_Loss: 6143.03857421875\n",
      "Epoch 121, Train_Loss: 5769.96533203125, Val_Loss: 6142.67919921875\n",
      "Epoch 122, Train_Loss: 5769.6279296875, Val_Loss: 6142.3076171875\n",
      "Epoch 123, Train_Loss: 5769.294921875, Val_Loss: 6141.921875\n",
      "Epoch 124, Train_Loss: 5768.97509765625, Val_Loss: 6141.5654296875\n",
      "Epoch 125, Train_Loss: 5768.6640625, Val_Loss: 6141.2119140625\n",
      "Epoch 126, Train_Loss: 5768.2900390625, Val_Loss: 6140.8583984375\n",
      "Epoch 127, Train_Loss: 5767.9521484375, Val_Loss: 6140.4970703125\n",
      "Epoch 128, Train_Loss: 5767.64013671875, Val_Loss: 6140.142578125\n",
      "Epoch 129, Train_Loss: 5767.31103515625, Val_Loss: 6139.7734375\n",
      "Epoch 130, Train_Loss: 5767.001953125, Val_Loss: 6139.40673828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131, Train_Loss: 5766.67138671875, Val_Loss: 6139.04296875\n",
      "Epoch 132, Train_Loss: 5766.3447265625, Val_Loss: 6138.67822265625\n",
      "Epoch 133, Train_Loss: 5766.025390625, Val_Loss: 6138.32568359375\n",
      "Epoch 134, Train_Loss: 5765.69091796875, Val_Loss: 6137.96435546875\n",
      "Epoch 135, Train_Loss: 5765.392578125, Val_Loss: 6137.5947265625\n",
      "Epoch 136, Train_Loss: 5765.0517578125, Val_Loss: 6137.2294921875\n",
      "Epoch 137, Train_Loss: 5764.728515625, Val_Loss: 6136.8671875\n",
      "Epoch 138, Train_Loss: 5764.408203125, Val_Loss: 6136.5244140625\n",
      "Epoch 139, Train_Loss: 5763.9931640625, Val_Loss: 6136.16552734375\n",
      "Epoch 140, Train_Loss: 5763.673828125, Val_Loss: 6135.77099609375\n",
      "Epoch 141, Train_Loss: 5763.36572265625, Val_Loss: 6135.4189453125\n",
      "Epoch 142, Train_Loss: 5763.04052734375, Val_Loss: 6135.044921875\n",
      "Epoch 143, Train_Loss: 5762.7216796875, Val_Loss: 6134.69384765625\n",
      "Epoch 144, Train_Loss: 5762.3935546875, Val_Loss: 6134.33349609375\n",
      "Epoch 145, Train_Loss: 5762.044921875, Val_Loss: 6133.96875\n",
      "Epoch 146, Train_Loss: 5761.72802734375, Val_Loss: 6133.6201171875\n",
      "Epoch 147, Train_Loss: 5761.41748046875, Val_Loss: 6133.25146484375\n",
      "Epoch 148, Train_Loss: 5761.09228515625, Val_Loss: 6132.87451171875\n",
      "Epoch 149, Train_Loss: 5760.76123046875, Val_Loss: 6132.50732421875\n",
      "Epoch 150, Train_Loss: 5760.43505859375, Val_Loss: 6132.15380859375\n",
      "Epoch 151, Train_Loss: 5760.09765625, Val_Loss: 6131.81005859375\n",
      "Epoch 152, Train_Loss: 5759.7919921875, Val_Loss: 6131.4453125\n",
      "Epoch 153, Train_Loss: 5759.48193359375, Val_Loss: 6131.08740234375\n",
      "Epoch 154, Train_Loss: 5759.138671875, Val_Loss: 6130.71435546875\n",
      "Epoch 155, Train_Loss: 5758.826171875, Val_Loss: 6130.36767578125\n",
      "Epoch 156, Train_Loss: 5758.498046875, Val_Loss: 6130.00341796875\n",
      "Epoch 157, Train_Loss: 5758.17578125, Val_Loss: 6129.63671875\n",
      "Epoch 158, Train_Loss: 5757.81396484375, Val_Loss: 6129.2841796875\n",
      "Epoch 159, Train_Loss: 5757.4755859375, Val_Loss: 6128.92626953125\n",
      "Epoch 160, Train_Loss: 5757.16259765625, Val_Loss: 6128.552734375\n",
      "Epoch 161, Train_Loss: 5756.841796875, Val_Loss: 6128.1904296875\n",
      "Epoch 162, Train_Loss: 5756.525390625, Val_Loss: 6127.84765625\n",
      "Epoch 163, Train_Loss: 5756.11962890625, Val_Loss: 6127.49169921875\n",
      "Epoch 164, Train_Loss: 5755.79150390625, Val_Loss: 6127.134765625\n",
      "Epoch 165, Train_Loss: 5755.45751953125, Val_Loss: 6126.7822265625\n",
      "Epoch 166, Train_Loss: 5755.14501953125, Val_Loss: 6126.40380859375\n",
      "Epoch 167, Train_Loss: 5754.81884765625, Val_Loss: 6126.0498046875\n",
      "Epoch 168, Train_Loss: 5754.49658203125, Val_Loss: 6125.69287109375\n",
      "Epoch 169, Train_Loss: 5754.16845703125, Val_Loss: 6125.32080078125\n",
      "Epoch 170, Train_Loss: 5753.8603515625, Val_Loss: 6124.9736328125\n",
      "Epoch 171, Train_Loss: 5753.55517578125, Val_Loss: 6124.62939453125\n",
      "Epoch 172, Train_Loss: 5753.23486328125, Val_Loss: 6124.24462890625\n",
      "Epoch 173, Train_Loss: 5752.92138671875, Val_Loss: 6123.88427734375\n",
      "Epoch 174, Train_Loss: 5752.57666015625, Val_Loss: 6123.53369140625\n",
      "Epoch 175, Train_Loss: 5752.19921875, Val_Loss: 6123.1455078125\n",
      "Epoch 176, Train_Loss: 5751.875, Val_Loss: 6122.810546875\n",
      "Epoch 177, Train_Loss: 5751.56591796875, Val_Loss: 6122.43994140625\n",
      "Epoch 178, Train_Loss: 5751.25048828125, Val_Loss: 6122.0712890625\n",
      "Epoch 179, Train_Loss: 5750.95166015625, Val_Loss: 6121.7177734375\n",
      "Epoch 180, Train_Loss: 5750.6455078125, Val_Loss: 6121.365234375\n",
      "Epoch 181, Train_Loss: 5750.3232421875, Val_Loss: 6120.9892578125\n",
      "Epoch 182, Train_Loss: 5749.9775390625, Val_Loss: 6120.64990234375\n",
      "Epoch 183, Train_Loss: 5749.66943359375, Val_Loss: 6120.2890625\n",
      "Epoch 184, Train_Loss: 5749.34423828125, Val_Loss: 6119.9013671875\n",
      "Epoch 185, Train_Loss: 5749.013671875, Val_Loss: 6119.55224609375\n",
      "Epoch 186, Train_Loss: 5748.69189453125, Val_Loss: 6119.2001953125\n",
      "Epoch 187, Train_Loss: 5748.31494140625, Val_Loss: 6118.83984375\n",
      "Epoch 188, Train_Loss: 5748.0009765625, Val_Loss: 6118.48876953125\n",
      "Epoch 189, Train_Loss: 5747.6875, Val_Loss: 6118.142578125\n",
      "Epoch 190, Train_Loss: 5747.365234375, Val_Loss: 6117.763671875\n",
      "Epoch 191, Train_Loss: 5747.0244140625, Val_Loss: 6117.40625\n",
      "Epoch 192, Train_Loss: 5746.685546875, Val_Loss: 6117.03173828125\n",
      "Epoch 193, Train_Loss: 5746.36865234375, Val_Loss: 6116.68115234375\n",
      "Epoch 194, Train_Loss: 5746.05712890625, Val_Loss: 6116.3369140625\n",
      "Epoch 195, Train_Loss: 5745.7373046875, Val_Loss: 6115.97607421875\n",
      "Epoch 196, Train_Loss: 5745.3994140625, Val_Loss: 6115.60693359375\n",
      "Epoch 197, Train_Loss: 5745.080078125, Val_Loss: 6115.25244140625\n",
      "Epoch 198, Train_Loss: 5744.74755859375, Val_Loss: 6114.88818359375\n",
      "Epoch 199, Train_Loss: 5744.36474609375, Val_Loss: 6114.5302734375\n",
      "Epoch 200, Train_Loss: 5744.0419921875, Val_Loss: 6114.15869140625\n",
      "Epoch 201, Train_Loss: 5743.71875, Val_Loss: 6113.8173828125\n",
      "Epoch 202, Train_Loss: 5743.40087890625, Val_Loss: 6113.4658203125\n",
      "Epoch 203, Train_Loss: 5743.08203125, Val_Loss: 6113.09033203125\n",
      "Epoch 204, Train_Loss: 5742.77490234375, Val_Loss: 6112.73681640625\n",
      "Epoch 205, Train_Loss: 5742.455078125, Val_Loss: 6112.3779296875\n",
      "Epoch 206, Train_Loss: 5742.13671875, Val_Loss: 6112.0302734375\n",
      "Epoch 207, Train_Loss: 5741.83154296875, Val_Loss: 6111.658203125\n",
      "Epoch 208, Train_Loss: 5741.5302734375, Val_Loss: 6111.32470703125\n",
      "Epoch 209, Train_Loss: 5741.2099609375, Val_Loss: 6110.91552734375\n",
      "Epoch 210, Train_Loss: 5740.86572265625, Val_Loss: 6110.564453125\n",
      "Epoch 211, Train_Loss: 5740.56396484375, Val_Loss: 6110.21435546875\n",
      "Epoch 212, Train_Loss: 5740.2041015625, Val_Loss: 6109.85498046875\n",
      "Epoch 213, Train_Loss: 5739.8798828125, Val_Loss: 6109.50048828125\n",
      "Epoch 214, Train_Loss: 5739.57666015625, Val_Loss: 6109.14892578125\n",
      "Epoch 215, Train_Loss: 5739.23291015625, Val_Loss: 6108.771484375\n",
      "Epoch 216, Train_Loss: 5738.92724609375, Val_Loss: 6108.4345703125\n",
      "Epoch 217, Train_Loss: 5738.59912109375, Val_Loss: 6108.076171875\n",
      "Epoch 218, Train_Loss: 5738.2001953125, Val_Loss: 6107.708984375\n",
      "Epoch 219, Train_Loss: 5737.88134765625, Val_Loss: 6107.35986328125\n",
      "Epoch 220, Train_Loss: 5737.5595703125, Val_Loss: 6106.99072265625\n",
      "Epoch 221, Train_Loss: 5737.23583984375, Val_Loss: 6106.61669921875\n",
      "Epoch 222, Train_Loss: 5736.91552734375, Val_Loss: 6106.2724609375\n",
      "Epoch 223, Train_Loss: 5736.60107421875, Val_Loss: 6105.91943359375\n",
      "Epoch 224, Train_Loss: 5736.240234375, Val_Loss: 6105.58056640625\n",
      "Epoch 225, Train_Loss: 5735.92822265625, Val_Loss: 6105.22900390625\n",
      "Epoch 226, Train_Loss: 5735.52783203125, Val_Loss: 6104.861328125\n",
      "Epoch 227, Train_Loss: 5735.2041015625, Val_Loss: 6104.5048828125\n",
      "Epoch 228, Train_Loss: 5734.87451171875, Val_Loss: 6104.1337890625\n",
      "Epoch 229, Train_Loss: 5734.54443359375, Val_Loss: 6103.7861328125\n",
      "Epoch 230, Train_Loss: 5734.232421875, Val_Loss: 6103.43017578125\n",
      "Epoch 231, Train_Loss: 5733.9169921875, Val_Loss: 6103.07763671875\n",
      "Epoch 232, Train_Loss: 5733.607421875, Val_Loss: 6102.71826171875\n",
      "Epoch 233, Train_Loss: 5733.2822265625, Val_Loss: 6102.34130859375\n",
      "Epoch 234, Train_Loss: 5732.97119140625, Val_Loss: 6101.9990234375\n",
      "Epoch 235, Train_Loss: 5732.62744140625, Val_Loss: 6101.6328125\n",
      "Epoch 236, Train_Loss: 5732.228515625, Val_Loss: 6101.27099609375\n",
      "Epoch 237, Train_Loss: 5731.8994140625, Val_Loss: 6100.93408203125\n",
      "Epoch 238, Train_Loss: 5731.5810546875, Val_Loss: 6100.56640625\n",
      "Epoch 239, Train_Loss: 5731.24755859375, Val_Loss: 6100.193359375\n",
      "Epoch 240, Train_Loss: 5730.935546875, Val_Loss: 6099.84423828125\n",
      "Epoch 241, Train_Loss: 5730.6298828125, Val_Loss: 6099.484375\n",
      "Epoch 242, Train_Loss: 5730.31201171875, Val_Loss: 6099.12744140625\n",
      "Epoch 243, Train_Loss: 5729.994140625, Val_Loss: 6098.77734375\n",
      "Epoch 244, Train_Loss: 5729.67529296875, Val_Loss: 6098.39990234375\n",
      "Epoch 245, Train_Loss: 5729.36865234375, Val_Loss: 6098.033203125\n",
      "Epoch 246, Train_Loss: 5729.0576171875, Val_Loss: 6097.66455078125\n",
      "Epoch 247, Train_Loss: 5728.72216796875, Val_Loss: 6097.32080078125\n",
      "Epoch 248, Train_Loss: 5728.390625, Val_Loss: 6096.9677734375\n",
      "Epoch 249, Train_Loss: 5728.078125, Val_Loss: 6096.6181640625\n",
      "Epoch 250, Train_Loss: 5727.75927734375, Val_Loss: 6096.26171875\n",
      "Epoch 251, Train_Loss: 5727.4482421875, Val_Loss: 6095.88720703125\n",
      "Epoch 252, Train_Loss: 5727.146484375, Val_Loss: 6095.5322265625\n",
      "Epoch 253, Train_Loss: 5726.83056640625, Val_Loss: 6095.177734375\n",
      "Epoch 254, Train_Loss: 5726.51318359375, Val_Loss: 6094.8271484375\n",
      "Epoch 255, Train_Loss: 5726.2021484375, Val_Loss: 6094.4765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256, Train_Loss: 5725.85009765625, Val_Loss: 6094.12158203125\n",
      "Epoch 257, Train_Loss: 5725.5244140625, Val_Loss: 6093.77392578125\n",
      "Epoch 258, Train_Loss: 5725.21728515625, Val_Loss: 6093.40966796875\n",
      "Epoch 259, Train_Loss: 5724.89404296875, Val_Loss: 6093.05517578125\n",
      "Epoch 260, Train_Loss: 5724.58642578125, Val_Loss: 6092.71875\n",
      "Epoch 261, Train_Loss: 5724.173828125, Val_Loss: 6092.3388671875\n",
      "Epoch 262, Train_Loss: 5723.86181640625, Val_Loss: 6091.98193359375\n",
      "Epoch 263, Train_Loss: 5723.54931640625, Val_Loss: 6091.65234375\n",
      "Epoch 264, Train_Loss: 5723.236328125, Val_Loss: 6091.27783203125\n",
      "Epoch 265, Train_Loss: 5722.89013671875, Val_Loss: 6090.91259765625\n",
      "Epoch 266, Train_Loss: 5722.57861328125, Val_Loss: 6090.5673828125\n",
      "Epoch 267, Train_Loss: 5722.259765625, Val_Loss: 6090.2099609375\n",
      "Epoch 268, Train_Loss: 5721.94921875, Val_Loss: 6089.8583984375\n",
      "Epoch 269, Train_Loss: 5721.64306640625, Val_Loss: 6089.49951171875\n",
      "Epoch 270, Train_Loss: 5721.3125, Val_Loss: 6089.12255859375\n",
      "Epoch 271, Train_Loss: 5720.998046875, Val_Loss: 6088.77783203125\n",
      "Epoch 272, Train_Loss: 5720.67626953125, Val_Loss: 6088.42919921875\n",
      "Epoch 273, Train_Loss: 5720.32275390625, Val_Loss: 6088.08935546875\n",
      "Epoch 274, Train_Loss: 5719.998046875, Val_Loss: 6087.73095703125\n",
      "Epoch 275, Train_Loss: 5719.68896484375, Val_Loss: 6087.3818359375\n",
      "Epoch 276, Train_Loss: 5719.349609375, Val_Loss: 6087.01123046875\n",
      "Epoch 277, Train_Loss: 5719.02587890625, Val_Loss: 6086.6572265625\n",
      "Epoch 278, Train_Loss: 5718.69970703125, Val_Loss: 6086.27880859375\n",
      "Epoch 279, Train_Loss: 5718.37060546875, Val_Loss: 6085.90673828125\n",
      "Epoch 280, Train_Loss: 5718.04931640625, Val_Loss: 6085.57080078125\n",
      "Epoch 281, Train_Loss: 5717.736328125, Val_Loss: 6085.224609375\n",
      "Epoch 282, Train_Loss: 5717.42822265625, Val_Loss: 6084.830078125\n",
      "Epoch 283, Train_Loss: 5717.13037109375, Val_Loss: 6084.48779296875\n",
      "Epoch 284, Train_Loss: 5716.82421875, Val_Loss: 6084.1416015625\n",
      "Epoch 285, Train_Loss: 5716.46484375, Val_Loss: 6083.78564453125\n",
      "Epoch 286, Train_Loss: 5716.15283203125, Val_Loss: 6083.4521484375\n",
      "Epoch 287, Train_Loss: 5715.83154296875, Val_Loss: 6083.08935546875\n",
      "Epoch 288, Train_Loss: 5715.50341796875, Val_Loss: 6082.7236328125\n",
      "Epoch 289, Train_Loss: 5715.18310546875, Val_Loss: 6082.37255859375\n",
      "Epoch 290, Train_Loss: 5714.85888671875, Val_Loss: 6082.03369140625\n",
      "Epoch 291, Train_Loss: 5714.525390625, Val_Loss: 6081.6669921875\n",
      "Epoch 292, Train_Loss: 5714.21923828125, Val_Loss: 6081.30615234375\n",
      "Epoch 293, Train_Loss: 5713.87353515625, Val_Loss: 6080.9482421875\n",
      "Epoch 294, Train_Loss: 5713.5732421875, Val_Loss: 6080.58154296875\n",
      "Epoch 295, Train_Loss: 5713.16064453125, Val_Loss: 6080.22705078125\n",
      "Epoch 296, Train_Loss: 5712.84228515625, Val_Loss: 6079.87255859375\n",
      "Epoch 297, Train_Loss: 5712.45458984375, Val_Loss: 6079.54052734375\n",
      "Epoch 298, Train_Loss: 5712.1318359375, Val_Loss: 6079.185546875\n",
      "Epoch 299, Train_Loss: 5711.83984375, Val_Loss: 6078.841796875\n",
      "Epoch 300, Train_Loss: 5711.51171875, Val_Loss: 6078.47216796875\n",
      "Epoch 301, Train_Loss: 5711.2080078125, Val_Loss: 6078.12255859375\n",
      "Epoch 302, Train_Loss: 5710.865234375, Val_Loss: 6077.7646484375\n",
      "Epoch 303, Train_Loss: 5710.54150390625, Val_Loss: 6077.4130859375\n",
      "Epoch 304, Train_Loss: 5710.22705078125, Val_Loss: 6077.04541015625\n",
      "Epoch 305, Train_Loss: 5709.93359375, Val_Loss: 6076.6962890625\n",
      "Epoch 306, Train_Loss: 5709.62353515625, Val_Loss: 6076.3369140625\n",
      "Epoch 307, Train_Loss: 5709.306640625, Val_Loss: 6075.9853515625\n",
      "Epoch 308, Train_Loss: 5708.98291015625, Val_Loss: 6075.64892578125\n",
      "Epoch 309, Train_Loss: 5708.63134765625, Val_Loss: 6075.2890625\n",
      "Epoch 310, Train_Loss: 5708.3095703125, Val_Loss: 6074.94775390625\n",
      "Epoch 311, Train_Loss: 5707.99169921875, Val_Loss: 6074.59912109375\n",
      "Epoch 312, Train_Loss: 5707.63916015625, Val_Loss: 6074.21142578125\n",
      "Epoch 313, Train_Loss: 5707.3125, Val_Loss: 6073.8505859375\n",
      "Epoch 314, Train_Loss: 5706.998046875, Val_Loss: 6073.5068359375\n",
      "Epoch 315, Train_Loss: 5706.67138671875, Val_Loss: 6073.15966796875\n",
      "Epoch 316, Train_Loss: 5706.34521484375, Val_Loss: 6072.79296875\n",
      "Epoch 317, Train_Loss: 5706.02880859375, Val_Loss: 6072.4375\n",
      "Epoch 318, Train_Loss: 5705.72216796875, Val_Loss: 6072.0966796875\n",
      "Epoch 319, Train_Loss: 5705.39990234375, Val_Loss: 6071.73095703125\n",
      "Epoch 320, Train_Loss: 5705.083984375, Val_Loss: 6071.37255859375\n",
      "Epoch 321, Train_Loss: 5704.75146484375, Val_Loss: 6071.0322265625\n",
      "Epoch 322, Train_Loss: 5704.37939453125, Val_Loss: 6070.66943359375\n",
      "Epoch 323, Train_Loss: 5704.0810546875, Val_Loss: 6070.31787109375\n",
      "Epoch 324, Train_Loss: 5703.77197265625, Val_Loss: 6069.9658203125\n",
      "Epoch 325, Train_Loss: 5703.45751953125, Val_Loss: 6069.61474609375\n",
      "Epoch 326, Train_Loss: 5703.158203125, Val_Loss: 6069.2607421875\n",
      "Epoch 327, Train_Loss: 5702.85888671875, Val_Loss: 6068.90625\n",
      "Epoch 328, Train_Loss: 5702.54833984375, Val_Loss: 6068.552734375\n",
      "Epoch 329, Train_Loss: 5702.228515625, Val_Loss: 6068.205078125\n",
      "Epoch 330, Train_Loss: 5701.8759765625, Val_Loss: 6067.8544921875\n",
      "Epoch 331, Train_Loss: 5701.55810546875, Val_Loss: 6067.48779296875\n",
      "Epoch 332, Train_Loss: 5701.24365234375, Val_Loss: 6067.13623046875\n",
      "Epoch 333, Train_Loss: 5700.93359375, Val_Loss: 6066.77978515625\n",
      "Epoch 334, Train_Loss: 5700.5263671875, Val_Loss: 6066.43994140625\n",
      "Epoch 335, Train_Loss: 5700.20703125, Val_Loss: 6066.0810546875\n",
      "Epoch 336, Train_Loss: 5699.9013671875, Val_Loss: 6065.74365234375\n",
      "Epoch 337, Train_Loss: 5699.58056640625, Val_Loss: 6065.35888671875\n",
      "Epoch 338, Train_Loss: 5699.28515625, Val_Loss: 6065.00634765625\n",
      "Epoch 339, Train_Loss: 5698.96826171875, Val_Loss: 6064.65283203125\n",
      "Epoch 340, Train_Loss: 5698.6416015625, Val_Loss: 6064.29736328125\n",
      "Epoch 341, Train_Loss: 5698.34033203125, Val_Loss: 6063.95947265625\n",
      "Epoch 342, Train_Loss: 5698.03173828125, Val_Loss: 6063.609375\n",
      "Epoch 343, Train_Loss: 5697.720703125, Val_Loss: 6063.23388671875\n",
      "Epoch 344, Train_Loss: 5697.373046875, Val_Loss: 6062.892578125\n",
      "Epoch 345, Train_Loss: 5697.03466796875, Val_Loss: 6062.548828125\n",
      "Epoch 346, Train_Loss: 5696.677734375, Val_Loss: 6062.2080078125\n",
      "Epoch 347, Train_Loss: 5696.3671875, Val_Loss: 6061.82373046875\n",
      "Epoch 348, Train_Loss: 5696.048828125, Val_Loss: 6061.4609375\n",
      "Epoch 349, Train_Loss: 5695.7353515625, Val_Loss: 6061.09765625\n",
      "Epoch 350, Train_Loss: 5695.42919921875, Val_Loss: 6060.75732421875\n",
      "Epoch 351, Train_Loss: 5695.130859375, Val_Loss: 6060.40380859375\n",
      "Epoch 352, Train_Loss: 5694.837890625, Val_Loss: 6060.048828125\n",
      "Epoch 353, Train_Loss: 5694.513671875, Val_Loss: 6059.708984375\n",
      "Epoch 354, Train_Loss: 5694.21826171875, Val_Loss: 6059.3564453125\n",
      "Epoch 355, Train_Loss: 5693.90478515625, Val_Loss: 6058.9833984375\n",
      "Epoch 356, Train_Loss: 5693.5693359375, Val_Loss: 6058.630859375\n",
      "Epoch 357, Train_Loss: 5693.24755859375, Val_Loss: 6058.2734375\n",
      "Epoch 358, Train_Loss: 5692.8359375, Val_Loss: 6057.93701171875\n",
      "Epoch 359, Train_Loss: 5692.53125, Val_Loss: 6057.595703125\n",
      "Epoch 360, Train_Loss: 5692.22607421875, Val_Loss: 6057.24462890625\n",
      "Epoch 361, Train_Loss: 5691.92431640625, Val_Loss: 6056.89501953125\n",
      "Epoch 362, Train_Loss: 5691.61328125, Val_Loss: 6056.53076171875\n",
      "Epoch 363, Train_Loss: 5691.29833984375, Val_Loss: 6056.173828125\n",
      "Epoch 364, Train_Loss: 5690.93115234375, Val_Loss: 6055.82958984375\n",
      "Epoch 365, Train_Loss: 5690.625, Val_Loss: 6055.4892578125\n",
      "Epoch 366, Train_Loss: 5690.31396484375, Val_Loss: 6055.13134765625\n",
      "Epoch 367, Train_Loss: 5689.99658203125, Val_Loss: 6054.77294921875\n",
      "Epoch 368, Train_Loss: 5689.68359375, Val_Loss: 6054.40966796875\n",
      "Epoch 369, Train_Loss: 5689.3544921875, Val_Loss: 6054.0712890625\n",
      "Epoch 370, Train_Loss: 5689.05224609375, Val_Loss: 6053.71728515625\n",
      "Epoch 371, Train_Loss: 5688.685546875, Val_Loss: 6053.37744140625\n",
      "Epoch 372, Train_Loss: 5688.357421875, Val_Loss: 6053.0234375\n",
      "Epoch 373, Train_Loss: 5688.03271484375, Val_Loss: 6052.65869140625\n",
      "Epoch 374, Train_Loss: 5687.72216796875, Val_Loss: 6052.29345703125\n",
      "Epoch 375, Train_Loss: 5687.4208984375, Val_Loss: 6051.9521484375\n",
      "Epoch 376, Train_Loss: 5687.0966796875, Val_Loss: 6051.60400390625\n",
      "Epoch 377, Train_Loss: 5686.76220703125, Val_Loss: 6051.24462890625\n",
      "Epoch 378, Train_Loss: 5686.44482421875, Val_Loss: 6050.90380859375\n",
      "Epoch 379, Train_Loss: 5686.1376953125, Val_Loss: 6050.55712890625\n",
      "Epoch 380, Train_Loss: 5685.81591796875, Val_Loss: 6050.193359375\n",
      "Epoch 381, Train_Loss: 5685.50341796875, Val_Loss: 6049.84326171875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 382, Train_Loss: 5685.18798828125, Val_Loss: 6049.4501953125\n",
      "Epoch 383, Train_Loss: 5684.85546875, Val_Loss: 6049.10302734375\n",
      "Epoch 384, Train_Loss: 5684.55810546875, Val_Loss: 6048.7626953125\n",
      "Epoch 385, Train_Loss: 5684.22607421875, Val_Loss: 6048.42138671875\n",
      "Epoch 386, Train_Loss: 5683.90380859375, Val_Loss: 6048.05078125\n",
      "Epoch 387, Train_Loss: 5683.58740234375, Val_Loss: 6047.7080078125\n",
      "Epoch 388, Train_Loss: 5683.28173828125, Val_Loss: 6047.35498046875\n",
      "Epoch 389, Train_Loss: 5682.9765625, Val_Loss: 6047.00048828125\n",
      "Epoch 390, Train_Loss: 5682.66015625, Val_Loss: 6046.65771484375\n",
      "Epoch 391, Train_Loss: 5682.3642578125, Val_Loss: 6046.306640625\n",
      "Epoch 392, Train_Loss: 5682.0400390625, Val_Loss: 6045.95166015625\n",
      "Epoch 393, Train_Loss: 5681.74658203125, Val_Loss: 6045.6025390625\n",
      "Epoch 394, Train_Loss: 5681.4228515625, Val_Loss: 6045.25341796875\n",
      "Epoch 395, Train_Loss: 5681.0302734375, Val_Loss: 6044.890625\n",
      "Epoch 396, Train_Loss: 5680.71630859375, Val_Loss: 6044.5498046875\n",
      "Epoch 397, Train_Loss: 5680.4072265625, Val_Loss: 6044.216796875\n",
      "Epoch 398, Train_Loss: 5680.08935546875, Val_Loss: 6043.84423828125\n",
      "Epoch 399, Train_Loss: 5679.76806640625, Val_Loss: 6043.49169921875\n",
      "Epoch 400, Train_Loss: 5679.451171875, Val_Loss: 6043.12744140625\n",
      "Epoch 401, Train_Loss: 5679.13037109375, Val_Loss: 6042.7880859375\n",
      "Epoch 402, Train_Loss: 5678.83056640625, Val_Loss: 6042.4296875\n",
      "Epoch 403, Train_Loss: 5678.51318359375, Val_Loss: 6042.080078125\n",
      "Epoch 404, Train_Loss: 5678.1884765625, Val_Loss: 6041.7265625\n",
      "Epoch 405, Train_Loss: 5677.87939453125, Val_Loss: 6041.376953125\n",
      "Epoch 406, Train_Loss: 5677.568359375, Val_Loss: 6041.029296875\n",
      "Epoch 407, Train_Loss: 5677.24853515625, Val_Loss: 6040.66943359375\n",
      "Epoch 408, Train_Loss: 5676.890625, Val_Loss: 6040.326171875\n",
      "Epoch 409, Train_Loss: 5676.57861328125, Val_Loss: 6039.990234375\n",
      "Epoch 410, Train_Loss: 5676.27587890625, Val_Loss: 6039.64404296875\n",
      "Epoch 411, Train_Loss: 5675.96044921875, Val_Loss: 6039.26904296875\n",
      "Epoch 412, Train_Loss: 5675.669921875, Val_Loss: 6038.92724609375\n",
      "Epoch 413, Train_Loss: 5675.32666015625, Val_Loss: 6038.57373046875\n",
      "Epoch 414, Train_Loss: 5675.00390625, Val_Loss: 6038.22314453125\n",
      "Epoch 415, Train_Loss: 5674.70556640625, Val_Loss: 6037.87890625\n",
      "Epoch 416, Train_Loss: 5674.40234375, Val_Loss: 6037.50439453125\n",
      "Epoch 417, Train_Loss: 5674.0859375, Val_Loss: 6037.13720703125\n",
      "Epoch 418, Train_Loss: 5673.75341796875, Val_Loss: 6036.7919921875\n",
      "Epoch 419, Train_Loss: 5673.4462890625, Val_Loss: 6036.453125\n",
      "Epoch 420, Train_Loss: 5673.06787109375, Val_Loss: 6036.107421875\n",
      "Epoch 421, Train_Loss: 5672.76806640625, Val_Loss: 6035.75341796875\n",
      "Epoch 422, Train_Loss: 5672.4619140625, Val_Loss: 6035.40966796875\n",
      "Epoch 423, Train_Loss: 5672.1474609375, Val_Loss: 6035.05712890625\n",
      "Epoch 424, Train_Loss: 5671.83642578125, Val_Loss: 6034.70849609375\n",
      "Epoch 425, Train_Loss: 5671.50830078125, Val_Loss: 6034.35498046875\n",
      "Epoch 426, Train_Loss: 5671.1923828125, Val_Loss: 6034.00146484375\n",
      "Epoch 427, Train_Loss: 5670.8603515625, Val_Loss: 6033.6494140625\n",
      "Epoch 428, Train_Loss: 5670.53662109375, Val_Loss: 6033.2978515625\n",
      "Epoch 429, Train_Loss: 5670.240234375, Val_Loss: 6032.935546875\n",
      "Epoch 430, Train_Loss: 5669.93212890625, Val_Loss: 6032.59814453125\n",
      "Epoch 431, Train_Loss: 5669.6328125, Val_Loss: 6032.25537109375\n",
      "Epoch 432, Train_Loss: 5669.2275390625, Val_Loss: 6031.90283203125\n",
      "Epoch 433, Train_Loss: 5668.68701171875, Val_Loss: 6031.54248046875\n",
      "Epoch 434, Train_Loss: 5668.3876953125, Val_Loss: 6031.197265625\n",
      "Epoch 435, Train_Loss: 5668.07958984375, Val_Loss: 6030.84423828125\n",
      "Epoch 436, Train_Loss: 5667.7646484375, Val_Loss: 6030.486328125\n",
      "Epoch 437, Train_Loss: 5667.4462890625, Val_Loss: 6030.1494140625\n",
      "Epoch 438, Train_Loss: 5667.14501953125, Val_Loss: 6029.80615234375\n",
      "Epoch 439, Train_Loss: 5666.8447265625, Val_Loss: 6029.45458984375\n",
      "Epoch 440, Train_Loss: 5666.5322265625, Val_Loss: 6029.10009765625\n",
      "Epoch 441, Train_Loss: 5666.19873046875, Val_Loss: 6028.73876953125\n",
      "Epoch 442, Train_Loss: 5665.8828125, Val_Loss: 6028.38427734375\n",
      "Epoch 443, Train_Loss: 5665.5439453125, Val_Loss: 6028.03662109375\n",
      "Epoch 444, Train_Loss: 5665.21484375, Val_Loss: 6027.70166015625\n",
      "Epoch 445, Train_Loss: 5664.89892578125, Val_Loss: 6027.3486328125\n",
      "Epoch 446, Train_Loss: 5664.583984375, Val_Loss: 6027.00732421875\n",
      "Epoch 447, Train_Loss: 5664.29296875, Val_Loss: 6026.640625\n",
      "Epoch 448, Train_Loss: 5663.98046875, Val_Loss: 6026.3125\n",
      "Epoch 449, Train_Loss: 5663.67333984375, Val_Loss: 6025.97216796875\n",
      "Epoch 450, Train_Loss: 5663.32470703125, Val_Loss: 6025.599609375\n",
      "Epoch 451, Train_Loss: 5662.97998046875, Val_Loss: 6025.2314453125\n",
      "Epoch 452, Train_Loss: 5662.67041015625, Val_Loss: 6024.87646484375\n",
      "Epoch 453, Train_Loss: 5662.376953125, Val_Loss: 6024.53564453125\n",
      "Epoch 454, Train_Loss: 5662.08056640625, Val_Loss: 6024.17578125\n",
      "Epoch 455, Train_Loss: 5661.7529296875, Val_Loss: 6023.84326171875\n",
      "Epoch 456, Train_Loss: 5661.43505859375, Val_Loss: 6023.49169921875\n",
      "Epoch 457, Train_Loss: 5661.03466796875, Val_Loss: 6023.1455078125\n",
      "Epoch 458, Train_Loss: 5660.73828125, Val_Loss: 6022.80224609375\n",
      "Epoch 459, Train_Loss: 5660.435546875, Val_Loss: 6022.45263671875\n",
      "Epoch 460, Train_Loss: 5660.11572265625, Val_Loss: 6022.08935546875\n",
      "Epoch 461, Train_Loss: 5659.80517578125, Val_Loss: 6021.7373046875\n",
      "Epoch 462, Train_Loss: 5659.49658203125, Val_Loss: 6021.4111328125\n",
      "Epoch 463, Train_Loss: 5659.201171875, Val_Loss: 6021.046875\n",
      "Epoch 464, Train_Loss: 5658.87744140625, Val_Loss: 6020.70703125\n",
      "Epoch 465, Train_Loss: 5658.56982421875, Val_Loss: 6020.36962890625\n",
      "Epoch 466, Train_Loss: 5658.26904296875, Val_Loss: 6020.00244140625\n",
      "Epoch 467, Train_Loss: 5657.95458984375, Val_Loss: 6019.650390625\n",
      "Epoch 468, Train_Loss: 5657.6494140625, Val_Loss: 6019.2958984375\n",
      "Epoch 469, Train_Loss: 5657.28173828125, Val_Loss: 6018.95458984375\n",
      "Epoch 470, Train_Loss: 5656.97119140625, Val_Loss: 6018.609375\n",
      "Epoch 471, Train_Loss: 5656.66455078125, Val_Loss: 6018.251953125\n",
      "Epoch 472, Train_Loss: 5656.359375, Val_Loss: 6017.9052734375\n",
      "Epoch 473, Train_Loss: 5656.05859375, Val_Loss: 6017.55224609375\n",
      "Epoch 474, Train_Loss: 5655.740234375, Val_Loss: 6017.20703125\n",
      "Epoch 475, Train_Loss: 5655.4150390625, Val_Loss: 6016.85009765625\n",
      "Epoch 476, Train_Loss: 5655.11083984375, Val_Loss: 6016.50439453125\n",
      "Epoch 477, Train_Loss: 5654.78857421875, Val_Loss: 6016.1396484375\n",
      "Epoch 478, Train_Loss: 5654.46044921875, Val_Loss: 6015.78662109375\n",
      "Epoch 479, Train_Loss: 5654.1630859375, Val_Loss: 6015.44873046875\n",
      "Epoch 480, Train_Loss: 5653.8486328125, Val_Loss: 6015.10400390625\n",
      "Epoch 481, Train_Loss: 5653.49462890625, Val_Loss: 6014.763671875\n",
      "Epoch 482, Train_Loss: 5653.20703125, Val_Loss: 6014.40576171875\n",
      "Epoch 483, Train_Loss: 5652.88818359375, Val_Loss: 6014.0712890625\n",
      "Epoch 484, Train_Loss: 5652.57861328125, Val_Loss: 6013.7080078125\n",
      "Epoch 485, Train_Loss: 5652.2646484375, Val_Loss: 6013.3359375\n",
      "Epoch 486, Train_Loss: 5651.943359375, Val_Loss: 6012.98779296875\n",
      "Epoch 487, Train_Loss: 5651.6435546875, Val_Loss: 6012.63720703125\n",
      "Epoch 488, Train_Loss: 5651.31494140625, Val_Loss: 6012.29736328125\n",
      "Epoch 489, Train_Loss: 5651.00439453125, Val_Loss: 6011.9453125\n",
      "Epoch 490, Train_Loss: 5650.7060546875, Val_Loss: 6011.58740234375\n",
      "Epoch 491, Train_Loss: 5650.4111328125, Val_Loss: 6011.244140625\n",
      "Epoch 492, Train_Loss: 5650.06591796875, Val_Loss: 6010.90234375\n",
      "Epoch 493, Train_Loss: 5649.77880859375, Val_Loss: 6010.55029296875\n",
      "Epoch 494, Train_Loss: 5649.3232421875, Val_Loss: 6010.20947265625\n",
      "Epoch 495, Train_Loss: 5649.0234375, Val_Loss: 6009.8671875\n",
      "Epoch 496, Train_Loss: 5648.7119140625, Val_Loss: 6009.525390625\n",
      "Epoch 497, Train_Loss: 5648.39453125, Val_Loss: 6009.16064453125\n",
      "Epoch 498, Train_Loss: 5648.0673828125, Val_Loss: 6008.81005859375\n",
      "Epoch 499, Train_Loss: 5647.75146484375, Val_Loss: 6008.48095703125\n",
      "Epoch 500, Train_Loss: 5647.4306640625, Val_Loss: 6008.12451171875\n",
      "Epoch 501, Train_Loss: 5647.1298828125, Val_Loss: 6007.77978515625\n",
      "Epoch 502, Train_Loss: 5646.81689453125, Val_Loss: 6007.4345703125\n",
      "Epoch 503, Train_Loss: 5646.47607421875, Val_Loss: 6007.06396484375\n",
      "Epoch 504, Train_Loss: 5646.18212890625, Val_Loss: 6006.72607421875\n",
      "Epoch 505, Train_Loss: 5645.8740234375, Val_Loss: 6006.3818359375\n",
      "Epoch 506, Train_Loss: 5645.51220703125, Val_Loss: 6006.0322265625\n",
      "Epoch 507, Train_Loss: 5645.21484375, Val_Loss: 6005.6875\n",
      "Epoch 508, Train_Loss: 5644.9033203125, Val_Loss: 6005.35595703125\n",
      "Epoch 509, Train_Loss: 5644.57080078125, Val_Loss: 6005.00341796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 510, Train_Loss: 5644.2763671875, Val_Loss: 6004.65576171875\n",
      "Epoch 511, Train_Loss: 5643.94287109375, Val_Loss: 6004.3037109375\n",
      "Epoch 512, Train_Loss: 5643.6259765625, Val_Loss: 6003.9482421875\n",
      "Epoch 513, Train_Loss: 5643.31103515625, Val_Loss: 6003.60791015625\n",
      "Epoch 514, Train_Loss: 5643.0068359375, Val_Loss: 6003.2529296875\n",
      "Epoch 515, Train_Loss: 5642.71142578125, Val_Loss: 6002.89501953125\n",
      "Epoch 516, Train_Loss: 5642.3984375, Val_Loss: 6002.55029296875\n",
      "Epoch 517, Train_Loss: 5642.09423828125, Val_Loss: 6002.2099609375\n",
      "Epoch 518, Train_Loss: 5641.74462890625, Val_Loss: 6001.869140625\n",
      "Epoch 519, Train_Loss: 5641.4541015625, Val_Loss: 6001.537109375\n",
      "Epoch 520, Train_Loss: 5641.13427734375, Val_Loss: 6001.1591796875\n",
      "Epoch 521, Train_Loss: 5640.8212890625, Val_Loss: 6000.79638671875\n",
      "Epoch 522, Train_Loss: 5640.52490234375, Val_Loss: 6000.45703125\n",
      "Epoch 523, Train_Loss: 5640.2275390625, Val_Loss: 6000.10986328125\n",
      "Epoch 524, Train_Loss: 5639.9248046875, Val_Loss: 5999.7607421875\n",
      "Epoch 525, Train_Loss: 5639.587890625, Val_Loss: 5999.4248046875\n",
      "Epoch 526, Train_Loss: 5639.2744140625, Val_Loss: 5999.076171875\n",
      "Epoch 527, Train_Loss: 5638.96484375, Val_Loss: 5998.7109375\n",
      "Epoch 528, Train_Loss: 5638.66162109375, Val_Loss: 5998.361328125\n",
      "Epoch 529, Train_Loss: 5638.322265625, Val_Loss: 5998.0205078125\n",
      "Epoch 530, Train_Loss: 5637.927734375, Val_Loss: 5997.67724609375\n",
      "Epoch 531, Train_Loss: 5637.62158203125, Val_Loss: 5997.3359375\n",
      "Epoch 532, Train_Loss: 5637.3291015625, Val_Loss: 5996.99462890625\n",
      "Epoch 533, Train_Loss: 5637.02734375, Val_Loss: 5996.64794921875\n",
      "Epoch 534, Train_Loss: 5636.7109375, Val_Loss: 5996.302734375\n",
      "Epoch 535, Train_Loss: 5636.41015625, Val_Loss: 5995.953125\n",
      "Epoch 536, Train_Loss: 5636.107421875, Val_Loss: 5995.60498046875\n",
      "Epoch 537, Train_Loss: 5635.79345703125, Val_Loss: 5995.2392578125\n",
      "Epoch 538, Train_Loss: 5635.47607421875, Val_Loss: 5994.90478515625\n",
      "Epoch 539, Train_Loss: 5635.16650390625, Val_Loss: 5994.56298828125\n",
      "Epoch 540, Train_Loss: 5634.859375, Val_Loss: 5994.18994140625\n",
      "Epoch 541, Train_Loss: 5634.533203125, Val_Loss: 5993.869140625\n",
      "Epoch 542, Train_Loss: 5634.21337890625, Val_Loss: 5993.51611328125\n",
      "Epoch 543, Train_Loss: 5633.873046875, Val_Loss: 5993.17822265625\n",
      "Epoch 544, Train_Loss: 5633.58251953125, Val_Loss: 5992.83984375\n",
      "Epoch 545, Train_Loss: 5633.2744140625, Val_Loss: 5992.482421875\n",
      "Epoch 546, Train_Loss: 5632.98095703125, Val_Loss: 5992.1201171875\n",
      "Epoch 547, Train_Loss: 5632.6650390625, Val_Loss: 5991.76513671875\n",
      "Epoch 548, Train_Loss: 5632.36181640625, Val_Loss: 5991.42626953125\n",
      "Epoch 549, Train_Loss: 5632.03466796875, Val_Loss: 5991.07373046875\n",
      "Epoch 550, Train_Loss: 5631.7529296875, Val_Loss: 5990.73583984375\n",
      "Epoch 551, Train_Loss: 5631.44091796875, Val_Loss: 5990.3984375\n",
      "Epoch 552, Train_Loss: 5631.140625, Val_Loss: 5990.02783203125\n",
      "Epoch 553, Train_Loss: 5630.8291015625, Val_Loss: 5989.68994140625\n",
      "Epoch 554, Train_Loss: 5630.51708984375, Val_Loss: 5989.34814453125\n",
      "Epoch 555, Train_Loss: 5630.11181640625, Val_Loss: 5988.9794921875\n",
      "Epoch 556, Train_Loss: 5629.798828125, Val_Loss: 5988.63427734375\n",
      "Epoch 557, Train_Loss: 5629.48876953125, Val_Loss: 5988.28759765625\n",
      "Epoch 558, Train_Loss: 5629.189453125, Val_Loss: 5987.93408203125\n",
      "Epoch 559, Train_Loss: 5628.87841796875, Val_Loss: 5987.6005859375\n",
      "Epoch 560, Train_Loss: 5628.57373046875, Val_Loss: 5987.2490234375\n",
      "Epoch 561, Train_Loss: 5628.2548828125, Val_Loss: 5986.90087890625\n",
      "Epoch 562, Train_Loss: 5627.9248046875, Val_Loss: 5986.5458984375\n",
      "Epoch 563, Train_Loss: 5627.61669921875, Val_Loss: 5986.2060546875\n",
      "Epoch 564, Train_Loss: 5627.30224609375, Val_Loss: 5985.845703125\n",
      "Epoch 565, Train_Loss: 5626.99853515625, Val_Loss: 5985.50244140625\n",
      "Epoch 566, Train_Loss: 5626.67724609375, Val_Loss: 5985.1611328125\n",
      "Epoch 567, Train_Loss: 5626.30859375, Val_Loss: 5984.8212890625\n",
      "Epoch 568, Train_Loss: 5625.99462890625, Val_Loss: 5984.48583984375\n",
      "Epoch 569, Train_Loss: 5625.673828125, Val_Loss: 5984.14501953125\n",
      "Epoch 570, Train_Loss: 5625.36474609375, Val_Loss: 5983.7783203125\n",
      "Epoch 571, Train_Loss: 5625.0732421875, Val_Loss: 5983.43408203125\n",
      "Epoch 572, Train_Loss: 5624.6025390625, Val_Loss: 5983.0869140625\n",
      "Epoch 573, Train_Loss: 5624.2841796875, Val_Loss: 5982.7578125\n",
      "Epoch 574, Train_Loss: 5623.97412109375, Val_Loss: 5982.41015625\n",
      "Epoch 575, Train_Loss: 5623.6904296875, Val_Loss: 5982.0576171875\n",
      "Epoch 576, Train_Loss: 5623.3916015625, Val_Loss: 5981.71875\n",
      "Epoch 577, Train_Loss: 5623.08203125, Val_Loss: 5981.36181640625\n",
      "Epoch 578, Train_Loss: 5622.7705078125, Val_Loss: 5981.02880859375\n",
      "Epoch 579, Train_Loss: 5622.4853515625, Val_Loss: 5980.69140625\n",
      "Epoch 580, Train_Loss: 5622.15283203125, Val_Loss: 5980.34716796875\n",
      "Epoch 581, Train_Loss: 5621.84033203125, Val_Loss: 5979.98583984375\n",
      "Epoch 582, Train_Loss: 5621.5234375, Val_Loss: 5979.65087890625\n",
      "Epoch 583, Train_Loss: 5621.21435546875, Val_Loss: 5979.28515625\n",
      "Epoch 584, Train_Loss: 5620.912109375, Val_Loss: 5978.94482421875\n",
      "Epoch 585, Train_Loss: 5620.61279296875, Val_Loss: 5978.60009765625\n",
      "Epoch 586, Train_Loss: 5620.3046875, Val_Loss: 5978.2578125\n",
      "Epoch 587, Train_Loss: 5620.00341796875, Val_Loss: 5977.91552734375\n",
      "Epoch 588, Train_Loss: 5619.6953125, Val_Loss: 5977.5732421875\n",
      "Epoch 589, Train_Loss: 5619.3994140625, Val_Loss: 5977.22216796875\n",
      "Epoch 590, Train_Loss: 5619.0537109375, Val_Loss: 5976.8349609375\n",
      "Epoch 591, Train_Loss: 5618.76025390625, Val_Loss: 5976.5078125\n",
      "Epoch 592, Train_Loss: 5618.3720703125, Val_Loss: 5976.1689453125\n",
      "Epoch 593, Train_Loss: 5618.07763671875, Val_Loss: 5975.82763671875\n",
      "Epoch 594, Train_Loss: 5617.76611328125, Val_Loss: 5975.486328125\n",
      "Epoch 595, Train_Loss: 5617.4375, Val_Loss: 5975.107421875\n",
      "Epoch 596, Train_Loss: 5617.13330078125, Val_Loss: 5974.7802734375\n",
      "Epoch 597, Train_Loss: 5616.8310546875, Val_Loss: 5974.447265625\n",
      "Epoch 598, Train_Loss: 5616.521484375, Val_Loss: 5974.095703125\n",
      "Epoch 599, Train_Loss: 5616.2001953125, Val_Loss: 5973.75634765625\n",
      "Epoch 600, Train_Loss: 5615.88720703125, Val_Loss: 5973.416015625\n",
      "Epoch 601, Train_Loss: 5615.5908203125, Val_Loss: 5973.05322265625\n",
      "Epoch 602, Train_Loss: 5615.302734375, Val_Loss: 5972.703125\n",
      "Epoch 603, Train_Loss: 5615.00341796875, Val_Loss: 5972.36181640625\n",
      "Epoch 604, Train_Loss: 5614.619140625, Val_Loss: 5972.01904296875\n",
      "Epoch 605, Train_Loss: 5614.3134765625, Val_Loss: 5971.6845703125\n",
      "Epoch 606, Train_Loss: 5614.01953125, Val_Loss: 5971.3388671875\n",
      "Epoch 607, Train_Loss: 5613.69921875, Val_Loss: 5970.97021484375\n",
      "Epoch 608, Train_Loss: 5613.40673828125, Val_Loss: 5970.63818359375\n",
      "Epoch 609, Train_Loss: 5613.0869140625, Val_Loss: 5970.3017578125\n",
      "Epoch 610, Train_Loss: 5612.7578125, Val_Loss: 5969.947265625\n",
      "Epoch 611, Train_Loss: 5612.455078125, Val_Loss: 5969.625\n",
      "Epoch 612, Train_Loss: 5612.14306640625, Val_Loss: 5969.27783203125\n",
      "Epoch 613, Train_Loss: 5611.830078125, Val_Loss: 5968.92822265625\n",
      "Epoch 614, Train_Loss: 5611.5146484375, Val_Loss: 5968.572265625\n",
      "Epoch 615, Train_Loss: 5611.20166015625, Val_Loss: 5968.2353515625\n",
      "Epoch 616, Train_Loss: 5610.90771484375, Val_Loss: 5967.8740234375\n",
      "Epoch 617, Train_Loss: 5610.55615234375, Val_Loss: 5967.5380859375\n",
      "Epoch 618, Train_Loss: 5610.24072265625, Val_Loss: 5967.19580078125\n",
      "Epoch 619, Train_Loss: 5609.951171875, Val_Loss: 5966.84521484375\n",
      "Epoch 620, Train_Loss: 5609.63232421875, Val_Loss: 5966.498046875\n",
      "Epoch 621, Train_Loss: 5609.32275390625, Val_Loss: 5966.16259765625\n",
      "Epoch 622, Train_Loss: 5609.0302734375, Val_Loss: 5965.82958984375\n",
      "Epoch 623, Train_Loss: 5608.72705078125, Val_Loss: 5965.474609375\n",
      "Epoch 624, Train_Loss: 5608.408203125, Val_Loss: 5965.099609375\n",
      "Epoch 625, Train_Loss: 5608.111328125, Val_Loss: 5964.7626953125\n",
      "Epoch 626, Train_Loss: 5607.82275390625, Val_Loss: 5964.4013671875\n",
      "Epoch 627, Train_Loss: 5607.5107421875, Val_Loss: 5964.0546875\n",
      "Epoch 628, Train_Loss: 5607.20654296875, Val_Loss: 5963.7138671875\n",
      "Epoch 629, Train_Loss: 5606.818359375, Val_Loss: 5963.38330078125\n",
      "Epoch 630, Train_Loss: 5606.525390625, Val_Loss: 5963.04638671875\n",
      "Epoch 631, Train_Loss: 5606.2255859375, Val_Loss: 5962.697265625\n",
      "Epoch 632, Train_Loss: 5605.91748046875, Val_Loss: 5962.3486328125\n",
      "Epoch 633, Train_Loss: 5605.515625, Val_Loss: 5962.01123046875\n",
      "Epoch 634, Train_Loss: 5605.21142578125, Val_Loss: 5961.67578125\n",
      "Epoch 635, Train_Loss: 5604.91845703125, Val_Loss: 5961.3251953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 636, Train_Loss: 5604.6240234375, Val_Loss: 5960.982421875\n",
      "Epoch 637, Train_Loss: 5604.28564453125, Val_Loss: 5960.654296875\n",
      "Epoch 638, Train_Loss: 5603.9619140625, Val_Loss: 5960.298828125\n",
      "Epoch 639, Train_Loss: 5603.65966796875, Val_Loss: 5959.9599609375\n",
      "Epoch 640, Train_Loss: 5603.33251953125, Val_Loss: 5959.6181640625\n",
      "Epoch 641, Train_Loss: 5602.990234375, Val_Loss: 5959.2880859375\n",
      "Epoch 642, Train_Loss: 5602.638671875, Val_Loss: 5958.93310546875\n",
      "Epoch 643, Train_Loss: 5602.341796875, Val_Loss: 5958.60302734375\n",
      "Epoch 644, Train_Loss: 5602.02734375, Val_Loss: 5958.24462890625\n",
      "Epoch 645, Train_Loss: 5601.74853515625, Val_Loss: 5957.90625\n",
      "Epoch 646, Train_Loss: 5601.44189453125, Val_Loss: 5957.5654296875\n",
      "Epoch 647, Train_Loss: 5601.12646484375, Val_Loss: 5957.22509765625\n",
      "Epoch 648, Train_Loss: 5600.822265625, Val_Loss: 5956.8818359375\n",
      "Epoch 649, Train_Loss: 5600.5185546875, Val_Loss: 5956.544921875\n",
      "Epoch 650, Train_Loss: 5600.2099609375, Val_Loss: 5956.19580078125\n",
      "Epoch 651, Train_Loss: 5599.8974609375, Val_Loss: 5955.84423828125\n",
      "Epoch 652, Train_Loss: 5599.5810546875, Val_Loss: 5955.49267578125\n",
      "Epoch 653, Train_Loss: 5599.28759765625, Val_Loss: 5955.16650390625\n",
      "Epoch 654, Train_Loss: 5598.900390625, Val_Loss: 5954.8330078125\n",
      "Epoch 655, Train_Loss: 5598.587890625, Val_Loss: 5954.49560546875\n",
      "Epoch 656, Train_Loss: 5598.2880859375, Val_Loss: 5954.16015625\n",
      "Epoch 657, Train_Loss: 5598.01220703125, Val_Loss: 5953.80712890625\n",
      "Epoch 658, Train_Loss: 5597.716796875, Val_Loss: 5953.4677734375\n",
      "Epoch 659, Train_Loss: 5597.41943359375, Val_Loss: 5953.08837890625\n",
      "Epoch 660, Train_Loss: 5597.12109375, Val_Loss: 5952.7421875\n",
      "Epoch 661, Train_Loss: 5596.8212890625, Val_Loss: 5952.408203125\n",
      "Epoch 662, Train_Loss: 5596.5205078125, Val_Loss: 5952.07421875\n",
      "Epoch 663, Train_Loss: 5596.23388671875, Val_Loss: 5951.72119140625\n",
      "Epoch 664, Train_Loss: 5595.9287109375, Val_Loss: 5951.392578125\n",
      "Epoch 665, Train_Loss: 5595.59423828125, Val_Loss: 5951.06298828125\n",
      "Epoch 666, Train_Loss: 5595.25830078125, Val_Loss: 5950.728515625\n",
      "Epoch 667, Train_Loss: 5594.95361328125, Val_Loss: 5950.384765625\n",
      "Epoch 668, Train_Loss: 5594.64306640625, Val_Loss: 5950.03662109375\n",
      "Epoch 669, Train_Loss: 5594.3125, Val_Loss: 5949.6826171875\n",
      "Epoch 670, Train_Loss: 5594.01513671875, Val_Loss: 5949.341796875\n",
      "Epoch 671, Train_Loss: 5593.71533203125, Val_Loss: 5949.025390625\n",
      "Epoch 672, Train_Loss: 5593.4091796875, Val_Loss: 5948.68896484375\n",
      "Epoch 673, Train_Loss: 5593.103515625, Val_Loss: 5948.33056640625\n",
      "Epoch 674, Train_Loss: 5592.7734375, Val_Loss: 5947.99169921875\n",
      "Epoch 675, Train_Loss: 5592.4765625, Val_Loss: 5947.640625\n",
      "Epoch 676, Train_Loss: 5592.17529296875, Val_Loss: 5947.31298828125\n",
      "Epoch 677, Train_Loss: 5591.86474609375, Val_Loss: 5946.96337890625\n",
      "Epoch 678, Train_Loss: 5591.533203125, Val_Loss: 5946.62939453125\n",
      "Epoch 679, Train_Loss: 5591.21337890625, Val_Loss: 5946.302734375\n",
      "Epoch 680, Train_Loss: 5590.90673828125, Val_Loss: 5945.95703125\n",
      "Epoch 681, Train_Loss: 5590.619140625, Val_Loss: 5945.61376953125\n",
      "Epoch 682, Train_Loss: 5590.3359375, Val_Loss: 5945.2666015625\n",
      "Epoch 683, Train_Loss: 5590.021484375, Val_Loss: 5944.93798828125\n",
      "Epoch 684, Train_Loss: 5589.72705078125, Val_Loss: 5944.60498046875\n",
      "Epoch 685, Train_Loss: 5589.4248046875, Val_Loss: 5944.2607421875\n",
      "Epoch 686, Train_Loss: 5589.1259765625, Val_Loss: 5943.9169921875\n",
      "Epoch 687, Train_Loss: 5588.8330078125, Val_Loss: 5943.5703125\n",
      "Epoch 688, Train_Loss: 5588.51123046875, Val_Loss: 5943.21728515625\n",
      "Epoch 689, Train_Loss: 5588.19287109375, Val_Loss: 5942.88232421875\n",
      "Epoch 690, Train_Loss: 5587.89208984375, Val_Loss: 5942.53271484375\n",
      "Epoch 691, Train_Loss: 5587.51611328125, Val_Loss: 5942.2158203125\n",
      "Epoch 692, Train_Loss: 5587.2255859375, Val_Loss: 5941.88623046875\n",
      "Epoch 693, Train_Loss: 5586.91455078125, Val_Loss: 5941.5595703125\n",
      "Epoch 694, Train_Loss: 5586.6142578125, Val_Loss: 5941.17138671875\n",
      "Epoch 695, Train_Loss: 5586.31396484375, Val_Loss: 5940.83203125\n",
      "Epoch 696, Train_Loss: 5586.01513671875, Val_Loss: 5940.4912109375\n",
      "Epoch 697, Train_Loss: 5585.7099609375, Val_Loss: 5940.166015625\n",
      "Epoch 698, Train_Loss: 5585.4150390625, Val_Loss: 5939.8271484375\n",
      "Epoch 699, Train_Loss: 5585.1123046875, Val_Loss: 5939.48046875\n",
      "Epoch 700, Train_Loss: 5584.7998046875, Val_Loss: 5939.13232421875\n",
      "Epoch 701, Train_Loss: 5584.4951171875, Val_Loss: 5938.79296875\n",
      "Epoch 702, Train_Loss: 5584.16650390625, Val_Loss: 5938.451171875\n",
      "Epoch 703, Train_Loss: 5583.8173828125, Val_Loss: 5938.12451171875\n",
      "Epoch 704, Train_Loss: 5583.5283203125, Val_Loss: 5937.78759765625\n",
      "Epoch 705, Train_Loss: 5583.22900390625, Val_Loss: 5937.45703125\n",
      "Epoch 706, Train_Loss: 5582.91650390625, Val_Loss: 5937.1044921875\n",
      "Epoch 707, Train_Loss: 5582.61572265625, Val_Loss: 5936.76171875\n",
      "Epoch 708, Train_Loss: 5582.30712890625, Val_Loss: 5936.435546875\n",
      "Epoch 709, Train_Loss: 5582.0009765625, Val_Loss: 5936.10302734375\n",
      "Epoch 710, Train_Loss: 5581.6943359375, Val_Loss: 5935.7578125\n",
      "Epoch 711, Train_Loss: 5581.31982421875, Val_Loss: 5935.42138671875\n",
      "Epoch 712, Train_Loss: 5581.0263671875, Val_Loss: 5935.0673828125\n",
      "Epoch 713, Train_Loss: 5580.720703125, Val_Loss: 5934.7373046875\n",
      "Epoch 714, Train_Loss: 5580.40478515625, Val_Loss: 5934.408203125\n",
      "Epoch 715, Train_Loss: 5580.119140625, Val_Loss: 5934.0849609375\n",
      "Epoch 716, Train_Loss: 5579.75341796875, Val_Loss: 5933.7421875\n",
      "Epoch 717, Train_Loss: 5579.453125, Val_Loss: 5933.41162109375\n",
      "Epoch 718, Train_Loss: 5579.1640625, Val_Loss: 5933.068359375\n",
      "Epoch 719, Train_Loss: 5578.86669921875, Val_Loss: 5932.72021484375\n",
      "Epoch 720, Train_Loss: 5578.57275390625, Val_Loss: 5932.3818359375\n",
      "Epoch 721, Train_Loss: 5578.267578125, Val_Loss: 5932.0498046875\n",
      "Epoch 722, Train_Loss: 5577.93505859375, Val_Loss: 5931.7109375\n",
      "Epoch 723, Train_Loss: 5577.64501953125, Val_Loss: 5931.3740234375\n",
      "Epoch 724, Train_Loss: 5577.3505859375, Val_Loss: 5931.0400390625\n",
      "Epoch 725, Train_Loss: 5577.0556640625, Val_Loss: 5930.6884765625\n",
      "Epoch 726, Train_Loss: 5576.724609375, Val_Loss: 5930.357421875\n",
      "Epoch 727, Train_Loss: 5576.43603515625, Val_Loss: 5930.02978515625\n",
      "Epoch 728, Train_Loss: 5576.03662109375, Val_Loss: 5929.681640625\n",
      "Epoch 729, Train_Loss: 5575.7099609375, Val_Loss: 5929.32666015625\n",
      "Epoch 730, Train_Loss: 5575.40869140625, Val_Loss: 5928.99658203125\n",
      "Epoch 731, Train_Loss: 5575.11279296875, Val_Loss: 5928.64892578125\n",
      "Epoch 732, Train_Loss: 5574.82373046875, Val_Loss: 5928.3212890625\n",
      "Epoch 733, Train_Loss: 5574.54248046875, Val_Loss: 5927.9794921875\n",
      "Epoch 734, Train_Loss: 5574.240234375, Val_Loss: 5927.64306640625\n",
      "Epoch 735, Train_Loss: 5573.93603515625, Val_Loss: 5927.32177734375\n",
      "Epoch 736, Train_Loss: 5573.64599609375, Val_Loss: 5926.97314453125\n",
      "Epoch 737, Train_Loss: 5573.34619140625, Val_Loss: 5926.6328125\n",
      "Epoch 738, Train_Loss: 5573.05126953125, Val_Loss: 5926.2900390625\n",
      "Epoch 739, Train_Loss: 5572.74609375, Val_Loss: 5925.95556640625\n",
      "Epoch 740, Train_Loss: 5572.40869140625, Val_Loss: 5925.63623046875\n",
      "Epoch 741, Train_Loss: 5572.107421875, Val_Loss: 5925.2939453125\n",
      "Epoch 742, Train_Loss: 5571.80810546875, Val_Loss: 5924.970703125\n",
      "Epoch 743, Train_Loss: 5571.5166015625, Val_Loss: 5924.60302734375\n",
      "Epoch 744, Train_Loss: 5571.201171875, Val_Loss: 5924.2763671875\n",
      "Epoch 745, Train_Loss: 5570.90869140625, Val_Loss: 5923.958984375\n",
      "Epoch 746, Train_Loss: 5570.61865234375, Val_Loss: 5923.60693359375\n",
      "Epoch 747, Train_Loss: 5570.31884765625, Val_Loss: 5923.27197265625\n",
      "Epoch 748, Train_Loss: 5570.01025390625, Val_Loss: 5922.94140625\n",
      "Epoch 749, Train_Loss: 5569.68603515625, Val_Loss: 5922.60009765625\n",
      "Epoch 750, Train_Loss: 5569.373046875, Val_Loss: 5922.25732421875\n",
      "Epoch 751, Train_Loss: 5569.060546875, Val_Loss: 5921.92138671875\n",
      "Epoch 752, Train_Loss: 5568.77978515625, Val_Loss: 5921.58935546875\n",
      "Epoch 753, Train_Loss: 5568.40625, Val_Loss: 5921.2607421875\n",
      "Epoch 754, Train_Loss: 5568.0986328125, Val_Loss: 5920.93359375\n",
      "Epoch 755, Train_Loss: 5567.7890625, Val_Loss: 5920.58203125\n",
      "Epoch 756, Train_Loss: 5567.48583984375, Val_Loss: 5920.23291015625\n",
      "Epoch 757, Train_Loss: 5567.1845703125, Val_Loss: 5919.890625\n",
      "Epoch 758, Train_Loss: 5566.88916015625, Val_Loss: 5919.55419921875\n",
      "Epoch 759, Train_Loss: 5566.60302734375, Val_Loss: 5919.23193359375\n",
      "Epoch 760, Train_Loss: 5566.306640625, Val_Loss: 5918.88818359375\n",
      "Epoch 761, Train_Loss: 5566.00537109375, Val_Loss: 5918.5615234375\n",
      "Epoch 762, Train_Loss: 5565.70361328125, Val_Loss: 5918.20458984375\n",
      "Epoch 763, Train_Loss: 5565.40625, Val_Loss: 5917.8828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 764, Train_Loss: 5565.10546875, Val_Loss: 5917.51171875\n",
      "Epoch 765, Train_Loss: 5564.7685546875, Val_Loss: 5917.1845703125\n",
      "Epoch 766, Train_Loss: 5564.4736328125, Val_Loss: 5916.85791015625\n",
      "Epoch 767, Train_Loss: 5564.17724609375, Val_Loss: 5916.5244140625\n",
      "Epoch 768, Train_Loss: 5563.8662109375, Val_Loss: 5916.1884765625\n",
      "Epoch 769, Train_Loss: 5563.572265625, Val_Loss: 5915.8564453125\n",
      "Epoch 770, Train_Loss: 5563.27978515625, Val_Loss: 5915.51708984375\n",
      "Epoch 771, Train_Loss: 5562.98583984375, Val_Loss: 5915.1787109375\n",
      "Epoch 772, Train_Loss: 5562.673828125, Val_Loss: 5914.84619140625\n",
      "Epoch 773, Train_Loss: 5562.3037109375, Val_Loss: 5914.5244140625\n",
      "Epoch 774, Train_Loss: 5562.02197265625, Val_Loss: 5914.20166015625\n",
      "Epoch 775, Train_Loss: 5561.716796875, Val_Loss: 5913.8388671875\n",
      "Epoch 776, Train_Loss: 5561.4375, Val_Loss: 5913.5126953125\n",
      "Epoch 777, Train_Loss: 5561.11279296875, Val_Loss: 5913.173828125\n",
      "Epoch 778, Train_Loss: 5560.775390625, Val_Loss: 5912.841796875\n",
      "Epoch 779, Train_Loss: 5560.49462890625, Val_Loss: 5912.50537109375\n",
      "Epoch 780, Train_Loss: 5560.203125, Val_Loss: 5912.19287109375\n",
      "Epoch 781, Train_Loss: 5559.83056640625, Val_Loss: 5911.82861328125\n",
      "Epoch 782, Train_Loss: 5559.525390625, Val_Loss: 5911.49609375\n",
      "Epoch 783, Train_Loss: 5559.25146484375, Val_Loss: 5911.17822265625\n",
      "Epoch 784, Train_Loss: 5558.9404296875, Val_Loss: 5910.837890625\n",
      "Epoch 785, Train_Loss: 5558.6591796875, Val_Loss: 5910.50341796875\n",
      "Epoch 786, Train_Loss: 5558.36962890625, Val_Loss: 5910.16796875\n",
      "Epoch 787, Train_Loss: 5558.0458984375, Val_Loss: 5909.82177734375\n",
      "Epoch 788, Train_Loss: 5557.73681640625, Val_Loss: 5909.5078125\n",
      "Epoch 789, Train_Loss: 5557.4248046875, Val_Loss: 5909.1669921875\n",
      "Epoch 790, Train_Loss: 5557.03173828125, Val_Loss: 5908.80859375\n",
      "Epoch 791, Train_Loss: 5556.71826171875, Val_Loss: 5908.490234375\n",
      "Epoch 792, Train_Loss: 5556.435546875, Val_Loss: 5908.16259765625\n",
      "Epoch 793, Train_Loss: 5556.134765625, Val_Loss: 5907.81298828125\n",
      "Epoch 794, Train_Loss: 5555.86083984375, Val_Loss: 5907.49462890625\n",
      "Epoch 795, Train_Loss: 5555.56884765625, Val_Loss: 5907.16162109375\n",
      "Epoch 796, Train_Loss: 5555.2626953125, Val_Loss: 5906.8193359375\n",
      "Epoch 797, Train_Loss: 5554.94970703125, Val_Loss: 5906.48828125\n",
      "Epoch 798, Train_Loss: 5554.65234375, Val_Loss: 5906.15478515625\n",
      "Epoch 799, Train_Loss: 5554.34521484375, Val_Loss: 5905.78271484375\n",
      "Epoch 800, Train_Loss: 5554.05908203125, Val_Loss: 5905.458984375\n",
      "Epoch 801, Train_Loss: 5553.77099609375, Val_Loss: 5905.12548828125\n",
      "Epoch 802, Train_Loss: 5553.46923828125, Val_Loss: 5904.791015625\n",
      "Epoch 803, Train_Loss: 5553.140625, Val_Loss: 5904.47216796875\n",
      "Epoch 804, Train_Loss: 5552.84912109375, Val_Loss: 5904.1240234375\n",
      "Epoch 805, Train_Loss: 5552.55029296875, Val_Loss: 5903.81298828125\n",
      "Epoch 806, Train_Loss: 5552.24072265625, Val_Loss: 5903.46240234375\n",
      "Epoch 807, Train_Loss: 5551.9609375, Val_Loss: 5903.1162109375\n",
      "Epoch 808, Train_Loss: 5551.6484375, Val_Loss: 5902.787109375\n",
      "Epoch 809, Train_Loss: 5551.3359375, Val_Loss: 5902.45263671875\n",
      "Epoch 810, Train_Loss: 5551.03271484375, Val_Loss: 5902.12744140625\n",
      "Epoch 811, Train_Loss: 5550.73388671875, Val_Loss: 5901.79541015625\n",
      "Epoch 812, Train_Loss: 5550.43701171875, Val_Loss: 5901.44482421875\n",
      "Epoch 813, Train_Loss: 5550.14013671875, Val_Loss: 5901.13134765625\n",
      "Epoch 814, Train_Loss: 5549.85009765625, Val_Loss: 5900.79833984375\n",
      "Epoch 815, Train_Loss: 5549.4833984375, Val_Loss: 5900.47509765625\n",
      "Epoch 816, Train_Loss: 5549.201171875, Val_Loss: 5900.13623046875\n",
      "Epoch 817, Train_Loss: 5548.88427734375, Val_Loss: 5899.78857421875\n",
      "Epoch 818, Train_Loss: 5548.5986328125, Val_Loss: 5899.44189453125\n",
      "Epoch 819, Train_Loss: 5548.3017578125, Val_Loss: 5899.12255859375\n",
      "Epoch 820, Train_Loss: 5548.017578125, Val_Loss: 5898.79296875\n",
      "Epoch 821, Train_Loss: 5547.72265625, Val_Loss: 5898.4609375\n",
      "Epoch 822, Train_Loss: 5547.435546875, Val_Loss: 5898.14501953125\n",
      "Epoch 823, Train_Loss: 5547.13427734375, Val_Loss: 5897.8017578125\n",
      "Epoch 824, Train_Loss: 5546.81005859375, Val_Loss: 5897.4599609375\n",
      "Epoch 825, Train_Loss: 5546.4892578125, Val_Loss: 5897.12060546875\n",
      "Epoch 826, Train_Loss: 5546.20751953125, Val_Loss: 5896.79736328125\n",
      "Epoch 827, Train_Loss: 5545.8251953125, Val_Loss: 5896.47216796875\n",
      "Epoch 828, Train_Loss: 5545.52783203125, Val_Loss: 5896.1328125\n",
      "Epoch 829, Train_Loss: 5545.22705078125, Val_Loss: 5895.8017578125\n",
      "Epoch 830, Train_Loss: 5544.9423828125, Val_Loss: 5895.48046875\n",
      "Epoch 831, Train_Loss: 5544.64208984375, Val_Loss: 5895.14111328125\n",
      "Epoch 832, Train_Loss: 5544.349609375, Val_Loss: 5894.79150390625\n",
      "Epoch 833, Train_Loss: 5544.0537109375, Val_Loss: 5894.47119140625\n",
      "Epoch 834, Train_Loss: 5543.7294921875, Val_Loss: 5894.1083984375\n",
      "Epoch 835, Train_Loss: 5543.4326171875, Val_Loss: 5893.775390625\n",
      "Epoch 836, Train_Loss: 5543.14453125, Val_Loss: 5893.45263671875\n",
      "Epoch 837, Train_Loss: 5542.85693359375, Val_Loss: 5893.11376953125\n",
      "Epoch 838, Train_Loss: 5542.544921875, Val_Loss: 5892.77783203125\n",
      "Epoch 839, Train_Loss: 5542.25830078125, Val_Loss: 5892.447265625\n",
      "Epoch 840, Train_Loss: 5541.95361328125, Val_Loss: 5892.12744140625\n",
      "Epoch 841, Train_Loss: 5541.66162109375, Val_Loss: 5891.79345703125\n",
      "Epoch 842, Train_Loss: 5541.37353515625, Val_Loss: 5891.44921875\n",
      "Epoch 843, Train_Loss: 5541.05859375, Val_Loss: 5891.10791015625\n",
      "Epoch 844, Train_Loss: 5540.76220703125, Val_Loss: 5890.79150390625\n",
      "Epoch 845, Train_Loss: 5540.4716796875, Val_Loss: 5890.45654296875\n",
      "Epoch 846, Train_Loss: 5540.166015625, Val_Loss: 5890.1328125\n",
      "Epoch 847, Train_Loss: 5539.88671875, Val_Loss: 5889.80078125\n",
      "Epoch 848, Train_Loss: 5539.5810546875, Val_Loss: 5889.4755859375\n",
      "Epoch 849, Train_Loss: 5539.291015625, Val_Loss: 5889.12060546875\n",
      "Epoch 850, Train_Loss: 5538.98291015625, Val_Loss: 5888.78955078125\n",
      "Epoch 851, Train_Loss: 5538.6943359375, Val_Loss: 5888.46923828125\n",
      "Epoch 852, Train_Loss: 5538.185546875, Val_Loss: 5888.13720703125\n",
      "Epoch 853, Train_Loss: 5537.90283203125, Val_Loss: 5887.82080078125\n",
      "Epoch 854, Train_Loss: 5537.61572265625, Val_Loss: 5887.4951171875\n",
      "Epoch 855, Train_Loss: 5537.3349609375, Val_Loss: 5887.13623046875\n",
      "Epoch 856, Train_Loss: 5537.0419921875, Val_Loss: 5886.8115234375\n",
      "Epoch 857, Train_Loss: 5536.744140625, Val_Loss: 5886.47607421875\n",
      "Epoch 858, Train_Loss: 5536.44921875, Val_Loss: 5886.1513671875\n",
      "Epoch 859, Train_Loss: 5536.154296875, Val_Loss: 5885.82177734375\n",
      "Epoch 860, Train_Loss: 5535.86572265625, Val_Loss: 5885.4833984375\n",
      "Epoch 861, Train_Loss: 5535.583984375, Val_Loss: 5885.1455078125\n",
      "Epoch 862, Train_Loss: 5535.26220703125, Val_Loss: 5884.7958984375\n",
      "Epoch 863, Train_Loss: 5534.95361328125, Val_Loss: 5884.4755859375\n",
      "Epoch 864, Train_Loss: 5534.65966796875, Val_Loss: 5884.16015625\n",
      "Epoch 865, Train_Loss: 5534.328125, Val_Loss: 5883.82958984375\n",
      "Epoch 866, Train_Loss: 5534.037109375, Val_Loss: 5883.50634765625\n",
      "Epoch 867, Train_Loss: 5533.73974609375, Val_Loss: 5883.1767578125\n",
      "Epoch 868, Train_Loss: 5533.4384765625, Val_Loss: 5882.837890625\n",
      "Epoch 869, Train_Loss: 5533.09912109375, Val_Loss: 5882.474609375\n",
      "Epoch 870, Train_Loss: 5532.81298828125, Val_Loss: 5882.140625\n",
      "Epoch 871, Train_Loss: 5532.52392578125, Val_Loss: 5881.818359375\n",
      "Epoch 872, Train_Loss: 5532.2138671875, Val_Loss: 5881.484375\n",
      "Epoch 873, Train_Loss: 5531.92431640625, Val_Loss: 5881.16845703125\n",
      "Epoch 874, Train_Loss: 5531.64306640625, Val_Loss: 5880.82373046875\n",
      "Epoch 875, Train_Loss: 5531.35107421875, Val_Loss: 5880.4921875\n",
      "Epoch 876, Train_Loss: 5531.04638671875, Val_Loss: 5880.1787109375\n",
      "Epoch 877, Train_Loss: 5530.7216796875, Val_Loss: 5879.83154296875\n",
      "Epoch 878, Train_Loss: 5530.421875, Val_Loss: 5879.50537109375\n",
      "Epoch 879, Train_Loss: 5530.12548828125, Val_Loss: 5879.169921875\n",
      "Epoch 880, Train_Loss: 5529.837890625, Val_Loss: 5878.8291015625\n",
      "Epoch 881, Train_Loss: 5529.54052734375, Val_Loss: 5878.51416015625\n",
      "Epoch 882, Train_Loss: 5529.2392578125, Val_Loss: 5878.19482421875\n",
      "Epoch 883, Train_Loss: 5528.927734375, Val_Loss: 5877.8681640625\n",
      "Epoch 884, Train_Loss: 5528.626953125, Val_Loss: 5877.5234375\n",
      "Epoch 885, Train_Loss: 5528.3515625, Val_Loss: 5877.20068359375\n",
      "Epoch 886, Train_Loss: 5528.0498046875, Val_Loss: 5876.8818359375\n",
      "Epoch 887, Train_Loss: 5527.73828125, Val_Loss: 5876.5107421875\n",
      "Epoch 888, Train_Loss: 5527.4453125, Val_Loss: 5876.18798828125\n",
      "Epoch 889, Train_Loss: 5527.15673828125, Val_Loss: 5875.857421875\n",
      "Epoch 890, Train_Loss: 5526.75927734375, Val_Loss: 5875.54443359375\n",
      "Epoch 891, Train_Loss: 5526.46484375, Val_Loss: 5875.20947265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 892, Train_Loss: 5526.16748046875, Val_Loss: 5874.8955078125\n",
      "Epoch 893, Train_Loss: 5525.87939453125, Val_Loss: 5874.55859375\n",
      "Epoch 894, Train_Loss: 5525.60498046875, Val_Loss: 5874.2216796875\n",
      "Epoch 895, Train_Loss: 5525.32470703125, Val_Loss: 5873.890625\n",
      "Epoch 896, Train_Loss: 5525.0390625, Val_Loss: 5873.552734375\n",
      "Epoch 897, Train_Loss: 5524.76318359375, Val_Loss: 5873.22314453125\n",
      "Epoch 898, Train_Loss: 5524.478515625, Val_Loss: 5872.88330078125\n",
      "Epoch 899, Train_Loss: 5524.19140625, Val_Loss: 5872.53857421875\n",
      "Epoch 900, Train_Loss: 5523.8603515625, Val_Loss: 5872.22900390625\n",
      "Epoch 901, Train_Loss: 5523.56201171875, Val_Loss: 5871.9033203125\n",
      "Epoch 902, Train_Loss: 5523.22265625, Val_Loss: 5871.58056640625\n",
      "Epoch 903, Train_Loss: 5522.92822265625, Val_Loss: 5871.2412109375\n",
      "Epoch 904, Train_Loss: 5522.62353515625, Val_Loss: 5870.890625\n",
      "Epoch 905, Train_Loss: 5522.32373046875, Val_Loss: 5870.53955078125\n",
      "Epoch 906, Train_Loss: 5522.03076171875, Val_Loss: 5870.21240234375\n",
      "Epoch 907, Train_Loss: 5521.74169921875, Val_Loss: 5869.8994140625\n",
      "Epoch 908, Train_Loss: 5521.44921875, Val_Loss: 5869.56640625\n",
      "Epoch 909, Train_Loss: 5521.1435546875, Val_Loss: 5869.2431640625\n",
      "Epoch 910, Train_Loss: 5520.8583984375, Val_Loss: 5868.908203125\n",
      "Epoch 911, Train_Loss: 5520.56689453125, Val_Loss: 5868.57958984375\n",
      "Epoch 912, Train_Loss: 5520.27734375, Val_Loss: 5868.240234375\n",
      "Epoch 913, Train_Loss: 5519.9609375, Val_Loss: 5867.9169921875\n",
      "Epoch 914, Train_Loss: 5519.6494140625, Val_Loss: 5867.6005859375\n",
      "Epoch 915, Train_Loss: 5519.31005859375, Val_Loss: 5867.2734375\n",
      "Epoch 916, Train_Loss: 5519.00390625, Val_Loss: 5866.943359375\n",
      "Epoch 917, Train_Loss: 5518.71728515625, Val_Loss: 5866.62353515625\n",
      "Epoch 918, Train_Loss: 5518.416015625, Val_Loss: 5866.2802734375\n",
      "Epoch 919, Train_Loss: 5518.134765625, Val_Loss: 5865.9580078125\n",
      "Epoch 920, Train_Loss: 5517.837890625, Val_Loss: 5865.62548828125\n",
      "Epoch 921, Train_Loss: 5517.53857421875, Val_Loss: 5865.30419921875\n",
      "Epoch 922, Train_Loss: 5517.1962890625, Val_Loss: 5864.978515625\n",
      "Epoch 923, Train_Loss: 5516.91259765625, Val_Loss: 5864.64697265625\n",
      "Epoch 924, Train_Loss: 5516.615234375, Val_Loss: 5864.29736328125\n",
      "Epoch 925, Train_Loss: 5516.33056640625, Val_Loss: 5863.98193359375\n",
      "Epoch 926, Train_Loss: 5516.05810546875, Val_Loss: 5863.66015625\n",
      "Epoch 927, Train_Loss: 5515.677734375, Val_Loss: 5863.337890625\n",
      "Epoch 928, Train_Loss: 5515.3623046875, Val_Loss: 5863.01416015625\n",
      "Epoch 929, Train_Loss: 5515.06982421875, Val_Loss: 5862.69140625\n",
      "Epoch 930, Train_Loss: 5514.7724609375, Val_Loss: 5862.34130859375\n",
      "Epoch 931, Train_Loss: 5514.48681640625, Val_Loss: 5862.01220703125\n",
      "Epoch 932, Train_Loss: 5514.2060546875, Val_Loss: 5861.693359375\n",
      "Epoch 933, Train_Loss: 5513.9111328125, Val_Loss: 5861.3564453125\n",
      "Epoch 934, Train_Loss: 5513.62841796875, Val_Loss: 5861.048828125\n",
      "Epoch 935, Train_Loss: 5513.32958984375, Val_Loss: 5860.708984375\n",
      "Epoch 936, Train_Loss: 5513.05126953125, Val_Loss: 5860.37646484375\n",
      "Epoch 937, Train_Loss: 5512.724609375, Val_Loss: 5860.04541015625\n",
      "Epoch 938, Train_Loss: 5512.42041015625, Val_Loss: 5859.72900390625\n",
      "Epoch 939, Train_Loss: 5512.12548828125, Val_Loss: 5859.41162109375\n",
      "Epoch 940, Train_Loss: 5511.8203125, Val_Loss: 5859.03662109375\n",
      "Epoch 941, Train_Loss: 5511.53515625, Val_Loss: 5858.71337890625\n",
      "Epoch 942, Train_Loss: 5511.22265625, Val_Loss: 5858.38916015625\n",
      "Epoch 943, Train_Loss: 5510.9404296875, Val_Loss: 5858.05029296875\n",
      "Epoch 944, Train_Loss: 5510.65478515625, Val_Loss: 5857.72705078125\n",
      "Epoch 945, Train_Loss: 5510.35498046875, Val_Loss: 5857.39697265625\n",
      "Epoch 946, Train_Loss: 5510.0771484375, Val_Loss: 5857.08740234375\n",
      "Epoch 947, Train_Loss: 5509.77392578125, Val_Loss: 5856.76708984375\n",
      "Epoch 948, Train_Loss: 5509.486328125, Val_Loss: 5856.4248046875\n",
      "Epoch 949, Train_Loss: 5509.208984375, Val_Loss: 5856.08740234375\n",
      "Epoch 950, Train_Loss: 5508.919921875, Val_Loss: 5855.755859375\n",
      "Epoch 951, Train_Loss: 5508.62744140625, Val_Loss: 5855.44482421875\n",
      "Epoch 952, Train_Loss: 5508.23291015625, Val_Loss: 5855.115234375\n",
      "Epoch 953, Train_Loss: 5507.9296875, Val_Loss: 5854.8017578125\n",
      "Epoch 954, Train_Loss: 5507.6513671875, Val_Loss: 5854.47802734375\n",
      "Epoch 955, Train_Loss: 5507.361328125, Val_Loss: 5854.1279296875\n",
      "Epoch 956, Train_Loss: 5507.0791015625, Val_Loss: 5853.806640625\n",
      "Epoch 957, Train_Loss: 5506.7763671875, Val_Loss: 5853.46826171875\n",
      "Epoch 958, Train_Loss: 5506.49609375, Val_Loss: 5853.15380859375\n",
      "Epoch 959, Train_Loss: 5506.20556640625, Val_Loss: 5852.8251953125\n",
      "Epoch 960, Train_Loss: 5505.900390625, Val_Loss: 5852.494140625\n",
      "Epoch 961, Train_Loss: 5505.6123046875, Val_Loss: 5852.1767578125\n",
      "Epoch 962, Train_Loss: 5505.337890625, Val_Loss: 5851.82421875\n",
      "Epoch 963, Train_Loss: 5505.04833984375, Val_Loss: 5851.50732421875\n",
      "Epoch 964, Train_Loss: 5504.7734375, Val_Loss: 5851.2041015625\n",
      "Epoch 965, Train_Loss: 5504.42626953125, Val_Loss: 5850.873046875\n",
      "Epoch 966, Train_Loss: 5504.12939453125, Val_Loss: 5850.5322265625\n",
      "Epoch 967, Train_Loss: 5503.84912109375, Val_Loss: 5850.19287109375\n",
      "Epoch 968, Train_Loss: 5503.55078125, Val_Loss: 5849.859375\n",
      "Epoch 969, Train_Loss: 5503.25830078125, Val_Loss: 5849.53125\n",
      "Epoch 970, Train_Loss: 5502.9619140625, Val_Loss: 5849.20751953125\n",
      "Epoch 971, Train_Loss: 5502.67333984375, Val_Loss: 5848.88232421875\n",
      "Epoch 972, Train_Loss: 5502.390625, Val_Loss: 5848.5615234375\n",
      "Epoch 973, Train_Loss: 5502.09912109375, Val_Loss: 5848.24267578125\n",
      "Epoch 974, Train_Loss: 5501.80859375, Val_Loss: 5847.90087890625\n",
      "Epoch 975, Train_Loss: 5501.48193359375, Val_Loss: 5847.53955078125\n",
      "Epoch 976, Train_Loss: 5501.17919921875, Val_Loss: 5847.22607421875\n",
      "Epoch 977, Train_Loss: 5500.84375, Val_Loss: 5846.8837890625\n",
      "Epoch 978, Train_Loss: 5500.56982421875, Val_Loss: 5846.57421875\n",
      "Epoch 979, Train_Loss: 5500.2734375, Val_Loss: 5846.26025390625\n",
      "Epoch 980, Train_Loss: 5499.96142578125, Val_Loss: 5845.90771484375\n",
      "Epoch 981, Train_Loss: 5499.689453125, Val_Loss: 5845.58984375\n",
      "Epoch 982, Train_Loss: 5499.4130859375, Val_Loss: 5845.28125\n",
      "Epoch 983, Train_Loss: 5499.13134765625, Val_Loss: 5844.95703125\n",
      "Epoch 984, Train_Loss: 5498.8310546875, Val_Loss: 5844.6142578125\n",
      "Epoch 985, Train_Loss: 5498.54052734375, Val_Loss: 5844.29541015625\n",
      "Epoch 986, Train_Loss: 5498.2548828125, Val_Loss: 5843.97607421875\n",
      "Epoch 987, Train_Loss: 5497.98388671875, Val_Loss: 5843.6337890625\n",
      "Epoch 988, Train_Loss: 5497.70654296875, Val_Loss: 5843.302734375\n",
      "Epoch 989, Train_Loss: 5497.4033203125, Val_Loss: 5842.99609375\n",
      "Epoch 990, Train_Loss: 5497.02392578125, Val_Loss: 5842.66943359375\n",
      "Epoch 991, Train_Loss: 5496.73388671875, Val_Loss: 5842.337890625\n",
      "Epoch 992, Train_Loss: 5496.45703125, Val_Loss: 5842.02978515625\n",
      "Epoch 993, Train_Loss: 5495.94677734375, Val_Loss: 5841.66943359375\n",
      "Epoch 994, Train_Loss: 5495.6533203125, Val_Loss: 5841.3388671875\n",
      "Epoch 995, Train_Loss: 5495.35595703125, Val_Loss: 5841.03271484375\n",
      "Epoch 996, Train_Loss: 5495.07861328125, Val_Loss: 5840.7080078125\n",
      "Epoch 997, Train_Loss: 5494.79248046875, Val_Loss: 5840.37744140625\n",
      "Epoch 998, Train_Loss: 5494.48583984375, Val_Loss: 5840.0400390625\n",
      "Epoch 999, Train_Loss: 5494.20263671875, Val_Loss: 5839.71728515625\n",
      "Epoch 1000, Train_Loss: 5493.9130859375, Val_Loss: 5839.392578125\n",
      "Epoch 1001, Train_Loss: 5493.61328125, Val_Loss: 5839.0625\n",
      "Epoch 1002, Train_Loss: 5493.27783203125, Val_Loss: 5838.724609375\n",
      "Epoch 1003, Train_Loss: 5492.98583984375, Val_Loss: 5838.4130859375\n",
      "Epoch 1004, Train_Loss: 5492.6826171875, Val_Loss: 5838.08349609375\n",
      "Epoch 1005, Train_Loss: 5492.37060546875, Val_Loss: 5837.73828125\n",
      "Epoch 1006, Train_Loss: 5492.0859375, Val_Loss: 5837.4287109375\n",
      "Epoch 1007, Train_Loss: 5491.80224609375, Val_Loss: 5837.10595703125\n",
      "Epoch 1008, Train_Loss: 5491.51708984375, Val_Loss: 5836.7861328125\n",
      "Epoch 1009, Train_Loss: 5491.2294921875, Val_Loss: 5836.46142578125\n",
      "Epoch 1010, Train_Loss: 5490.92822265625, Val_Loss: 5836.0966796875\n",
      "Epoch 1011, Train_Loss: 5490.64404296875, Val_Loss: 5835.77783203125\n",
      "Epoch 1012, Train_Loss: 5490.36669921875, Val_Loss: 5835.443359375\n",
      "Epoch 1013, Train_Loss: 5490.03564453125, Val_Loss: 5835.1279296875\n",
      "Epoch 1014, Train_Loss: 5489.732421875, Val_Loss: 5834.796875\n",
      "Epoch 1015, Train_Loss: 5489.404296875, Val_Loss: 5834.47607421875\n",
      "Epoch 1016, Train_Loss: 5489.13623046875, Val_Loss: 5834.15283203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1017, Train_Loss: 5488.84912109375, Val_Loss: 5833.8349609375\n",
      "Epoch 1018, Train_Loss: 5488.5361328125, Val_Loss: 5833.48779296875\n",
      "Epoch 1019, Train_Loss: 5488.23486328125, Val_Loss: 5833.15966796875\n",
      "Epoch 1020, Train_Loss: 5487.95751953125, Val_Loss: 5832.85009765625\n",
      "Epoch 1021, Train_Loss: 5487.6748046875, Val_Loss: 5832.51220703125\n",
      "Epoch 1022, Train_Loss: 5487.39208984375, Val_Loss: 5832.205078125\n",
      "Epoch 1023, Train_Loss: 5487.09619140625, Val_Loss: 5831.888671875\n",
      "Epoch 1024, Train_Loss: 5486.80810546875, Val_Loss: 5831.546875\n",
      "Epoch 1025, Train_Loss: 5486.5322265625, Val_Loss: 5831.23193359375\n",
      "Epoch 1026, Train_Loss: 5486.25537109375, Val_Loss: 5830.90478515625\n",
      "Epoch 1027, Train_Loss: 5485.86328125, Val_Loss: 5830.5908203125\n",
      "Epoch 1028, Train_Loss: 5485.572265625, Val_Loss: 5830.2646484375\n",
      "Epoch 1029, Train_Loss: 5485.28759765625, Val_Loss: 5829.9423828125\n",
      "Epoch 1030, Train_Loss: 5485.01318359375, Val_Loss: 5829.6259765625\n",
      "Epoch 1031, Train_Loss: 5484.73193359375, Val_Loss: 5829.27587890625\n",
      "Epoch 1032, Train_Loss: 5484.44091796875, Val_Loss: 5828.95703125\n",
      "Epoch 1033, Train_Loss: 5484.162109375, Val_Loss: 5828.64599609375\n",
      "Epoch 1034, Train_Loss: 5483.87158203125, Val_Loss: 5828.3154296875\n",
      "Epoch 1035, Train_Loss: 5483.58837890625, Val_Loss: 5827.9970703125\n",
      "Epoch 1036, Train_Loss: 5483.31201171875, Val_Loss: 5827.67822265625\n",
      "Epoch 1037, Train_Loss: 5483.01513671875, Val_Loss: 5827.326171875\n",
      "Epoch 1038, Train_Loss: 5482.72314453125, Val_Loss: 5827.00537109375\n",
      "Epoch 1039, Train_Loss: 5482.4267578125, Val_Loss: 5826.69140625\n",
      "Epoch 1040, Train_Loss: 5482.115234375, Val_Loss: 5826.36669921875\n",
      "Epoch 1041, Train_Loss: 5481.83837890625, Val_Loss: 5826.048828125\n",
      "Epoch 1042, Train_Loss: 5481.541015625, Val_Loss: 5825.72509765625\n",
      "Epoch 1043, Train_Loss: 5481.25048828125, Val_Loss: 5825.375\n",
      "Epoch 1044, Train_Loss: 5480.96630859375, Val_Loss: 5825.07275390625\n",
      "Epoch 1045, Train_Loss: 5480.67333984375, Val_Loss: 5824.7490234375\n",
      "Epoch 1046, Train_Loss: 5480.3857421875, Val_Loss: 5824.392578125\n",
      "Epoch 1047, Train_Loss: 5480.11767578125, Val_Loss: 5824.06982421875\n",
      "Epoch 1048, Train_Loss: 5479.822265625, Val_Loss: 5823.7607421875\n",
      "Epoch 1049, Train_Loss: 5479.544921875, Val_Loss: 5823.4208984375\n",
      "Epoch 1050, Train_Loss: 5479.259765625, Val_Loss: 5823.0908203125\n",
      "Epoch 1051, Train_Loss: 5478.9287109375, Val_Loss: 5822.77197265625\n",
      "Epoch 1052, Train_Loss: 5478.6337890625, Val_Loss: 5822.4560546875\n",
      "Epoch 1053, Train_Loss: 5478.27099609375, Val_Loss: 5822.14306640625\n",
      "Epoch 1054, Train_Loss: 5477.9990234375, Val_Loss: 5821.8388671875\n",
      "Epoch 1055, Train_Loss: 5477.6591796875, Val_Loss: 5821.498046875\n",
      "Epoch 1056, Train_Loss: 5477.3515625, Val_Loss: 5821.16943359375\n",
      "Epoch 1057, Train_Loss: 5477.06787109375, Val_Loss: 5820.849609375\n",
      "Epoch 1058, Train_Loss: 5476.78662109375, Val_Loss: 5820.5341796875\n",
      "Epoch 1059, Train_Loss: 5476.49365234375, Val_Loss: 5820.2109375\n",
      "Epoch 1060, Train_Loss: 5476.220703125, Val_Loss: 5819.9013671875\n",
      "Epoch 1061, Train_Loss: 5475.9267578125, Val_Loss: 5819.5703125\n",
      "Epoch 1062, Train_Loss: 5475.6376953125, Val_Loss: 5819.23681640625\n",
      "Epoch 1063, Train_Loss: 5475.31591796875, Val_Loss: 5818.91162109375\n",
      "Epoch 1064, Train_Loss: 5475.04052734375, Val_Loss: 5818.58544921875\n",
      "Epoch 1065, Train_Loss: 5474.6953125, Val_Loss: 5818.2685546875\n",
      "Epoch 1066, Train_Loss: 5474.40185546875, Val_Loss: 5817.96923828125\n",
      "Epoch 1067, Train_Loss: 5474.13720703125, Val_Loss: 5817.640625\n",
      "Epoch 1068, Train_Loss: 5473.845703125, Val_Loss: 5817.3037109375\n",
      "Epoch 1069, Train_Loss: 5473.56103515625, Val_Loss: 5816.97216796875\n",
      "Epoch 1070, Train_Loss: 5473.2802734375, Val_Loss: 5816.65478515625\n",
      "Epoch 1071, Train_Loss: 5473.0009765625, Val_Loss: 5816.33056640625\n",
      "Epoch 1072, Train_Loss: 5472.703125, Val_Loss: 5816.00439453125\n",
      "Epoch 1073, Train_Loss: 5472.412109375, Val_Loss: 5815.69140625\n",
      "Epoch 1074, Train_Loss: 5472.13671875, Val_Loss: 5815.3740234375\n",
      "Epoch 1075, Train_Loss: 5471.85302734375, Val_Loss: 5815.029296875\n",
      "Epoch 1076, Train_Loss: 5471.56884765625, Val_Loss: 5814.70703125\n",
      "Epoch 1077, Train_Loss: 5471.29150390625, Val_Loss: 5814.38232421875\n",
      "Epoch 1078, Train_Loss: 5470.9619140625, Val_Loss: 5814.06982421875\n",
      "Epoch 1079, Train_Loss: 5470.677734375, Val_Loss: 5813.75390625\n",
      "Epoch 1080, Train_Loss: 5470.37646484375, Val_Loss: 5813.43896484375\n",
      "Epoch 1081, Train_Loss: 5470.0927734375, Val_Loss: 5813.07421875\n",
      "Epoch 1082, Train_Loss: 5469.81298828125, Val_Loss: 5812.7470703125\n",
      "Epoch 1083, Train_Loss: 5469.53759765625, Val_Loss: 5812.4296875\n",
      "Epoch 1084, Train_Loss: 5469.25, Val_Loss: 5812.1162109375\n",
      "Epoch 1085, Train_Loss: 5468.98193359375, Val_Loss: 5811.78564453125\n",
      "Epoch 1086, Train_Loss: 5468.70654296875, Val_Loss: 5811.45947265625\n",
      "Epoch 1087, Train_Loss: 5468.427734375, Val_Loss: 5811.14501953125\n",
      "Epoch 1088, Train_Loss: 5468.1376953125, Val_Loss: 5810.82421875\n",
      "Epoch 1089, Train_Loss: 5467.82373046875, Val_Loss: 5810.5078125\n",
      "Epoch 1090, Train_Loss: 5467.4443359375, Val_Loss: 5810.1826171875\n",
      "Epoch 1091, Train_Loss: 5467.15380859375, Val_Loss: 5809.86767578125\n",
      "Epoch 1092, Train_Loss: 5466.8984375, Val_Loss: 5809.5498046875\n",
      "Epoch 1093, Train_Loss: 5466.61376953125, Val_Loss: 5809.21728515625\n",
      "Epoch 1094, Train_Loss: 5466.314453125, Val_Loss: 5808.89990234375\n",
      "Epoch 1095, Train_Loss: 5466.03515625, Val_Loss: 5808.57958984375\n",
      "Epoch 1096, Train_Loss: 5465.7392578125, Val_Loss: 5808.26611328125\n",
      "Epoch 1097, Train_Loss: 5465.453125, Val_Loss: 5807.9326171875\n",
      "Epoch 1098, Train_Loss: 5465.18701171875, Val_Loss: 5807.62353515625\n",
      "Epoch 1099, Train_Loss: 5464.87890625, Val_Loss: 5807.3125\n",
      "Epoch 1100, Train_Loss: 5464.58056640625, Val_Loss: 5806.97021484375\n",
      "Epoch 1101, Train_Loss: 5464.31396484375, Val_Loss: 5806.64794921875\n",
      "Epoch 1102, Train_Loss: 5464.0185546875, Val_Loss: 5806.3173828125\n",
      "Epoch 1103, Train_Loss: 5463.68505859375, Val_Loss: 5806.0107421875\n",
      "Epoch 1104, Train_Loss: 5463.3857421875, Val_Loss: 5805.6943359375\n",
      "Epoch 1105, Train_Loss: 5463.10400390625, Val_Loss: 5805.373046875\n",
      "Epoch 1106, Train_Loss: 5462.79443359375, Val_Loss: 5805.03759765625\n",
      "Epoch 1107, Train_Loss: 5462.5068359375, Val_Loss: 5804.72216796875\n",
      "Epoch 1108, Train_Loss: 5462.22216796875, Val_Loss: 5804.38427734375\n",
      "Epoch 1109, Train_Loss: 5461.93701171875, Val_Loss: 5804.06298828125\n",
      "Epoch 1110, Train_Loss: 5461.67578125, Val_Loss: 5803.76025390625\n",
      "Epoch 1111, Train_Loss: 5461.39794921875, Val_Loss: 5803.43359375\n",
      "Epoch 1112, Train_Loss: 5461.10888671875, Val_Loss: 5803.10400390625\n",
      "Epoch 1113, Train_Loss: 5460.8349609375, Val_Loss: 5802.79736328125\n",
      "Epoch 1114, Train_Loss: 5460.5576171875, Val_Loss: 5802.47119140625\n",
      "Epoch 1115, Train_Loss: 5460.2333984375, Val_Loss: 5802.14404296875\n",
      "Epoch 1116, Train_Loss: 5459.95166015625, Val_Loss: 5801.826171875\n",
      "Epoch 1117, Train_Loss: 5459.65771484375, Val_Loss: 5801.47216796875\n",
      "Epoch 1118, Train_Loss: 5459.35107421875, Val_Loss: 5801.15771484375\n",
      "Epoch 1119, Train_Loss: 5459.06298828125, Val_Loss: 5800.8154296875\n",
      "Epoch 1120, Train_Loss: 5458.79052734375, Val_Loss: 5800.50439453125\n",
      "Epoch 1121, Train_Loss: 5458.515625, Val_Loss: 5800.1865234375\n",
      "Epoch 1122, Train_Loss: 5458.2138671875, Val_Loss: 5799.86376953125\n",
      "Epoch 1123, Train_Loss: 5457.94189453125, Val_Loss: 5799.5537109375\n",
      "Epoch 1124, Train_Loss: 5457.66943359375, Val_Loss: 5799.22802734375\n",
      "Epoch 1125, Train_Loss: 5457.3818359375, Val_Loss: 5798.9052734375\n",
      "Epoch 1126, Train_Loss: 5457.0927734375, Val_Loss: 5798.58984375\n",
      "Epoch 1127, Train_Loss: 5456.7724609375, Val_Loss: 5798.26220703125\n",
      "Epoch 1128, Train_Loss: 5456.388671875, Val_Loss: 5797.9482421875\n",
      "Epoch 1129, Train_Loss: 5456.11328125, Val_Loss: 5797.64208984375\n",
      "Epoch 1130, Train_Loss: 5455.8369140625, Val_Loss: 5797.322265625\n",
      "Epoch 1131, Train_Loss: 5455.56103515625, Val_Loss: 5796.98095703125\n",
      "Epoch 1132, Train_Loss: 5455.271484375, Val_Loss: 5796.67041015625\n",
      "Epoch 1133, Train_Loss: 5455.0068359375, Val_Loss: 5796.35107421875\n",
      "Epoch 1134, Train_Loss: 5454.5576171875, Val_Loss: 5796.015625\n",
      "Epoch 1135, Train_Loss: 5454.28173828125, Val_Loss: 5795.70703125\n",
      "Epoch 1136, Train_Loss: 5454.001953125, Val_Loss: 5795.38623046875\n",
      "Epoch 1137, Train_Loss: 5453.70849609375, Val_Loss: 5795.0751953125\n",
      "Epoch 1138, Train_Loss: 5453.427734375, Val_Loss: 5794.73779296875\n",
      "Epoch 1139, Train_Loss: 5453.16015625, Val_Loss: 5794.423828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1140, Train_Loss: 5452.88623046875, Val_Loss: 5794.12255859375\n",
      "Epoch 1141, Train_Loss: 5452.59521484375, Val_Loss: 5793.80224609375\n",
      "Epoch 1142, Train_Loss: 5452.30517578125, Val_Loss: 5793.49658203125\n",
      "Epoch 1143, Train_Loss: 5452.015625, Val_Loss: 5793.15771484375\n",
      "Epoch 1144, Train_Loss: 5451.732421875, Val_Loss: 5792.8291015625\n",
      "Epoch 1145, Train_Loss: 5451.4541015625, Val_Loss: 5792.51904296875\n",
      "Epoch 1146, Train_Loss: 5451.16259765625, Val_Loss: 5792.19384765625\n",
      "Epoch 1147, Train_Loss: 5450.88720703125, Val_Loss: 5791.88818359375\n",
      "Epoch 1148, Train_Loss: 5450.61328125, Val_Loss: 5791.568359375\n",
      "Epoch 1149, Train_Loss: 5450.3359375, Val_Loss: 5791.25439453125\n",
      "Epoch 1150, Train_Loss: 5450.06787109375, Val_Loss: 5790.919921875\n",
      "Epoch 1151, Train_Loss: 5449.7822265625, Val_Loss: 5790.609375\n",
      "Epoch 1152, Train_Loss: 5449.4970703125, Val_Loss: 5790.25830078125\n",
      "Epoch 1153, Train_Loss: 5449.1259765625, Val_Loss: 5789.935546875\n",
      "Epoch 1154, Train_Loss: 5448.853515625, Val_Loss: 5789.6171875\n",
      "Epoch 1155, Train_Loss: 5448.57421875, Val_Loss: 5789.29443359375\n",
      "Epoch 1156, Train_Loss: 5448.2578125, Val_Loss: 5788.9921875\n",
      "Epoch 1157, Train_Loss: 5447.97119140625, Val_Loss: 5788.6513671875\n",
      "Epoch 1158, Train_Loss: 5447.6826171875, Val_Loss: 5788.33642578125\n",
      "Epoch 1159, Train_Loss: 5447.41455078125, Val_Loss: 5788.01513671875\n",
      "Epoch 1160, Train_Loss: 5447.13720703125, Val_Loss: 5787.693359375\n",
      "Epoch 1161, Train_Loss: 5446.86181640625, Val_Loss: 5787.37939453125\n",
      "Epoch 1162, Train_Loss: 5446.58740234375, Val_Loss: 5787.06982421875\n",
      "Epoch 1163, Train_Loss: 5446.32421875, Val_Loss: 5786.734375\n",
      "Epoch 1164, Train_Loss: 5446.03662109375, Val_Loss: 5786.4287109375\n",
      "Epoch 1165, Train_Loss: 5445.72607421875, Val_Loss: 5786.10693359375\n",
      "Epoch 1166, Train_Loss: 5445.4013671875, Val_Loss: 5785.79248046875\n",
      "Epoch 1167, Train_Loss: 5445.12890625, Val_Loss: 5785.478515625\n",
      "Epoch 1168, Train_Loss: 5444.85791015625, Val_Loss: 5785.16943359375\n",
      "Epoch 1169, Train_Loss: 5444.5751953125, Val_Loss: 5784.82421875\n",
      "Epoch 1170, Train_Loss: 5444.2763671875, Val_Loss: 5784.5009765625\n",
      "Epoch 1171, Train_Loss: 5443.99853515625, Val_Loss: 5784.18505859375\n",
      "Epoch 1172, Train_Loss: 5443.7099609375, Val_Loss: 5783.87109375\n",
      "Epoch 1173, Train_Loss: 5443.4248046875, Val_Loss: 5783.56298828125\n",
      "Epoch 1174, Train_Loss: 5443.150390625, Val_Loss: 5783.23388671875\n",
      "Epoch 1175, Train_Loss: 5442.85595703125, Val_Loss: 5782.90869140625\n",
      "Epoch 1176, Train_Loss: 5442.5703125, Val_Loss: 5782.58984375\n",
      "Epoch 1177, Train_Loss: 5442.3076171875, Val_Loss: 5782.27685546875\n",
      "Epoch 1178, Train_Loss: 5442.02392578125, Val_Loss: 5781.9560546875\n",
      "Epoch 1179, Train_Loss: 5441.70654296875, Val_Loss: 5781.64306640625\n",
      "Epoch 1180, Train_Loss: 5441.4443359375, Val_Loss: 5781.33447265625\n",
      "Epoch 1181, Train_Loss: 5441.17431640625, Val_Loss: 5781.00341796875\n",
      "Epoch 1182, Train_Loss: 5440.892578125, Val_Loss: 5780.6787109375\n",
      "Epoch 1183, Train_Loss: 5440.60009765625, Val_Loss: 5780.359375\n",
      "Epoch 1184, Train_Loss: 5440.32275390625, Val_Loss: 5780.04833984375\n",
      "Epoch 1185, Train_Loss: 5440.03955078125, Val_Loss: 5779.72607421875\n",
      "Epoch 1186, Train_Loss: 5439.76025390625, Val_Loss: 5779.42138671875\n",
      "Epoch 1187, Train_Loss: 5439.48291015625, Val_Loss: 5779.10400390625\n",
      "Epoch 1188, Train_Loss: 5439.2041015625, Val_Loss: 5778.73876953125\n",
      "Epoch 1189, Train_Loss: 5438.927734375, Val_Loss: 5778.4248046875\n",
      "Epoch 1190, Train_Loss: 5438.65185546875, Val_Loss: 5778.11474609375\n",
      "Epoch 1191, Train_Loss: 5438.29931640625, Val_Loss: 5777.7998046875\n",
      "Epoch 1192, Train_Loss: 5438.0126953125, Val_Loss: 5777.48974609375\n",
      "Epoch 1193, Train_Loss: 5437.744140625, Val_Loss: 5777.16650390625\n",
      "Epoch 1194, Train_Loss: 5437.43798828125, Val_Loss: 5776.83544921875\n",
      "Epoch 1195, Train_Loss: 5437.15234375, Val_Loss: 5776.52587890625\n",
      "Epoch 1196, Train_Loss: 5436.8564453125, Val_Loss: 5776.2158203125\n",
      "Epoch 1197, Train_Loss: 5436.48486328125, Val_Loss: 5775.9072265625\n",
      "Epoch 1198, Train_Loss: 5436.197265625, Val_Loss: 5775.58056640625\n",
      "Epoch 1199, Train_Loss: 5435.91748046875, Val_Loss: 5775.2724609375\n",
      "Epoch 1200, Train_Loss: 5435.63623046875, Val_Loss: 5774.966796875\n",
      "Epoch 1201, Train_Loss: 5435.3583984375, Val_Loss: 5774.626953125\n",
      "Epoch 1202, Train_Loss: 5435.1005859375, Val_Loss: 5774.32177734375\n",
      "Epoch 1203, Train_Loss: 5434.83154296875, Val_Loss: 5774.01611328125\n",
      "Epoch 1204, Train_Loss: 5434.47314453125, Val_Loss: 5773.7041015625\n",
      "Epoch 1205, Train_Loss: 5434.1865234375, Val_Loss: 5773.38525390625\n",
      "Epoch 1206, Train_Loss: 5433.85791015625, Val_Loss: 5773.0673828125\n",
      "Epoch 1207, Train_Loss: 5433.59814453125, Val_Loss: 5772.7373046875\n",
      "Epoch 1208, Train_Loss: 5433.3076171875, Val_Loss: 5772.423828125\n",
      "Epoch 1209, Train_Loss: 5433.02734375, Val_Loss: 5772.10498046875\n",
      "Epoch 1210, Train_Loss: 5432.7314453125, Val_Loss: 5771.7880859375\n",
      "Epoch 1211, Train_Loss: 5432.453125, Val_Loss: 5771.4794921875\n",
      "Epoch 1212, Train_Loss: 5432.17578125, Val_Loss: 5771.17724609375\n",
      "Epoch 1213, Train_Loss: 5431.88916015625, Val_Loss: 5770.8447265625\n",
      "Epoch 1214, Train_Loss: 5431.61474609375, Val_Loss: 5770.53857421875\n",
      "Epoch 1215, Train_Loss: 5431.31005859375, Val_Loss: 5770.20703125\n",
      "Epoch 1216, Train_Loss: 5431.02685546875, Val_Loss: 5769.896484375\n",
      "Epoch 1217, Train_Loss: 5430.712890625, Val_Loss: 5769.57470703125\n",
      "Epoch 1218, Train_Loss: 5430.41650390625, Val_Loss: 5769.26171875\n",
      "Epoch 1219, Train_Loss: 5430.1328125, Val_Loss: 5768.955078125\n",
      "Epoch 1220, Train_Loss: 5429.86279296875, Val_Loss: 5768.6279296875\n",
      "Epoch 1221, Train_Loss: 5429.5869140625, Val_Loss: 5768.33203125\n",
      "Epoch 1222, Train_Loss: 5429.3203125, Val_Loss: 5767.99609375\n",
      "Epoch 1223, Train_Loss: 5429.0283203125, Val_Loss: 5767.68359375\n",
      "Epoch 1224, Train_Loss: 5428.7568359375, Val_Loss: 5767.3388671875\n",
      "Epoch 1225, Train_Loss: 5428.4931640625, Val_Loss: 5767.02392578125\n",
      "Epoch 1226, Train_Loss: 5428.2314453125, Val_Loss: 5766.68603515625\n",
      "Epoch 1227, Train_Loss: 5427.9375, Val_Loss: 5766.37890625\n",
      "Epoch 1228, Train_Loss: 5427.6572265625, Val_Loss: 5766.06396484375\n",
      "Epoch 1229, Train_Loss: 5427.27978515625, Val_Loss: 5765.7451171875\n",
      "Epoch 1230, Train_Loss: 5427.0126953125, Val_Loss: 5765.43701171875\n",
      "Epoch 1231, Train_Loss: 5426.74755859375, Val_Loss: 5765.12841796875\n",
      "Epoch 1232, Train_Loss: 5426.4345703125, Val_Loss: 5764.79052734375\n",
      "Epoch 1233, Train_Loss: 5426.154296875, Val_Loss: 5764.482421875\n",
      "Epoch 1234, Train_Loss: 5425.88427734375, Val_Loss: 5764.16845703125\n",
      "Epoch 1235, Train_Loss: 5425.62158203125, Val_Loss: 5763.86767578125\n",
      "Epoch 1236, Train_Loss: 5425.33642578125, Val_Loss: 5763.54541015625\n",
      "Epoch 1237, Train_Loss: 5425.0498046875, Val_Loss: 5763.23779296875\n",
      "Epoch 1238, Train_Loss: 5424.78369140625, Val_Loss: 5762.9267578125\n",
      "Epoch 1239, Train_Loss: 5424.5, Val_Loss: 5762.5849609375\n",
      "Epoch 1240, Train_Loss: 5424.203125, Val_Loss: 5762.27880859375\n",
      "Epoch 1241, Train_Loss: 5423.91259765625, Val_Loss: 5761.96435546875\n",
      "Epoch 1242, Train_Loss: 5423.58642578125, Val_Loss: 5761.66015625\n",
      "Epoch 1243, Train_Loss: 5423.31982421875, Val_Loss: 5761.33984375\n",
      "Epoch 1244, Train_Loss: 5423.04638671875, Val_Loss: 5761.03271484375\n",
      "Epoch 1245, Train_Loss: 5422.7646484375, Val_Loss: 5760.7021484375\n",
      "Epoch 1246, Train_Loss: 5422.4970703125, Val_Loss: 5760.3818359375\n",
      "Epoch 1247, Train_Loss: 5422.21142578125, Val_Loss: 5760.08154296875\n",
      "Epoch 1248, Train_Loss: 5421.919921875, Val_Loss: 5759.7470703125\n",
      "Epoch 1249, Train_Loss: 5421.64208984375, Val_Loss: 5759.44384765625\n",
      "Epoch 1250, Train_Loss: 5421.37451171875, Val_Loss: 5759.1142578125\n",
      "Epoch 1251, Train_Loss: 5421.09033203125, Val_Loss: 5758.796875\n",
      "Epoch 1252, Train_Loss: 5420.806640625, Val_Loss: 5758.48291015625\n",
      "Epoch 1253, Train_Loss: 5420.53173828125, Val_Loss: 5758.17626953125\n",
      "Epoch 1254, Train_Loss: 5420.2607421875, Val_Loss: 5757.87060546875\n",
      "Epoch 1255, Train_Loss: 5419.89892578125, Val_Loss: 5757.556640625\n",
      "Epoch 1256, Train_Loss: 5419.6103515625, Val_Loss: 5757.2490234375\n",
      "Epoch 1257, Train_Loss: 5419.33837890625, Val_Loss: 5756.94580078125\n",
      "Epoch 1258, Train_Loss: 5419.06396484375, Val_Loss: 5756.6123046875\n",
      "Epoch 1259, Train_Loss: 5418.7978515625, Val_Loss: 5756.2646484375\n",
      "Epoch 1260, Train_Loss: 5418.52685546875, Val_Loss: 5755.9560546875\n",
      "Epoch 1261, Train_Loss: 5418.2392578125, Val_Loss: 5755.6259765625\n",
      "Epoch 1262, Train_Loss: 5417.96533203125, Val_Loss: 5755.3271484375\n",
      "Epoch 1263, Train_Loss: 5417.701171875, Val_Loss: 5755.00732421875\n",
      "Epoch 1264, Train_Loss: 5417.431640625, Val_Loss: 5754.68359375\n",
      "Epoch 1265, Train_Loss: 5417.16064453125, Val_Loss: 5754.375\n",
      "Epoch 1266, Train_Loss: 5416.88818359375, Val_Loss: 5754.0703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1267, Train_Loss: 5416.57421875, Val_Loss: 5753.7529296875\n",
      "Epoch 1268, Train_Loss: 5416.283203125, Val_Loss: 5753.4326171875\n",
      "Epoch 1269, Train_Loss: 5416.02685546875, Val_Loss: 5753.13232421875\n",
      "Epoch 1270, Train_Loss: 5415.75244140625, Val_Loss: 5752.81103515625\n",
      "Epoch 1271, Train_Loss: 5415.453125, Val_Loss: 5752.5048828125\n",
      "Epoch 1272, Train_Loss: 5415.17431640625, Val_Loss: 5752.19775390625\n",
      "Epoch 1273, Train_Loss: 5414.89697265625, Val_Loss: 5751.89599609375\n",
      "Epoch 1274, Train_Loss: 5414.630859375, Val_Loss: 5751.57177734375\n",
      "Epoch 1275, Train_Loss: 5414.34716796875, Val_Loss: 5751.2568359375\n",
      "Epoch 1276, Train_Loss: 5414.0849609375, Val_Loss: 5750.95703125\n",
      "Epoch 1277, Train_Loss: 5413.7373046875, Val_Loss: 5750.60546875\n",
      "Epoch 1278, Train_Loss: 5413.44775390625, Val_Loss: 5750.3037109375\n",
      "Epoch 1279, Train_Loss: 5413.173828125, Val_Loss: 5749.98779296875\n",
      "Epoch 1280, Train_Loss: 5412.85498046875, Val_Loss: 5749.6806640625\n",
      "Epoch 1281, Train_Loss: 5412.57373046875, Val_Loss: 5749.36279296875\n",
      "Epoch 1282, Train_Loss: 5412.2958984375, Val_Loss: 5749.064453125\n",
      "Epoch 1283, Train_Loss: 5412.02001953125, Val_Loss: 5748.72900390625\n",
      "Epoch 1284, Train_Loss: 5411.74951171875, Val_Loss: 5748.423828125\n",
      "Epoch 1285, Train_Loss: 5411.455078125, Val_Loss: 5748.11474609375\n",
      "Epoch 1286, Train_Loss: 5411.17919921875, Val_Loss: 5747.7998046875\n",
      "Epoch 1287, Train_Loss: 5410.88525390625, Val_Loss: 5747.47900390625\n",
      "Epoch 1288, Train_Loss: 5410.6162109375, Val_Loss: 5747.17529296875\n",
      "Epoch 1289, Train_Loss: 5410.33935546875, Val_Loss: 5746.85302734375\n",
      "Epoch 1290, Train_Loss: 5410.0625, Val_Loss: 5746.53564453125\n",
      "Epoch 1291, Train_Loss: 5409.78173828125, Val_Loss: 5746.22900390625\n",
      "Epoch 1292, Train_Loss: 5409.5087890625, Val_Loss: 5745.9208984375\n",
      "Epoch 1293, Train_Loss: 5409.15380859375, Val_Loss: 5745.615234375\n",
      "Epoch 1294, Train_Loss: 5408.884765625, Val_Loss: 5745.31884765625\n",
      "Epoch 1295, Train_Loss: 5408.56494140625, Val_Loss: 5744.97509765625\n",
      "Epoch 1296, Train_Loss: 5408.28076171875, Val_Loss: 5744.6416015625\n",
      "Epoch 1297, Train_Loss: 5408.0126953125, Val_Loss: 5744.341796875\n",
      "Epoch 1298, Train_Loss: 5407.736328125, Val_Loss: 5744.0341796875\n",
      "Epoch 1299, Train_Loss: 5407.451171875, Val_Loss: 5743.7158203125\n",
      "Epoch 1300, Train_Loss: 5407.17822265625, Val_Loss: 5743.4052734375\n",
      "Epoch 1301, Train_Loss: 5406.90380859375, Val_Loss: 5743.08984375\n",
      "Epoch 1302, Train_Loss: 5406.61572265625, Val_Loss: 5742.76123046875\n",
      "Epoch 1303, Train_Loss: 5406.34521484375, Val_Loss: 5742.46240234375\n",
      "Epoch 1304, Train_Loss: 5406.0859375, Val_Loss: 5742.16015625\n",
      "Epoch 1305, Train_Loss: 5405.759765625, Val_Loss: 5741.8466796875\n",
      "Epoch 1306, Train_Loss: 5405.48583984375, Val_Loss: 5741.54541015625\n",
      "Epoch 1307, Train_Loss: 5405.22216796875, Val_Loss: 5741.23876953125\n",
      "Epoch 1308, Train_Loss: 5404.92626953125, Val_Loss: 5740.89697265625\n",
      "Epoch 1309, Train_Loss: 5404.63818359375, Val_Loss: 5740.59375\n",
      "Epoch 1310, Train_Loss: 5404.37255859375, Val_Loss: 5740.2919921875\n",
      "Epoch 1311, Train_Loss: 5404.0908203125, Val_Loss: 5739.9873046875\n",
      "Epoch 1312, Train_Loss: 5403.80859375, Val_Loss: 5739.67333984375\n",
      "Epoch 1313, Train_Loss: 5403.51611328125, Val_Loss: 5739.3544921875\n",
      "Epoch 1314, Train_Loss: 5403.24560546875, Val_Loss: 5739.04833984375\n",
      "Epoch 1315, Train_Loss: 5402.982421875, Val_Loss: 5738.73291015625\n",
      "Epoch 1316, Train_Loss: 5402.70556640625, Val_Loss: 5738.408203125\n",
      "Epoch 1317, Train_Loss: 5402.4267578125, Val_Loss: 5738.10302734375\n",
      "Epoch 1318, Train_Loss: 5402.09130859375, Val_Loss: 5737.80224609375\n",
      "Epoch 1319, Train_Loss: 5401.802734375, Val_Loss: 5737.498046875\n",
      "Epoch 1320, Train_Loss: 5401.513671875, Val_Loss: 5737.18798828125\n",
      "Epoch 1321, Train_Loss: 5401.236328125, Val_Loss: 5736.86474609375\n",
      "Epoch 1322, Train_Loss: 5400.96240234375, Val_Loss: 5736.54736328125\n",
      "Epoch 1323, Train_Loss: 5400.689453125, Val_Loss: 5736.23486328125\n",
      "Epoch 1324, Train_Loss: 5400.41015625, Val_Loss: 5735.92431640625\n",
      "Epoch 1325, Train_Loss: 5400.14501953125, Val_Loss: 5735.6162109375\n",
      "Epoch 1326, Train_Loss: 5399.88232421875, Val_Loss: 5735.318359375\n",
      "Epoch 1327, Train_Loss: 5399.61572265625, Val_Loss: 5734.982421875\n",
      "Epoch 1328, Train_Loss: 5399.330078125, Val_Loss: 5734.6845703125\n",
      "Epoch 1329, Train_Loss: 5399.0595703125, Val_Loss: 5734.361328125\n",
      "Epoch 1330, Train_Loss: 5398.79443359375, Val_Loss: 5734.05859375\n",
      "Epoch 1331, Train_Loss: 5398.42919921875, Val_Loss: 5733.71728515625\n",
      "Epoch 1332, Train_Loss: 5398.1650390625, Val_Loss: 5733.42138671875\n",
      "Epoch 1333, Train_Loss: 5397.8876953125, Val_Loss: 5733.1123046875\n",
      "Epoch 1334, Train_Loss: 5397.6083984375, Val_Loss: 5732.78173828125\n",
      "Epoch 1335, Train_Loss: 5397.32958984375, Val_Loss: 5732.4697265625\n",
      "Epoch 1336, Train_Loss: 5397.04296875, Val_Loss: 5732.1572265625\n",
      "Epoch 1337, Train_Loss: 5396.755859375, Val_Loss: 5731.84765625\n",
      "Epoch 1338, Train_Loss: 5396.4755859375, Val_Loss: 5731.53857421875\n",
      "Epoch 1339, Train_Loss: 5396.2041015625, Val_Loss: 5731.22021484375\n",
      "Epoch 1340, Train_Loss: 5395.88330078125, Val_Loss: 5730.908203125\n",
      "Epoch 1341, Train_Loss: 5395.609375, Val_Loss: 5730.60302734375\n",
      "Epoch 1342, Train_Loss: 5395.32958984375, Val_Loss: 5730.29736328125\n",
      "Epoch 1343, Train_Loss: 5395.03564453125, Val_Loss: 5729.98876953125\n",
      "Epoch 1344, Train_Loss: 5394.7666015625, Val_Loss: 5729.673828125\n",
      "Epoch 1345, Train_Loss: 5394.51220703125, Val_Loss: 5729.37060546875\n",
      "Epoch 1346, Train_Loss: 5394.24072265625, Val_Loss: 5729.0478515625\n",
      "Epoch 1347, Train_Loss: 5393.95751953125, Val_Loss: 5728.75732421875\n",
      "Epoch 1348, Train_Loss: 5393.669921875, Val_Loss: 5728.4501953125\n",
      "Epoch 1349, Train_Loss: 5393.306640625, Val_Loss: 5728.13525390625\n",
      "Epoch 1350, Train_Loss: 5393.04345703125, Val_Loss: 5727.8251953125\n",
      "Epoch 1351, Train_Loss: 5392.7763671875, Val_Loss: 5727.50634765625\n",
      "Epoch 1352, Train_Loss: 5392.50341796875, Val_Loss: 5727.2001953125\n",
      "Epoch 1353, Train_Loss: 5392.23583984375, Val_Loss: 5726.869140625\n",
      "Epoch 1354, Train_Loss: 5391.98388671875, Val_Loss: 5726.5830078125\n",
      "Epoch 1355, Train_Loss: 5391.69921875, Val_Loss: 5726.27978515625\n",
      "Epoch 1356, Train_Loss: 5391.3447265625, Val_Loss: 5725.9833984375\n",
      "Epoch 1357, Train_Loss: 5391.05078125, Val_Loss: 5725.677734375\n",
      "Epoch 1358, Train_Loss: 5390.7568359375, Val_Loss: 5725.34326171875\n",
      "Epoch 1359, Train_Loss: 5390.50537109375, Val_Loss: 5725.0126953125\n",
      "Epoch 1360, Train_Loss: 5390.23974609375, Val_Loss: 5724.70263671875\n",
      "Epoch 1361, Train_Loss: 5389.94873046875, Val_Loss: 5724.408203125\n",
      "Epoch 1362, Train_Loss: 5389.6552734375, Val_Loss: 5724.0908203125\n",
      "Epoch 1363, Train_Loss: 5389.380859375, Val_Loss: 5723.79052734375\n",
      "Epoch 1364, Train_Loss: 5389.1044921875, Val_Loss: 5723.4794921875\n",
      "Epoch 1365, Train_Loss: 5388.83984375, Val_Loss: 5723.17822265625\n",
      "Epoch 1366, Train_Loss: 5388.5546875, Val_Loss: 5722.8359375\n",
      "Epoch 1367, Train_Loss: 5388.26806640625, Val_Loss: 5722.51806640625\n",
      "Epoch 1368, Train_Loss: 5387.994140625, Val_Loss: 5722.21875\n",
      "Epoch 1369, Train_Loss: 5387.68798828125, Val_Loss: 5721.919921875\n",
      "Epoch 1370, Train_Loss: 5387.4443359375, Val_Loss: 5721.5986328125\n",
      "Epoch 1371, Train_Loss: 5387.1611328125, Val_Loss: 5721.30322265625\n",
      "Epoch 1372, Train_Loss: 5386.89013671875, Val_Loss: 5720.98388671875\n",
      "Epoch 1373, Train_Loss: 5386.61279296875, Val_Loss: 5720.68359375\n",
      "Epoch 1374, Train_Loss: 5386.34130859375, Val_Loss: 5720.3759765625\n",
      "Epoch 1375, Train_Loss: 5386.06103515625, Val_Loss: 5720.0673828125\n",
      "Epoch 1376, Train_Loss: 5385.79443359375, Val_Loss: 5719.7373046875\n",
      "Epoch 1377, Train_Loss: 5385.5234375, Val_Loss: 5719.443359375\n",
      "Epoch 1378, Train_Loss: 5385.26708984375, Val_Loss: 5719.11279296875\n",
      "Epoch 1379, Train_Loss: 5385.00146484375, Val_Loss: 5718.80859375\n",
      "Epoch 1380, Train_Loss: 5384.73583984375, Val_Loss: 5718.50537109375\n",
      "Epoch 1381, Train_Loss: 5384.427734375, Val_Loss: 5718.208984375\n",
      "Epoch 1382, Train_Loss: 5384.1513671875, Val_Loss: 5717.89599609375\n",
      "Epoch 1383, Train_Loss: 5383.8798828125, Val_Loss: 5717.5927734375\n",
      "Epoch 1384, Train_Loss: 5383.6025390625, Val_Loss: 5717.28564453125\n",
      "Epoch 1385, Train_Loss: 5383.31884765625, Val_Loss: 5716.95556640625\n",
      "Epoch 1386, Train_Loss: 5383.02734375, Val_Loss: 5716.65087890625\n",
      "Epoch 1387, Train_Loss: 5382.74658203125, Val_Loss: 5716.34326171875\n",
      "Epoch 1388, Train_Loss: 5382.47314453125, Val_Loss: 5716.041015625\n",
      "Epoch 1389, Train_Loss: 5382.1943359375, Val_Loss: 5715.732421875\n",
      "Epoch 1390, Train_Loss: 5381.9208984375, Val_Loss: 5715.43896484375\n",
      "Epoch 1391, Train_Loss: 5381.654296875, Val_Loss: 5715.1044921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1392, Train_Loss: 5381.3876953125, Val_Loss: 5714.79638671875\n",
      "Epoch 1393, Train_Loss: 5381.123046875, Val_Loss: 5714.47509765625\n",
      "Epoch 1394, Train_Loss: 5380.763671875, Val_Loss: 5714.16357421875\n",
      "Epoch 1395, Train_Loss: 5380.466796875, Val_Loss: 5713.87109375\n",
      "Epoch 1396, Train_Loss: 5380.2021484375, Val_Loss: 5713.58056640625\n",
      "Epoch 1397, Train_Loss: 5379.91015625, Val_Loss: 5713.2626953125\n",
      "Epoch 1398, Train_Loss: 5379.63623046875, Val_Loss: 5712.95556640625\n",
      "Epoch 1399, Train_Loss: 5379.36767578125, Val_Loss: 5712.6494140625\n",
      "Epoch 1400, Train_Loss: 5379.078125, Val_Loss: 5712.3427734375\n",
      "Epoch 1401, Train_Loss: 5378.818359375, Val_Loss: 5712.0283203125\n",
      "Epoch 1402, Train_Loss: 5378.56298828125, Val_Loss: 5711.685546875\n",
      "Epoch 1403, Train_Loss: 5378.29541015625, Val_Loss: 5711.38720703125\n",
      "Epoch 1404, Train_Loss: 5378.0390625, Val_Loss: 5711.068359375\n",
      "Epoch 1405, Train_Loss: 5377.744140625, Val_Loss: 5710.76123046875\n",
      "Epoch 1406, Train_Loss: 5377.45654296875, Val_Loss: 5710.4658203125\n",
      "Epoch 1407, Train_Loss: 5377.1318359375, Val_Loss: 5710.158203125\n",
      "Epoch 1408, Train_Loss: 5376.8720703125, Val_Loss: 5709.86083984375\n",
      "Epoch 1409, Train_Loss: 5376.625, Val_Loss: 5709.544921875\n",
      "Epoch 1410, Train_Loss: 5376.34521484375, Val_Loss: 5709.224609375\n",
      "Epoch 1411, Train_Loss: 5376.046875, Val_Loss: 5708.923828125\n",
      "Epoch 1412, Train_Loss: 5375.78173828125, Val_Loss: 5708.6103515625\n",
      "Epoch 1413, Train_Loss: 5375.5166015625, Val_Loss: 5708.30517578125\n",
      "Epoch 1414, Train_Loss: 5375.24658203125, Val_Loss: 5708.01513671875\n",
      "Epoch 1415, Train_Loss: 5374.96337890625, Val_Loss: 5707.6962890625\n",
      "Epoch 1416, Train_Loss: 5374.6845703125, Val_Loss: 5707.3798828125\n",
      "Epoch 1417, Train_Loss: 5374.42138671875, Val_Loss: 5707.08251953125\n",
      "Epoch 1418, Train_Loss: 5374.14404296875, Val_Loss: 5706.7705078125\n",
      "Epoch 1419, Train_Loss: 5373.87646484375, Val_Loss: 5706.470703125\n",
      "Epoch 1420, Train_Loss: 5373.45361328125, Val_Loss: 5706.16748046875\n",
      "Epoch 1421, Train_Loss: 5373.18896484375, Val_Loss: 5705.86083984375\n",
      "Epoch 1422, Train_Loss: 5372.92138671875, Val_Loss: 5705.5546875\n",
      "Epoch 1423, Train_Loss: 5372.6611328125, Val_Loss: 5705.234375\n",
      "Epoch 1424, Train_Loss: 5372.3662109375, Val_Loss: 5704.93359375\n",
      "Epoch 1425, Train_Loss: 5372.10107421875, Val_Loss: 5704.62255859375\n",
      "Epoch 1426, Train_Loss: 5371.82958984375, Val_Loss: 5704.31494140625\n",
      "Epoch 1427, Train_Loss: 5371.5517578125, Val_Loss: 5704.0166015625\n",
      "Epoch 1428, Train_Loss: 5371.27978515625, Val_Loss: 5703.71240234375\n",
      "Epoch 1429, Train_Loss: 5370.99853515625, Val_Loss: 5703.3681640625\n",
      "Epoch 1430, Train_Loss: 5370.72216796875, Val_Loss: 5703.0830078125\n",
      "Epoch 1431, Train_Loss: 5370.455078125, Val_Loss: 5702.78369140625\n",
      "Epoch 1432, Train_Loss: 5370.1044921875, Val_Loss: 5702.4814453125\n",
      "Epoch 1433, Train_Loss: 5369.8447265625, Val_Loss: 5702.18701171875\n",
      "Epoch 1434, Train_Loss: 5369.54541015625, Val_Loss: 5701.880859375\n",
      "Epoch 1435, Train_Loss: 5369.2587890625, Val_Loss: 5701.5546875\n",
      "Epoch 1436, Train_Loss: 5368.98876953125, Val_Loss: 5701.25537109375\n",
      "Epoch 1437, Train_Loss: 5368.71923828125, Val_Loss: 5700.947265625\n",
      "Epoch 1438, Train_Loss: 5368.41455078125, Val_Loss: 5700.62255859375\n",
      "Epoch 1439, Train_Loss: 5368.12890625, Val_Loss: 5700.31494140625\n",
      "Epoch 1440, Train_Loss: 5367.8701171875, Val_Loss: 5700.00732421875\n",
      "Epoch 1441, Train_Loss: 5367.62255859375, Val_Loss: 5699.70361328125\n",
      "Epoch 1442, Train_Loss: 5367.35791015625, Val_Loss: 5699.37744140625\n",
      "Epoch 1443, Train_Loss: 5367.08837890625, Val_Loss: 5699.103515625\n",
      "Epoch 1444, Train_Loss: 5366.80322265625, Val_Loss: 5698.79443359375\n",
      "Epoch 1445, Train_Loss: 5366.50439453125, Val_Loss: 5698.48095703125\n",
      "Epoch 1446, Train_Loss: 5366.25146484375, Val_Loss: 5698.18701171875\n",
      "Epoch 1447, Train_Loss: 5365.990234375, Val_Loss: 5697.88818359375\n",
      "Epoch 1448, Train_Loss: 5365.720703125, Val_Loss: 5697.568359375\n",
      "Epoch 1449, Train_Loss: 5365.44580078125, Val_Loss: 5697.26611328125\n",
      "Epoch 1450, Train_Loss: 5365.17236328125, Val_Loss: 5696.9658203125\n",
      "Epoch 1451, Train_Loss: 5364.89599609375, Val_Loss: 5696.654296875\n",
      "Epoch 1452, Train_Loss: 5364.63671875, Val_Loss: 5696.37353515625\n",
      "Epoch 1453, Train_Loss: 5364.37060546875, Val_Loss: 5696.07080078125\n",
      "Epoch 1454, Train_Loss: 5364.1162109375, Val_Loss: 5695.7587890625\n",
      "Epoch 1455, Train_Loss: 5363.845703125, Val_Loss: 5695.44384765625\n",
      "Epoch 1456, Train_Loss: 5363.55517578125, Val_Loss: 5695.1376953125\n",
      "Epoch 1457, Train_Loss: 5363.28271484375, Val_Loss: 5694.82763671875\n",
      "Epoch 1458, Train_Loss: 5362.9189453125, Val_Loss: 5694.52685546875\n",
      "Epoch 1459, Train_Loss: 5362.66943359375, Val_Loss: 5694.24169921875\n",
      "Epoch 1460, Train_Loss: 5362.42431640625, Val_Loss: 5693.93701171875\n",
      "Epoch 1461, Train_Loss: 5362.1376953125, Val_Loss: 5693.61376953125\n",
      "Epoch 1462, Train_Loss: 5361.85791015625, Val_Loss: 5693.31396484375\n",
      "Epoch 1463, Train_Loss: 5361.5673828125, Val_Loss: 5693.021484375\n",
      "Epoch 1464, Train_Loss: 5361.291015625, Val_Loss: 5692.7080078125\n",
      "Epoch 1465, Train_Loss: 5361.02392578125, Val_Loss: 5692.39501953125\n",
      "Epoch 1466, Train_Loss: 5360.7666015625, Val_Loss: 5692.0947265625\n",
      "Epoch 1467, Train_Loss: 5360.50244140625, Val_Loss: 5691.76904296875\n",
      "Epoch 1468, Train_Loss: 5360.24560546875, Val_Loss: 5691.47900390625\n",
      "Epoch 1469, Train_Loss: 5359.98828125, Val_Loss: 5691.18701171875\n",
      "Epoch 1470, Train_Loss: 5359.7294921875, Val_Loss: 5690.88623046875\n",
      "Epoch 1471, Train_Loss: 5359.42724609375, Val_Loss: 5690.5751953125\n",
      "Epoch 1472, Train_Loss: 5359.15234375, Val_Loss: 5690.2744140625\n",
      "Epoch 1473, Train_Loss: 5358.875, Val_Loss: 5689.98291015625\n",
      "Epoch 1474, Train_Loss: 5358.58642578125, Val_Loss: 5689.60986328125\n",
      "Epoch 1475, Train_Loss: 5358.322265625, Val_Loss: 5689.31494140625\n",
      "Epoch 1476, Train_Loss: 5358.06396484375, Val_Loss: 5689.029296875\n",
      "Epoch 1477, Train_Loss: 5357.7783203125, Val_Loss: 5688.7177734375\n",
      "Epoch 1478, Train_Loss: 5357.5205078125, Val_Loss: 5688.42822265625\n",
      "Epoch 1479, Train_Loss: 5357.255859375, Val_Loss: 5688.11962890625\n",
      "Epoch 1480, Train_Loss: 5356.9970703125, Val_Loss: 5687.81005859375\n",
      "Epoch 1481, Train_Loss: 5356.732421875, Val_Loss: 5687.5087890625\n",
      "Epoch 1482, Train_Loss: 5356.45166015625, Val_Loss: 5687.21484375\n",
      "Epoch 1483, Train_Loss: 5356.12060546875, Val_Loss: 5686.91455078125\n",
      "Epoch 1484, Train_Loss: 5355.853515625, Val_Loss: 5686.62060546875\n",
      "Epoch 1485, Train_Loss: 5355.591796875, Val_Loss: 5686.32177734375\n",
      "Epoch 1486, Train_Loss: 5355.326171875, Val_Loss: 5686.00341796875\n",
      "Epoch 1487, Train_Loss: 5355.0537109375, Val_Loss: 5685.7099609375\n",
      "Epoch 1488, Train_Loss: 5354.796875, Val_Loss: 5685.3896484375\n",
      "Epoch 1489, Train_Loss: 5354.525390625, Val_Loss: 5685.0986328125\n",
      "Epoch 1490, Train_Loss: 5354.2646484375, Val_Loss: 5684.8037109375\n",
      "Epoch 1491, Train_Loss: 5353.99267578125, Val_Loss: 5684.51025390625\n",
      "Epoch 1492, Train_Loss: 5353.669921875, Val_Loss: 5684.20947265625\n",
      "Epoch 1493, Train_Loss: 5353.4208984375, Val_Loss: 5683.88623046875\n",
      "Epoch 1494, Train_Loss: 5353.1376953125, Val_Loss: 5683.58203125\n",
      "Epoch 1495, Train_Loss: 5352.86767578125, Val_Loss: 5683.2841796875\n",
      "Epoch 1496, Train_Loss: 5352.50048828125, Val_Loss: 5682.9755859375\n",
      "Epoch 1497, Train_Loss: 5352.232421875, Val_Loss: 5682.68701171875\n",
      "Epoch 1498, Train_Loss: 5351.97216796875, Val_Loss: 5682.39892578125\n",
      "Epoch 1499, Train_Loss: 5351.7109375, Val_Loss: 5682.08349609375\n",
      "Epoch 1500, Train_Loss: 5351.45703125, Val_Loss: 5681.783203125\n",
      "Epoch 1501, Train_Loss: 5351.1533203125, Val_Loss: 5681.4765625\n",
      "Epoch 1502, Train_Loss: 5350.892578125, Val_Loss: 5681.17041015625\n",
      "Epoch 1503, Train_Loss: 5350.6328125, Val_Loss: 5680.87060546875\n",
      "Epoch 1504, Train_Loss: 5350.359375, Val_Loss: 5680.57373046875\n",
      "Epoch 1505, Train_Loss: 5350.1025390625, Val_Loss: 5680.2666015625\n",
      "Epoch 1506, Train_Loss: 5349.82861328125, Val_Loss: 5679.96142578125\n",
      "Epoch 1507, Train_Loss: 5349.58447265625, Val_Loss: 5679.66357421875\n",
      "Epoch 1508, Train_Loss: 5349.3154296875, Val_Loss: 5679.36376953125\n",
      "Epoch 1509, Train_Loss: 5348.9892578125, Val_Loss: 5679.07177734375\n",
      "Epoch 1510, Train_Loss: 5348.71044921875, Val_Loss: 5678.73779296875\n",
      "Epoch 1511, Train_Loss: 5348.42578125, Val_Loss: 5678.44140625\n",
      "Epoch 1512, Train_Loss: 5348.14697265625, Val_Loss: 5678.1279296875\n",
      "Epoch 1513, Train_Loss: 5347.87890625, Val_Loss: 5677.833984375\n",
      "Epoch 1514, Train_Loss: 5347.6201171875, Val_Loss: 5677.5341796875\n",
      "Epoch 1515, Train_Loss: 5347.3525390625, Val_Loss: 5677.22900390625\n",
      "Epoch 1516, Train_Loss: 5347.07373046875, Val_Loss: 5676.93701171875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1517, Train_Loss: 5346.81396484375, Val_Loss: 5676.634765625\n",
      "Epoch 1518, Train_Loss: 5346.55126953125, Val_Loss: 5676.3154296875\n",
      "Epoch 1519, Train_Loss: 5346.287109375, Val_Loss: 5676.01220703125\n",
      "Epoch 1520, Train_Loss: 5346.01123046875, Val_Loss: 5675.71923828125\n",
      "Epoch 1521, Train_Loss: 5345.73193359375, Val_Loss: 5675.4208984375\n",
      "Epoch 1522, Train_Loss: 5345.423828125, Val_Loss: 5675.1279296875\n",
      "Epoch 1523, Train_Loss: 5345.16357421875, Val_Loss: 5674.828125\n",
      "Epoch 1524, Train_Loss: 5344.89990234375, Val_Loss: 5674.537109375\n",
      "Epoch 1525, Train_Loss: 5344.63671875, Val_Loss: 5674.21337890625\n",
      "Epoch 1526, Train_Loss: 5344.36181640625, Val_Loss: 5673.9130859375\n",
      "Epoch 1527, Train_Loss: 5344.0966796875, Val_Loss: 5673.6181640625\n",
      "Epoch 1528, Train_Loss: 5343.82373046875, Val_Loss: 5673.30859375\n",
      "Epoch 1529, Train_Loss: 5343.55615234375, Val_Loss: 5673.00341796875\n",
      "Epoch 1530, Train_Loss: 5343.29052734375, Val_Loss: 5672.7099609375\n",
      "Epoch 1531, Train_Loss: 5343.033203125, Val_Loss: 5672.39599609375\n",
      "Epoch 1532, Train_Loss: 5342.77587890625, Val_Loss: 5672.10498046875\n",
      "Epoch 1533, Train_Loss: 5342.5087890625, Val_Loss: 5671.8203125\n",
      "Epoch 1534, Train_Loss: 5342.13916015625, Val_Loss: 5671.51220703125\n",
      "Epoch 1535, Train_Loss: 5341.89306640625, Val_Loss: 5671.22216796875\n",
      "Epoch 1536, Train_Loss: 5341.625, Val_Loss: 5670.9140625\n",
      "Epoch 1537, Train_Loss: 5341.349609375, Val_Loss: 5670.60205078125\n",
      "Epoch 1538, Train_Loss: 5341.09326171875, Val_Loss: 5670.28857421875\n",
      "Epoch 1539, Train_Loss: 5340.830078125, Val_Loss: 5669.990234375\n",
      "Epoch 1540, Train_Loss: 5340.544921875, Val_Loss: 5669.70556640625\n",
      "Epoch 1541, Train_Loss: 5340.27734375, Val_Loss: 5669.39697265625\n",
      "Epoch 1542, Train_Loss: 5340.017578125, Val_Loss: 5669.095703125\n",
      "Epoch 1543, Train_Loss: 5339.7392578125, Val_Loss: 5668.80810546875\n",
      "Epoch 1544, Train_Loss: 5339.482421875, Val_Loss: 5668.4912109375\n",
      "Epoch 1545, Train_Loss: 5339.21728515625, Val_Loss: 5668.193359375\n",
      "Epoch 1546, Train_Loss: 5338.939453125, Val_Loss: 5667.87060546875\n",
      "Epoch 1547, Train_Loss: 5338.64892578125, Val_Loss: 5667.578125\n",
      "Epoch 1548, Train_Loss: 5338.40087890625, Val_Loss: 5667.279296875\n",
      "Epoch 1549, Train_Loss: 5338.154296875, Val_Loss: 5666.9873046875\n",
      "Epoch 1550, Train_Loss: 5337.849609375, Val_Loss: 5666.68115234375\n",
      "Epoch 1551, Train_Loss: 5337.57177734375, Val_Loss: 5666.3720703125\n",
      "Epoch 1552, Train_Loss: 5337.32666015625, Val_Loss: 5666.07421875\n",
      "Epoch 1553, Train_Loss: 5337.06640625, Val_Loss: 5665.775390625\n",
      "Epoch 1554, Train_Loss: 5336.791015625, Val_Loss: 5665.4833984375\n",
      "Epoch 1555, Train_Loss: 5336.494140625, Val_Loss: 5665.185546875\n",
      "Epoch 1556, Train_Loss: 5336.24365234375, Val_Loss: 5664.88671875\n",
      "Epoch 1557, Train_Loss: 5335.9765625, Val_Loss: 5664.57861328125\n",
      "Epoch 1558, Train_Loss: 5335.70068359375, Val_Loss: 5664.2861328125\n",
      "Epoch 1559, Train_Loss: 5335.41943359375, Val_Loss: 5663.9833984375\n",
      "Epoch 1560, Train_Loss: 5335.07177734375, Val_Loss: 5663.685546875\n",
      "Epoch 1561, Train_Loss: 5334.81005859375, Val_Loss: 5663.4033203125\n",
      "Epoch 1562, Train_Loss: 5334.55712890625, Val_Loss: 5663.10205078125\n",
      "Epoch 1563, Train_Loss: 5334.310546875, Val_Loss: 5662.7822265625\n",
      "Epoch 1564, Train_Loss: 5333.8173828125, Val_Loss: 5662.49462890625\n",
      "Epoch 1565, Train_Loss: 5333.5498046875, Val_Loss: 5662.2001953125\n",
      "Epoch 1566, Train_Loss: 5333.28125, Val_Loss: 5661.89892578125\n",
      "Epoch 1567, Train_Loss: 5333.0048828125, Val_Loss: 5661.59912109375\n",
      "Epoch 1568, Train_Loss: 5332.74365234375, Val_Loss: 5661.29443359375\n",
      "Epoch 1569, Train_Loss: 5332.47216796875, Val_Loss: 5661.00634765625\n",
      "Epoch 1570, Train_Loss: 5332.21533203125, Val_Loss: 5660.68603515625\n",
      "Epoch 1571, Train_Loss: 5331.9501953125, Val_Loss: 5660.39599609375\n",
      "Epoch 1572, Train_Loss: 5331.68212890625, Val_Loss: 5660.09619140625\n",
      "Epoch 1573, Train_Loss: 5331.37109375, Val_Loss: 5659.79296875\n",
      "Epoch 1574, Train_Loss: 5331.12109375, Val_Loss: 5659.4970703125\n",
      "Epoch 1575, Train_Loss: 5330.85791015625, Val_Loss: 5659.189453125\n",
      "Epoch 1576, Train_Loss: 5330.58154296875, Val_Loss: 5658.88134765625\n",
      "Epoch 1577, Train_Loss: 5330.30859375, Val_Loss: 5658.59912109375\n",
      "Epoch 1578, Train_Loss: 5330.04052734375, Val_Loss: 5658.31396484375\n",
      "Epoch 1579, Train_Loss: 5329.76171875, Val_Loss: 5658.00537109375\n",
      "Epoch 1580, Train_Loss: 5329.501953125, Val_Loss: 5657.70458984375\n",
      "Epoch 1581, Train_Loss: 5329.232421875, Val_Loss: 5657.4140625\n",
      "Epoch 1582, Train_Loss: 5328.9482421875, Val_Loss: 5657.05712890625\n",
      "Epoch 1583, Train_Loss: 5328.69921875, Val_Loss: 5656.767578125\n",
      "Epoch 1584, Train_Loss: 5328.419921875, Val_Loss: 5656.478515625\n",
      "Epoch 1585, Train_Loss: 5328.154296875, Val_Loss: 5656.1865234375\n",
      "Epoch 1586, Train_Loss: 5327.85693359375, Val_Loss: 5655.88623046875\n",
      "Epoch 1587, Train_Loss: 5327.623046875, Val_Loss: 5655.587890625\n",
      "Epoch 1588, Train_Loss: 5327.328125, Val_Loss: 5655.30078125\n",
      "Epoch 1589, Train_Loss: 5327.06591796875, Val_Loss: 5654.97998046875\n",
      "Epoch 1590, Train_Loss: 5326.794921875, Val_Loss: 5654.69970703125\n",
      "Epoch 1591, Train_Loss: 5326.5361328125, Val_Loss: 5654.39892578125\n",
      "Epoch 1592, Train_Loss: 5326.2705078125, Val_Loss: 5654.09375\n",
      "Epoch 1593, Train_Loss: 5326.0009765625, Val_Loss: 5653.7998046875\n",
      "Epoch 1594, Train_Loss: 5325.7421875, Val_Loss: 5653.50927734375\n",
      "Epoch 1595, Train_Loss: 5325.4873046875, Val_Loss: 5653.1982421875\n",
      "Epoch 1596, Train_Loss: 5325.20068359375, Val_Loss: 5652.9091796875\n",
      "Epoch 1597, Train_Loss: 5324.93994140625, Val_Loss: 5652.607421875\n",
      "Epoch 1598, Train_Loss: 5324.5888671875, Val_Loss: 5652.31787109375\n",
      "Epoch 1599, Train_Loss: 5324.32421875, Val_Loss: 5652.01611328125\n",
      "Epoch 1600, Train_Loss: 5324.05322265625, Val_Loss: 5651.71484375\n",
      "Epoch 1601, Train_Loss: 5323.7958984375, Val_Loss: 5651.4296875\n",
      "Epoch 1602, Train_Loss: 5323.544921875, Val_Loss: 5651.10595703125\n",
      "Epoch 1603, Train_Loss: 5323.28271484375, Val_Loss: 5650.79541015625\n",
      "Epoch 1604, Train_Loss: 5323.0263671875, Val_Loss: 5650.505859375\n",
      "Epoch 1605, Train_Loss: 5322.7548828125, Val_Loss: 5650.2099609375\n",
      "Epoch 1606, Train_Loss: 5322.50537109375, Val_Loss: 5649.9287109375\n",
      "Epoch 1607, Train_Loss: 5322.2490234375, Val_Loss: 5649.640625\n",
      "Epoch 1608, Train_Loss: 5321.98828125, Val_Loss: 5649.328125\n",
      "Epoch 1609, Train_Loss: 5321.7138671875, Val_Loss: 5649.01025390625\n",
      "Epoch 1610, Train_Loss: 5321.4580078125, Val_Loss: 5648.720703125\n",
      "Epoch 1611, Train_Loss: 5321.15576171875, Val_Loss: 5648.42041015625\n",
      "Epoch 1612, Train_Loss: 5320.88134765625, Val_Loss: 5648.12890625\n",
      "Epoch 1613, Train_Loss: 5320.611328125, Val_Loss: 5647.83056640625\n",
      "Epoch 1614, Train_Loss: 5320.32275390625, Val_Loss: 5647.51904296875\n",
      "Epoch 1615, Train_Loss: 5320.05712890625, Val_Loss: 5647.240234375\n",
      "Epoch 1616, Train_Loss: 5319.79736328125, Val_Loss: 5646.9375\n",
      "Epoch 1617, Train_Loss: 5319.51904296875, Val_Loss: 5646.64794921875\n",
      "Epoch 1618, Train_Loss: 5319.2529296875, Val_Loss: 5646.3017578125\n",
      "Epoch 1619, Train_Loss: 5318.99560546875, Val_Loss: 5646.00732421875\n",
      "Epoch 1620, Train_Loss: 5318.7373046875, Val_Loss: 5645.71630859375\n",
      "Epoch 1621, Train_Loss: 5318.4775390625, Val_Loss: 5645.4013671875\n",
      "Epoch 1622, Train_Loss: 5318.2001953125, Val_Loss: 5645.1220703125\n",
      "Epoch 1623, Train_Loss: 5317.9404296875, Val_Loss: 5644.82861328125\n",
      "Epoch 1624, Train_Loss: 5317.64599609375, Val_Loss: 5644.53759765625\n",
      "Epoch 1625, Train_Loss: 5317.38720703125, Val_Loss: 5644.2421875\n",
      "Epoch 1626, Train_Loss: 5317.13525390625, Val_Loss: 5643.95068359375\n",
      "Epoch 1627, Train_Loss: 5316.79931640625, Val_Loss: 5643.63330078125\n",
      "Epoch 1628, Train_Loss: 5316.52783203125, Val_Loss: 5643.34619140625\n",
      "Epoch 1629, Train_Loss: 5316.27294921875, Val_Loss: 5643.0556640625\n",
      "Epoch 1630, Train_Loss: 5316.0068359375, Val_Loss: 5642.75537109375\n",
      "Epoch 1631, Train_Loss: 5315.7470703125, Val_Loss: 5642.46923828125\n",
      "Epoch 1632, Train_Loss: 5315.4697265625, Val_Loss: 5642.1630859375\n",
      "Epoch 1633, Train_Loss: 5315.2138671875, Val_Loss: 5641.8759765625\n",
      "Epoch 1634, Train_Loss: 5314.953125, Val_Loss: 5641.55712890625\n",
      "Epoch 1635, Train_Loss: 5314.7001953125, Val_Loss: 5641.28076171875\n",
      "Epoch 1636, Train_Loss: 5314.38134765625, Val_Loss: 5640.9658203125\n",
      "Epoch 1637, Train_Loss: 5314.01611328125, Val_Loss: 5640.6826171875\n",
      "Epoch 1638, Train_Loss: 5313.77099609375, Val_Loss: 5640.384765625\n",
      "Epoch 1639, Train_Loss: 5313.5107421875, Val_Loss: 5640.0986328125\n",
      "Epoch 1640, Train_Loss: 5313.2470703125, Val_Loss: 5639.78857421875\n",
      "Epoch 1641, Train_Loss: 5312.97216796875, Val_Loss: 5639.49658203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1642, Train_Loss: 5312.697265625, Val_Loss: 5639.201171875\n",
      "Epoch 1643, Train_Loss: 5312.4345703125, Val_Loss: 5638.908203125\n",
      "Epoch 1644, Train_Loss: 5312.16845703125, Val_Loss: 5638.60498046875\n",
      "Epoch 1645, Train_Loss: 5311.89208984375, Val_Loss: 5638.30419921875\n",
      "Epoch 1646, Train_Loss: 5311.64013671875, Val_Loss: 5637.994140625\n",
      "Epoch 1647, Train_Loss: 5311.36279296875, Val_Loss: 5637.70166015625\n",
      "Epoch 1648, Train_Loss: 5311.1015625, Val_Loss: 5637.41455078125\n",
      "Epoch 1649, Train_Loss: 5310.8466796875, Val_Loss: 5637.1103515625\n",
      "Epoch 1650, Train_Loss: 5310.56005859375, Val_Loss: 5636.82080078125\n",
      "Epoch 1651, Train_Loss: 5310.3046875, Val_Loss: 5636.54248046875\n",
      "Epoch 1652, Train_Loss: 5310.0478515625, Val_Loss: 5636.24267578125\n",
      "Epoch 1653, Train_Loss: 5309.78076171875, Val_Loss: 5635.9345703125\n",
      "Epoch 1654, Train_Loss: 5309.51416015625, Val_Loss: 5635.6005859375\n",
      "Epoch 1655, Train_Loss: 5309.248046875, Val_Loss: 5635.30859375\n",
      "Epoch 1656, Train_Loss: 5308.97314453125, Val_Loss: 5634.9990234375\n",
      "Epoch 1657, Train_Loss: 5308.728515625, Val_Loss: 5634.71630859375\n",
      "Epoch 1658, Train_Loss: 5308.47802734375, Val_Loss: 5634.43212890625\n",
      "Epoch 1659, Train_Loss: 5308.2109375, Val_Loss: 5634.12109375\n",
      "Epoch 1660, Train_Loss: 5307.9619140625, Val_Loss: 5633.8330078125\n",
      "Epoch 1661, Train_Loss: 5307.693359375, Val_Loss: 5633.52978515625\n",
      "Epoch 1662, Train_Loss: 5307.34033203125, Val_Loss: 5633.24365234375\n",
      "Epoch 1663, Train_Loss: 5307.08984375, Val_Loss: 5632.95458984375\n",
      "Epoch 1664, Train_Loss: 5306.845703125, Val_Loss: 5632.66015625\n",
      "Epoch 1665, Train_Loss: 5306.59619140625, Val_Loss: 5632.3798828125\n",
      "Epoch 1666, Train_Loss: 5306.3046875, Val_Loss: 5632.06396484375\n",
      "Epoch 1667, Train_Loss: 5306.037109375, Val_Loss: 5631.7763671875\n",
      "Epoch 1668, Train_Loss: 5305.76953125, Val_Loss: 5631.478515625\n",
      "Epoch 1669, Train_Loss: 5305.49755859375, Val_Loss: 5631.17919921875\n",
      "Epoch 1670, Train_Loss: 5305.2333984375, Val_Loss: 5630.88671875\n",
      "Epoch 1671, Train_Loss: 5304.958984375, Val_Loss: 5630.60302734375\n",
      "Epoch 1672, Train_Loss: 5304.7080078125, Val_Loss: 5630.2822265625\n",
      "Epoch 1673, Train_Loss: 5304.447265625, Val_Loss: 5629.986328125\n",
      "Epoch 1674, Train_Loss: 5304.1826171875, Val_Loss: 5629.7001953125\n",
      "Epoch 1675, Train_Loss: 5303.88671875, Val_Loss: 5629.3984375\n",
      "Epoch 1676, Train_Loss: 5303.6015625, Val_Loss: 5629.11083984375\n",
      "Epoch 1677, Train_Loss: 5303.3564453125, Val_Loss: 5628.82958984375\n",
      "Epoch 1678, Train_Loss: 5303.08056640625, Val_Loss: 5628.5185546875\n",
      "Epoch 1679, Train_Loss: 5302.82275390625, Val_Loss: 5628.224609375\n",
      "Epoch 1680, Train_Loss: 5302.55419921875, Val_Loss: 5627.93115234375\n",
      "Epoch 1681, Train_Loss: 5302.30859375, Val_Loss: 5627.6494140625\n",
      "Epoch 1682, Train_Loss: 5302.04052734375, Val_Loss: 5627.33984375\n",
      "Epoch 1683, Train_Loss: 5301.79052734375, Val_Loss: 5627.04833984375\n",
      "Epoch 1684, Train_Loss: 5301.546875, Val_Loss: 5626.744140625\n",
      "Epoch 1685, Train_Loss: 5301.2880859375, Val_Loss: 5626.431640625\n",
      "Epoch 1686, Train_Loss: 5301.03564453125, Val_Loss: 5626.14697265625\n",
      "Epoch 1687, Train_Loss: 5300.7734375, Val_Loss: 5625.8525390625\n",
      "Epoch 1688, Train_Loss: 5300.4697265625, Val_Loss: 5625.55859375\n",
      "Epoch 1689, Train_Loss: 5300.22314453125, Val_Loss: 5625.271484375\n",
      "Epoch 1690, Train_Loss: 5299.97119140625, Val_Loss: 5624.9853515625\n",
      "Epoch 1691, Train_Loss: 5299.70068359375, Val_Loss: 5624.63818359375\n",
      "Epoch 1692, Train_Loss: 5299.4404296875, Val_Loss: 5624.3466796875\n",
      "Epoch 1693, Train_Loss: 5299.18408203125, Val_Loss: 5624.056640625\n",
      "Epoch 1694, Train_Loss: 5298.93115234375, Val_Loss: 5623.7666015625\n",
      "Epoch 1695, Train_Loss: 5298.658203125, Val_Loss: 5623.47802734375\n",
      "Epoch 1696, Train_Loss: 5298.40673828125, Val_Loss: 5623.18701171875\n",
      "Epoch 1697, Train_Loss: 5298.14306640625, Val_Loss: 5622.89794921875\n",
      "Epoch 1698, Train_Loss: 5297.88037109375, Val_Loss: 5622.587890625\n",
      "Epoch 1699, Train_Loss: 5297.623046875, Val_Loss: 5622.2998046875\n",
      "Epoch 1700, Train_Loss: 5297.3330078125, Val_Loss: 5622.005859375\n",
      "Epoch 1701, Train_Loss: 5296.9921875, Val_Loss: 5621.70703125\n",
      "Epoch 1702, Train_Loss: 5296.73486328125, Val_Loss: 5621.42138671875\n",
      "Epoch 1703, Train_Loss: 5296.484375, Val_Loss: 5621.1279296875\n",
      "Epoch 1704, Train_Loss: 5296.20751953125, Val_Loss: 5620.81640625\n",
      "Epoch 1705, Train_Loss: 5295.9169921875, Val_Loss: 5620.5205078125\n",
      "Epoch 1706, Train_Loss: 5295.6650390625, Val_Loss: 5620.2578125\n",
      "Epoch 1707, Train_Loss: 5295.38671875, Val_Loss: 5619.9541015625\n",
      "Epoch 1708, Train_Loss: 5295.1357421875, Val_Loss: 5619.6513671875\n",
      "Epoch 1709, Train_Loss: 5294.73486328125, Val_Loss: 5619.36376953125\n",
      "Epoch 1710, Train_Loss: 5294.4599609375, Val_Loss: 5619.078125\n",
      "Epoch 1711, Train_Loss: 5294.1875, Val_Loss: 5618.759765625\n",
      "Epoch 1712, Train_Loss: 5293.935546875, Val_Loss: 5618.474609375\n",
      "Epoch 1713, Train_Loss: 5293.6806640625, Val_Loss: 5618.18603515625\n",
      "Epoch 1714, Train_Loss: 5293.3623046875, Val_Loss: 5617.89208984375\n",
      "Epoch 1715, Train_Loss: 5293.1083984375, Val_Loss: 5617.6015625\n",
      "Epoch 1716, Train_Loss: 5292.8583984375, Val_Loss: 5617.3017578125\n",
      "Epoch 1717, Train_Loss: 5292.59716796875, Val_Loss: 5616.9990234375\n",
      "Epoch 1718, Train_Loss: 5292.34814453125, Val_Loss: 5616.6982421875\n",
      "Epoch 1719, Train_Loss: 5292.07958984375, Val_Loss: 5616.408203125\n",
      "Epoch 1720, Train_Loss: 5291.82421875, Val_Loss: 5616.1044921875\n",
      "Epoch 1721, Train_Loss: 5291.56494140625, Val_Loss: 5615.8251953125\n",
      "Epoch 1722, Train_Loss: 5291.3056640625, Val_Loss: 5615.537109375\n",
      "Epoch 1723, Train_Loss: 5291.056640625, Val_Loss: 5615.22705078125\n",
      "Epoch 1724, Train_Loss: 5290.794921875, Val_Loss: 5614.94091796875\n",
      "Epoch 1725, Train_Loss: 5290.5390625, Val_Loss: 5614.64599609375\n",
      "Epoch 1726, Train_Loss: 5290.2783203125, Val_Loss: 5614.3662109375\n",
      "Epoch 1727, Train_Loss: 5289.94970703125, Val_Loss: 5614.0322265625\n",
      "Epoch 1728, Train_Loss: 5289.70068359375, Val_Loss: 5613.74609375\n",
      "Epoch 1729, Train_Loss: 5289.42724609375, Val_Loss: 5613.443359375\n",
      "Epoch 1730, Train_Loss: 5289.16943359375, Val_Loss: 5613.1474609375\n",
      "Epoch 1731, Train_Loss: 5288.90869140625, Val_Loss: 5612.8564453125\n",
      "Epoch 1732, Train_Loss: 5288.66259765625, Val_Loss: 5612.56884765625\n",
      "Epoch 1733, Train_Loss: 5288.4013671875, Val_Loss: 5612.27197265625\n",
      "Epoch 1734, Train_Loss: 5288.119140625, Val_Loss: 5611.99560546875\n",
      "Epoch 1735, Train_Loss: 5287.85400390625, Val_Loss: 5611.69189453125\n",
      "Epoch 1736, Train_Loss: 5287.591796875, Val_Loss: 5611.38623046875\n",
      "Epoch 1737, Train_Loss: 5287.35400390625, Val_Loss: 5611.09423828125\n",
      "Epoch 1738, Train_Loss: 5287.09814453125, Val_Loss: 5610.81298828125\n",
      "Epoch 1739, Train_Loss: 5286.74462890625, Val_Loss: 5610.52001953125\n",
      "Epoch 1740, Train_Loss: 5286.48828125, Val_Loss: 5610.2275390625\n",
      "Epoch 1741, Train_Loss: 5286.23291015625, Val_Loss: 5609.93701171875\n",
      "Epoch 1742, Train_Loss: 5285.99853515625, Val_Loss: 5609.6484375\n",
      "Epoch 1743, Train_Loss: 5285.7314453125, Val_Loss: 5609.33837890625\n",
      "Epoch 1744, Train_Loss: 5285.4453125, Val_Loss: 5609.05419921875\n",
      "Epoch 1745, Train_Loss: 5285.1630859375, Val_Loss: 5608.7421875\n",
      "Epoch 1746, Train_Loss: 5284.900390625, Val_Loss: 5608.4580078125\n",
      "Epoch 1747, Train_Loss: 5284.6357421875, Val_Loss: 5608.1689453125\n",
      "Epoch 1748, Train_Loss: 5284.36865234375, Val_Loss: 5607.88134765625\n",
      "Epoch 1749, Train_Loss: 5284.1044921875, Val_Loss: 5607.5703125\n",
      "Epoch 1750, Train_Loss: 5283.8525390625, Val_Loss: 5607.275390625\n",
      "Epoch 1751, Train_Loss: 5283.595703125, Val_Loss: 5606.98486328125\n",
      "Epoch 1752, Train_Loss: 5283.3125, Val_Loss: 5606.69921875\n",
      "Epoch 1753, Train_Loss: 5283.05078125, Val_Loss: 5606.39990234375\n",
      "Epoch 1754, Train_Loss: 5282.802734375, Val_Loss: 5606.10400390625\n",
      "Epoch 1755, Train_Loss: 5282.556640625, Val_Loss: 5605.80419921875\n",
      "Epoch 1756, Train_Loss: 5282.28857421875, Val_Loss: 5605.5146484375\n",
      "Epoch 1757, Train_Loss: 5282.03271484375, Val_Loss: 5605.224609375\n",
      "Epoch 1758, Train_Loss: 5281.77001953125, Val_Loss: 5604.9375\n",
      "Epoch 1759, Train_Loss: 5281.509765625, Val_Loss: 5604.64794921875\n",
      "Epoch 1760, Train_Loss: 5281.25732421875, Val_Loss: 5604.35546875\n",
      "Epoch 1761, Train_Loss: 5281.01123046875, Val_Loss: 5604.0673828125\n",
      "Epoch 1762, Train_Loss: 5280.74951171875, Val_Loss: 5603.759765625\n",
      "Epoch 1763, Train_Loss: 5280.494140625, Val_Loss: 5603.4375\n",
      "Epoch 1764, Train_Loss: 5280.22998046875, Val_Loss: 5603.14697265625\n",
      "Epoch 1765, Train_Loss: 5279.8857421875, Val_Loss: 5602.861328125\n",
      "Epoch 1766, Train_Loss: 5279.6279296875, Val_Loss: 5602.5771484375\n",
      "Epoch 1767, Train_Loss: 5279.36572265625, Val_Loss: 5602.275390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1768, Train_Loss: 5279.10400390625, Val_Loss: 5601.97216796875\n",
      "Epoch 1769, Train_Loss: 5278.84814453125, Val_Loss: 5601.7021484375\n",
      "Epoch 1770, Train_Loss: 5278.59619140625, Val_Loss: 5601.41015625\n",
      "Epoch 1771, Train_Loss: 5278.349609375, Val_Loss: 5601.1181640625\n",
      "Epoch 1772, Train_Loss: 5277.9853515625, Val_Loss: 5600.8154296875\n",
      "Epoch 1773, Train_Loss: 5277.7080078125, Val_Loss: 5600.52978515625\n",
      "Epoch 1774, Train_Loss: 5277.45458984375, Val_Loss: 5600.259765625\n",
      "Epoch 1775, Train_Loss: 5277.19287109375, Val_Loss: 5599.95361328125\n",
      "Epoch 1776, Train_Loss: 5276.9345703125, Val_Loss: 5599.65771484375\n",
      "Epoch 1777, Train_Loss: 5276.6845703125, Val_Loss: 5599.37548828125\n",
      "Epoch 1778, Train_Loss: 5276.3720703125, Val_Loss: 5599.0869140625\n",
      "Epoch 1779, Train_Loss: 5276.1298828125, Val_Loss: 5598.79296875\n",
      "Epoch 1780, Train_Loss: 5275.88525390625, Val_Loss: 5598.51123046875\n",
      "Epoch 1781, Train_Loss: 5275.564453125, Val_Loss: 5598.193359375\n",
      "Epoch 1782, Train_Loss: 5275.30517578125, Val_Loss: 5597.92138671875\n",
      "Epoch 1783, Train_Loss: 5275.04296875, Val_Loss: 5597.62939453125\n",
      "Epoch 1784, Train_Loss: 5274.7685546875, Val_Loss: 5597.32177734375\n",
      "Epoch 1785, Train_Loss: 5274.5224609375, Val_Loss: 5597.04150390625\n",
      "Epoch 1786, Train_Loss: 5274.2724609375, Val_Loss: 5596.75244140625\n",
      "Epoch 1787, Train_Loss: 5273.99267578125, Val_Loss: 5596.46337890625\n",
      "Epoch 1788, Train_Loss: 5273.7421875, Val_Loss: 5596.15771484375\n",
      "Epoch 1789, Train_Loss: 5273.4794921875, Val_Loss: 5595.87841796875\n",
      "Epoch 1790, Train_Loss: 5273.21826171875, Val_Loss: 5595.5693359375\n",
      "Epoch 1791, Train_Loss: 5272.919921875, Val_Loss: 5595.2900390625\n",
      "Epoch 1792, Train_Loss: 5272.64794921875, Val_Loss: 5595.00634765625\n",
      "Epoch 1793, Train_Loss: 5272.39794921875, Val_Loss: 5594.7275390625\n",
      "Epoch 1794, Train_Loss: 5272.12255859375, Val_Loss: 5594.41552734375\n",
      "Epoch 1795, Train_Loss: 5271.87451171875, Val_Loss: 5594.12841796875\n",
      "Epoch 1796, Train_Loss: 5271.62255859375, Val_Loss: 5593.8349609375\n",
      "Epoch 1797, Train_Loss: 5271.35546875, Val_Loss: 5593.53857421875\n",
      "Epoch 1798, Train_Loss: 5271.0908203125, Val_Loss: 5593.2578125\n",
      "Epoch 1799, Train_Loss: 5270.83447265625, Val_Loss: 5592.9345703125\n",
      "Epoch 1800, Train_Loss: 5270.5791015625, Val_Loss: 5592.6259765625\n",
      "Epoch 1801, Train_Loss: 5270.32861328125, Val_Loss: 5592.34375\n",
      "Epoch 1802, Train_Loss: 5270.052734375, Val_Loss: 5592.0556640625\n",
      "Epoch 1803, Train_Loss: 5269.81591796875, Val_Loss: 5591.77978515625\n",
      "Epoch 1804, Train_Loss: 5269.484375, Val_Loss: 5591.48095703125\n",
      "Epoch 1805, Train_Loss: 5269.23388671875, Val_Loss: 5591.20166015625\n",
      "Epoch 1806, Train_Loss: 5268.97119140625, Val_Loss: 5590.919921875\n",
      "Epoch 1807, Train_Loss: 5268.7158203125, Val_Loss: 5590.62841796875\n",
      "Epoch 1808, Train_Loss: 5268.46142578125, Val_Loss: 5590.3212890625\n",
      "Epoch 1809, Train_Loss: 5268.20654296875, Val_Loss: 5590.03271484375\n",
      "Epoch 1810, Train_Loss: 5267.9609375, Val_Loss: 5589.74560546875\n",
      "Epoch 1811, Train_Loss: 5267.69677734375, Val_Loss: 5589.45947265625\n",
      "Epoch 1812, Train_Loss: 5267.43701171875, Val_Loss: 5589.17822265625\n",
      "Epoch 1813, Train_Loss: 5267.18359375, Val_Loss: 5588.86669921875\n",
      "Epoch 1814, Train_Loss: 5266.927734375, Val_Loss: 5588.58984375\n",
      "Epoch 1815, Train_Loss: 5266.65380859375, Val_Loss: 5588.30419921875\n",
      "Epoch 1816, Train_Loss: 5266.39306640625, Val_Loss: 5588.01708984375\n",
      "Epoch 1817, Train_Loss: 5266.064453125, Val_Loss: 5587.7177734375\n",
      "Epoch 1818, Train_Loss: 5265.82421875, Val_Loss: 5587.435546875\n",
      "Epoch 1819, Train_Loss: 5265.5712890625, Val_Loss: 5587.14697265625\n",
      "Epoch 1820, Train_Loss: 5265.330078125, Val_Loss: 5586.83447265625\n",
      "Epoch 1821, Train_Loss: 5265.0615234375, Val_Loss: 5586.55078125\n",
      "Epoch 1822, Train_Loss: 5264.8056640625, Val_Loss: 5586.27197265625\n",
      "Epoch 1823, Train_Loss: 5264.5478515625, Val_Loss: 5585.97998046875\n",
      "Epoch 1824, Train_Loss: 5264.294921875, Val_Loss: 5585.70556640625\n",
      "Epoch 1825, Train_Loss: 5264.05029296875, Val_Loss: 5585.42138671875\n",
      "Epoch 1826, Train_Loss: 5263.7734375, Val_Loss: 5585.10498046875\n",
      "Epoch 1827, Train_Loss: 5263.5087890625, Val_Loss: 5584.82470703125\n",
      "Epoch 1828, Train_Loss: 5263.248046875, Val_Loss: 5584.53759765625\n",
      "Epoch 1829, Train_Loss: 5262.95068359375, Val_Loss: 5584.24609375\n",
      "Epoch 1830, Train_Loss: 5262.69873046875, Val_Loss: 5583.953125\n",
      "Epoch 1831, Train_Loss: 5262.435546875, Val_Loss: 5583.673828125\n",
      "Epoch 1832, Train_Loss: 5262.181640625, Val_Loss: 5583.38134765625\n",
      "Epoch 1833, Train_Loss: 5261.931640625, Val_Loss: 5583.07666015625\n",
      "Epoch 1834, Train_Loss: 5261.67138671875, Val_Loss: 5582.783203125\n",
      "Epoch 1835, Train_Loss: 5261.42529296875, Val_Loss: 5582.5048828125\n",
      "Epoch 1836, Train_Loss: 5261.1376953125, Val_Loss: 5582.1884765625\n",
      "Epoch 1837, Train_Loss: 5260.8779296875, Val_Loss: 5581.8994140625\n",
      "Epoch 1838, Train_Loss: 5260.6201171875, Val_Loss: 5581.61279296875\n",
      "Epoch 1839, Train_Loss: 5260.3681640625, Val_Loss: 5581.30322265625\n",
      "Epoch 1840, Train_Loss: 5260.1181640625, Val_Loss: 5581.01416015625\n",
      "Epoch 1841, Train_Loss: 5259.85791015625, Val_Loss: 5580.73974609375\n",
      "Epoch 1842, Train_Loss: 5259.52587890625, Val_Loss: 5580.4501953125\n",
      "Epoch 1843, Train_Loss: 5259.2724609375, Val_Loss: 5580.16455078125\n",
      "Epoch 1844, Train_Loss: 5259.02001953125, Val_Loss: 5579.869140625\n",
      "Epoch 1845, Train_Loss: 5258.7314453125, Val_Loss: 5579.5703125\n",
      "Epoch 1846, Train_Loss: 5258.4736328125, Val_Loss: 5579.2919921875\n",
      "Epoch 1847, Train_Loss: 5258.21728515625, Val_Loss: 5579.0009765625\n",
      "Epoch 1848, Train_Loss: 5257.962890625, Val_Loss: 5578.72900390625\n",
      "Epoch 1849, Train_Loss: 5257.70458984375, Val_Loss: 5578.42919921875\n",
      "Epoch 1850, Train_Loss: 5257.41943359375, Val_Loss: 5578.15283203125\n",
      "Epoch 1851, Train_Loss: 5257.171875, Val_Loss: 5577.859375\n",
      "Epoch 1852, Train_Loss: 5256.94091796875, Val_Loss: 5577.56201171875\n",
      "Epoch 1853, Train_Loss: 5256.69189453125, Val_Loss: 5577.2763671875\n",
      "Epoch 1854, Train_Loss: 5256.38037109375, Val_Loss: 5576.98095703125\n",
      "Epoch 1855, Train_Loss: 5256.08544921875, Val_Loss: 5576.7041015625\n",
      "Epoch 1856, Train_Loss: 5255.822265625, Val_Loss: 5576.427734375\n",
      "Epoch 1857, Train_Loss: 5255.5830078125, Val_Loss: 5576.13330078125\n",
      "Epoch 1858, Train_Loss: 5255.33203125, Val_Loss: 5575.83544921875\n",
      "Epoch 1859, Train_Loss: 5255.07373046875, Val_Loss: 5575.5439453125\n",
      "Epoch 1860, Train_Loss: 5254.80322265625, Val_Loss: 5575.2509765625\n",
      "Epoch 1861, Train_Loss: 5254.53515625, Val_Loss: 5574.9677734375\n",
      "Epoch 1862, Train_Loss: 5254.2822265625, Val_Loss: 5574.6865234375\n",
      "Epoch 1863, Train_Loss: 5254.02734375, Val_Loss: 5574.388671875\n",
      "Epoch 1864, Train_Loss: 5253.7646484375, Val_Loss: 5574.10888671875\n",
      "Epoch 1865, Train_Loss: 5253.50244140625, Val_Loss: 5573.80078125\n",
      "Epoch 1866, Train_Loss: 5253.2421875, Val_Loss: 5573.51318359375\n",
      "Epoch 1867, Train_Loss: 5252.98779296875, Val_Loss: 5573.236328125\n",
      "Epoch 1868, Train_Loss: 5252.65966796875, Val_Loss: 5572.94873046875\n",
      "Epoch 1869, Train_Loss: 5252.41015625, Val_Loss: 5572.66796875\n",
      "Epoch 1870, Train_Loss: 5252.13427734375, Val_Loss: 5572.37451171875\n",
      "Epoch 1871, Train_Loss: 5251.87744140625, Val_Loss: 5572.06982421875\n",
      "Epoch 1872, Train_Loss: 5251.619140625, Val_Loss: 5571.75537109375\n",
      "Epoch 1873, Train_Loss: 5251.37060546875, Val_Loss: 5571.46240234375\n",
      "Epoch 1874, Train_Loss: 5251.1123046875, Val_Loss: 5571.193359375\n",
      "Epoch 1875, Train_Loss: 5250.8525390625, Val_Loss: 5570.90283203125\n",
      "Epoch 1876, Train_Loss: 5250.587890625, Val_Loss: 5570.62646484375\n",
      "Epoch 1877, Train_Loss: 5250.3408203125, Val_Loss: 5570.3525390625\n",
      "Epoch 1878, Train_Loss: 5250.07177734375, Val_Loss: 5570.041015625\n",
      "Epoch 1879, Train_Loss: 5249.8193359375, Val_Loss: 5569.771484375\n",
      "Epoch 1880, Train_Loss: 5249.57568359375, Val_Loss: 5569.4921875\n",
      "Epoch 1881, Train_Loss: 5249.27294921875, Val_Loss: 5569.2041015625\n",
      "Epoch 1882, Train_Loss: 5249.01953125, Val_Loss: 5568.9169921875\n",
      "Epoch 1883, Train_Loss: 5248.77880859375, Val_Loss: 5568.6259765625\n",
      "Epoch 1884, Train_Loss: 5248.52685546875, Val_Loss: 5568.326171875\n",
      "Epoch 1885, Train_Loss: 5248.2783203125, Val_Loss: 5568.04931640625\n",
      "Epoch 1886, Train_Loss: 5248.02294921875, Val_Loss: 5567.75048828125\n",
      "Epoch 1887, Train_Loss: 5247.759765625, Val_Loss: 5567.47314453125\n",
      "Epoch 1888, Train_Loss: 5247.51025390625, Val_Loss: 5567.20654296875\n",
      "Epoch 1889, Train_Loss: 5247.23876953125, Val_Loss: 5566.92138671875\n",
      "Epoch 1890, Train_Loss: 5246.982421875, Val_Loss: 5566.6220703125\n",
      "Epoch 1891, Train_Loss: 5246.7216796875, Val_Loss: 5566.33154296875\n",
      "Epoch 1892, Train_Loss: 5246.45947265625, Val_Loss: 5566.05224609375\n",
      "Epoch 1893, Train_Loss: 5246.22021484375, Val_Loss: 5565.75830078125\n",
      "Epoch 1894, Train_Loss: 5245.91943359375, Val_Loss: 5565.46923828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1895, Train_Loss: 5245.67431640625, Val_Loss: 5565.20166015625\n",
      "Epoch 1896, Train_Loss: 5245.42578125, Val_Loss: 5564.9169921875\n",
      "Epoch 1897, Train_Loss: 5245.1650390625, Val_Loss: 5564.609375\n",
      "Epoch 1898, Train_Loss: 5244.923828125, Val_Loss: 5564.3212890625\n",
      "Epoch 1899, Train_Loss: 5244.64697265625, Val_Loss: 5564.02880859375\n",
      "Epoch 1900, Train_Loss: 5244.39208984375, Val_Loss: 5563.7490234375\n",
      "Epoch 1901, Train_Loss: 5244.15771484375, Val_Loss: 5563.47509765625\n",
      "Epoch 1902, Train_Loss: 5243.89794921875, Val_Loss: 5563.1865234375\n",
      "Epoch 1903, Train_Loss: 5243.64208984375, Val_Loss: 5562.8837890625\n",
      "Epoch 1904, Train_Loss: 5243.369140625, Val_Loss: 5562.59814453125\n",
      "Epoch 1905, Train_Loss: 5243.10595703125, Val_Loss: 5562.32861328125\n",
      "Epoch 1906, Train_Loss: 5242.85595703125, Val_Loss: 5562.05126953125\n",
      "Epoch 1907, Train_Loss: 5242.51953125, Val_Loss: 5561.755859375\n",
      "Epoch 1908, Train_Loss: 5242.27880859375, Val_Loss: 5561.43896484375\n",
      "Epoch 1909, Train_Loss: 5242.0224609375, Val_Loss: 5561.1494140625\n",
      "Epoch 1910, Train_Loss: 5241.7841796875, Val_Loss: 5560.8603515625\n",
      "Epoch 1911, Train_Loss: 5241.55712890625, Val_Loss: 5560.57861328125\n",
      "Epoch 1912, Train_Loss: 5241.32080078125, Val_Loss: 5560.2919921875\n",
      "Epoch 1913, Train_Loss: 5241.05908203125, Val_Loss: 5560.00830078125\n",
      "Epoch 1914, Train_Loss: 5240.8095703125, Val_Loss: 5559.72265625\n",
      "Epoch 1915, Train_Loss: 5240.5517578125, Val_Loss: 5559.443359375\n",
      "Epoch 1916, Train_Loss: 5240.28955078125, Val_Loss: 5559.13623046875\n",
      "Epoch 1917, Train_Loss: 5239.98095703125, Val_Loss: 5558.857421875\n",
      "Epoch 1918, Train_Loss: 5239.7216796875, Val_Loss: 5558.58349609375\n",
      "Epoch 1919, Train_Loss: 5239.4013671875, Val_Loss: 5558.29931640625\n",
      "Epoch 1920, Train_Loss: 5239.16455078125, Val_Loss: 5558.01171875\n",
      "Epoch 1921, Train_Loss: 5238.92626953125, Val_Loss: 5557.73291015625\n",
      "Epoch 1922, Train_Loss: 5238.68115234375, Val_Loss: 5557.455078125\n",
      "Epoch 1923, Train_Loss: 5238.41162109375, Val_Loss: 5557.15283203125\n",
      "Epoch 1924, Train_Loss: 5238.17236328125, Val_Loss: 5556.8818359375\n",
      "Epoch 1925, Train_Loss: 5237.9150390625, Val_Loss: 5556.59619140625\n",
      "Epoch 1926, Train_Loss: 5237.59033203125, Val_Loss: 5556.318359375\n",
      "Epoch 1927, Train_Loss: 5237.3369140625, Val_Loss: 5556.0263671875\n",
      "Epoch 1928, Train_Loss: 5237.06591796875, Val_Loss: 5555.7509765625\n",
      "Epoch 1929, Train_Loss: 5236.8134765625, Val_Loss: 5555.4482421875\n",
      "Epoch 1930, Train_Loss: 5236.56201171875, Val_Loss: 5555.1708984375\n",
      "Epoch 1931, Train_Loss: 5236.31494140625, Val_Loss: 5554.8896484375\n",
      "Epoch 1932, Train_Loss: 5236.0, Val_Loss: 5554.60791015625\n",
      "Epoch 1933, Train_Loss: 5235.72216796875, Val_Loss: 5554.3251953125\n",
      "Epoch 1934, Train_Loss: 5235.48046875, Val_Loss: 5554.04296875\n",
      "Epoch 1935, Train_Loss: 5235.2265625, Val_Loss: 5553.74267578125\n",
      "Epoch 1936, Train_Loss: 5234.95703125, Val_Loss: 5553.44140625\n",
      "Epoch 1937, Train_Loss: 5234.7041015625, Val_Loss: 5553.18310546875\n",
      "Epoch 1938, Train_Loss: 5234.42138671875, Val_Loss: 5552.9033203125\n",
      "Epoch 1939, Train_Loss: 5234.15771484375, Val_Loss: 5552.61181640625\n",
      "Epoch 1940, Train_Loss: 5233.91455078125, Val_Loss: 5552.3388671875\n",
      "Epoch 1941, Train_Loss: 5233.68115234375, Val_Loss: 5552.0556640625\n",
      "Epoch 1942, Train_Loss: 5233.43896484375, Val_Loss: 5551.74755859375\n",
      "Epoch 1943, Train_Loss: 5233.18115234375, Val_Loss: 5551.47314453125\n",
      "Epoch 1944, Train_Loss: 5232.92138671875, Val_Loss: 5551.158203125\n",
      "Epoch 1945, Train_Loss: 5232.580078125, Val_Loss: 5550.87744140625\n",
      "Epoch 1946, Train_Loss: 5232.3427734375, Val_Loss: 5550.607421875\n",
      "Epoch 1947, Train_Loss: 5232.09423828125, Val_Loss: 5550.31689453125\n",
      "Epoch 1948, Train_Loss: 5231.83154296875, Val_Loss: 5550.0244140625\n",
      "Epoch 1949, Train_Loss: 5231.5888671875, Val_Loss: 5549.73486328125\n",
      "Epoch 1950, Train_Loss: 5231.3447265625, Val_Loss: 5549.46484375\n",
      "Epoch 1951, Train_Loss: 5231.08447265625, Val_Loss: 5549.17626953125\n",
      "Epoch 1952, Train_Loss: 5230.8271484375, Val_Loss: 5548.888671875\n",
      "Epoch 1953, Train_Loss: 5230.56005859375, Val_Loss: 5548.61962890625\n",
      "Epoch 1954, Train_Loss: 5230.2978515625, Val_Loss: 5548.34228515625\n",
      "Epoch 1955, Train_Loss: 5230.04638671875, Val_Loss: 5548.03662109375\n",
      "Epoch 1956, Train_Loss: 5229.7939453125, Val_Loss: 5547.76123046875\n",
      "Epoch 1957, Train_Loss: 5229.5595703125, Val_Loss: 5547.48046875\n",
      "Epoch 1958, Train_Loss: 5229.26806640625, Val_Loss: 5547.19189453125\n",
      "Epoch 1959, Train_Loss: 5229.01513671875, Val_Loss: 5546.91552734375\n",
      "Epoch 1960, Train_Loss: 5228.779296875, Val_Loss: 5546.63134765625\n",
      "Epoch 1961, Train_Loss: 5228.53076171875, Val_Loss: 5546.34912109375\n",
      "Epoch 1962, Train_Loss: 5228.2861328125, Val_Loss: 5546.06396484375\n",
      "Epoch 1963, Train_Loss: 5228.021484375, Val_Loss: 5545.77783203125\n",
      "Epoch 1964, Train_Loss: 5227.783203125, Val_Loss: 5545.4990234375\n",
      "Epoch 1965, Train_Loss: 5227.5166015625, Val_Loss: 5545.21630859375\n",
      "Epoch 1966, Train_Loss: 5227.2841796875, Val_Loss: 5544.94140625\n",
      "Epoch 1967, Train_Loss: 5227.01513671875, Val_Loss: 5544.654296875\n",
      "Epoch 1968, Train_Loss: 5226.76953125, Val_Loss: 5544.35546875\n",
      "Epoch 1969, Train_Loss: 5226.5107421875, Val_Loss: 5544.08447265625\n",
      "Epoch 1970, Train_Loss: 5226.2607421875, Val_Loss: 5543.806640625\n",
      "Epoch 1971, Train_Loss: 5225.91748046875, Val_Loss: 5543.52880859375\n",
      "Epoch 1972, Train_Loss: 5225.65576171875, Val_Loss: 5543.24755859375\n",
      "Epoch 1973, Train_Loss: 5225.41650390625, Val_Loss: 5542.953125\n",
      "Epoch 1974, Train_Loss: 5225.1669921875, Val_Loss: 5542.6494140625\n",
      "Epoch 1975, Train_Loss: 5224.92138671875, Val_Loss: 5542.3740234375\n",
      "Epoch 1976, Train_Loss: 5224.69873046875, Val_Loss: 5542.09423828125\n",
      "Epoch 1977, Train_Loss: 5224.4306640625, Val_Loss: 5541.8212890625\n",
      "Epoch 1978, Train_Loss: 5224.1748046875, Val_Loss: 5541.52978515625\n",
      "Epoch 1979, Train_Loss: 5223.93310546875, Val_Loss: 5541.26416015625\n",
      "Epoch 1980, Train_Loss: 5223.68798828125, Val_Loss: 5540.97802734375\n",
      "Epoch 1981, Train_Loss: 5223.43310546875, Val_Loss: 5540.63671875\n",
      "Epoch 1982, Train_Loss: 5223.177734375, Val_Loss: 5540.37939453125\n",
      "Epoch 1983, Train_Loss: 5222.93603515625, Val_Loss: 5540.0986328125\n",
      "Epoch 1984, Train_Loss: 5222.63916015625, Val_Loss: 5539.8173828125\n",
      "Epoch 1985, Train_Loss: 5222.40087890625, Val_Loss: 5539.5400390625\n",
      "Epoch 1986, Train_Loss: 5222.17724609375, Val_Loss: 5539.25390625\n",
      "Epoch 1987, Train_Loss: 5221.90869140625, Val_Loss: 5538.9609375\n",
      "Epoch 1988, Train_Loss: 5221.66748046875, Val_Loss: 5538.6767578125\n",
      "Epoch 1989, Train_Loss: 5221.4140625, Val_Loss: 5538.41259765625\n",
      "Epoch 1990, Train_Loss: 5221.13525390625, Val_Loss: 5538.12939453125\n",
      "Epoch 1991, Train_Loss: 5220.88525390625, Val_Loss: 5537.84619140625\n",
      "Epoch 1992, Train_Loss: 5220.6279296875, Val_Loss: 5537.57470703125\n",
      "Epoch 1993, Train_Loss: 5220.369140625, Val_Loss: 5537.29296875\n",
      "Epoch 1994, Train_Loss: 5220.11865234375, Val_Loss: 5537.0029296875\n",
      "Epoch 1995, Train_Loss: 5219.87451171875, Val_Loss: 5536.72265625\n",
      "Epoch 1996, Train_Loss: 5219.63037109375, Val_Loss: 5536.4404296875\n",
      "Epoch 1997, Train_Loss: 5219.3359375, Val_Loss: 5536.1650390625\n",
      "Epoch 1998, Train_Loss: 5219.1025390625, Val_Loss: 5535.8876953125\n",
      "Epoch 1999, Train_Loss: 5218.75537109375, Val_Loss: 5535.61669921875\n",
      "Epoch 2000, Train_Loss: 5218.498046875, Val_Loss: 5535.30517578125\n",
      "Epoch 2001, Train_Loss: 5218.24072265625, Val_Loss: 5535.04052734375\n",
      "Epoch 2002, Train_Loss: 5217.97509765625, Val_Loss: 5534.76123046875\n",
      "Epoch 2003, Train_Loss: 5217.72119140625, Val_Loss: 5534.47900390625\n",
      "Epoch 2004, Train_Loss: 5217.4619140625, Val_Loss: 5534.21142578125\n",
      "Epoch 2005, Train_Loss: 5217.22265625, Val_Loss: 5533.92822265625\n",
      "Epoch 2006, Train_Loss: 5216.96728515625, Val_Loss: 5533.634765625\n",
      "Epoch 2007, Train_Loss: 5216.72021484375, Val_Loss: 5533.34765625\n",
      "Epoch 2008, Train_Loss: 5216.455078125, Val_Loss: 5533.0498046875\n",
      "Epoch 2009, Train_Loss: 5216.21142578125, Val_Loss: 5532.77490234375\n",
      "Epoch 2010, Train_Loss: 5215.90283203125, Val_Loss: 5532.5009765625\n",
      "Epoch 2011, Train_Loss: 5215.66259765625, Val_Loss: 5532.22021484375\n",
      "Epoch 2012, Train_Loss: 5215.4111328125, Val_Loss: 5531.95703125\n",
      "Epoch 2013, Train_Loss: 5215.15771484375, Val_Loss: 5531.6494140625\n",
      "Epoch 2014, Train_Loss: 5214.9150390625, Val_Loss: 5531.37646484375\n",
      "Epoch 2015, Train_Loss: 5214.68115234375, Val_Loss: 5531.09326171875\n",
      "Epoch 2016, Train_Loss: 5214.400390625, Val_Loss: 5530.81494140625\n",
      "Epoch 2017, Train_Loss: 5214.12060546875, Val_Loss: 5530.49951171875\n",
      "Epoch 2018, Train_Loss: 5213.87451171875, Val_Loss: 5530.22998046875\n",
      "Epoch 2019, Train_Loss: 5213.6376953125, Val_Loss: 5529.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2020, Train_Loss: 5213.3720703125, Val_Loss: 5529.66259765625\n",
      "Epoch 2021, Train_Loss: 5213.11572265625, Val_Loss: 5529.38671875\n",
      "Epoch 2022, Train_Loss: 5212.86328125, Val_Loss: 5529.10546875\n",
      "Epoch 2023, Train_Loss: 5212.5517578125, Val_Loss: 5528.82861328125\n",
      "Epoch 2024, Train_Loss: 5212.28564453125, Val_Loss: 5528.54443359375\n",
      "Epoch 2025, Train_Loss: 5212.04052734375, Val_Loss: 5528.26318359375\n",
      "Epoch 2026, Train_Loss: 5211.78759765625, Val_Loss: 5527.97607421875\n",
      "Epoch 2027, Train_Loss: 5211.552734375, Val_Loss: 5527.69580078125\n",
      "Epoch 2028, Train_Loss: 5211.310546875, Val_Loss: 5527.423828125\n",
      "Epoch 2029, Train_Loss: 5211.05859375, Val_Loss: 5527.14208984375\n",
      "Epoch 2030, Train_Loss: 5210.8173828125, Val_Loss: 5526.86279296875\n",
      "Epoch 2031, Train_Loss: 5210.5634765625, Val_Loss: 5526.59619140625\n",
      "Epoch 2032, Train_Loss: 5210.31982421875, Val_Loss: 5526.3154296875\n",
      "Epoch 2033, Train_Loss: 5210.068359375, Val_Loss: 5526.0166015625\n",
      "Epoch 2034, Train_Loss: 5209.81494140625, Val_Loss: 5525.74072265625\n",
      "Epoch 2035, Train_Loss: 5209.55810546875, Val_Loss: 5525.46337890625\n",
      "Epoch 2036, Train_Loss: 5209.24609375, Val_Loss: 5525.18701171875\n",
      "Epoch 2037, Train_Loss: 5208.99755859375, Val_Loss: 5524.90380859375\n",
      "Epoch 2038, Train_Loss: 5208.76123046875, Val_Loss: 5524.630859375\n",
      "Epoch 2039, Train_Loss: 5208.5166015625, Val_Loss: 5524.3203125\n",
      "Epoch 2040, Train_Loss: 5208.27783203125, Val_Loss: 5524.05419921875\n",
      "Epoch 2041, Train_Loss: 5208.01611328125, Val_Loss: 5523.771484375\n",
      "Epoch 2042, Train_Loss: 5207.76416015625, Val_Loss: 5523.4951171875\n",
      "Epoch 2043, Train_Loss: 5207.52978515625, Val_Loss: 5523.23779296875\n",
      "Epoch 2044, Train_Loss: 5207.29345703125, Val_Loss: 5522.9580078125\n",
      "Epoch 2045, Train_Loss: 5207.04052734375, Val_Loss: 5522.6455078125\n",
      "Epoch 2046, Train_Loss: 5206.77001953125, Val_Loss: 5522.3662109375\n",
      "Epoch 2047, Train_Loss: 5206.50927734375, Val_Loss: 5522.08740234375\n",
      "Epoch 2048, Train_Loss: 5206.26318359375, Val_Loss: 5521.8203125\n",
      "Epoch 2049, Train_Loss: 5205.9423828125, Val_Loss: 5521.5390625\n",
      "Epoch 2050, Train_Loss: 5205.69287109375, Val_Loss: 5521.26513671875\n",
      "Epoch 2051, Train_Loss: 5205.44580078125, Val_Loss: 5520.9970703125\n",
      "Epoch 2052, Train_Loss: 5205.19580078125, Val_Loss: 5520.70703125\n",
      "Epoch 2053, Train_Loss: 5204.95947265625, Val_Loss: 5520.427734375\n",
      "Epoch 2054, Train_Loss: 5204.71337890625, Val_Loss: 5520.1171875\n",
      "Epoch 2055, Train_Loss: 5204.41650390625, Val_Loss: 5519.82421875\n",
      "Epoch 2056, Train_Loss: 5204.1845703125, Val_Loss: 5519.56298828125\n",
      "Epoch 2057, Train_Loss: 5203.9384765625, Val_Loss: 5519.28515625\n",
      "Epoch 2058, Train_Loss: 5203.69921875, Val_Loss: 5518.97314453125\n",
      "Epoch 2059, Train_Loss: 5203.455078125, Val_Loss: 5518.71044921875\n",
      "Epoch 2060, Train_Loss: 5203.19287109375, Val_Loss: 5518.43310546875\n",
      "Epoch 2061, Train_Loss: 5202.958984375, Val_Loss: 5518.15869140625\n",
      "Epoch 2062, Train_Loss: 5202.6904296875, Val_Loss: 5517.89453125\n",
      "Epoch 2063, Train_Loss: 5202.44091796875, Val_Loss: 5517.60546875\n",
      "Epoch 2064, Train_Loss: 5202.20068359375, Val_Loss: 5517.32275390625\n",
      "Epoch 2065, Train_Loss: 5201.93603515625, Val_Loss: 5517.04443359375\n",
      "Epoch 2066, Train_Loss: 5201.701171875, Val_Loss: 5516.7626953125\n",
      "Epoch 2067, Train_Loss: 5201.46142578125, Val_Loss: 5516.484375\n",
      "Epoch 2068, Train_Loss: 5201.2119140625, Val_Loss: 5516.212890625\n",
      "Epoch 2069, Train_Loss: 5200.9775390625, Val_Loss: 5515.939453125\n",
      "Epoch 2070, Train_Loss: 5200.72314453125, Val_Loss: 5515.6650390625\n",
      "Epoch 2071, Train_Loss: 5200.46484375, Val_Loss: 5515.3701171875\n",
      "Epoch 2072, Train_Loss: 5200.14697265625, Val_Loss: 5515.0947265625\n",
      "Epoch 2073, Train_Loss: 5199.90576171875, Val_Loss: 5514.8271484375\n",
      "Epoch 2074, Train_Loss: 5199.67236328125, Val_Loss: 5514.55126953125\n",
      "Epoch 2075, Train_Loss: 5199.33203125, Val_Loss: 5514.2734375\n",
      "Epoch 2076, Train_Loss: 5199.08251953125, Val_Loss: 5513.99609375\n",
      "Epoch 2077, Train_Loss: 5198.845703125, Val_Loss: 5513.72509765625\n",
      "Epoch 2078, Train_Loss: 5198.60302734375, Val_Loss: 5513.4296875\n",
      "Epoch 2079, Train_Loss: 5198.35302734375, Val_Loss: 5513.15283203125\n",
      "Epoch 2080, Train_Loss: 5198.1044921875, Val_Loss: 5512.8720703125\n",
      "Epoch 2081, Train_Loss: 5197.85546875, Val_Loss: 5512.57958984375\n",
      "Epoch 2082, Train_Loss: 5197.6259765625, Val_Loss: 5512.306640625\n",
      "Epoch 2083, Train_Loss: 5197.380859375, Val_Loss: 5512.0380859375\n",
      "Epoch 2084, Train_Loss: 5197.1279296875, Val_Loss: 5511.73583984375\n",
      "Epoch 2085, Train_Loss: 5196.865234375, Val_Loss: 5511.46923828125\n",
      "Epoch 2086, Train_Loss: 5196.626953125, Val_Loss: 5511.19091796875\n",
      "Epoch 2087, Train_Loss: 5196.3779296875, Val_Loss: 5510.9248046875\n",
      "Epoch 2088, Train_Loss: 5196.0771484375, Val_Loss: 5510.63916015625\n",
      "Epoch 2089, Train_Loss: 5195.85888671875, Val_Loss: 5510.37451171875\n",
      "Epoch 2090, Train_Loss: 5195.58837890625, Val_Loss: 5510.0576171875\n",
      "Epoch 2091, Train_Loss: 5195.34375, Val_Loss: 5509.76025390625\n",
      "Epoch 2092, Train_Loss: 5195.0908203125, Val_Loss: 5509.50341796875\n",
      "Epoch 2093, Train_Loss: 5194.869140625, Val_Loss: 5509.2158203125\n",
      "Epoch 2094, Train_Loss: 5194.58349609375, Val_Loss: 5508.94482421875\n",
      "Epoch 2095, Train_Loss: 5194.33740234375, Val_Loss: 5508.67578125\n",
      "Epoch 2096, Train_Loss: 5194.091796875, Val_Loss: 5508.3935546875\n",
      "Epoch 2097, Train_Loss: 5193.85400390625, Val_Loss: 5508.09423828125\n",
      "Epoch 2098, Train_Loss: 5193.6201171875, Val_Loss: 5507.828125\n",
      "Epoch 2099, Train_Loss: 5193.3505859375, Val_Loss: 5507.54541015625\n",
      "Epoch 2100, Train_Loss: 5193.0625, Val_Loss: 5507.25927734375\n",
      "Epoch 2101, Train_Loss: 5192.82470703125, Val_Loss: 5506.99755859375\n",
      "Epoch 2102, Train_Loss: 5192.58154296875, Val_Loss: 5506.7255859375\n",
      "Epoch 2103, Train_Loss: 5192.33203125, Val_Loss: 5506.4521484375\n",
      "Epoch 2104, Train_Loss: 5192.06103515625, Val_Loss: 5506.16064453125\n",
      "Epoch 2105, Train_Loss: 5191.7978515625, Val_Loss: 5505.89501953125\n",
      "Epoch 2106, Train_Loss: 5191.54736328125, Val_Loss: 5505.60302734375\n",
      "Epoch 2107, Train_Loss: 5191.30859375, Val_Loss: 5505.32958984375\n",
      "Epoch 2108, Train_Loss: 5191.07470703125, Val_Loss: 5505.064453125\n",
      "Epoch 2109, Train_Loss: 5190.82373046875, Val_Loss: 5504.77978515625\n",
      "Epoch 2110, Train_Loss: 5190.57861328125, Val_Loss: 5504.4931640625\n",
      "Epoch 2111, Train_Loss: 5190.337890625, Val_Loss: 5504.2294921875\n",
      "Epoch 2112, Train_Loss: 5190.09912109375, Val_Loss: 5503.9482421875\n",
      "Epoch 2113, Train_Loss: 5189.7880859375, Val_Loss: 5503.671875\n",
      "Epoch 2114, Train_Loss: 5189.5283203125, Val_Loss: 5503.3935546875\n",
      "Epoch 2115, Train_Loss: 5189.30322265625, Val_Loss: 5503.11572265625\n",
      "Epoch 2116, Train_Loss: 5189.0556640625, Val_Loss: 5502.8388671875\n",
      "Epoch 2117, Train_Loss: 5188.81591796875, Val_Loss: 5502.54931640625\n",
      "Epoch 2118, Train_Loss: 5188.564453125, Val_Loss: 5502.2666015625\n",
      "Epoch 2119, Train_Loss: 5188.32177734375, Val_Loss: 5501.990234375\n",
      "Epoch 2120, Train_Loss: 5188.064453125, Val_Loss: 5501.71337890625\n",
      "Epoch 2121, Train_Loss: 5187.8154296875, Val_Loss: 5501.44580078125\n",
      "Epoch 2122, Train_Loss: 5187.5673828125, Val_Loss: 5501.16259765625\n",
      "Epoch 2123, Train_Loss: 5187.3359375, Val_Loss: 5500.875\n",
      "Epoch 2124, Train_Loss: 5187.08447265625, Val_Loss: 5500.60546875\n",
      "Epoch 2125, Train_Loss: 5186.84228515625, Val_Loss: 5500.33154296875\n",
      "Epoch 2126, Train_Loss: 5186.5361328125, Val_Loss: 5500.04736328125\n",
      "Epoch 2127, Train_Loss: 5186.2939453125, Val_Loss: 5499.7529296875\n",
      "Epoch 2128, Train_Loss: 5186.05322265625, Val_Loss: 5499.4755859375\n",
      "Epoch 2129, Train_Loss: 5185.78515625, Val_Loss: 5499.2021484375\n",
      "Epoch 2130, Train_Loss: 5185.54638671875, Val_Loss: 5498.900390625\n",
      "Epoch 2131, Train_Loss: 5185.298828125, Val_Loss: 5498.63623046875\n",
      "Epoch 2132, Train_Loss: 5185.05712890625, Val_Loss: 5498.353515625\n",
      "Epoch 2133, Train_Loss: 5184.80908203125, Val_Loss: 5498.08447265625\n",
      "Epoch 2134, Train_Loss: 5184.5390625, Val_Loss: 5497.80712890625\n",
      "Epoch 2135, Train_Loss: 5184.27978515625, Val_Loss: 5497.53173828125\n",
      "Epoch 2136, Train_Loss: 5184.02197265625, Val_Loss: 5497.2470703125\n",
      "Epoch 2137, Train_Loss: 5183.7705078125, Val_Loss: 5496.966796875\n",
      "Epoch 2138, Train_Loss: 5183.52197265625, Val_Loss: 5496.70166015625\n",
      "Epoch 2139, Train_Loss: 5183.21875, Val_Loss: 5496.43896484375\n",
      "Epoch 2140, Train_Loss: 5182.97998046875, Val_Loss: 5496.16357421875\n",
      "Epoch 2141, Train_Loss: 5182.73046875, Val_Loss: 5495.8876953125\n",
      "Epoch 2142, Train_Loss: 5182.5, Val_Loss: 5495.607421875\n",
      "Epoch 2143, Train_Loss: 5182.23974609375, Val_Loss: 5495.3203125\n",
      "Epoch 2144, Train_Loss: 5182.00244140625, Val_Loss: 5495.05712890625\n",
      "Epoch 2145, Train_Loss: 5181.56396484375, Val_Loss: 5494.76904296875\n",
      "Epoch 2146, Train_Loss: 5181.32275390625, Val_Loss: 5494.48974609375\n",
      "Epoch 2147, Train_Loss: 5181.07861328125, Val_Loss: 5494.21337890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2148, Train_Loss: 5180.828125, Val_Loss: 5493.9541015625\n",
      "Epoch 2149, Train_Loss: 5180.56689453125, Val_Loss: 5493.669921875\n",
      "Epoch 2150, Train_Loss: 5180.32373046875, Val_Loss: 5493.39501953125\n",
      "Epoch 2151, Train_Loss: 5180.07373046875, Val_Loss: 5493.13330078125\n",
      "Epoch 2152, Train_Loss: 5179.751953125, Val_Loss: 5492.8525390625\n",
      "Epoch 2153, Train_Loss: 5179.5068359375, Val_Loss: 5492.57568359375\n",
      "Epoch 2154, Train_Loss: 5179.26611328125, Val_Loss: 5492.28515625\n",
      "Epoch 2155, Train_Loss: 5179.03369140625, Val_Loss: 5492.015625\n",
      "Epoch 2156, Train_Loss: 5178.794921875, Val_Loss: 5491.71923828125\n",
      "Epoch 2157, Train_Loss: 5178.55078125, Val_Loss: 5491.45166015625\n",
      "Epoch 2158, Train_Loss: 5178.310546875, Val_Loss: 5491.17041015625\n",
      "Epoch 2159, Train_Loss: 5178.0615234375, Val_Loss: 5490.896484375\n",
      "Epoch 2160, Train_Loss: 5177.8193359375, Val_Loss: 5490.63623046875\n",
      "Epoch 2161, Train_Loss: 5177.587890625, Val_Loss: 5490.36865234375\n",
      "Epoch 2162, Train_Loss: 5177.36376953125, Val_Loss: 5490.07177734375\n",
      "Epoch 2163, Train_Loss: 5177.07470703125, Val_Loss: 5489.7666015625\n",
      "Epoch 2164, Train_Loss: 5176.828125, Val_Loss: 5489.50244140625\n",
      "Epoch 2165, Train_Loss: 5176.56787109375, Val_Loss: 5489.23388671875\n",
      "Epoch 2166, Train_Loss: 5176.33740234375, Val_Loss: 5488.96044921875\n",
      "Epoch 2167, Train_Loss: 5176.10107421875, Val_Loss: 5488.69580078125\n",
      "Epoch 2168, Train_Loss: 5175.84765625, Val_Loss: 5488.41455078125\n",
      "Epoch 2169, Train_Loss: 5175.6201171875, Val_Loss: 5488.13427734375\n",
      "Epoch 2170, Train_Loss: 5175.37548828125, Val_Loss: 5487.86962890625\n",
      "Epoch 2171, Train_Loss: 5175.125, Val_Loss: 5487.5888671875\n",
      "Epoch 2172, Train_Loss: 5174.88671875, Val_Loss: 5487.29541015625\n",
      "Epoch 2173, Train_Loss: 5174.63134765625, Val_Loss: 5487.03955078125\n",
      "Epoch 2174, Train_Loss: 5174.37890625, Val_Loss: 5486.7666015625\n",
      "Epoch 2175, Train_Loss: 5174.12451171875, Val_Loss: 5486.470703125\n",
      "Epoch 2176, Train_Loss: 5173.8798828125, Val_Loss: 5486.21044921875\n",
      "Epoch 2177, Train_Loss: 5173.62548828125, Val_Loss: 5485.9345703125\n",
      "Epoch 2178, Train_Loss: 5173.2890625, Val_Loss: 5485.658203125\n",
      "Epoch 2179, Train_Loss: 5173.0458984375, Val_Loss: 5485.37841796875\n",
      "Epoch 2180, Train_Loss: 5172.82470703125, Val_Loss: 5485.10595703125\n",
      "Epoch 2181, Train_Loss: 5172.57568359375, Val_Loss: 5484.81298828125\n",
      "Epoch 2182, Train_Loss: 5172.3203125, Val_Loss: 5484.53857421875\n",
      "Epoch 2183, Train_Loss: 5172.0703125, Val_Loss: 5484.26416015625\n",
      "Epoch 2184, Train_Loss: 5171.80810546875, Val_Loss: 5483.9990234375\n",
      "Epoch 2185, Train_Loss: 5171.5693359375, Val_Loss: 5483.71875\n",
      "Epoch 2186, Train_Loss: 5171.32275390625, Val_Loss: 5483.44287109375\n",
      "Epoch 2187, Train_Loss: 5171.095703125, Val_Loss: 5483.17822265625\n",
      "Epoch 2188, Train_Loss: 5170.8583984375, Val_Loss: 5482.88525390625\n",
      "Epoch 2189, Train_Loss: 5170.6240234375, Val_Loss: 5482.60986328125\n",
      "Epoch 2190, Train_Loss: 5170.38623046875, Val_Loss: 5482.34521484375\n",
      "Epoch 2191, Train_Loss: 5170.0908203125, Val_Loss: 5482.05615234375\n",
      "Epoch 2192, Train_Loss: 5169.857421875, Val_Loss: 5481.7900390625\n",
      "Epoch 2193, Train_Loss: 5169.62646484375, Val_Loss: 5481.5166015625\n",
      "Epoch 2194, Train_Loss: 5169.376953125, Val_Loss: 5481.23583984375\n",
      "Epoch 2195, Train_Loss: 5169.1552734375, Val_Loss: 5480.9609375\n",
      "Epoch 2196, Train_Loss: 5168.8994140625, Val_Loss: 5480.68505859375\n",
      "Epoch 2197, Train_Loss: 5168.6474609375, Val_Loss: 5480.423828125\n",
      "Epoch 2198, Train_Loss: 5168.40625, Val_Loss: 5480.14697265625\n",
      "Epoch 2199, Train_Loss: 5168.16455078125, Val_Loss: 5479.873046875\n",
      "Epoch 2200, Train_Loss: 5167.91357421875, Val_Loss: 5479.56201171875\n",
      "Epoch 2201, Train_Loss: 5167.68701171875, Val_Loss: 5479.27734375\n",
      "Epoch 2202, Train_Loss: 5167.41748046875, Val_Loss: 5479.00634765625\n",
      "Epoch 2203, Train_Loss: 5167.18896484375, Val_Loss: 5478.74462890625\n",
      "Epoch 2204, Train_Loss: 5166.89990234375, Val_Loss: 5478.4736328125\n",
      "Epoch 2205, Train_Loss: 5166.6630859375, Val_Loss: 5478.20263671875\n",
      "Epoch 2206, Train_Loss: 5166.4375, Val_Loss: 5477.92333984375\n",
      "Epoch 2207, Train_Loss: 5166.16552734375, Val_Loss: 5477.6396484375\n",
      "Epoch 2208, Train_Loss: 5165.9248046875, Val_Loss: 5477.37890625\n",
      "Epoch 2209, Train_Loss: 5165.61181640625, Val_Loss: 5477.10693359375\n",
      "Epoch 2210, Train_Loss: 5165.37158203125, Val_Loss: 5476.83740234375\n",
      "Epoch 2211, Train_Loss: 5165.1298828125, Val_Loss: 5476.56103515625\n",
      "Epoch 2212, Train_Loss: 5164.87109375, Val_Loss: 5476.28857421875\n",
      "Epoch 2213, Train_Loss: 5164.630859375, Val_Loss: 5476.02490234375\n",
      "Epoch 2214, Train_Loss: 5164.392578125, Val_Loss: 5475.7373046875\n",
      "Epoch 2215, Train_Loss: 5164.1640625, Val_Loss: 5475.46875\n",
      "Epoch 2216, Train_Loss: 5163.92236328125, Val_Loss: 5475.1943359375\n",
      "Epoch 2217, Train_Loss: 5163.5986328125, Val_Loss: 5474.9208984375\n",
      "Epoch 2218, Train_Loss: 5163.291015625, Val_Loss: 5474.6494140625\n",
      "Epoch 2219, Train_Loss: 5163.0595703125, Val_Loss: 5474.37890625\n",
      "Epoch 2220, Train_Loss: 5162.8115234375, Val_Loss: 5474.0947265625\n",
      "Epoch 2221, Train_Loss: 5162.5771484375, Val_Loss: 5473.826171875\n",
      "Epoch 2222, Train_Loss: 5162.3193359375, Val_Loss: 5473.5615234375\n",
      "Epoch 2223, Train_Loss: 5162.08447265625, Val_Loss: 5473.2822265625\n",
      "Epoch 2224, Train_Loss: 5161.8388671875, Val_Loss: 5473.009765625\n",
      "Epoch 2225, Train_Loss: 5161.609375, Val_Loss: 5472.751953125\n",
      "Epoch 2226, Train_Loss: 5161.36083984375, Val_Loss: 5472.466796875\n",
      "Epoch 2227, Train_Loss: 5161.10302734375, Val_Loss: 5472.17431640625\n",
      "Epoch 2228, Train_Loss: 5160.85595703125, Val_Loss: 5471.90576171875\n",
      "Epoch 2229, Train_Loss: 5160.6171875, Val_Loss: 5471.62451171875\n",
      "Epoch 2230, Train_Loss: 5160.3232421875, Val_Loss: 5471.361328125\n",
      "Epoch 2231, Train_Loss: 5160.0859375, Val_Loss: 5471.07958984375\n",
      "Epoch 2232, Train_Loss: 5159.86083984375, Val_Loss: 5470.8173828125\n",
      "Epoch 2233, Train_Loss: 5159.62841796875, Val_Loss: 5470.52490234375\n",
      "Epoch 2234, Train_Loss: 5159.38916015625, Val_Loss: 5470.26123046875\n",
      "Epoch 2235, Train_Loss: 5159.14794921875, Val_Loss: 5469.9951171875\n",
      "Epoch 2236, Train_Loss: 5158.91650390625, Val_Loss: 5469.71240234375\n",
      "Epoch 2237, Train_Loss: 5158.6591796875, Val_Loss: 5469.404296875\n",
      "Epoch 2238, Train_Loss: 5158.4287109375, Val_Loss: 5469.14697265625\n",
      "Epoch 2239, Train_Loss: 5158.17041015625, Val_Loss: 5468.8720703125\n",
      "Epoch 2240, Train_Loss: 5157.93408203125, Val_Loss: 5468.5830078125\n",
      "Epoch 2241, Train_Loss: 5157.7021484375, Val_Loss: 5468.3173828125\n",
      "Epoch 2242, Train_Loss: 5157.431640625, Val_Loss: 5468.04541015625\n",
      "Epoch 2243, Train_Loss: 5157.119140625, Val_Loss: 5467.77099609375\n",
      "Epoch 2244, Train_Loss: 5156.89208984375, Val_Loss: 5467.5146484375\n",
      "Epoch 2245, Train_Loss: 5156.642578125, Val_Loss: 5467.23974609375\n",
      "Epoch 2246, Train_Loss: 5156.3935546875, Val_Loss: 5466.94677734375\n",
      "Epoch 2247, Train_Loss: 5156.16064453125, Val_Loss: 5466.67626953125\n",
      "Epoch 2248, Train_Loss: 5155.9296875, Val_Loss: 5466.4150390625\n",
      "Epoch 2249, Train_Loss: 5155.67626953125, Val_Loss: 5466.14306640625\n",
      "Epoch 2250, Train_Loss: 5155.44189453125, Val_Loss: 5465.86474609375\n",
      "Epoch 2251, Train_Loss: 5155.1689453125, Val_Loss: 5465.58447265625\n",
      "Epoch 2252, Train_Loss: 5154.912109375, Val_Loss: 5465.3203125\n",
      "Epoch 2253, Train_Loss: 5154.6630859375, Val_Loss: 5465.033203125\n",
      "Epoch 2254, Train_Loss: 5154.4140625, Val_Loss: 5464.76025390625\n",
      "Epoch 2255, Train_Loss: 5154.15966796875, Val_Loss: 5464.47705078125\n",
      "Epoch 2256, Train_Loss: 5153.8193359375, Val_Loss: 5464.21533203125\n",
      "Epoch 2257, Train_Loss: 5153.58349609375, Val_Loss: 5463.94580078125\n",
      "Epoch 2258, Train_Loss: 5153.34765625, Val_Loss: 5463.693359375\n",
      "Epoch 2259, Train_Loss: 5153.111328125, Val_Loss: 5463.4052734375\n",
      "Epoch 2260, Train_Loss: 5152.8857421875, Val_Loss: 5463.1240234375\n",
      "Epoch 2261, Train_Loss: 5152.62353515625, Val_Loss: 5462.8623046875\n",
      "Epoch 2262, Train_Loss: 5152.38671875, Val_Loss: 5462.6015625\n",
      "Epoch 2263, Train_Loss: 5152.154296875, Val_Loss: 5462.33251953125\n",
      "Epoch 2264, Train_Loss: 5151.919921875, Val_Loss: 5462.0537109375\n",
      "Epoch 2265, Train_Loss: 5151.68896484375, Val_Loss: 5461.7802734375\n",
      "Epoch 2266, Train_Loss: 5151.4423828125, Val_Loss: 5461.48486328125\n",
      "Epoch 2267, Train_Loss: 5151.19140625, Val_Loss: 5461.203125\n",
      "Epoch 2268, Train_Loss: 5150.94970703125, Val_Loss: 5460.953125\n",
      "Epoch 2269, Train_Loss: 5150.69189453125, Val_Loss: 5460.66796875\n",
      "Epoch 2270, Train_Loss: 5150.4755859375, Val_Loss: 5460.416015625\n",
      "Epoch 2271, Train_Loss: 5150.23828125, Val_Loss: 5460.1513671875\n",
      "Epoch 2272, Train_Loss: 5149.98486328125, Val_Loss: 5459.86767578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2273, Train_Loss: 5149.74267578125, Val_Loss: 5459.5546875\n",
      "Epoch 2274, Train_Loss: 5149.50439453125, Val_Loss: 5459.28857421875\n",
      "Epoch 2275, Train_Loss: 5149.27099609375, Val_Loss: 5459.0107421875\n",
      "Epoch 2276, Train_Loss: 5149.04345703125, Val_Loss: 5458.74609375\n",
      "Epoch 2277, Train_Loss: 5148.81005859375, Val_Loss: 5458.4765625\n",
      "Epoch 2278, Train_Loss: 5148.57177734375, Val_Loss: 5458.20556640625\n",
      "Epoch 2279, Train_Loss: 5148.3330078125, Val_Loss: 5457.9228515625\n",
      "Epoch 2280, Train_Loss: 5148.0947265625, Val_Loss: 5457.65283203125\n",
      "Epoch 2281, Train_Loss: 5147.8369140625, Val_Loss: 5457.3916015625\n",
      "Epoch 2282, Train_Loss: 5147.49169921875, Val_Loss: 5457.11669921875\n",
      "Epoch 2283, Train_Loss: 5147.27294921875, Val_Loss: 5456.8486328125\n",
      "Epoch 2284, Train_Loss: 5147.01904296875, Val_Loss: 5456.5927734375\n",
      "Epoch 2285, Train_Loss: 5146.779296875, Val_Loss: 5456.306640625\n",
      "Epoch 2286, Train_Loss: 5146.53564453125, Val_Loss: 5456.02880859375\n",
      "Epoch 2287, Train_Loss: 5146.30224609375, Val_Loss: 5455.7705078125\n",
      "Epoch 2288, Train_Loss: 5146.06103515625, Val_Loss: 5455.5029296875\n",
      "Epoch 2289, Train_Loss: 5145.82080078125, Val_Loss: 5455.21826171875\n",
      "Epoch 2290, Train_Loss: 5145.59521484375, Val_Loss: 5454.94677734375\n",
      "Epoch 2291, Train_Loss: 5145.18994140625, Val_Loss: 5454.68212890625\n",
      "Epoch 2292, Train_Loss: 5144.95556640625, Val_Loss: 5454.392578125\n",
      "Epoch 2293, Train_Loss: 5144.72314453125, Val_Loss: 5454.123046875\n",
      "Epoch 2294, Train_Loss: 5144.49951171875, Val_Loss: 5453.8662109375\n",
      "Epoch 2295, Train_Loss: 5144.20703125, Val_Loss: 5453.59033203125\n",
      "Epoch 2296, Train_Loss: 5143.9560546875, Val_Loss: 5453.32470703125\n",
      "Epoch 2297, Train_Loss: 5143.70556640625, Val_Loss: 5453.0400390625\n",
      "Epoch 2298, Train_Loss: 5143.44677734375, Val_Loss: 5452.7646484375\n",
      "Epoch 2299, Train_Loss: 5143.205078125, Val_Loss: 5452.49853515625\n",
      "Epoch 2300, Train_Loss: 5142.96044921875, Val_Loss: 5452.23828125\n",
      "Epoch 2301, Train_Loss: 5142.72216796875, Val_Loss: 5451.95654296875\n",
      "Epoch 2302, Train_Loss: 5142.47802734375, Val_Loss: 5451.68310546875\n",
      "Epoch 2303, Train_Loss: 5142.2587890625, Val_Loss: 5451.42138671875\n",
      "Epoch 2304, Train_Loss: 5142.00830078125, Val_Loss: 5451.1572265625\n",
      "Epoch 2305, Train_Loss: 5141.76513671875, Val_Loss: 5450.857421875\n",
      "Epoch 2306, Train_Loss: 5141.53173828125, Val_Loss: 5450.5986328125\n",
      "Epoch 2307, Train_Loss: 5141.29443359375, Val_Loss: 5450.32568359375\n",
      "Epoch 2308, Train_Loss: 5141.02294921875, Val_Loss: 5450.048828125\n",
      "Epoch 2309, Train_Loss: 5140.79296875, Val_Loss: 5449.79150390625\n",
      "Epoch 2310, Train_Loss: 5140.5341796875, Val_Loss: 5449.49560546875\n",
      "Epoch 2311, Train_Loss: 5140.27587890625, Val_Loss: 5449.2060546875\n",
      "Epoch 2312, Train_Loss: 5140.05126953125, Val_Loss: 5448.93896484375\n",
      "Epoch 2313, Train_Loss: 5139.81396484375, Val_Loss: 5448.6650390625\n",
      "Epoch 2314, Train_Loss: 5139.5869140625, Val_Loss: 5448.3974609375\n",
      "Epoch 2315, Train_Loss: 5139.34033203125, Val_Loss: 5448.12939453125\n",
      "Epoch 2316, Train_Loss: 5139.0927734375, Val_Loss: 5447.86181640625\n",
      "Epoch 2317, Train_Loss: 5138.86865234375, Val_Loss: 5447.59765625\n",
      "Epoch 2318, Train_Loss: 5138.62353515625, Val_Loss: 5447.3037109375\n",
      "Epoch 2319, Train_Loss: 5138.38330078125, Val_Loss: 5447.02490234375\n",
      "Epoch 2320, Train_Loss: 5138.1201171875, Val_Loss: 5446.76416015625\n",
      "Epoch 2321, Train_Loss: 5137.81201171875, Val_Loss: 5446.49609375\n",
      "Epoch 2322, Train_Loss: 5137.576171875, Val_Loss: 5446.234375\n",
      "Epoch 2323, Train_Loss: 5137.345703125, Val_Loss: 5445.96923828125\n",
      "Epoch 2324, Train_Loss: 5137.1083984375, Val_Loss: 5445.67822265625\n",
      "Epoch 2325, Train_Loss: 5136.8564453125, Val_Loss: 5445.40380859375\n",
      "Epoch 2326, Train_Loss: 5136.63134765625, Val_Loss: 5445.146484375\n",
      "Epoch 2327, Train_Loss: 5136.3828125, Val_Loss: 5444.88037109375\n",
      "Epoch 2328, Train_Loss: 5136.12548828125, Val_Loss: 5444.60107421875\n",
      "Epoch 2329, Train_Loss: 5135.896484375, Val_Loss: 5444.34423828125\n",
      "Epoch 2330, Train_Loss: 5135.63232421875, Val_Loss: 5444.0673828125\n",
      "Epoch 2331, Train_Loss: 5135.38525390625, Val_Loss: 5443.7734375\n",
      "Epoch 2332, Train_Loss: 5135.1689453125, Val_Loss: 5443.50390625\n",
      "Epoch 2333, Train_Loss: 5134.9345703125, Val_Loss: 5443.24267578125\n",
      "Epoch 2334, Train_Loss: 5134.63818359375, Val_Loss: 5442.974609375\n",
      "Epoch 2335, Train_Loss: 5134.3876953125, Val_Loss: 5442.7041015625\n",
      "Epoch 2336, Train_Loss: 5134.15234375, Val_Loss: 5442.44091796875\n",
      "Epoch 2337, Train_Loss: 5133.9140625, Val_Loss: 5442.1337890625\n",
      "Epoch 2338, Train_Loss: 5133.67626953125, Val_Loss: 5441.8740234375\n",
      "Epoch 2339, Train_Loss: 5133.4345703125, Val_Loss: 5441.6044921875\n",
      "Epoch 2340, Train_Loss: 5133.18798828125, Val_Loss: 5441.3525390625\n",
      "Epoch 2341, Train_Loss: 5132.95703125, Val_Loss: 5441.07177734375\n",
      "Epoch 2342, Train_Loss: 5132.7119140625, Val_Loss: 5440.80517578125\n",
      "Epoch 2343, Train_Loss: 5132.48583984375, Val_Loss: 5440.53466796875\n",
      "Epoch 2344, Train_Loss: 5132.25830078125, Val_Loss: 5440.24658203125\n",
      "Epoch 2345, Train_Loss: 5132.00927734375, Val_Loss: 5439.9921875\n",
      "Epoch 2346, Train_Loss: 5131.76123046875, Val_Loss: 5439.69140625\n",
      "Epoch 2347, Train_Loss: 5131.466796875, Val_Loss: 5439.42724609375\n",
      "Epoch 2348, Train_Loss: 5131.232421875, Val_Loss: 5439.16162109375\n",
      "Epoch 2349, Train_Loss: 5130.9970703125, Val_Loss: 5438.88427734375\n",
      "Epoch 2350, Train_Loss: 5130.7626953125, Val_Loss: 5438.587890625\n",
      "Epoch 2351, Train_Loss: 5130.54443359375, Val_Loss: 5438.33056640625\n",
      "Epoch 2352, Train_Loss: 5130.30517578125, Val_Loss: 5438.06787109375\n",
      "Epoch 2353, Train_Loss: 5130.07421875, Val_Loss: 5437.78857421875\n",
      "Epoch 2354, Train_Loss: 5129.82958984375, Val_Loss: 5437.52587890625\n",
      "Epoch 2355, Train_Loss: 5129.505859375, Val_Loss: 5437.259765625\n",
      "Epoch 2356, Train_Loss: 5129.26806640625, Val_Loss: 5436.9833984375\n",
      "Epoch 2357, Train_Loss: 5129.03466796875, Val_Loss: 5436.7099609375\n",
      "Epoch 2358, Train_Loss: 5128.787109375, Val_Loss: 5436.4453125\n",
      "Epoch 2359, Train_Loss: 5128.52880859375, Val_Loss: 5436.1728515625\n",
      "Epoch 2360, Train_Loss: 5128.21630859375, Val_Loss: 5435.912109375\n",
      "Epoch 2361, Train_Loss: 5127.9716796875, Val_Loss: 5435.65380859375\n",
      "Epoch 2362, Train_Loss: 5127.74951171875, Val_Loss: 5435.38818359375\n",
      "Epoch 2363, Train_Loss: 5127.49267578125, Val_Loss: 5435.09814453125\n",
      "Epoch 2364, Train_Loss: 5127.23095703125, Val_Loss: 5434.8310546875\n",
      "Epoch 2365, Train_Loss: 5126.92431640625, Val_Loss: 5434.556640625\n",
      "Epoch 2366, Train_Loss: 5126.68701171875, Val_Loss: 5434.29296875\n",
      "Epoch 2367, Train_Loss: 5126.439453125, Val_Loss: 5434.02490234375\n",
      "Epoch 2368, Train_Loss: 5126.2109375, Val_Loss: 5433.75341796875\n",
      "Epoch 2369, Train_Loss: 5125.94384765625, Val_Loss: 5433.48486328125\n",
      "Epoch 2370, Train_Loss: 5125.6943359375, Val_Loss: 5433.20458984375\n",
      "Epoch 2371, Train_Loss: 5125.4560546875, Val_Loss: 5432.9345703125\n",
      "Epoch 2372, Train_Loss: 5125.23193359375, Val_Loss: 5432.685546875\n",
      "Epoch 2373, Train_Loss: 5124.95654296875, Val_Loss: 5432.42236328125\n",
      "Epoch 2374, Train_Loss: 5124.69921875, Val_Loss: 5432.13037109375\n",
      "Epoch 2375, Train_Loss: 5124.47900390625, Val_Loss: 5431.85888671875\n",
      "Epoch 2376, Train_Loss: 5124.24853515625, Val_Loss: 5431.58544921875\n",
      "Epoch 2377, Train_Loss: 5124.02490234375, Val_Loss: 5431.31103515625\n",
      "Epoch 2378, Train_Loss: 5123.79052734375, Val_Loss: 5431.0390625\n",
      "Epoch 2379, Train_Loss: 5123.53271484375, Val_Loss: 5430.7724609375\n",
      "Epoch 2380, Train_Loss: 5123.2890625, Val_Loss: 5430.5126953125\n",
      "Epoch 2381, Train_Loss: 5123.0537109375, Val_Loss: 5430.248046875\n",
      "Epoch 2382, Train_Loss: 5122.830078125, Val_Loss: 5429.994140625\n",
      "Epoch 2383, Train_Loss: 5122.57958984375, Val_Loss: 5429.6689453125\n",
      "Epoch 2384, Train_Loss: 5122.33935546875, Val_Loss: 5429.41259765625\n",
      "Epoch 2385, Train_Loss: 5122.1044921875, Val_Loss: 5429.1416015625\n",
      "Epoch 2386, Train_Loss: 5121.79736328125, Val_Loss: 5428.86572265625\n",
      "Epoch 2387, Train_Loss: 5121.56884765625, Val_Loss: 5428.60595703125\n",
      "Epoch 2388, Train_Loss: 5121.3466796875, Val_Loss: 5428.3505859375\n",
      "Epoch 2389, Train_Loss: 5121.10400390625, Val_Loss: 5428.06787109375\n",
      "Epoch 2390, Train_Loss: 5120.86767578125, Val_Loss: 5427.7978515625\n",
      "Epoch 2391, Train_Loss: 5120.6376953125, Val_Loss: 5427.5400390625\n",
      "Epoch 2392, Train_Loss: 5120.40185546875, Val_Loss: 5427.27099609375\n",
      "Epoch 2393, Train_Loss: 5120.1533203125, Val_Loss: 5427.00244140625\n",
      "Epoch 2394, Train_Loss: 5119.912109375, Val_Loss: 5426.7421875\n",
      "Epoch 2395, Train_Loss: 5119.65966796875, Val_Loss: 5426.45947265625\n",
      "Epoch 2396, Train_Loss: 5119.41162109375, Val_Loss: 5426.1767578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2397, Train_Loss: 5119.16943359375, Val_Loss: 5425.92138671875\n",
      "Epoch 2398, Train_Loss: 5118.93359375, Val_Loss: 5425.65673828125\n",
      "Epoch 2399, Train_Loss: 5118.6240234375, Val_Loss: 5425.39501953125\n",
      "Epoch 2400, Train_Loss: 5118.3896484375, Val_Loss: 5425.125\n",
      "Epoch 2401, Train_Loss: 5118.16357421875, Val_Loss: 5424.85400390625\n",
      "Epoch 2402, Train_Loss: 5117.9345703125, Val_Loss: 5424.5576171875\n",
      "Epoch 2403, Train_Loss: 5117.68896484375, Val_Loss: 5424.302734375\n",
      "Epoch 2404, Train_Loss: 5117.45166015625, Val_Loss: 5424.03662109375\n",
      "Epoch 2405, Train_Loss: 5117.21240234375, Val_Loss: 5423.77197265625\n",
      "Epoch 2406, Train_Loss: 5116.97119140625, Val_Loss: 5423.51416015625\n",
      "Epoch 2407, Train_Loss: 5116.73974609375, Val_Loss: 5423.25146484375\n",
      "Epoch 2408, Train_Loss: 5116.47412109375, Val_Loss: 5422.98486328125\n",
      "Epoch 2409, Train_Loss: 5116.23291015625, Val_Loss: 5422.701171875\n",
      "Epoch 2410, Train_Loss: 5115.990234375, Val_Loss: 5422.42529296875\n",
      "Epoch 2411, Train_Loss: 5115.74658203125, Val_Loss: 5422.1611328125\n",
      "Epoch 2412, Train_Loss: 5115.48876953125, Val_Loss: 5421.896484375\n",
      "Epoch 2413, Train_Loss: 5115.240234375, Val_Loss: 5421.634765625\n",
      "Epoch 2414, Train_Loss: 5115.02294921875, Val_Loss: 5421.36865234375\n",
      "Epoch 2415, Train_Loss: 5114.787109375, Val_Loss: 5421.0927734375\n",
      "Epoch 2416, Train_Loss: 5114.5556640625, Val_Loss: 5420.82861328125\n",
      "Epoch 2417, Train_Loss: 5114.322265625, Val_Loss: 5420.57080078125\n",
      "Epoch 2418, Train_Loss: 5114.0751953125, Val_Loss: 5420.30615234375\n",
      "Epoch 2419, Train_Loss: 5113.8486328125, Val_Loss: 5420.04443359375\n",
      "Epoch 2420, Train_Loss: 5113.61279296875, Val_Loss: 5419.7333984375\n",
      "Epoch 2421, Train_Loss: 5113.3828125, Val_Loss: 5419.47412109375\n",
      "Epoch 2422, Train_Loss: 5113.13818359375, Val_Loss: 5419.1845703125\n",
      "Epoch 2423, Train_Loss: 5112.87060546875, Val_Loss: 5418.92578125\n",
      "Epoch 2424, Train_Loss: 5112.623046875, Val_Loss: 5418.65576171875\n",
      "Epoch 2425, Train_Loss: 5112.32275390625, Val_Loss: 5418.3994140625\n",
      "Epoch 2426, Train_Loss: 5112.0927734375, Val_Loss: 5418.13671875\n",
      "Epoch 2427, Train_Loss: 5111.86328125, Val_Loss: 5417.87353515625\n",
      "Epoch 2428, Train_Loss: 5111.62646484375, Val_Loss: 5417.59423828125\n",
      "Epoch 2429, Train_Loss: 5111.365234375, Val_Loss: 5417.32861328125\n",
      "Epoch 2430, Train_Loss: 5111.13330078125, Val_Loss: 5417.06982421875\n",
      "Epoch 2431, Train_Loss: 5110.888671875, Val_Loss: 5416.80029296875\n",
      "Epoch 2432, Train_Loss: 5110.662109375, Val_Loss: 5416.53076171875\n",
      "Epoch 2433, Train_Loss: 5110.4091796875, Val_Loss: 5416.26318359375\n",
      "Epoch 2434, Train_Loss: 5110.18798828125, Val_Loss: 5416.005859375\n",
      "Epoch 2435, Train_Loss: 5109.9638671875, Val_Loss: 5415.7294921875\n",
      "Epoch 2436, Train_Loss: 5109.72998046875, Val_Loss: 5415.44921875\n",
      "Epoch 2437, Train_Loss: 5109.50048828125, Val_Loss: 5415.193359375\n",
      "Epoch 2438, Train_Loss: 5109.13037109375, Val_Loss: 5414.9287109375\n",
      "Epoch 2439, Train_Loss: 5108.8994140625, Val_Loss: 5414.6650390625\n",
      "Epoch 2440, Train_Loss: 5108.6689453125, Val_Loss: 5414.39599609375\n",
      "Epoch 2441, Train_Loss: 5108.4345703125, Val_Loss: 5414.125\n",
      "Epoch 2442, Train_Loss: 5108.208984375, Val_Loss: 5413.86572265625\n",
      "Epoch 2443, Train_Loss: 5107.96533203125, Val_Loss: 5413.59619140625\n",
      "Epoch 2444, Train_Loss: 5107.72802734375, Val_Loss: 5413.3349609375\n",
      "Epoch 2445, Train_Loss: 5107.501953125, Val_Loss: 5413.07177734375\n",
      "Epoch 2446, Train_Loss: 5107.2724609375, Val_Loss: 5412.81689453125\n",
      "Epoch 2447, Train_Loss: 5107.04345703125, Val_Loss: 5412.541015625\n",
      "Epoch 2448, Train_Loss: 5106.7880859375, Val_Loss: 5412.24169921875\n",
      "Epoch 2449, Train_Loss: 5106.54345703125, Val_Loss: 5411.98779296875\n",
      "Epoch 2450, Train_Loss: 5106.30908203125, Val_Loss: 5411.7275390625\n",
      "Epoch 2451, Train_Loss: 5106.02099609375, Val_Loss: 5411.455078125\n",
      "Epoch 2452, Train_Loss: 5105.77587890625, Val_Loss: 5411.19873046875\n",
      "Epoch 2453, Train_Loss: 5105.5234375, Val_Loss: 5410.94140625\n",
      "Epoch 2454, Train_Loss: 5105.2763671875, Val_Loss: 5410.65966796875\n",
      "Epoch 2455, Train_Loss: 5105.06103515625, Val_Loss: 5410.3896484375\n",
      "Epoch 2456, Train_Loss: 5104.81005859375, Val_Loss: 5410.091796875\n",
      "Epoch 2457, Train_Loss: 5104.56591796875, Val_Loss: 5409.83544921875\n",
      "Epoch 2458, Train_Loss: 5104.32080078125, Val_Loss: 5409.568359375\n",
      "Epoch 2459, Train_Loss: 5104.0830078125, Val_Loss: 5409.30419921875\n",
      "Epoch 2460, Train_Loss: 5103.8447265625, Val_Loss: 5409.03369140625\n",
      "Epoch 2461, Train_Loss: 5103.62158203125, Val_Loss: 5408.76025390625\n",
      "Epoch 2462, Train_Loss: 5103.3671875, Val_Loss: 5408.49609375\n",
      "Epoch 2463, Train_Loss: 5103.1064453125, Val_Loss: 5408.24462890625\n",
      "Epoch 2464, Train_Loss: 5102.78369140625, Val_Loss: 5407.98388671875\n",
      "Epoch 2465, Train_Loss: 5102.5498046875, Val_Loss: 5407.71484375\n",
      "Epoch 2466, Train_Loss: 5102.3271484375, Val_Loss: 5407.43994140625\n",
      "Epoch 2467, Train_Loss: 5102.07568359375, Val_Loss: 5407.1630859375\n",
      "Epoch 2468, Train_Loss: 5101.83544921875, Val_Loss: 5406.9072265625\n",
      "Epoch 2469, Train_Loss: 5101.60400390625, Val_Loss: 5406.64404296875\n",
      "Epoch 2470, Train_Loss: 5101.37548828125, Val_Loss: 5406.37548828125\n",
      "Epoch 2471, Train_Loss: 5101.14501953125, Val_Loss: 5406.1181640625\n",
      "Epoch 2472, Train_Loss: 5100.90234375, Val_Loss: 5405.84912109375\n",
      "Epoch 2473, Train_Loss: 5100.673828125, Val_Loss: 5405.59326171875\n",
      "Epoch 2474, Train_Loss: 5100.4345703125, Val_Loss: 5405.2939453125\n",
      "Epoch 2475, Train_Loss: 5100.201171875, Val_Loss: 5405.0380859375\n",
      "Epoch 2476, Train_Loss: 5099.96630859375, Val_Loss: 5404.77880859375\n",
      "Epoch 2477, Train_Loss: 5099.68701171875, Val_Loss: 5404.517578125\n",
      "Epoch 2478, Train_Loss: 5099.4658203125, Val_Loss: 5404.2509765625\n",
      "Epoch 2479, Train_Loss: 5099.2265625, Val_Loss: 5403.98828125\n",
      "Epoch 2480, Train_Loss: 5098.99609375, Val_Loss: 5403.69482421875\n",
      "Epoch 2481, Train_Loss: 5098.7705078125, Val_Loss: 5403.43212890625\n",
      "Epoch 2482, Train_Loss: 5098.53564453125, Val_Loss: 5403.1728515625\n",
      "Epoch 2483, Train_Loss: 5098.3017578125, Val_Loss: 5402.91015625\n",
      "Epoch 2484, Train_Loss: 5098.078125, Val_Loss: 5402.63916015625\n",
      "Epoch 2485, Train_Loss: 5097.84326171875, Val_Loss: 5402.3720703125\n",
      "Epoch 2486, Train_Loss: 5097.615234375, Val_Loss: 5402.11328125\n",
      "Epoch 2487, Train_Loss: 5097.3603515625, Val_Loss: 5401.83642578125\n",
      "Epoch 2488, Train_Loss: 5097.1171875, Val_Loss: 5401.572265625\n",
      "Epoch 2489, Train_Loss: 5096.87744140625, Val_Loss: 5401.3017578125\n",
      "Epoch 2490, Train_Loss: 5096.5673828125, Val_Loss: 5401.04931640625\n",
      "Epoch 2491, Train_Loss: 5096.32666015625, Val_Loss: 5400.77783203125\n",
      "Epoch 2492, Train_Loss: 5096.0859375, Val_Loss: 5400.52587890625\n",
      "Epoch 2493, Train_Loss: 5095.84521484375, Val_Loss: 5400.20263671875\n",
      "Epoch 2494, Train_Loss: 5095.60205078125, Val_Loss: 5399.93994140625\n",
      "Epoch 2495, Train_Loss: 5095.365234375, Val_Loss: 5399.6767578125\n",
      "Epoch 2496, Train_Loss: 5095.12841796875, Val_Loss: 5399.4111328125\n",
      "Epoch 2497, Train_Loss: 5094.86669921875, Val_Loss: 5399.15673828125\n",
      "Epoch 2498, Train_Loss: 5094.64892578125, Val_Loss: 5398.8955078125\n",
      "Epoch 2499, Train_Loss: 5094.4248046875, Val_Loss: 5398.62939453125\n",
      "Epoch 2500, Train_Loss: 5094.2001953125, Val_Loss: 5398.35595703125\n",
      "Epoch 2501, Train_Loss: 5093.96435546875, Val_Loss: 5398.08935546875\n",
      "Epoch 2502, Train_Loss: 5093.65869140625, Val_Loss: 5397.81982421875\n",
      "Epoch 2503, Train_Loss: 5093.36474609375, Val_Loss: 5397.5654296875\n",
      "Epoch 2504, Train_Loss: 5093.1455078125, Val_Loss: 5397.298828125\n",
      "Epoch 2505, Train_Loss: 5092.91259765625, Val_Loss: 5397.0419921875\n",
      "Epoch 2506, Train_Loss: 5092.6640625, Val_Loss: 5396.75244140625\n",
      "Epoch 2507, Train_Loss: 5092.42333984375, Val_Loss: 5396.49169921875\n",
      "Epoch 2508, Train_Loss: 5092.18798828125, Val_Loss: 5396.2333984375\n",
      "Epoch 2509, Train_Loss: 5091.958984375, Val_Loss: 5395.9697265625\n",
      "Epoch 2510, Train_Loss: 5091.7216796875, Val_Loss: 5395.703125\n",
      "Epoch 2511, Train_Loss: 5091.40283203125, Val_Loss: 5395.4453125\n",
      "Epoch 2512, Train_Loss: 5091.166015625, Val_Loss: 5395.17822265625\n",
      "Epoch 2513, Train_Loss: 5090.9453125, Val_Loss: 5394.88818359375\n",
      "Epoch 2514, Train_Loss: 5090.71337890625, Val_Loss: 5394.62646484375\n",
      "Epoch 2515, Train_Loss: 5090.4775390625, Val_Loss: 5394.37841796875\n",
      "Epoch 2516, Train_Loss: 5090.18701171875, Val_Loss: 5394.12109375\n",
      "Epoch 2517, Train_Loss: 5089.9658203125, Val_Loss: 5393.8515625\n",
      "Epoch 2518, Train_Loss: 5089.73291015625, Val_Loss: 5393.5966796875\n",
      "Epoch 2519, Train_Loss: 5089.5078125, Val_Loss: 5393.32568359375\n",
      "Epoch 2520, Train_Loss: 5089.27734375, Val_Loss: 5393.0283203125\n",
      "Epoch 2521, Train_Loss: 5089.025390625, Val_Loss: 5392.77294921875\n",
      "Epoch 2522, Train_Loss: 5088.79296875, Val_Loss: 5392.5166015625\n",
      "Epoch 2523, Train_Loss: 5088.5732421875, Val_Loss: 5392.240234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2524, Train_Loss: 5088.33642578125, Val_Loss: 5391.97021484375\n",
      "Epoch 2525, Train_Loss: 5088.1015625, Val_Loss: 5391.7216796875\n",
      "Epoch 2526, Train_Loss: 5087.84423828125, Val_Loss: 5391.43896484375\n",
      "Epoch 2527, Train_Loss: 5087.59716796875, Val_Loss: 5391.17578125\n",
      "Epoch 2528, Train_Loss: 5087.353515625, Val_Loss: 5390.9111328125\n",
      "Epoch 2529, Train_Loss: 5087.0517578125, Val_Loss: 5390.626953125\n",
      "Epoch 2530, Train_Loss: 5086.81982421875, Val_Loss: 5390.3681640625\n",
      "Epoch 2531, Train_Loss: 5086.57763671875, Val_Loss: 5390.0966796875\n",
      "Epoch 2532, Train_Loss: 5086.34912109375, Val_Loss: 5389.79931640625\n",
      "Epoch 2533, Train_Loss: 5086.119140625, Val_Loss: 5389.55126953125\n",
      "Epoch 2534, Train_Loss: 5085.8876953125, Val_Loss: 5389.30419921875\n",
      "Epoch 2535, Train_Loss: 5085.6533203125, Val_Loss: 5389.0341796875\n",
      "Epoch 2536, Train_Loss: 5085.4072265625, Val_Loss: 5388.78173828125\n",
      "Epoch 2537, Train_Loss: 5085.173828125, Val_Loss: 5388.5048828125\n",
      "Epoch 2538, Train_Loss: 5084.9365234375, Val_Loss: 5388.2509765625\n",
      "Epoch 2539, Train_Loss: 5084.70166015625, Val_Loss: 5387.96533203125\n",
      "Epoch 2540, Train_Loss: 5084.47998046875, Val_Loss: 5387.7138671875\n",
      "Epoch 2541, Train_Loss: 5084.2412109375, Val_Loss: 5387.4521484375\n",
      "Epoch 2542, Train_Loss: 5083.95556640625, Val_Loss: 5387.18310546875\n",
      "Epoch 2543, Train_Loss: 5083.72412109375, Val_Loss: 5386.923828125\n",
      "Epoch 2544, Train_Loss: 5083.5048828125, Val_Loss: 5386.669921875\n",
      "Epoch 2545, Train_Loss: 5083.28515625, Val_Loss: 5386.38623046875\n",
      "Epoch 2546, Train_Loss: 5083.048828125, Val_Loss: 5386.123046875\n",
      "Epoch 2547, Train_Loss: 5082.826171875, Val_Loss: 5385.85986328125\n",
      "Epoch 2548, Train_Loss: 5082.56982421875, Val_Loss: 5385.583984375\n",
      "Epoch 2549, Train_Loss: 5082.34033203125, Val_Loss: 5385.3359375\n",
      "Epoch 2550, Train_Loss: 5082.09521484375, Val_Loss: 5385.076171875\n",
      "Epoch 2551, Train_Loss: 5081.85400390625, Val_Loss: 5384.8095703125\n",
      "Epoch 2552, Train_Loss: 5081.625, Val_Loss: 5384.5244140625\n",
      "Epoch 2553, Train_Loss: 5081.39404296875, Val_Loss: 5384.26025390625\n",
      "Epoch 2554, Train_Loss: 5081.15380859375, Val_Loss: 5384.01953125\n",
      "Epoch 2555, Train_Loss: 5080.85693359375, Val_Loss: 5383.75048828125\n",
      "Epoch 2556, Train_Loss: 5080.60595703125, Val_Loss: 5383.49267578125\n",
      "Epoch 2557, Train_Loss: 5080.37255859375, Val_Loss: 5383.21533203125\n",
      "Epoch 2558, Train_Loss: 5080.14453125, Val_Loss: 5382.92822265625\n",
      "Epoch 2559, Train_Loss: 5079.91357421875, Val_Loss: 5382.67626953125\n",
      "Epoch 2560, Train_Loss: 5079.677734375, Val_Loss: 5382.4150390625\n",
      "Epoch 2561, Train_Loss: 5079.4384765625, Val_Loss: 5382.142578125\n",
      "Epoch 2562, Train_Loss: 5079.22265625, Val_Loss: 5381.87841796875\n",
      "Epoch 2563, Train_Loss: 5078.990234375, Val_Loss: 5381.62255859375\n",
      "Epoch 2564, Train_Loss: 5078.775390625, Val_Loss: 5381.35400390625\n",
      "Epoch 2565, Train_Loss: 5078.53173828125, Val_Loss: 5381.0693359375\n",
      "Epoch 2566, Train_Loss: 5078.263671875, Val_Loss: 5380.78076171875\n",
      "Epoch 2567, Train_Loss: 5078.025390625, Val_Loss: 5380.5185546875\n",
      "Epoch 2568, Train_Loss: 5077.70166015625, Val_Loss: 5380.2587890625\n",
      "Epoch 2569, Train_Loss: 5077.4736328125, Val_Loss: 5379.9931640625\n",
      "Epoch 2570, Train_Loss: 5077.23095703125, Val_Loss: 5379.73779296875\n",
      "Epoch 2571, Train_Loss: 5076.99169921875, Val_Loss: 5379.46533203125\n",
      "Epoch 2572, Train_Loss: 5076.76611328125, Val_Loss: 5379.20458984375\n",
      "Epoch 2573, Train_Loss: 5076.52880859375, Val_Loss: 5378.94970703125\n",
      "Epoch 2574, Train_Loss: 5076.29296875, Val_Loss: 5378.6845703125\n",
      "Epoch 2575, Train_Loss: 5076.03173828125, Val_Loss: 5378.43017578125\n",
      "Epoch 2576, Train_Loss: 5075.8134765625, Val_Loss: 5378.17138671875\n",
      "Epoch 2577, Train_Loss: 5075.572265625, Val_Loss: 5377.9052734375\n",
      "Epoch 2578, Train_Loss: 5075.34375, Val_Loss: 5377.6220703125\n",
      "Epoch 2579, Train_Loss: 5075.10986328125, Val_Loss: 5377.3564453125\n",
      "Epoch 2580, Train_Loss: 5074.8623046875, Val_Loss: 5377.10400390625\n",
      "Epoch 2581, Train_Loss: 5074.6123046875, Val_Loss: 5376.841796875\n",
      "Epoch 2582, Train_Loss: 5074.39453125, Val_Loss: 5376.57568359375\n",
      "Epoch 2583, Train_Loss: 5074.162109375, Val_Loss: 5376.31640625\n",
      "Epoch 2584, Train_Loss: 5073.8193359375, Val_Loss: 5376.0322265625\n",
      "Epoch 2585, Train_Loss: 5073.57763671875, Val_Loss: 5375.77294921875\n",
      "Epoch 2586, Train_Loss: 5073.35888671875, Val_Loss: 5375.5029296875\n",
      "Epoch 2587, Train_Loss: 5073.126953125, Val_Loss: 5375.24462890625\n",
      "Epoch 2588, Train_Loss: 5072.88818359375, Val_Loss: 5374.9892578125\n",
      "Epoch 2589, Train_Loss: 5072.6611328125, Val_Loss: 5374.72900390625\n",
      "Epoch 2590, Train_Loss: 5072.4306640625, Val_Loss: 5374.4501953125\n",
      "Epoch 2591, Train_Loss: 5072.201171875, Val_Loss: 5374.18310546875\n",
      "Epoch 2592, Train_Loss: 5071.97119140625, Val_Loss: 5373.93408203125\n",
      "Epoch 2593, Train_Loss: 5071.74365234375, Val_Loss: 5373.65283203125\n",
      "Epoch 2594, Train_Loss: 5071.4306640625, Val_Loss: 5373.40234375\n",
      "Epoch 2595, Train_Loss: 5071.18359375, Val_Loss: 5373.140625\n",
      "Epoch 2596, Train_Loss: 5070.95263671875, Val_Loss: 5372.888671875\n",
      "Epoch 2597, Train_Loss: 5070.71923828125, Val_Loss: 5372.61767578125\n",
      "Epoch 2598, Train_Loss: 5070.494140625, Val_Loss: 5372.36181640625\n",
      "Epoch 2599, Train_Loss: 5070.27587890625, Val_Loss: 5372.1083984375\n",
      "Epoch 2600, Train_Loss: 5070.0322265625, Val_Loss: 5371.82666015625\n",
      "Epoch 2601, Train_Loss: 5069.80224609375, Val_Loss: 5371.5625\n",
      "Epoch 2602, Train_Loss: 5069.5751953125, Val_Loss: 5371.2998046875\n",
      "Epoch 2603, Train_Loss: 5069.3095703125, Val_Loss: 5371.00341796875\n",
      "Epoch 2604, Train_Loss: 5069.091796875, Val_Loss: 5370.724609375\n",
      "Epoch 2605, Train_Loss: 5068.82958984375, Val_Loss: 5370.474609375\n",
      "Epoch 2606, Train_Loss: 5068.5908203125, Val_Loss: 5370.21728515625\n",
      "Epoch 2607, Train_Loss: 5068.3017578125, Val_Loss: 5369.95263671875\n",
      "Epoch 2608, Train_Loss: 5068.08056640625, Val_Loss: 5369.70849609375\n",
      "Epoch 2609, Train_Loss: 5067.8369140625, Val_Loss: 5369.43408203125\n",
      "Epoch 2610, Train_Loss: 5067.5791015625, Val_Loss: 5369.15966796875\n",
      "Epoch 2611, Train_Loss: 5067.36328125, Val_Loss: 5368.8974609375\n",
      "Epoch 2612, Train_Loss: 5067.14111328125, Val_Loss: 5368.64404296875\n",
      "Epoch 2613, Train_Loss: 5066.89306640625, Val_Loss: 5368.36767578125\n",
      "Epoch 2614, Train_Loss: 5066.66943359375, Val_Loss: 5368.11865234375\n",
      "Epoch 2615, Train_Loss: 5066.4306640625, Val_Loss: 5367.87060546875\n",
      "Epoch 2616, Train_Loss: 5066.19287109375, Val_Loss: 5367.59326171875\n",
      "Epoch 2617, Train_Loss: 5065.9619140625, Val_Loss: 5367.31103515625\n",
      "Epoch 2618, Train_Loss: 5065.7294921875, Val_Loss: 5367.05615234375\n",
      "Epoch 2619, Train_Loss: 5065.50244140625, Val_Loss: 5366.79736328125\n",
      "Epoch 2620, Train_Loss: 5065.23095703125, Val_Loss: 5366.5283203125\n",
      "Epoch 2621, Train_Loss: 5064.99365234375, Val_Loss: 5366.26171875\n",
      "Epoch 2622, Train_Loss: 5064.76123046875, Val_Loss: 5366.01416015625\n",
      "Epoch 2623, Train_Loss: 5064.51904296875, Val_Loss: 5365.74072265625\n",
      "Epoch 2624, Train_Loss: 5064.3037109375, Val_Loss: 5365.48828125\n",
      "Epoch 2625, Train_Loss: 5064.06787109375, Val_Loss: 5365.23779296875\n",
      "Epoch 2626, Train_Loss: 5063.8349609375, Val_Loss: 5364.97265625\n",
      "Epoch 2627, Train_Loss: 5063.62158203125, Val_Loss: 5364.71435546875\n",
      "Epoch 2628, Train_Loss: 5063.40185546875, Val_Loss: 5364.458984375\n",
      "Epoch 2629, Train_Loss: 5063.17578125, Val_Loss: 5364.2099609375\n",
      "Epoch 2630, Train_Loss: 5062.95263671875, Val_Loss: 5363.93310546875\n",
      "Epoch 2631, Train_Loss: 5062.7265625, Val_Loss: 5363.67724609375\n",
      "Epoch 2632, Train_Loss: 5062.50341796875, Val_Loss: 5363.42529296875\n",
      "Epoch 2633, Train_Loss: 5062.23291015625, Val_Loss: 5363.17724609375\n",
      "Epoch 2634, Train_Loss: 5061.97900390625, Val_Loss: 5362.92578125\n",
      "Epoch 2635, Train_Loss: 5061.759765625, Val_Loss: 5362.669921875\n",
      "Epoch 2636, Train_Loss: 5061.544921875, Val_Loss: 5362.4013671875\n",
      "Epoch 2637, Train_Loss: 5061.32275390625, Val_Loss: 5362.1455078125\n",
      "Epoch 2638, Train_Loss: 5061.08642578125, Val_Loss: 5361.88134765625\n",
      "Epoch 2639, Train_Loss: 5060.84326171875, Val_Loss: 5361.59912109375\n",
      "Epoch 2640, Train_Loss: 5060.62353515625, Val_Loss: 5361.35498046875\n",
      "Epoch 2641, Train_Loss: 5060.3984375, Val_Loss: 5361.0927734375\n",
      "Epoch 2642, Train_Loss: 5060.17431640625, Val_Loss: 5360.84375\n",
      "Epoch 2643, Train_Loss: 5059.9462890625, Val_Loss: 5360.578125\n",
      "Epoch 2644, Train_Loss: 5059.71435546875, Val_Loss: 5360.3232421875\n",
      "Epoch 2645, Train_Loss: 5059.48291015625, Val_Loss: 5360.05908203125\n",
      "Epoch 2646, Train_Loss: 5059.22900390625, Val_Loss: 5359.81640625\n",
      "Epoch 2647, Train_Loss: 5059.01220703125, Val_Loss: 5359.56787109375\n",
      "Epoch 2648, Train_Loss: 5058.78564453125, Val_Loss: 5359.314453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2649, Train_Loss: 5058.55517578125, Val_Loss: 5359.04736328125\n",
      "Epoch 2650, Train_Loss: 5058.3310546875, Val_Loss: 5358.794921875\n",
      "Epoch 2651, Train_Loss: 5058.1162109375, Val_Loss: 5358.541015625\n",
      "Epoch 2652, Train_Loss: 5057.89697265625, Val_Loss: 5358.29345703125\n",
      "Epoch 2653, Train_Loss: 5057.65966796875, Val_Loss: 5358.03955078125\n",
      "Epoch 2654, Train_Loss: 5057.4267578125, Val_Loss: 5357.78857421875\n",
      "Epoch 2655, Train_Loss: 5057.1923828125, Val_Loss: 5357.5341796875\n",
      "Epoch 2656, Train_Loss: 5056.96240234375, Val_Loss: 5357.26220703125\n",
      "Epoch 2657, Train_Loss: 5056.74560546875, Val_Loss: 5357.009765625\n",
      "Epoch 2658, Train_Loss: 5056.46240234375, Val_Loss: 5356.75341796875\n",
      "Epoch 2659, Train_Loss: 5056.171875, Val_Loss: 5356.509765625\n",
      "Epoch 2660, Train_Loss: 5055.93896484375, Val_Loss: 5356.26123046875\n",
      "Epoch 2661, Train_Loss: 5055.71240234375, Val_Loss: 5356.01220703125\n",
      "Epoch 2662, Train_Loss: 5055.4873046875, Val_Loss: 5355.75341796875\n",
      "Epoch 2663, Train_Loss: 5055.27490234375, Val_Loss: 5355.4873046875\n",
      "Epoch 2664, Train_Loss: 5055.041015625, Val_Loss: 5355.24072265625\n",
      "Epoch 2665, Train_Loss: 5054.8291015625, Val_Loss: 5354.97900390625\n",
      "Epoch 2666, Train_Loss: 5054.6201171875, Val_Loss: 5354.72216796875\n",
      "Epoch 2667, Train_Loss: 5054.39453125, Val_Loss: 5354.466796875\n",
      "Epoch 2668, Train_Loss: 5054.16064453125, Val_Loss: 5354.20458984375\n",
      "Epoch 2669, Train_Loss: 5053.92724609375, Val_Loss: 5353.94677734375\n",
      "Epoch 2670, Train_Loss: 5053.7060546875, Val_Loss: 5353.69287109375\n",
      "Epoch 2671, Train_Loss: 5053.48876953125, Val_Loss: 5353.431640625\n",
      "Epoch 2672, Train_Loss: 5053.23681640625, Val_Loss: 5353.18603515625\n",
      "Epoch 2673, Train_Loss: 5053.0087890625, Val_Loss: 5352.927734375\n",
      "Epoch 2674, Train_Loss: 5052.763671875, Val_Loss: 5352.68212890625\n",
      "Epoch 2675, Train_Loss: 5052.54296875, Val_Loss: 5352.43896484375\n",
      "Epoch 2676, Train_Loss: 5052.3486328125, Val_Loss: 5352.1640625\n",
      "Epoch 2677, Train_Loss: 5052.12451171875, Val_Loss: 5351.888671875\n",
      "Epoch 2678, Train_Loss: 5051.90380859375, Val_Loss: 5351.6318359375\n",
      "Epoch 2679, Train_Loss: 5051.654296875, Val_Loss: 5351.3916015625\n",
      "Epoch 2680, Train_Loss: 5051.42822265625, Val_Loss: 5351.13427734375\n",
      "Epoch 2681, Train_Loss: 5051.19677734375, Val_Loss: 5350.88330078125\n",
      "Epoch 2682, Train_Loss: 5050.9814453125, Val_Loss: 5350.61669921875\n",
      "Epoch 2683, Train_Loss: 5050.76611328125, Val_Loss: 5350.35498046875\n",
      "Epoch 2684, Train_Loss: 5050.5087890625, Val_Loss: 5350.103515625\n",
      "Epoch 2685, Train_Loss: 5050.28466796875, Val_Loss: 5349.85009765625\n",
      "Epoch 2686, Train_Loss: 5050.044921875, Val_Loss: 5349.60107421875\n",
      "Epoch 2687, Train_Loss: 5049.818359375, Val_Loss: 5349.341796875\n",
      "Epoch 2688, Train_Loss: 5049.60498046875, Val_Loss: 5349.10205078125\n",
      "Epoch 2689, Train_Loss: 5049.37255859375, Val_Loss: 5348.8330078125\n",
      "Epoch 2690, Train_Loss: 5049.15380859375, Val_Loss: 5348.5849609375\n",
      "Epoch 2691, Train_Loss: 5048.94921875, Val_Loss: 5348.31982421875\n",
      "Epoch 2692, Train_Loss: 5048.72119140625, Val_Loss: 5348.0859375\n",
      "Epoch 2693, Train_Loss: 5048.4912109375, Val_Loss: 5347.83740234375\n",
      "Epoch 2694, Train_Loss: 5048.24853515625, Val_Loss: 5347.572265625\n",
      "Epoch 2695, Train_Loss: 5048.025390625, Val_Loss: 5347.31982421875\n",
      "Epoch 2696, Train_Loss: 5047.7958984375, Val_Loss: 5347.05224609375\n",
      "Epoch 2697, Train_Loss: 5047.578125, Val_Loss: 5346.794921875\n",
      "Epoch 2698, Train_Loss: 5047.34716796875, Val_Loss: 5346.5556640625\n",
      "Epoch 2699, Train_Loss: 5047.064453125, Val_Loss: 5346.31591796875\n",
      "Epoch 2700, Train_Loss: 5046.8232421875, Val_Loss: 5346.056640625\n",
      "Epoch 2701, Train_Loss: 5046.587890625, Val_Loss: 5345.7958984375\n",
      "Epoch 2702, Train_Loss: 5046.39697265625, Val_Loss: 5345.537109375\n",
      "Epoch 2703, Train_Loss: 5046.18310546875, Val_Loss: 5345.27587890625\n",
      "Epoch 2704, Train_Loss: 5045.9619140625, Val_Loss: 5345.03515625\n",
      "Epoch 2705, Train_Loss: 5045.7421875, Val_Loss: 5344.78662109375\n",
      "Epoch 2706, Train_Loss: 5045.52197265625, Val_Loss: 5344.52392578125\n",
      "Epoch 2707, Train_Loss: 5045.3046875, Val_Loss: 5344.26904296875\n",
      "Epoch 2708, Train_Loss: 5045.07373046875, Val_Loss: 5344.01318359375\n",
      "Epoch 2709, Train_Loss: 5044.8466796875, Val_Loss: 5343.75146484375\n",
      "Epoch 2710, Train_Loss: 5044.62890625, Val_Loss: 5343.4990234375\n",
      "Epoch 2711, Train_Loss: 5044.416015625, Val_Loss: 5343.2568359375\n",
      "Epoch 2712, Train_Loss: 5044.14013671875, Val_Loss: 5343.0\n",
      "Epoch 2713, Train_Loss: 5043.9169921875, Val_Loss: 5342.73974609375\n",
      "Epoch 2714, Train_Loss: 5043.66015625, Val_Loss: 5342.45703125\n",
      "Epoch 2715, Train_Loss: 5043.44287109375, Val_Loss: 5342.19580078125\n",
      "Epoch 2716, Train_Loss: 5043.21484375, Val_Loss: 5341.94091796875\n",
      "Epoch 2717, Train_Loss: 5042.98388671875, Val_Loss: 5341.67919921875\n",
      "Epoch 2718, Train_Loss: 5042.7578125, Val_Loss: 5341.4404296875\n",
      "Epoch 2719, Train_Loss: 5042.517578125, Val_Loss: 5341.1845703125\n",
      "Epoch 2720, Train_Loss: 5042.31005859375, Val_Loss: 5340.94580078125\n",
      "Epoch 2721, Train_Loss: 5042.07861328125, Val_Loss: 5340.69873046875\n",
      "Epoch 2722, Train_Loss: 5041.86279296875, Val_Loss: 5340.4326171875\n",
      "Epoch 2723, Train_Loss: 5041.62353515625, Val_Loss: 5340.1796875\n",
      "Epoch 2724, Train_Loss: 5041.36767578125, Val_Loss: 5339.92578125\n",
      "Epoch 2725, Train_Loss: 5041.1259765625, Val_Loss: 5339.685546875\n",
      "Epoch 2726, Train_Loss: 5040.89501953125, Val_Loss: 5339.4267578125\n",
      "Epoch 2727, Train_Loss: 5040.68701171875, Val_Loss: 5339.1669921875\n",
      "Epoch 2728, Train_Loss: 5040.45166015625, Val_Loss: 5338.9130859375\n",
      "Epoch 2729, Train_Loss: 5040.2041015625, Val_Loss: 5338.6474609375\n",
      "Epoch 2730, Train_Loss: 5039.97900390625, Val_Loss: 5338.3935546875\n",
      "Epoch 2731, Train_Loss: 5039.75, Val_Loss: 5338.13916015625\n",
      "Epoch 2732, Train_Loss: 5039.36767578125, Val_Loss: 5337.89697265625\n",
      "Epoch 2733, Train_Loss: 5039.11474609375, Val_Loss: 5337.63623046875\n",
      "Epoch 2734, Train_Loss: 5038.8701171875, Val_Loss: 5337.3818359375\n",
      "Epoch 2735, Train_Loss: 5038.6611328125, Val_Loss: 5337.1162109375\n",
      "Epoch 2736, Train_Loss: 5038.45703125, Val_Loss: 5336.86865234375\n",
      "Epoch 2737, Train_Loss: 5038.21875, Val_Loss: 5336.62060546875\n",
      "Epoch 2738, Train_Loss: 5037.94921875, Val_Loss: 5336.37060546875\n",
      "Epoch 2739, Train_Loss: 5037.72119140625, Val_Loss: 5336.1171875\n",
      "Epoch 2740, Train_Loss: 5037.50244140625, Val_Loss: 5335.859375\n",
      "Epoch 2741, Train_Loss: 5037.27392578125, Val_Loss: 5335.6220703125\n",
      "Epoch 2742, Train_Loss: 5037.04931640625, Val_Loss: 5335.3408203125\n",
      "Epoch 2743, Train_Loss: 5036.81884765625, Val_Loss: 5335.0986328125\n",
      "Epoch 2744, Train_Loss: 5036.5888671875, Val_Loss: 5334.8447265625\n",
      "Epoch 2745, Train_Loss: 5036.38134765625, Val_Loss: 5334.59375\n",
      "Epoch 2746, Train_Loss: 5036.16943359375, Val_Loss: 5334.34619140625\n",
      "Epoch 2747, Train_Loss: 5035.95654296875, Val_Loss: 5334.09423828125\n",
      "Epoch 2748, Train_Loss: 5035.73193359375, Val_Loss: 5333.828125\n",
      "Epoch 2749, Train_Loss: 5035.50390625, Val_Loss: 5333.5810546875\n",
      "Epoch 2750, Train_Loss: 5035.2744140625, Val_Loss: 5333.32421875\n",
      "Epoch 2751, Train_Loss: 5035.021484375, Val_Loss: 5333.03857421875\n",
      "Epoch 2752, Train_Loss: 5034.76611328125, Val_Loss: 5332.80078125\n",
      "Epoch 2753, Train_Loss: 5034.552734375, Val_Loss: 5332.54638671875\n",
      "Epoch 2754, Train_Loss: 5034.29638671875, Val_Loss: 5332.29833984375\n",
      "Epoch 2755, Train_Loss: 5034.1181640625, Val_Loss: 5332.01318359375\n",
      "Epoch 2756, Train_Loss: 5033.888671875, Val_Loss: 5331.76318359375\n",
      "Epoch 2757, Train_Loss: 5033.6708984375, Val_Loss: 5331.5244140625\n",
      "Epoch 2758, Train_Loss: 5033.4482421875, Val_Loss: 5331.2890625\n",
      "Epoch 2759, Train_Loss: 5033.20556640625, Val_Loss: 5331.03173828125\n",
      "Epoch 2760, Train_Loss: 5032.9794921875, Val_Loss: 5330.77392578125\n",
      "Epoch 2761, Train_Loss: 5032.763671875, Val_Loss: 5330.5126953125\n",
      "Epoch 2762, Train_Loss: 5032.54150390625, Val_Loss: 5330.2578125\n",
      "Epoch 2763, Train_Loss: 5032.32421875, Val_Loss: 5330.0205078125\n",
      "Epoch 2764, Train_Loss: 5032.07861328125, Val_Loss: 5329.77294921875\n",
      "Epoch 2765, Train_Loss: 5031.80322265625, Val_Loss: 5329.5224609375\n",
      "Epoch 2766, Train_Loss: 5031.572265625, Val_Loss: 5329.265625\n",
      "Epoch 2767, Train_Loss: 5031.32666015625, Val_Loss: 5329.009765625\n",
      "Epoch 2768, Train_Loss: 5031.1064453125, Val_Loss: 5328.748046875\n",
      "Epoch 2769, Train_Loss: 5030.86669921875, Val_Loss: 5328.48779296875\n",
      "Epoch 2770, Train_Loss: 5030.63525390625, Val_Loss: 5328.23291015625\n",
      "Epoch 2771, Train_Loss: 5030.4140625, Val_Loss: 5327.98095703125\n",
      "Epoch 2772, Train_Loss: 5030.20947265625, Val_Loss: 5327.73583984375\n",
      "Epoch 2773, Train_Loss: 5029.9853515625, Val_Loss: 5327.48095703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2774, Train_Loss: 5029.75732421875, Val_Loss: 5327.2177734375\n",
      "Epoch 2775, Train_Loss: 5029.54638671875, Val_Loss: 5326.9638671875\n",
      "Epoch 2776, Train_Loss: 5029.32421875, Val_Loss: 5326.71630859375\n",
      "Epoch 2777, Train_Loss: 5029.10693359375, Val_Loss: 5326.4609375\n",
      "Epoch 2778, Train_Loss: 5028.86083984375, Val_Loss: 5326.21728515625\n",
      "Epoch 2779, Train_Loss: 5028.63623046875, Val_Loss: 5325.9677734375\n",
      "Epoch 2780, Train_Loss: 5028.42724609375, Val_Loss: 5325.70458984375\n",
      "Epoch 2781, Train_Loss: 5028.2646484375, Val_Loss: 5325.4453125\n",
      "Epoch 2782, Train_Loss: 5028.052734375, Val_Loss: 5325.18603515625\n",
      "Epoch 2783, Train_Loss: 5027.8173828125, Val_Loss: 5324.935546875\n",
      "Epoch 2784, Train_Loss: 5027.5830078125, Val_Loss: 5324.69140625\n",
      "Epoch 2785, Train_Loss: 5027.35205078125, Val_Loss: 5324.43212890625\n",
      "Epoch 2786, Train_Loss: 5027.13623046875, Val_Loss: 5324.17578125\n",
      "Epoch 2787, Train_Loss: 5026.912109375, Val_Loss: 5323.93115234375\n",
      "Epoch 2788, Train_Loss: 5026.68701171875, Val_Loss: 5323.64013671875\n",
      "Epoch 2789, Train_Loss: 5026.435546875, Val_Loss: 5323.3818359375\n",
      "Epoch 2790, Train_Loss: 5026.2119140625, Val_Loss: 5323.1396484375\n",
      "Epoch 2791, Train_Loss: 5025.982421875, Val_Loss: 5322.90576171875\n",
      "Epoch 2792, Train_Loss: 5025.77001953125, Val_Loss: 5322.64111328125\n",
      "Epoch 2793, Train_Loss: 5025.546875, Val_Loss: 5322.38134765625\n",
      "Epoch 2794, Train_Loss: 5025.3193359375, Val_Loss: 5322.13525390625\n",
      "Epoch 2795, Train_Loss: 5025.1025390625, Val_Loss: 5321.88037109375\n",
      "Epoch 2796, Train_Loss: 5024.869140625, Val_Loss: 5321.62158203125\n",
      "Epoch 2797, Train_Loss: 5024.6455078125, Val_Loss: 5321.37109375\n",
      "Epoch 2798, Train_Loss: 5024.36767578125, Val_Loss: 5321.1240234375\n",
      "Epoch 2799, Train_Loss: 5024.1337890625, Val_Loss: 5320.88427734375\n",
      "Epoch 2800, Train_Loss: 5023.91650390625, Val_Loss: 5320.630859375\n",
      "Epoch 2801, Train_Loss: 5023.68798828125, Val_Loss: 5320.35546875\n",
      "Epoch 2802, Train_Loss: 5023.47119140625, Val_Loss: 5320.11376953125\n",
      "Epoch 2803, Train_Loss: 5023.26708984375, Val_Loss: 5319.86279296875\n",
      "Epoch 2804, Train_Loss: 5022.9775390625, Val_Loss: 5319.61669921875\n",
      "Epoch 2805, Train_Loss: 5022.76025390625, Val_Loss: 5319.375\n",
      "Epoch 2806, Train_Loss: 5022.54638671875, Val_Loss: 5319.11767578125\n",
      "Epoch 2807, Train_Loss: 5022.271484375, Val_Loss: 5318.869140625\n",
      "Epoch 2808, Train_Loss: 5022.06689453125, Val_Loss: 5318.6005859375\n",
      "Epoch 2809, Train_Loss: 5021.82421875, Val_Loss: 5318.357421875\n",
      "Epoch 2810, Train_Loss: 5021.5986328125, Val_Loss: 5318.0908203125\n",
      "Epoch 2811, Train_Loss: 5021.37109375, Val_Loss: 5317.85009765625\n",
      "Epoch 2812, Train_Loss: 5021.140625, Val_Loss: 5317.60302734375\n",
      "Epoch 2813, Train_Loss: 5020.9267578125, Val_Loss: 5317.34716796875\n",
      "Epoch 2814, Train_Loss: 5020.6904296875, Val_Loss: 5317.0927734375\n",
      "Epoch 2815, Train_Loss: 5020.4755859375, Val_Loss: 5316.82666015625\n",
      "Epoch 2816, Train_Loss: 5020.2568359375, Val_Loss: 5316.57373046875\n",
      "Epoch 2817, Train_Loss: 5020.02978515625, Val_Loss: 5316.31689453125\n",
      "Epoch 2818, Train_Loss: 5019.767578125, Val_Loss: 5316.06591796875\n",
      "Epoch 2819, Train_Loss: 5019.54638671875, Val_Loss: 5315.81591796875\n",
      "Epoch 2820, Train_Loss: 5019.31494140625, Val_Loss: 5315.56396484375\n",
      "Epoch 2821, Train_Loss: 5019.10546875, Val_Loss: 5315.31298828125\n",
      "Epoch 2822, Train_Loss: 5018.892578125, Val_Loss: 5315.06201171875\n",
      "Epoch 2823, Train_Loss: 5018.67041015625, Val_Loss: 5314.80712890625\n",
      "Epoch 2824, Train_Loss: 5018.439453125, Val_Loss: 5314.56396484375\n",
      "Epoch 2825, Train_Loss: 5018.21826171875, Val_Loss: 5314.2822265625\n",
      "Epoch 2826, Train_Loss: 5017.9892578125, Val_Loss: 5314.03076171875\n",
      "Epoch 2827, Train_Loss: 5017.7734375, Val_Loss: 5313.7705078125\n",
      "Epoch 2828, Train_Loss: 5017.56298828125, Val_Loss: 5313.51953125\n",
      "Epoch 2829, Train_Loss: 5017.32763671875, Val_Loss: 5313.263671875\n",
      "Epoch 2830, Train_Loss: 5017.1162109375, Val_Loss: 5313.01513671875\n",
      "Epoch 2831, Train_Loss: 5016.8505859375, Val_Loss: 5312.7705078125\n",
      "Epoch 2832, Train_Loss: 5016.6552734375, Val_Loss: 5312.52099609375\n",
      "Epoch 2833, Train_Loss: 5016.42333984375, Val_Loss: 5312.2685546875\n",
      "Epoch 2834, Train_Loss: 5016.16943359375, Val_Loss: 5311.994140625\n",
      "Epoch 2835, Train_Loss: 5015.951171875, Val_Loss: 5311.73876953125\n",
      "Epoch 2836, Train_Loss: 5015.73486328125, Val_Loss: 5311.50341796875\n",
      "Epoch 2837, Train_Loss: 5015.505859375, Val_Loss: 5311.2548828125\n",
      "Epoch 2838, Train_Loss: 5015.28466796875, Val_Loss: 5311.005859375\n",
      "Epoch 2839, Train_Loss: 5015.05126953125, Val_Loss: 5310.74853515625\n",
      "Epoch 2840, Train_Loss: 5014.8291015625, Val_Loss: 5310.50537109375\n",
      "Epoch 2841, Train_Loss: 5014.61328125, Val_Loss: 5310.25\n",
      "Epoch 2842, Train_Loss: 5014.39501953125, Val_Loss: 5310.00244140625\n",
      "Epoch 2843, Train_Loss: 5014.18408203125, Val_Loss: 5309.75\n",
      "Epoch 2844, Train_Loss: 5013.8828125, Val_Loss: 5309.49462890625\n",
      "Epoch 2845, Train_Loss: 5013.64697265625, Val_Loss: 5309.2509765625\n",
      "Epoch 2846, Train_Loss: 5013.4462890625, Val_Loss: 5309.00634765625\n",
      "Epoch 2847, Train_Loss: 5013.23291015625, Val_Loss: 5308.74462890625\n",
      "Epoch 2848, Train_Loss: 5013.0146484375, Val_Loss: 5308.4931640625\n",
      "Epoch 2849, Train_Loss: 5012.78125, Val_Loss: 5308.2421875\n",
      "Epoch 2850, Train_Loss: 5012.5625, Val_Loss: 5307.9912109375\n",
      "Epoch 2851, Train_Loss: 5012.34814453125, Val_Loss: 5307.7421875\n",
      "Epoch 2852, Train_Loss: 5012.13525390625, Val_Loss: 5307.48779296875\n",
      "Epoch 2853, Train_Loss: 5011.919921875, Val_Loss: 5307.23095703125\n",
      "Epoch 2854, Train_Loss: 5011.685546875, Val_Loss: 5306.974609375\n",
      "Epoch 2855, Train_Loss: 5011.458984375, Val_Loss: 5306.72509765625\n",
      "Epoch 2856, Train_Loss: 5011.2353515625, Val_Loss: 5306.46923828125\n",
      "Epoch 2857, Train_Loss: 5010.982421875, Val_Loss: 5306.2353515625\n",
      "Epoch 2858, Train_Loss: 5010.765625, Val_Loss: 5305.9892578125\n",
      "Epoch 2859, Train_Loss: 5010.5439453125, Val_Loss: 5305.74658203125\n",
      "Epoch 2860, Train_Loss: 5010.3447265625, Val_Loss: 5305.47021484375\n",
      "Epoch 2861, Train_Loss: 5010.126953125, Val_Loss: 5305.2255859375\n",
      "Epoch 2862, Train_Loss: 5009.90576171875, Val_Loss: 5304.97021484375\n",
      "Epoch 2863, Train_Loss: 5009.6748046875, Val_Loss: 5304.68994140625\n",
      "Epoch 2864, Train_Loss: 5009.4228515625, Val_Loss: 5304.451171875\n",
      "Epoch 2865, Train_Loss: 5009.20556640625, Val_Loss: 5304.18896484375\n",
      "Epoch 2866, Train_Loss: 5008.9853515625, Val_Loss: 5303.93603515625\n",
      "Epoch 2867, Train_Loss: 5008.76416015625, Val_Loss: 5303.68359375\n",
      "Epoch 2868, Train_Loss: 5008.54833984375, Val_Loss: 5303.4375\n",
      "Epoch 2869, Train_Loss: 5008.3212890625, Val_Loss: 5303.197265625\n",
      "Epoch 2870, Train_Loss: 5008.06640625, Val_Loss: 5302.95263671875\n",
      "Epoch 2871, Train_Loss: 5007.857421875, Val_Loss: 5302.6982421875\n",
      "Epoch 2872, Train_Loss: 5007.61572265625, Val_Loss: 5302.4443359375\n",
      "Epoch 2873, Train_Loss: 5007.39208984375, Val_Loss: 5302.1982421875\n",
      "Epoch 2874, Train_Loss: 5007.14404296875, Val_Loss: 5301.93701171875\n",
      "Epoch 2875, Train_Loss: 5006.92822265625, Val_Loss: 5301.689453125\n",
      "Epoch 2876, Train_Loss: 5006.712890625, Val_Loss: 5301.431640625\n",
      "Epoch 2877, Train_Loss: 5006.50244140625, Val_Loss: 5301.193359375\n",
      "Epoch 2878, Train_Loss: 5006.287109375, Val_Loss: 5300.94775390625\n",
      "Epoch 2879, Train_Loss: 5006.05029296875, Val_Loss: 5300.705078125\n",
      "Epoch 2880, Train_Loss: 5005.83544921875, Val_Loss: 5300.44140625\n",
      "Epoch 2881, Train_Loss: 5005.4814453125, Val_Loss: 5300.181640625\n",
      "Epoch 2882, Train_Loss: 5005.251953125, Val_Loss: 5299.93896484375\n",
      "Epoch 2883, Train_Loss: 5005.00439453125, Val_Loss: 5299.69677734375\n",
      "Epoch 2884, Train_Loss: 5004.7685546875, Val_Loss: 5299.46142578125\n",
      "Epoch 2885, Train_Loss: 5004.5419921875, Val_Loss: 5299.205078125\n",
      "Epoch 2886, Train_Loss: 5004.333984375, Val_Loss: 5298.9501953125\n",
      "Epoch 2887, Train_Loss: 5004.1474609375, Val_Loss: 5298.69189453125\n",
      "Epoch 2888, Train_Loss: 5003.92431640625, Val_Loss: 5298.447265625\n",
      "Epoch 2889, Train_Loss: 5003.69287109375, Val_Loss: 5298.18701171875\n",
      "Epoch 2890, Train_Loss: 5003.458984375, Val_Loss: 5297.93994140625\n",
      "Epoch 2891, Train_Loss: 5003.23583984375, Val_Loss: 5297.6884765625\n",
      "Epoch 2892, Train_Loss: 5003.01318359375, Val_Loss: 5297.4462890625\n",
      "Epoch 2893, Train_Loss: 5002.7822265625, Val_Loss: 5297.17822265625\n",
      "Epoch 2894, Train_Loss: 5002.552734375, Val_Loss: 5296.9326171875\n",
      "Epoch 2895, Train_Loss: 5002.3251953125, Val_Loss: 5296.68798828125\n",
      "Epoch 2896, Train_Loss: 5002.11328125, Val_Loss: 5296.43505859375\n",
      "Epoch 2897, Train_Loss: 5001.87255859375, Val_Loss: 5296.19384765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2898, Train_Loss: 5001.67626953125, Val_Loss: 5295.94677734375\n",
      "Epoch 2899, Train_Loss: 5001.45556640625, Val_Loss: 5295.6982421875\n",
      "Epoch 2900, Train_Loss: 5001.23388671875, Val_Loss: 5295.4013671875\n",
      "Epoch 2901, Train_Loss: 5001.01806640625, Val_Loss: 5295.15087890625\n",
      "Epoch 2902, Train_Loss: 5000.81005859375, Val_Loss: 5294.9150390625\n",
      "Epoch 2903, Train_Loss: 5000.59228515625, Val_Loss: 5294.6845703125\n",
      "Epoch 2904, Train_Loss: 5000.36474609375, Val_Loss: 5294.43505859375\n",
      "Epoch 2905, Train_Loss: 5000.14404296875, Val_Loss: 5294.17236328125\n",
      "Epoch 2906, Train_Loss: 4999.91748046875, Val_Loss: 5293.92822265625\n",
      "Epoch 2907, Train_Loss: 4999.69140625, Val_Loss: 5293.66455078125\n",
      "Epoch 2908, Train_Loss: 4999.474609375, Val_Loss: 5293.42529296875\n",
      "Epoch 2909, Train_Loss: 4999.2490234375, Val_Loss: 5293.17578125\n",
      "Epoch 2910, Train_Loss: 4998.97607421875, Val_Loss: 5292.93505859375\n",
      "Epoch 2911, Train_Loss: 4998.75927734375, Val_Loss: 5292.68603515625\n",
      "Epoch 2912, Train_Loss: 4998.53857421875, Val_Loss: 5292.43701171875\n",
      "Epoch 2913, Train_Loss: 4998.34912109375, Val_Loss: 5292.1748046875\n",
      "Epoch 2914, Train_Loss: 4998.11279296875, Val_Loss: 5291.9326171875\n",
      "Epoch 2915, Train_Loss: 4997.9052734375, Val_Loss: 5291.68310546875\n",
      "Epoch 2916, Train_Loss: 4997.6689453125, Val_Loss: 5291.44287109375\n",
      "Epoch 2917, Train_Loss: 4997.4375, Val_Loss: 5291.19140625\n",
      "Epoch 2918, Train_Loss: 4997.19921875, Val_Loss: 5290.93701171875\n",
      "Epoch 2919, Train_Loss: 4996.9716796875, Val_Loss: 5290.68408203125\n",
      "Epoch 2920, Train_Loss: 4996.76123046875, Val_Loss: 5290.42578125\n",
      "Epoch 2921, Train_Loss: 4996.5517578125, Val_Loss: 5290.1982421875\n",
      "Epoch 2922, Train_Loss: 4996.330078125, Val_Loss: 5289.935546875\n",
      "Epoch 2923, Train_Loss: 4996.0703125, Val_Loss: 5289.69970703125\n",
      "Epoch 2924, Train_Loss: 4995.837890625, Val_Loss: 5289.4501953125\n",
      "Epoch 2925, Train_Loss: 4995.6298828125, Val_Loss: 5289.19970703125\n",
      "Epoch 2926, Train_Loss: 4995.42138671875, Val_Loss: 5288.9482421875\n",
      "Epoch 2927, Train_Loss: 4995.19482421875, Val_Loss: 5288.69287109375\n",
      "Epoch 2928, Train_Loss: 4994.9765625, Val_Loss: 5288.42822265625\n",
      "Epoch 2929, Train_Loss: 4994.751953125, Val_Loss: 5288.18896484375\n",
      "Epoch 2930, Train_Loss: 4994.54150390625, Val_Loss: 5287.939453125\n",
      "Epoch 2931, Train_Loss: 4994.3115234375, Val_Loss: 5287.693359375\n",
      "Epoch 2932, Train_Loss: 4994.08544921875, Val_Loss: 5287.44580078125\n",
      "Epoch 2933, Train_Loss: 4993.8671875, Val_Loss: 5287.193359375\n",
      "Epoch 2934, Train_Loss: 4993.64013671875, Val_Loss: 5286.94189453125\n",
      "Epoch 2935, Train_Loss: 4993.4140625, Val_Loss: 5286.69189453125\n",
      "Epoch 2936, Train_Loss: 4993.15869140625, Val_Loss: 5286.4638671875\n",
      "Epoch 2937, Train_Loss: 4992.9326171875, Val_Loss: 5286.1787109375\n",
      "Epoch 2938, Train_Loss: 4992.7177734375, Val_Loss: 5285.92236328125\n",
      "Epoch 2939, Train_Loss: 4992.47021484375, Val_Loss: 5285.66796875\n",
      "Epoch 2940, Train_Loss: 4992.26171875, Val_Loss: 5285.41845703125\n",
      "Epoch 2941, Train_Loss: 4992.0537109375, Val_Loss: 5285.1708984375\n",
      "Epoch 2942, Train_Loss: 4991.8388671875, Val_Loss: 5284.916015625\n",
      "Epoch 2943, Train_Loss: 4991.61767578125, Val_Loss: 5284.68017578125\n",
      "Epoch 2944, Train_Loss: 4991.39013671875, Val_Loss: 5284.43408203125\n",
      "Epoch 2945, Train_Loss: 4991.171875, Val_Loss: 5284.1943359375\n",
      "Epoch 2946, Train_Loss: 4990.89794921875, Val_Loss: 5283.939453125\n",
      "Epoch 2947, Train_Loss: 4990.6796875, Val_Loss: 5283.68359375\n",
      "Epoch 2948, Train_Loss: 4990.4609375, Val_Loss: 5283.44921875\n",
      "Epoch 2949, Train_Loss: 4990.18408203125, Val_Loss: 5283.20458984375\n",
      "Epoch 2950, Train_Loss: 4989.9677734375, Val_Loss: 5282.953125\n",
      "Epoch 2951, Train_Loss: 4989.7578125, Val_Loss: 5282.7138671875\n",
      "Epoch 2952, Train_Loss: 4989.53857421875, Val_Loss: 5282.47216796875\n",
      "Epoch 2953, Train_Loss: 4989.32568359375, Val_Loss: 5282.21337890625\n",
      "Epoch 2954, Train_Loss: 4989.08984375, Val_Loss: 5281.96337890625\n",
      "Epoch 2955, Train_Loss: 4988.7978515625, Val_Loss: 5281.72216796875\n",
      "Epoch 2956, Train_Loss: 4988.5927734375, Val_Loss: 5281.48193359375\n",
      "Epoch 2957, Train_Loss: 4988.37060546875, Val_Loss: 5281.2353515625\n",
      "Epoch 2958, Train_Loss: 4988.14501953125, Val_Loss: 5280.9794921875\n",
      "Epoch 2959, Train_Loss: 4987.92431640625, Val_Loss: 5280.72900390625\n",
      "Epoch 2960, Train_Loss: 4987.6953125, Val_Loss: 5280.48486328125\n",
      "Epoch 2961, Train_Loss: 4987.466796875, Val_Loss: 5280.2275390625\n",
      "Epoch 2962, Train_Loss: 4987.24365234375, Val_Loss: 5279.986328125\n",
      "Epoch 2963, Train_Loss: 4986.97998046875, Val_Loss: 5279.75439453125\n",
      "Epoch 2964, Train_Loss: 4986.76513671875, Val_Loss: 5279.49609375\n",
      "Epoch 2965, Train_Loss: 4986.5341796875, Val_Loss: 5279.244140625\n",
      "Epoch 2966, Train_Loss: 4986.3447265625, Val_Loss: 5278.98095703125\n",
      "Epoch 2967, Train_Loss: 4986.13671875, Val_Loss: 5278.732421875\n",
      "Epoch 2968, Train_Loss: 4985.9033203125, Val_Loss: 5278.5048828125\n",
      "Epoch 2969, Train_Loss: 4985.69091796875, Val_Loss: 5278.25390625\n",
      "Epoch 2970, Train_Loss: 4985.482421875, Val_Loss: 5278.0126953125\n",
      "Epoch 2971, Train_Loss: 4985.26611328125, Val_Loss: 5277.76513671875\n",
      "Epoch 2972, Train_Loss: 4985.04541015625, Val_Loss: 5277.5087890625\n",
      "Epoch 2973, Train_Loss: 4984.80810546875, Val_Loss: 5277.26220703125\n",
      "Epoch 2974, Train_Loss: 4984.5703125, Val_Loss: 5276.984375\n",
      "Epoch 2975, Train_Loss: 4984.36083984375, Val_Loss: 5276.7373046875\n",
      "Epoch 2976, Train_Loss: 4984.10009765625, Val_Loss: 5276.49658203125\n",
      "Epoch 2977, Train_Loss: 4983.88037109375, Val_Loss: 5276.24658203125\n",
      "Epoch 2978, Train_Loss: 4983.6533203125, Val_Loss: 5276.01220703125\n",
      "Epoch 2979, Train_Loss: 4983.4375, Val_Loss: 5275.751953125\n",
      "Epoch 2980, Train_Loss: 4983.2236328125, Val_Loss: 5275.51123046875\n",
      "Epoch 2981, Train_Loss: 4983.00830078125, Val_Loss: 5275.2578125\n",
      "Epoch 2982, Train_Loss: 4982.80126953125, Val_Loss: 5275.0322265625\n",
      "Epoch 2983, Train_Loss: 4982.5751953125, Val_Loss: 5274.7744140625\n",
      "Epoch 2984, Train_Loss: 4982.34765625, Val_Loss: 5274.52978515625\n",
      "Epoch 2985, Train_Loss: 4982.13818359375, Val_Loss: 5274.2861328125\n",
      "Epoch 2986, Train_Loss: 4981.9248046875, Val_Loss: 5274.02099609375\n",
      "Epoch 2987, Train_Loss: 4981.70068359375, Val_Loss: 5273.79248046875\n",
      "Epoch 2988, Train_Loss: 4981.47900390625, Val_Loss: 5273.54638671875\n",
      "Epoch 2989, Train_Loss: 4981.236328125, Val_Loss: 5273.30517578125\n",
      "Epoch 2990, Train_Loss: 4981.02392578125, Val_Loss: 5273.0517578125\n",
      "Epoch 2991, Train_Loss: 4980.80419921875, Val_Loss: 5272.8037109375\n",
      "Epoch 2992, Train_Loss: 4980.63427734375, Val_Loss: 5272.552734375\n",
      "Epoch 2993, Train_Loss: 4980.376953125, Val_Loss: 5272.2998046875\n",
      "Epoch 2994, Train_Loss: 4980.162109375, Val_Loss: 5272.0478515625\n",
      "Epoch 2995, Train_Loss: 4979.9423828125, Val_Loss: 5271.81005859375\n",
      "Epoch 2996, Train_Loss: 4979.7265625, Val_Loss: 5271.5673828125\n",
      "Epoch 2997, Train_Loss: 4979.50927734375, Val_Loss: 5271.31201171875\n",
      "Epoch 2998, Train_Loss: 4979.28515625, Val_Loss: 5271.080078125\n",
      "Epoch 2999, Train_Loss: 4979.07861328125, Val_Loss: 5270.8251953125\n",
      "Epoch 3000, Train_Loss: 4978.86181640625, Val_Loss: 5270.5791015625\n",
      "Epoch 3001, Train_Loss: 4978.64013671875, Val_Loss: 5270.3388671875\n",
      "Epoch 3002, Train_Loss: 4978.40283203125, Val_Loss: 5270.08740234375\n",
      "Epoch 3003, Train_Loss: 4978.16650390625, Val_Loss: 5269.84423828125\n",
      "Epoch 3004, Train_Loss: 4977.96875, Val_Loss: 5269.59228515625\n",
      "Epoch 3005, Train_Loss: 4977.7490234375, Val_Loss: 5269.3271484375\n",
      "Epoch 3006, Train_Loss: 4977.53857421875, Val_Loss: 5269.08642578125\n",
      "Epoch 3007, Train_Loss: 4977.30810546875, Val_Loss: 5268.83740234375\n",
      "Epoch 3008, Train_Loss: 4977.08251953125, Val_Loss: 5268.5966796875\n",
      "Epoch 3009, Train_Loss: 4976.857421875, Val_Loss: 5268.35302734375\n",
      "Epoch 3010, Train_Loss: 4976.634765625, Val_Loss: 5268.10302734375\n",
      "Epoch 3011, Train_Loss: 4976.4306640625, Val_Loss: 5267.83203125\n",
      "Epoch 3012, Train_Loss: 4976.22998046875, Val_Loss: 5267.57275390625\n",
      "Epoch 3013, Train_Loss: 4975.98681640625, Val_Loss: 5267.33984375\n",
      "Epoch 3014, Train_Loss: 4975.77587890625, Val_Loss: 5267.08056640625\n",
      "Epoch 3015, Train_Loss: 4975.501953125, Val_Loss: 5266.8486328125\n",
      "Epoch 3016, Train_Loss: 4975.28662109375, Val_Loss: 5266.61181640625\n",
      "Epoch 3017, Train_Loss: 4975.064453125, Val_Loss: 5266.36083984375\n",
      "Epoch 3018, Train_Loss: 4974.84619140625, Val_Loss: 5266.11865234375\n",
      "Epoch 3019, Train_Loss: 4974.65283203125, Val_Loss: 5265.86083984375\n",
      "Epoch 3020, Train_Loss: 4974.4111328125, Val_Loss: 5265.62060546875\n",
      "Epoch 3021, Train_Loss: 4974.20263671875, Val_Loss: 5265.37890625\n",
      "Epoch 3022, Train_Loss: 4973.9951171875, Val_Loss: 5265.13525390625\n",
      "Epoch 3023, Train_Loss: 4973.779296875, Val_Loss: 5264.89697265625\n",
      "Epoch 3024, Train_Loss: 4973.552734375, Val_Loss: 5264.6533203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3025, Train_Loss: 4973.3408203125, Val_Loss: 5264.38818359375\n",
      "Epoch 3026, Train_Loss: 4973.13720703125, Val_Loss: 5264.14599609375\n",
      "Epoch 3027, Train_Loss: 4972.91748046875, Val_Loss: 5263.892578125\n",
      "Epoch 3028, Train_Loss: 4972.66552734375, Val_Loss: 5263.6640625\n",
      "Epoch 3029, Train_Loss: 4972.45556640625, Val_Loss: 5263.408203125\n",
      "Epoch 3030, Train_Loss: 4972.19140625, Val_Loss: 5263.16748046875\n",
      "Epoch 3031, Train_Loss: 4971.97265625, Val_Loss: 5262.9345703125\n",
      "Epoch 3032, Train_Loss: 4971.75830078125, Val_Loss: 5262.67822265625\n",
      "Epoch 3033, Train_Loss: 4971.5087890625, Val_Loss: 5262.4287109375\n",
      "Epoch 3034, Train_Loss: 4971.296875, Val_Loss: 5262.1796875\n",
      "Epoch 3035, Train_Loss: 4971.07958984375, Val_Loss: 5261.93505859375\n",
      "Epoch 3036, Train_Loss: 4970.8671875, Val_Loss: 5261.693359375\n",
      "Epoch 3037, Train_Loss: 4970.650390625, Val_Loss: 5261.4501953125\n",
      "Epoch 3038, Train_Loss: 4970.423828125, Val_Loss: 5261.193359375\n",
      "Epoch 3039, Train_Loss: 4970.21142578125, Val_Loss: 5260.93994140625\n",
      "Epoch 3040, Train_Loss: 4969.99169921875, Val_Loss: 5260.69970703125\n",
      "Epoch 3041, Train_Loss: 4969.78662109375, Val_Loss: 5260.44921875\n",
      "Epoch 3042, Train_Loss: 4969.54248046875, Val_Loss: 5260.21435546875\n",
      "Epoch 3043, Train_Loss: 4969.30078125, Val_Loss: 5259.98193359375\n",
      "Epoch 3044, Train_Loss: 4969.09619140625, Val_Loss: 5259.72802734375\n",
      "Epoch 3045, Train_Loss: 4968.875, Val_Loss: 5259.47509765625\n",
      "Epoch 3046, Train_Loss: 4968.65673828125, Val_Loss: 5259.23291015625\n",
      "Epoch 3047, Train_Loss: 4968.43505859375, Val_Loss: 5258.9921875\n",
      "Epoch 3048, Train_Loss: 4968.18408203125, Val_Loss: 5258.7265625\n",
      "Epoch 3049, Train_Loss: 4967.955078125, Val_Loss: 5258.4736328125\n",
      "Epoch 3050, Train_Loss: 4967.74755859375, Val_Loss: 5258.22607421875\n",
      "Epoch 3051, Train_Loss: 4967.5390625, Val_Loss: 5257.9755859375\n",
      "Epoch 3052, Train_Loss: 4967.32421875, Val_Loss: 5257.724609375\n",
      "Epoch 3053, Train_Loss: 4967.091796875, Val_Loss: 5257.4794921875\n",
      "Epoch 3054, Train_Loss: 4966.87255859375, Val_Loss: 5257.23291015625\n",
      "Epoch 3055, Train_Loss: 4966.60498046875, Val_Loss: 5257.00244140625\n",
      "Epoch 3056, Train_Loss: 4966.40185546875, Val_Loss: 5256.76318359375\n",
      "Epoch 3057, Train_Loss: 4966.1796875, Val_Loss: 5256.52099609375\n",
      "Epoch 3058, Train_Loss: 4965.96142578125, Val_Loss: 5256.27099609375\n",
      "Epoch 3059, Train_Loss: 4965.74072265625, Val_Loss: 5256.0048828125\n",
      "Epoch 3060, Train_Loss: 4965.51611328125, Val_Loss: 5255.771484375\n",
      "Epoch 3061, Train_Loss: 4965.302734375, Val_Loss: 5255.5380859375\n",
      "Epoch 3062, Train_Loss: 4965.08056640625, Val_Loss: 5255.29296875\n",
      "Epoch 3063, Train_Loss: 4964.85205078125, Val_Loss: 5255.04931640625\n",
      "Epoch 3064, Train_Loss: 4964.64892578125, Val_Loss: 5254.80029296875\n",
      "Epoch 3065, Train_Loss: 4964.4384765625, Val_Loss: 5254.5546875\n",
      "Epoch 3066, Train_Loss: 4964.23291015625, Val_Loss: 5254.3046875\n",
      "Epoch 3067, Train_Loss: 4964.01025390625, Val_Loss: 5254.04833984375\n",
      "Epoch 3068, Train_Loss: 4963.75927734375, Val_Loss: 5253.81103515625\n",
      "Epoch 3069, Train_Loss: 4963.544921875, Val_Loss: 5253.56787109375\n",
      "Epoch 3070, Train_Loss: 4963.318359375, Val_Loss: 5253.326171875\n",
      "Epoch 3071, Train_Loss: 4963.13916015625, Val_Loss: 5253.06005859375\n",
      "Epoch 3072, Train_Loss: 4962.92138671875, Val_Loss: 5252.82470703125\n",
      "Epoch 3073, Train_Loss: 4962.67822265625, Val_Loss: 5252.5830078125\n",
      "Epoch 3074, Train_Loss: 4962.4658203125, Val_Loss: 5252.3388671875\n",
      "Epoch 3075, Train_Loss: 4962.25634765625, Val_Loss: 5252.10009765625\n",
      "Epoch 3076, Train_Loss: 4962.05078125, Val_Loss: 5251.84326171875\n",
      "Epoch 3077, Train_Loss: 4961.8310546875, Val_Loss: 5251.59521484375\n",
      "Epoch 3078, Train_Loss: 4961.60888671875, Val_Loss: 5251.3427734375\n",
      "Epoch 3079, Train_Loss: 4961.4052734375, Val_Loss: 5251.109375\n",
      "Epoch 3080, Train_Loss: 4961.197265625, Val_Loss: 5250.85986328125\n",
      "Epoch 3081, Train_Loss: 4960.9384765625, Val_Loss: 5250.61962890625\n",
      "Epoch 3082, Train_Loss: 4960.73681640625, Val_Loss: 5250.38720703125\n",
      "Epoch 3083, Train_Loss: 4960.4912109375, Val_Loss: 5250.1337890625\n",
      "Epoch 3084, Train_Loss: 4960.28515625, Val_Loss: 5249.875\n",
      "Epoch 3085, Train_Loss: 4960.06591796875, Val_Loss: 5249.64111328125\n",
      "Epoch 3086, Train_Loss: 4959.845703125, Val_Loss: 5249.35595703125\n",
      "Epoch 3087, Train_Loss: 4959.63232421875, Val_Loss: 5249.10595703125\n",
      "Epoch 3088, Train_Loss: 4959.41552734375, Val_Loss: 5248.87109375\n",
      "Epoch 3089, Train_Loss: 4959.2001953125, Val_Loss: 5248.63134765625\n",
      "Epoch 3090, Train_Loss: 4958.98779296875, Val_Loss: 5248.38818359375\n",
      "Epoch 3091, Train_Loss: 4958.78271484375, Val_Loss: 5248.14111328125\n",
      "Epoch 3092, Train_Loss: 4958.5517578125, Val_Loss: 5247.89453125\n",
      "Epoch 3093, Train_Loss: 4958.326171875, Val_Loss: 5247.6513671875\n",
      "Epoch 3094, Train_Loss: 4958.0693359375, Val_Loss: 5247.4169921875\n",
      "Epoch 3095, Train_Loss: 4957.78955078125, Val_Loss: 5247.17724609375\n",
      "Epoch 3096, Train_Loss: 4957.5732421875, Val_Loss: 5246.92529296875\n",
      "Epoch 3097, Train_Loss: 4957.365234375, Val_Loss: 5246.6806640625\n",
      "Epoch 3098, Train_Loss: 4957.17822265625, Val_Loss: 5246.42529296875\n",
      "Epoch 3099, Train_Loss: 4956.9658203125, Val_Loss: 5246.18408203125\n",
      "Epoch 3100, Train_Loss: 4956.751953125, Val_Loss: 5245.9501953125\n",
      "Epoch 3101, Train_Loss: 4956.541015625, Val_Loss: 5245.7236328125\n",
      "Epoch 3102, Train_Loss: 4956.32861328125, Val_Loss: 5245.46337890625\n",
      "Epoch 3103, Train_Loss: 4956.11181640625, Val_Loss: 5245.2236328125\n",
      "Epoch 3104, Train_Loss: 4955.82373046875, Val_Loss: 5244.95849609375\n",
      "Epoch 3105, Train_Loss: 4955.6162109375, Val_Loss: 5244.72021484375\n",
      "Epoch 3106, Train_Loss: 4955.400390625, Val_Loss: 5244.48779296875\n",
      "Epoch 3107, Train_Loss: 4955.1962890625, Val_Loss: 5244.23388671875\n",
      "Epoch 3108, Train_Loss: 4954.9658203125, Val_Loss: 5244.0029296875\n",
      "Epoch 3109, Train_Loss: 4954.75927734375, Val_Loss: 5243.7578125\n",
      "Epoch 3110, Train_Loss: 4954.54833984375, Val_Loss: 5243.5166015625\n",
      "Epoch 3111, Train_Loss: 4954.33837890625, Val_Loss: 5243.259765625\n",
      "Epoch 3112, Train_Loss: 4954.11572265625, Val_Loss: 5243.0205078125\n",
      "Epoch 3113, Train_Loss: 4953.8720703125, Val_Loss: 5242.76416015625\n",
      "Epoch 3114, Train_Loss: 4953.66015625, Val_Loss: 5242.52783203125\n",
      "Epoch 3115, Train_Loss: 4953.44921875, Val_Loss: 5242.275390625\n",
      "Epoch 3116, Train_Loss: 4953.23974609375, Val_Loss: 5242.0419921875\n",
      "Epoch 3117, Train_Loss: 4953.02490234375, Val_Loss: 5241.78662109375\n",
      "Epoch 3118, Train_Loss: 4952.80517578125, Val_Loss: 5241.53515625\n",
      "Epoch 3119, Train_Loss: 4952.5849609375, Val_Loss: 5241.2802734375\n",
      "Epoch 3120, Train_Loss: 4952.37158203125, Val_Loss: 5241.0458984375\n",
      "Epoch 3121, Train_Loss: 4952.09912109375, Val_Loss: 5240.81787109375\n",
      "Epoch 3122, Train_Loss: 4951.8916015625, Val_Loss: 5240.57421875\n",
      "Epoch 3123, Train_Loss: 4951.642578125, Val_Loss: 5240.2998046875\n",
      "Epoch 3124, Train_Loss: 4951.46044921875, Val_Loss: 5240.03857421875\n",
      "Epoch 3125, Train_Loss: 4951.26171875, Val_Loss: 5239.79931640625\n",
      "Epoch 3126, Train_Loss: 4951.04541015625, Val_Loss: 5239.53662109375\n",
      "Epoch 3127, Train_Loss: 4950.83203125, Val_Loss: 5239.25927734375\n",
      "Epoch 3128, Train_Loss: 4950.587890625, Val_Loss: 5239.01953125\n",
      "Epoch 3129, Train_Loss: 4950.3642578125, Val_Loss: 5238.7744140625\n",
      "Epoch 3130, Train_Loss: 4950.1572265625, Val_Loss: 5238.52294921875\n",
      "Epoch 3131, Train_Loss: 4949.94140625, Val_Loss: 5238.26416015625\n",
      "Epoch 3132, Train_Loss: 4949.71630859375, Val_Loss: 5238.01904296875\n",
      "Epoch 3133, Train_Loss: 4949.49072265625, Val_Loss: 5237.78564453125\n",
      "Epoch 3134, Train_Loss: 4949.2353515625, Val_Loss: 5237.54150390625\n",
      "Epoch 3135, Train_Loss: 4949.0390625, Val_Loss: 5237.30419921875\n",
      "Epoch 3136, Train_Loss: 4948.8388671875, Val_Loss: 5237.06494140625\n",
      "Epoch 3137, Train_Loss: 4948.62744140625, Val_Loss: 5236.80419921875\n",
      "Epoch 3138, Train_Loss: 4948.4052734375, Val_Loss: 5236.56103515625\n",
      "Epoch 3139, Train_Loss: 4948.18798828125, Val_Loss: 5236.32421875\n",
      "Epoch 3140, Train_Loss: 4947.9716796875, Val_Loss: 5236.08740234375\n",
      "Epoch 3141, Train_Loss: 4947.7548828125, Val_Loss: 5235.8408203125\n",
      "Epoch 3142, Train_Loss: 4947.544921875, Val_Loss: 5235.60009765625\n",
      "Epoch 3143, Train_Loss: 4947.32177734375, Val_Loss: 5235.3515625\n",
      "Epoch 3144, Train_Loss: 4947.11669921875, Val_Loss: 5235.09912109375\n",
      "Epoch 3145, Train_Loss: 4946.89697265625, Val_Loss: 5234.85693359375\n",
      "Epoch 3146, Train_Loss: 4946.70654296875, Val_Loss: 5234.62255859375\n",
      "Epoch 3147, Train_Loss: 4946.44970703125, Val_Loss: 5234.39697265625\n",
      "Epoch 3148, Train_Loss: 4946.21630859375, Val_Loss: 5234.1513671875\n",
      "Epoch 3149, Train_Loss: 4946.01513671875, Val_Loss: 5233.90185546875\n",
      "Epoch 3150, Train_Loss: 4945.79345703125, Val_Loss: 5233.6494140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3151, Train_Loss: 4945.5693359375, Val_Loss: 5233.38623046875\n",
      "Epoch 3152, Train_Loss: 4945.32373046875, Val_Loss: 5233.14697265625\n",
      "Epoch 3153, Train_Loss: 4945.0966796875, Val_Loss: 5232.90869140625\n",
      "Epoch 3154, Train_Loss: 4944.884765625, Val_Loss: 5232.6708984375\n",
      "Epoch 3155, Train_Loss: 4944.6748046875, Val_Loss: 5232.4248046875\n",
      "Epoch 3156, Train_Loss: 4944.47021484375, Val_Loss: 5232.18408203125\n",
      "Epoch 3157, Train_Loss: 4944.26025390625, Val_Loss: 5231.93212890625\n",
      "Epoch 3158, Train_Loss: 4944.037109375, Val_Loss: 5231.6982421875\n",
      "Epoch 3159, Train_Loss: 4943.83154296875, Val_Loss: 5231.453125\n",
      "Epoch 3160, Train_Loss: 4943.5615234375, Val_Loss: 5231.18603515625\n",
      "Epoch 3161, Train_Loss: 4943.34423828125, Val_Loss: 5230.93994140625\n",
      "Epoch 3162, Train_Loss: 4943.1376953125, Val_Loss: 5230.69580078125\n",
      "Epoch 3163, Train_Loss: 4942.89208984375, Val_Loss: 5230.4482421875\n",
      "Epoch 3164, Train_Loss: 4942.69482421875, Val_Loss: 5230.1845703125\n",
      "Epoch 3165, Train_Loss: 4942.48486328125, Val_Loss: 5229.95263671875\n",
      "Epoch 3166, Train_Loss: 4942.26953125, Val_Loss: 5229.72021484375\n",
      "Epoch 3167, Train_Loss: 4942.064453125, Val_Loss: 5229.47998046875\n",
      "Epoch 3168, Train_Loss: 4941.8310546875, Val_Loss: 5229.23828125\n",
      "Epoch 3169, Train_Loss: 4941.6005859375, Val_Loss: 5228.99658203125\n",
      "Epoch 3170, Train_Loss: 4941.38720703125, Val_Loss: 5228.74755859375\n",
      "Epoch 3171, Train_Loss: 4941.177734375, Val_Loss: 5228.505859375\n",
      "Epoch 3172, Train_Loss: 4940.978515625, Val_Loss: 5228.2685546875\n",
      "Epoch 3173, Train_Loss: 4940.7236328125, Val_Loss: 5228.041015625\n",
      "Epoch 3174, Train_Loss: 4940.51953125, Val_Loss: 5227.78271484375\n",
      "Epoch 3175, Train_Loss: 4940.33154296875, Val_Loss: 5227.55224609375\n",
      "Epoch 3176, Train_Loss: 4940.1181640625, Val_Loss: 5227.3173828125\n",
      "Epoch 3177, Train_Loss: 4939.9228515625, Val_Loss: 5227.068359375\n",
      "Epoch 3178, Train_Loss: 4939.68896484375, Val_Loss: 5226.82177734375\n",
      "Epoch 3179, Train_Loss: 4939.35107421875, Val_Loss: 5226.5712890625\n",
      "Epoch 3180, Train_Loss: 4939.14892578125, Val_Loss: 5226.3388671875\n",
      "Epoch 3181, Train_Loss: 4938.9453125, Val_Loss: 5226.1005859375\n",
      "Epoch 3182, Train_Loss: 4938.74072265625, Val_Loss: 5225.84912109375\n",
      "Epoch 3183, Train_Loss: 4938.52099609375, Val_Loss: 5225.59619140625\n",
      "Epoch 3184, Train_Loss: 4938.32666015625, Val_Loss: 5225.34814453125\n",
      "Epoch 3185, Train_Loss: 4938.1064453125, Val_Loss: 5225.10498046875\n",
      "Epoch 3186, Train_Loss: 4937.900390625, Val_Loss: 5224.8740234375\n",
      "Epoch 3187, Train_Loss: 4937.64404296875, Val_Loss: 5224.642578125\n",
      "Epoch 3188, Train_Loss: 4937.42578125, Val_Loss: 5224.39404296875\n",
      "Epoch 3189, Train_Loss: 4937.2041015625, Val_Loss: 5224.14306640625\n",
      "Epoch 3190, Train_Loss: 4937.00244140625, Val_Loss: 5223.90087890625\n",
      "Epoch 3191, Train_Loss: 4936.7763671875, Val_Loss: 5223.6611328125\n",
      "Epoch 3192, Train_Loss: 4936.5693359375, Val_Loss: 5223.42138671875\n",
      "Epoch 3193, Train_Loss: 4936.330078125, Val_Loss: 5223.18359375\n",
      "Epoch 3194, Train_Loss: 4936.11376953125, Val_Loss: 5222.93017578125\n",
      "Epoch 3195, Train_Loss: 4935.9130859375, Val_Loss: 5222.6982421875\n",
      "Epoch 3196, Train_Loss: 4935.71240234375, Val_Loss: 5222.46484375\n",
      "Epoch 3197, Train_Loss: 4935.46240234375, Val_Loss: 5222.1826171875\n",
      "Epoch 3198, Train_Loss: 4935.24658203125, Val_Loss: 5221.94140625\n",
      "Epoch 3199, Train_Loss: 4935.0400390625, Val_Loss: 5221.7021484375\n",
      "Epoch 3200, Train_Loss: 4934.80224609375, Val_Loss: 5221.46533203125\n",
      "Epoch 3201, Train_Loss: 4934.60205078125, Val_Loss: 5221.2158203125\n",
      "Epoch 3202, Train_Loss: 4934.3857421875, Val_Loss: 5220.97900390625\n",
      "Epoch 3203, Train_Loss: 4934.1923828125, Val_Loss: 5220.7392578125\n",
      "Epoch 3204, Train_Loss: 4933.96923828125, Val_Loss: 5220.48388671875\n",
      "Epoch 3205, Train_Loss: 4933.763671875, Val_Loss: 5220.25146484375\n",
      "Epoch 3206, Train_Loss: 4933.5546875, Val_Loss: 5220.01611328125\n",
      "Epoch 3207, Train_Loss: 4933.33544921875, Val_Loss: 5219.775390625\n",
      "Epoch 3208, Train_Loss: 4933.1064453125, Val_Loss: 5219.54248046875\n",
      "Epoch 3209, Train_Loss: 4932.8994140625, Val_Loss: 5219.30419921875\n",
      "Epoch 3210, Train_Loss: 4932.693359375, Val_Loss: 5219.0517578125\n",
      "Epoch 3211, Train_Loss: 4932.4794921875, Val_Loss: 5218.806640625\n",
      "Epoch 3212, Train_Loss: 4932.27490234375, Val_Loss: 5218.56591796875\n",
      "Epoch 3213, Train_Loss: 4932.0205078125, Val_Loss: 5218.3388671875\n",
      "Epoch 3214, Train_Loss: 4931.80224609375, Val_Loss: 5218.0966796875\n",
      "Epoch 3215, Train_Loss: 4931.599609375, Val_Loss: 5217.84375\n",
      "Epoch 3216, Train_Loss: 4931.3818359375, Val_Loss: 5217.583984375\n",
      "Epoch 3217, Train_Loss: 4931.17333984375, Val_Loss: 5217.34716796875\n",
      "Epoch 3218, Train_Loss: 4930.9609375, Val_Loss: 5217.11181640625\n",
      "Epoch 3219, Train_Loss: 4930.74365234375, Val_Loss: 5216.8671875\n",
      "Epoch 3220, Train_Loss: 4930.53759765625, Val_Loss: 5216.63037109375\n",
      "Epoch 3221, Train_Loss: 4930.341796875, Val_Loss: 5216.392578125\n",
      "Epoch 3222, Train_Loss: 4930.13671875, Val_Loss: 5216.16455078125\n",
      "Epoch 3223, Train_Loss: 4929.9296875, Val_Loss: 5215.90380859375\n",
      "Epoch 3224, Train_Loss: 4929.72802734375, Val_Loss: 5215.666015625\n",
      "Epoch 3225, Train_Loss: 4929.51953125, Val_Loss: 5215.404296875\n",
      "Epoch 3226, Train_Loss: 4929.2685546875, Val_Loss: 5215.1767578125\n",
      "Epoch 3227, Train_Loss: 4929.0517578125, Val_Loss: 5214.94189453125\n",
      "Epoch 3228, Train_Loss: 4928.83984375, Val_Loss: 5214.7001953125\n",
      "Epoch 3229, Train_Loss: 4928.6123046875, Val_Loss: 5214.458984375\n",
      "Epoch 3230, Train_Loss: 4928.421875, Val_Loss: 5214.2099609375\n",
      "Epoch 3231, Train_Loss: 4928.21728515625, Val_Loss: 5213.9765625\n",
      "Epoch 3232, Train_Loss: 4928.01416015625, Val_Loss: 5213.732421875\n",
      "Epoch 3233, Train_Loss: 4927.7880859375, Val_Loss: 5213.50439453125\n",
      "Epoch 3234, Train_Loss: 4927.58203125, Val_Loss: 5213.22412109375\n",
      "Epoch 3235, Train_Loss: 4927.38037109375, Val_Loss: 5212.97119140625\n",
      "Epoch 3236, Train_Loss: 4927.17724609375, Val_Loss: 5212.72998046875\n",
      "Epoch 3237, Train_Loss: 4926.96435546875, Val_Loss: 5212.48876953125\n",
      "Epoch 3238, Train_Loss: 4926.74951171875, Val_Loss: 5212.25634765625\n",
      "Epoch 3239, Train_Loss: 4926.49072265625, Val_Loss: 5212.013671875\n",
      "Epoch 3240, Train_Loss: 4926.283203125, Val_Loss: 5211.775390625\n",
      "Epoch 3241, Train_Loss: 4926.0791015625, Val_Loss: 5211.5341796875\n",
      "Epoch 3242, Train_Loss: 4925.8642578125, Val_Loss: 5211.30517578125\n",
      "Epoch 3243, Train_Loss: 4925.63916015625, Val_Loss: 5211.0498046875\n",
      "Epoch 3244, Train_Loss: 4925.423828125, Val_Loss: 5210.8154296875\n",
      "Epoch 3245, Train_Loss: 4925.20263671875, Val_Loss: 5210.57763671875\n",
      "Epoch 3246, Train_Loss: 4924.99267578125, Val_Loss: 5210.3359375\n",
      "Epoch 3247, Train_Loss: 4924.77783203125, Val_Loss: 5210.10009765625\n",
      "Epoch 3248, Train_Loss: 4924.560546875, Val_Loss: 5209.8544921875\n",
      "Epoch 3249, Train_Loss: 4924.333984375, Val_Loss: 5209.60693359375\n",
      "Epoch 3250, Train_Loss: 4924.1328125, Val_Loss: 5209.35986328125\n",
      "Epoch 3251, Train_Loss: 4923.9033203125, Val_Loss: 5209.1220703125\n",
      "Epoch 3252, Train_Loss: 4923.6943359375, Val_Loss: 5208.888671875\n",
      "Epoch 3253, Train_Loss: 4923.3564453125, Val_Loss: 5208.65576171875\n",
      "Epoch 3254, Train_Loss: 4923.1591796875, Val_Loss: 5208.41845703125\n",
      "Epoch 3255, Train_Loss: 4922.95458984375, Val_Loss: 5208.18408203125\n",
      "Epoch 3256, Train_Loss: 4922.73095703125, Val_Loss: 5207.91650390625\n",
      "Epoch 3257, Train_Loss: 4922.533203125, Val_Loss: 5207.6826171875\n",
      "Epoch 3258, Train_Loss: 4922.314453125, Val_Loss: 5207.44482421875\n",
      "Epoch 3259, Train_Loss: 4922.1025390625, Val_Loss: 5207.20361328125\n",
      "Epoch 3260, Train_Loss: 4921.89306640625, Val_Loss: 5206.97021484375\n",
      "Epoch 3261, Train_Loss: 4921.6923828125, Val_Loss: 5206.73583984375\n",
      "Epoch 3262, Train_Loss: 4921.484375, Val_Loss: 5206.48779296875\n",
      "Epoch 3263, Train_Loss: 4921.26708984375, Val_Loss: 5206.24169921875\n",
      "Epoch 3264, Train_Loss: 4921.056640625, Val_Loss: 5205.98876953125\n",
      "Epoch 3265, Train_Loss: 4920.84912109375, Val_Loss: 5205.751953125\n",
      "Epoch 3266, Train_Loss: 4920.58056640625, Val_Loss: 5205.51708984375\n",
      "Epoch 3267, Train_Loss: 4920.3583984375, Val_Loss: 5205.2900390625\n",
      "Epoch 3268, Train_Loss: 4920.15234375, Val_Loss: 5205.03662109375\n",
      "Epoch 3269, Train_Loss: 4919.9462890625, Val_Loss: 5204.7783203125\n",
      "Epoch 3270, Train_Loss: 4919.7294921875, Val_Loss: 5204.54736328125\n",
      "Epoch 3271, Train_Loss: 4919.5107421875, Val_Loss: 5204.3046875\n",
      "Epoch 3272, Train_Loss: 4919.2900390625, Val_Loss: 5204.044921875\n",
      "Epoch 3273, Train_Loss: 4919.06201171875, Val_Loss: 5203.798828125\n",
      "Epoch 3274, Train_Loss: 4918.83837890625, Val_Loss: 5203.56005859375\n",
      "Epoch 3275, Train_Loss: 4918.6298828125, Val_Loss: 5203.33056640625\n",
      "Epoch 3276, Train_Loss: 4918.4501953125, Val_Loss: 5203.0810546875\n",
      "Epoch 3277, Train_Loss: 4918.23583984375, Val_Loss: 5202.83837890625\n",
      "Epoch 3278, Train_Loss: 4918.02490234375, Val_Loss: 5202.607421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3279, Train_Loss: 4917.8017578125, Val_Loss: 5202.3740234375\n",
      "Epoch 3280, Train_Loss: 4917.60107421875, Val_Loss: 5202.13720703125\n",
      "Epoch 3281, Train_Loss: 4917.38330078125, Val_Loss: 5201.8935546875\n",
      "Epoch 3282, Train_Loss: 4917.20068359375, Val_Loss: 5201.63037109375\n",
      "Epoch 3283, Train_Loss: 4916.97412109375, Val_Loss: 5201.3896484375\n",
      "Epoch 3284, Train_Loss: 4916.75634765625, Val_Loss: 5201.15185546875\n",
      "Epoch 3285, Train_Loss: 4916.54345703125, Val_Loss: 5200.91748046875\n",
      "Epoch 3286, Train_Loss: 4916.34033203125, Val_Loss: 5200.6826171875\n",
      "Epoch 3287, Train_Loss: 4916.134765625, Val_Loss: 5200.451171875\n",
      "Epoch 3288, Train_Loss: 4915.9140625, Val_Loss: 5200.22216796875\n",
      "Epoch 3289, Train_Loss: 4915.69775390625, Val_Loss: 5199.97509765625\n",
      "Epoch 3290, Train_Loss: 4915.48388671875, Val_Loss: 5199.72607421875\n",
      "Epoch 3291, Train_Loss: 4915.27001953125, Val_Loss: 5199.4775390625\n",
      "Epoch 3292, Train_Loss: 4915.0, Val_Loss: 5199.244140625\n",
      "Epoch 3293, Train_Loss: 4914.77587890625, Val_Loss: 5199.0146484375\n",
      "Epoch 3294, Train_Loss: 4914.5703125, Val_Loss: 5198.76904296875\n",
      "Epoch 3295, Train_Loss: 4914.373046875, Val_Loss: 5198.53173828125\n",
      "Epoch 3296, Train_Loss: 4914.17333984375, Val_Loss: 5198.2724609375\n",
      "Epoch 3297, Train_Loss: 4913.9619140625, Val_Loss: 5198.0517578125\n",
      "Epoch 3298, Train_Loss: 4913.7421875, Val_Loss: 5197.8095703125\n",
      "Epoch 3299, Train_Loss: 4913.53759765625, Val_Loss: 5197.56787109375\n",
      "Epoch 3300, Train_Loss: 4913.3173828125, Val_Loss: 5197.32177734375\n",
      "Epoch 3301, Train_Loss: 4913.11865234375, Val_Loss: 5197.08984375\n",
      "Epoch 3302, Train_Loss: 4912.9140625, Val_Loss: 5196.845703125\n",
      "Epoch 3303, Train_Loss: 4912.70849609375, Val_Loss: 5196.60400390625\n",
      "Epoch 3304, Train_Loss: 4912.498046875, Val_Loss: 5196.35546875\n",
      "Epoch 3305, Train_Loss: 4912.25, Val_Loss: 5196.12890625\n",
      "Epoch 3306, Train_Loss: 4912.052734375, Val_Loss: 5195.8837890625\n",
      "Epoch 3307, Train_Loss: 4911.85205078125, Val_Loss: 5195.65380859375\n",
      "Epoch 3308, Train_Loss: 4911.63525390625, Val_Loss: 5195.42626953125\n",
      "Epoch 3309, Train_Loss: 4911.47021484375, Val_Loss: 5195.12353515625\n",
      "Epoch 3310, Train_Loss: 4911.2529296875, Val_Loss: 5194.888671875\n",
      "Epoch 3311, Train_Loss: 4911.03955078125, Val_Loss: 5194.65380859375\n",
      "Epoch 3312, Train_Loss: 4910.8359375, Val_Loss: 5194.43408203125\n",
      "Epoch 3313, Train_Loss: 4910.6064453125, Val_Loss: 5194.18017578125\n",
      "Epoch 3314, Train_Loss: 4910.39697265625, Val_Loss: 5193.94287109375\n",
      "Epoch 3315, Train_Loss: 4910.19287109375, Val_Loss: 5193.68994140625\n",
      "Epoch 3316, Train_Loss: 4909.99072265625, Val_Loss: 5193.4521484375\n",
      "Epoch 3317, Train_Loss: 4909.77978515625, Val_Loss: 5193.22119140625\n",
      "Epoch 3318, Train_Loss: 4909.5419921875, Val_Loss: 5192.9775390625\n",
      "Epoch 3319, Train_Loss: 4909.3125, Val_Loss: 5192.75341796875\n",
      "Epoch 3320, Train_Loss: 4909.10791015625, Val_Loss: 5192.5048828125\n",
      "Epoch 3321, Train_Loss: 4908.90087890625, Val_Loss: 5192.25390625\n",
      "Epoch 3322, Train_Loss: 4908.67822265625, Val_Loss: 5192.0234375\n",
      "Epoch 3323, Train_Loss: 4908.46337890625, Val_Loss: 5191.78857421875\n",
      "Epoch 3324, Train_Loss: 4908.2626953125, Val_Loss: 5191.544921875\n",
      "Epoch 3325, Train_Loss: 4908.05712890625, Val_Loss: 5191.3095703125\n",
      "Epoch 3326, Train_Loss: 4907.84130859375, Val_Loss: 5191.0849609375\n",
      "Epoch 3327, Train_Loss: 4907.43505859375, Val_Loss: 5190.8408203125\n",
      "Epoch 3328, Train_Loss: 4907.2138671875, Val_Loss: 5190.59814453125\n",
      "Epoch 3329, Train_Loss: 4907.00732421875, Val_Loss: 5190.3369140625\n",
      "Epoch 3330, Train_Loss: 4906.81396484375, Val_Loss: 5190.10595703125\n",
      "Epoch 3331, Train_Loss: 4906.62060546875, Val_Loss: 5189.86376953125\n",
      "Epoch 3332, Train_Loss: 4906.345703125, Val_Loss: 5189.626953125\n",
      "Epoch 3333, Train_Loss: 4906.1376953125, Val_Loss: 5189.3994140625\n",
      "Epoch 3334, Train_Loss: 4905.927734375, Val_Loss: 5189.16162109375\n",
      "Epoch 3335, Train_Loss: 4905.744140625, Val_Loss: 5188.91943359375\n",
      "Epoch 3336, Train_Loss: 4905.544921875, Val_Loss: 5188.66552734375\n",
      "Epoch 3337, Train_Loss: 4905.31787109375, Val_Loss: 5188.431640625\n",
      "Epoch 3338, Train_Loss: 4905.11279296875, Val_Loss: 5188.19384765625\n",
      "Epoch 3339, Train_Loss: 4904.89599609375, Val_Loss: 5187.9609375\n",
      "Epoch 3340, Train_Loss: 4904.69873046875, Val_Loss: 5187.7236328125\n",
      "Epoch 3341, Train_Loss: 4904.48388671875, Val_Loss: 5187.48486328125\n",
      "Epoch 3342, Train_Loss: 4904.27294921875, Val_Loss: 5187.2373046875\n",
      "Epoch 3343, Train_Loss: 4904.06103515625, Val_Loss: 5187.0029296875\n",
      "Epoch 3344, Train_Loss: 4903.85888671875, Val_Loss: 5186.7509765625\n",
      "Epoch 3345, Train_Loss: 4903.61572265625, Val_Loss: 5186.53076171875\n",
      "Epoch 3346, Train_Loss: 4903.37158203125, Val_Loss: 5186.2578125\n",
      "Epoch 3347, Train_Loss: 4903.17919921875, Val_Loss: 5186.01953125\n",
      "Epoch 3348, Train_Loss: 4902.990234375, Val_Loss: 5185.767578125\n",
      "Epoch 3349, Train_Loss: 4902.794921875, Val_Loss: 5185.52734375\n",
      "Epoch 3350, Train_Loss: 4902.5771484375, Val_Loss: 5185.294921875\n",
      "Epoch 3351, Train_Loss: 4902.3603515625, Val_Loss: 5185.052734375\n",
      "Epoch 3352, Train_Loss: 4902.142578125, Val_Loss: 5184.828125\n",
      "Epoch 3353, Train_Loss: 4901.94873046875, Val_Loss: 5184.58984375\n",
      "Epoch 3354, Train_Loss: 4901.7373046875, Val_Loss: 5184.3564453125\n",
      "Epoch 3355, Train_Loss: 4901.52099609375, Val_Loss: 5184.109375\n",
      "Epoch 3356, Train_Loss: 4901.33154296875, Val_Loss: 5183.86328125\n",
      "Epoch 3357, Train_Loss: 4901.1044921875, Val_Loss: 5183.6318359375\n",
      "Epoch 3358, Train_Loss: 4900.8681640625, Val_Loss: 5183.40625\n",
      "Epoch 3359, Train_Loss: 4900.65234375, Val_Loss: 5183.1650390625\n",
      "Epoch 3360, Train_Loss: 4900.44287109375, Val_Loss: 5182.91845703125\n",
      "Epoch 3361, Train_Loss: 4900.22607421875, Val_Loss: 5182.68896484375\n",
      "Epoch 3362, Train_Loss: 4899.99560546875, Val_Loss: 5182.43115234375\n",
      "Epoch 3363, Train_Loss: 4899.79736328125, Val_Loss: 5182.2001953125\n",
      "Epoch 3364, Train_Loss: 4899.5849609375, Val_Loss: 5181.96484375\n",
      "Epoch 3365, Train_Loss: 4899.37255859375, Val_Loss: 5181.7275390625\n",
      "Epoch 3366, Train_Loss: 4899.15771484375, Val_Loss: 5181.4912109375\n",
      "Epoch 3367, Train_Loss: 4898.9267578125, Val_Loss: 5181.255859375\n",
      "Epoch 3368, Train_Loss: 4898.7314453125, Val_Loss: 5181.009765625\n",
      "Epoch 3369, Train_Loss: 4898.513671875, Val_Loss: 5180.7705078125\n",
      "Epoch 3370, Train_Loss: 4898.314453125, Val_Loss: 5180.53076171875\n",
      "Epoch 3371, Train_Loss: 4898.07568359375, Val_Loss: 5180.291015625\n",
      "Epoch 3372, Train_Loss: 4897.84521484375, Val_Loss: 5180.07080078125\n",
      "Epoch 3373, Train_Loss: 4897.63818359375, Val_Loss: 5179.8330078125\n",
      "Epoch 3374, Train_Loss: 4897.44775390625, Val_Loss: 5179.580078125\n",
      "Epoch 3375, Train_Loss: 4897.236328125, Val_Loss: 5179.32861328125\n",
      "Epoch 3376, Train_Loss: 4897.02783203125, Val_Loss: 5179.09130859375\n",
      "Epoch 3377, Train_Loss: 4896.8212890625, Val_Loss: 5178.85888671875\n",
      "Epoch 3378, Train_Loss: 4896.58349609375, Val_Loss: 5178.6337890625\n",
      "Epoch 3379, Train_Loss: 4896.3857421875, Val_Loss: 5178.39794921875\n",
      "Epoch 3380, Train_Loss: 4896.1953125, Val_Loss: 5178.1494140625\n",
      "Epoch 3381, Train_Loss: 4895.974609375, Val_Loss: 5177.904296875\n",
      "Epoch 3382, Train_Loss: 4895.763671875, Val_Loss: 5177.6708984375\n",
      "Epoch 3383, Train_Loss: 4895.56982421875, Val_Loss: 5177.4013671875\n",
      "Epoch 3384, Train_Loss: 4895.37548828125, Val_Loss: 5177.16015625\n",
      "Epoch 3385, Train_Loss: 4895.154296875, Val_Loss: 5176.94140625\n",
      "Epoch 3386, Train_Loss: 4894.94775390625, Val_Loss: 5176.6982421875\n",
      "Epoch 3387, Train_Loss: 4894.71923828125, Val_Loss: 5176.4501953125\n",
      "Epoch 3388, Train_Loss: 4894.54296875, Val_Loss: 5176.2177734375\n",
      "Epoch 3389, Train_Loss: 4894.3603515625, Val_Loss: 5175.96728515625\n",
      "Epoch 3390, Train_Loss: 4894.1484375, Val_Loss: 5175.7392578125\n",
      "Epoch 3391, Train_Loss: 4893.9375, Val_Loss: 5175.50537109375\n",
      "Epoch 3392, Train_Loss: 4893.69775390625, Val_Loss: 5175.2685546875\n",
      "Epoch 3393, Train_Loss: 4893.4287109375, Val_Loss: 5175.03857421875\n",
      "Epoch 3394, Train_Loss: 4893.22412109375, Val_Loss: 5174.802734375\n",
      "Epoch 3395, Train_Loss: 4893.02685546875, Val_Loss: 5174.5595703125\n",
      "Epoch 3396, Train_Loss: 4892.81640625, Val_Loss: 5174.31494140625\n",
      "Epoch 3397, Train_Loss: 4892.5986328125, Val_Loss: 5174.08544921875\n",
      "Epoch 3398, Train_Loss: 4892.3447265625, Val_Loss: 5173.8642578125\n",
      "Epoch 3399, Train_Loss: 4892.1396484375, Val_Loss: 5173.623046875\n",
      "Epoch 3400, Train_Loss: 4891.9326171875, Val_Loss: 5173.388671875\n",
      "Epoch 3401, Train_Loss: 4891.734375, Val_Loss: 5173.1416015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3402, Train_Loss: 4891.44775390625, Val_Loss: 5172.90380859375\n",
      "Epoch 3403, Train_Loss: 4891.23876953125, Val_Loss: 5172.66845703125\n",
      "Epoch 3404, Train_Loss: 4891.0283203125, Val_Loss: 5172.43017578125\n",
      "Epoch 3405, Train_Loss: 4890.8310546875, Val_Loss: 5172.20068359375\n",
      "Epoch 3406, Train_Loss: 4890.61474609375, Val_Loss: 5171.9658203125\n",
      "Epoch 3407, Train_Loss: 4890.3955078125, Val_Loss: 5171.71630859375\n",
      "Epoch 3408, Train_Loss: 4890.1943359375, Val_Loss: 5171.474609375\n",
      "Epoch 3409, Train_Loss: 4889.98583984375, Val_Loss: 5171.23583984375\n",
      "Epoch 3410, Train_Loss: 4889.78857421875, Val_Loss: 5171.01318359375\n",
      "Epoch 3411, Train_Loss: 4889.54150390625, Val_Loss: 5170.77294921875\n",
      "Epoch 3412, Train_Loss: 4889.32080078125, Val_Loss: 5170.52734375\n",
      "Epoch 3413, Train_Loss: 4889.1328125, Val_Loss: 5170.291015625\n",
      "Epoch 3414, Train_Loss: 4888.95654296875, Val_Loss: 5170.05517578125\n",
      "Epoch 3415, Train_Loss: 4888.7490234375, Val_Loss: 5169.80859375\n",
      "Epoch 3416, Train_Loss: 4888.54296875, Val_Loss: 5169.56396484375\n",
      "Epoch 3417, Train_Loss: 4888.326171875, Val_Loss: 5169.33203125\n",
      "Epoch 3418, Train_Loss: 4888.1318359375, Val_Loss: 5169.1025390625\n",
      "Epoch 3419, Train_Loss: 4887.91845703125, Val_Loss: 5168.86962890625\n",
      "Epoch 3420, Train_Loss: 4887.716796875, Val_Loss: 5168.60693359375\n",
      "Epoch 3421, Train_Loss: 4887.50830078125, Val_Loss: 5168.3505859375\n",
      "Epoch 3422, Train_Loss: 4887.3046875, Val_Loss: 5168.12109375\n",
      "Epoch 3423, Train_Loss: 4887.09619140625, Val_Loss: 5167.88623046875\n",
      "Epoch 3424, Train_Loss: 4886.87939453125, Val_Loss: 5167.6572265625\n",
      "Epoch 3425, Train_Loss: 4886.67919921875, Val_Loss: 5167.42578125\n",
      "Epoch 3426, Train_Loss: 4886.4873046875, Val_Loss: 5167.19384765625\n",
      "Epoch 3427, Train_Loss: 4886.283203125, Val_Loss: 5166.9560546875\n",
      "Epoch 3428, Train_Loss: 4886.07958984375, Val_Loss: 5166.71142578125\n",
      "Epoch 3429, Train_Loss: 4885.87646484375, Val_Loss: 5166.47900390625\n",
      "Epoch 3430, Train_Loss: 4885.6689453125, Val_Loss: 5166.234375\n",
      "Epoch 3431, Train_Loss: 4885.47509765625, Val_Loss: 5166.00439453125\n",
      "Epoch 3432, Train_Loss: 4885.25341796875, Val_Loss: 5165.771484375\n",
      "Epoch 3433, Train_Loss: 4885.04150390625, Val_Loss: 5165.5380859375\n",
      "Epoch 3434, Train_Loss: 4884.83447265625, Val_Loss: 5165.28515625\n",
      "Epoch 3435, Train_Loss: 4884.6376953125, Val_Loss: 5165.05029296875\n",
      "Epoch 3436, Train_Loss: 4884.42919921875, Val_Loss: 5164.80615234375\n",
      "Epoch 3437, Train_Loss: 4884.220703125, Val_Loss: 5164.5849609375\n",
      "Epoch 3438, Train_Loss: 4883.97119140625, Val_Loss: 5164.357421875\n",
      "Epoch 3439, Train_Loss: 4883.75537109375, Val_Loss: 5164.1044921875\n",
      "Epoch 3440, Train_Loss: 4883.55908203125, Val_Loss: 5163.880859375\n",
      "Epoch 3441, Train_Loss: 4883.3828125, Val_Loss: 5163.62744140625\n",
      "Epoch 3442, Train_Loss: 4883.1552734375, Val_Loss: 5163.39794921875\n",
      "Epoch 3443, Train_Loss: 4882.9296875, Val_Loss: 5163.1611328125\n",
      "Epoch 3444, Train_Loss: 4882.71630859375, Val_Loss: 5162.92529296875\n",
      "Epoch 3445, Train_Loss: 4882.50537109375, Val_Loss: 5162.69775390625\n",
      "Epoch 3446, Train_Loss: 4882.3095703125, Val_Loss: 5162.46044921875\n",
      "Epoch 3447, Train_Loss: 4882.09326171875, Val_Loss: 5162.2197265625\n",
      "Epoch 3448, Train_Loss: 4881.888671875, Val_Loss: 5161.97607421875\n",
      "Epoch 3449, Train_Loss: 4881.6826171875, Val_Loss: 5161.73095703125\n",
      "Epoch 3450, Train_Loss: 4881.48828125, Val_Loss: 5161.49462890625\n",
      "Epoch 3451, Train_Loss: 4881.2265625, Val_Loss: 5161.27294921875\n",
      "Epoch 3452, Train_Loss: 4881.0283203125, Val_Loss: 5161.0302734375\n",
      "Epoch 3453, Train_Loss: 4880.8369140625, Val_Loss: 5160.81103515625\n",
      "Epoch 3454, Train_Loss: 4880.6328125, Val_Loss: 5160.56640625\n",
      "Epoch 3455, Train_Loss: 4880.435546875, Val_Loss: 5160.322265625\n",
      "Epoch 3456, Train_Loss: 4880.23583984375, Val_Loss: 5160.091796875\n",
      "Epoch 3457, Train_Loss: 4880.01416015625, Val_Loss: 5159.87158203125\n",
      "Epoch 3458, Train_Loss: 4879.80419921875, Val_Loss: 5159.59375\n",
      "Epoch 3459, Train_Loss: 4879.59130859375, Val_Loss: 5159.35498046875\n",
      "Epoch 3460, Train_Loss: 4879.3974609375, Val_Loss: 5159.1162109375\n",
      "Epoch 3461, Train_Loss: 4879.20068359375, Val_Loss: 5158.8828125\n",
      "Epoch 3462, Train_Loss: 4878.97314453125, Val_Loss: 5158.6494140625\n",
      "Epoch 3463, Train_Loss: 4878.77392578125, Val_Loss: 5158.4150390625\n",
      "Epoch 3464, Train_Loss: 4878.5400390625, Val_Loss: 5158.18896484375\n",
      "Epoch 3465, Train_Loss: 4878.3447265625, Val_Loss: 5157.95361328125\n",
      "Epoch 3466, Train_Loss: 4878.13818359375, Val_Loss: 5157.70654296875\n",
      "Epoch 3467, Train_Loss: 4877.91064453125, Val_Loss: 5157.455078125\n",
      "Epoch 3468, Train_Loss: 4877.7080078125, Val_Loss: 5157.234375\n",
      "Epoch 3469, Train_Loss: 4877.50390625, Val_Loss: 5156.994140625\n",
      "Epoch 3470, Train_Loss: 4877.31640625, Val_Loss: 5156.75341796875\n",
      "Epoch 3471, Train_Loss: 4877.1123046875, Val_Loss: 5156.52978515625\n",
      "Epoch 3472, Train_Loss: 4876.8681640625, Val_Loss: 5156.29541015625\n",
      "Epoch 3473, Train_Loss: 4876.66064453125, Val_Loss: 5156.0634765625\n",
      "Epoch 3474, Train_Loss: 4876.4619140625, Val_Loss: 5155.81298828125\n",
      "Epoch 3475, Train_Loss: 4876.263671875, Val_Loss: 5155.58349609375\n",
      "Epoch 3476, Train_Loss: 4875.89013671875, Val_Loss: 5155.34130859375\n",
      "Epoch 3477, Train_Loss: 4875.64501953125, Val_Loss: 5155.12158203125\n",
      "Epoch 3478, Train_Loss: 4875.43505859375, Val_Loss: 5154.88671875\n",
      "Epoch 3479, Train_Loss: 4875.2373046875, Val_Loss: 5154.64306640625\n",
      "Epoch 3480, Train_Loss: 4875.041015625, Val_Loss: 5154.40576171875\n",
      "Epoch 3481, Train_Loss: 4874.83544921875, Val_Loss: 5154.1650390625\n",
      "Epoch 3482, Train_Loss: 4874.615234375, Val_Loss: 5153.9296875\n",
      "Epoch 3483, Train_Loss: 4874.41796875, Val_Loss: 5153.703125\n",
      "Epoch 3484, Train_Loss: 4874.2119140625, Val_Loss: 5153.46875\n",
      "Epoch 3485, Train_Loss: 4874.01318359375, Val_Loss: 5153.22705078125\n",
      "Epoch 3486, Train_Loss: 4873.81982421875, Val_Loss: 5152.9736328125\n",
      "Epoch 3487, Train_Loss: 4873.5986328125, Val_Loss: 5152.7412109375\n",
      "Epoch 3488, Train_Loss: 4873.390625, Val_Loss: 5152.50537109375\n",
      "Epoch 3489, Train_Loss: 4873.18994140625, Val_Loss: 5152.275390625\n",
      "Epoch 3490, Train_Loss: 4872.94921875, Val_Loss: 5152.04248046875\n",
      "Epoch 3491, Train_Loss: 4872.7529296875, Val_Loss: 5151.80908203125\n",
      "Epoch 3492, Train_Loss: 4872.52685546875, Val_Loss: 5151.5703125\n",
      "Epoch 3493, Train_Loss: 4872.326171875, Val_Loss: 5151.34619140625\n",
      "Epoch 3494, Train_Loss: 4872.142578125, Val_Loss: 5151.08984375\n",
      "Epoch 3495, Train_Loss: 4871.939453125, Val_Loss: 5150.82958984375\n",
      "Epoch 3496, Train_Loss: 4871.72998046875, Val_Loss: 5150.59228515625\n",
      "Epoch 3497, Train_Loss: 4871.50732421875, Val_Loss: 5150.35693359375\n",
      "Epoch 3498, Train_Loss: 4871.3076171875, Val_Loss: 5150.12548828125\n",
      "Epoch 3499, Train_Loss: 4871.1103515625, Val_Loss: 5149.88427734375\n",
      "Epoch 3500, Train_Loss: 4870.91259765625, Val_Loss: 5149.6416015625\n",
      "Epoch 3501, Train_Loss: 4870.6962890625, Val_Loss: 5149.4150390625\n",
      "Epoch 3502, Train_Loss: 4870.4794921875, Val_Loss: 5149.181640625\n",
      "Epoch 3503, Train_Loss: 4870.2861328125, Val_Loss: 5148.94775390625\n",
      "Epoch 3504, Train_Loss: 4870.0400390625, Val_Loss: 5148.71630859375\n",
      "Epoch 3505, Train_Loss: 4869.82958984375, Val_Loss: 5148.478515625\n",
      "Epoch 3506, Train_Loss: 4869.6259765625, Val_Loss: 5148.25\n",
      "Epoch 3507, Train_Loss: 4869.4189453125, Val_Loss: 5148.0234375\n",
      "Epoch 3508, Train_Loss: 4869.21044921875, Val_Loss: 5147.7841796875\n",
      "Epoch 3509, Train_Loss: 4868.990234375, Val_Loss: 5147.541015625\n",
      "Epoch 3510, Train_Loss: 4868.78515625, Val_Loss: 5147.32080078125\n",
      "Epoch 3511, Train_Loss: 4868.5810546875, Val_Loss: 5147.078125\n",
      "Epoch 3512, Train_Loss: 4868.353515625, Val_Loss: 5146.84326171875\n",
      "Epoch 3513, Train_Loss: 4868.1611328125, Val_Loss: 5146.61083984375\n",
      "Epoch 3514, Train_Loss: 4867.94873046875, Val_Loss: 5146.3583984375\n",
      "Epoch 3515, Train_Loss: 4867.74853515625, Val_Loss: 5146.12451171875\n",
      "Epoch 3516, Train_Loss: 4867.54345703125, Val_Loss: 5145.89404296875\n",
      "Epoch 3517, Train_Loss: 4867.29345703125, Val_Loss: 5145.68310546875\n",
      "Epoch 3518, Train_Loss: 4867.09423828125, Val_Loss: 5145.431640625\n",
      "Epoch 3519, Train_Loss: 4866.8974609375, Val_Loss: 5145.19580078125\n",
      "Epoch 3520, Train_Loss: 4866.7412109375, Val_Loss: 5144.9482421875\n",
      "Epoch 3521, Train_Loss: 4866.52783203125, Val_Loss: 5144.70654296875\n",
      "Epoch 3522, Train_Loss: 4866.30078125, Val_Loss: 5144.48095703125\n",
      "Epoch 3523, Train_Loss: 4866.09375, Val_Loss: 5144.2421875\n",
      "Epoch 3524, Train_Loss: 4865.89892578125, Val_Loss: 5144.0087890625\n",
      "Epoch 3525, Train_Loss: 4865.6953125, Val_Loss: 5143.77099609375\n",
      "Epoch 3526, Train_Loss: 4865.50634765625, Val_Loss: 5143.541015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3527, Train_Loss: 4865.2939453125, Val_Loss: 5143.29443359375\n",
      "Epoch 3528, Train_Loss: 4865.0859375, Val_Loss: 5143.06103515625\n",
      "Epoch 3529, Train_Loss: 4864.88916015625, Val_Loss: 5142.8349609375\n",
      "Epoch 3530, Train_Loss: 4864.67236328125, Val_Loss: 5142.595703125\n",
      "Epoch 3531, Train_Loss: 4864.4716796875, Val_Loss: 5142.36181640625\n",
      "Epoch 3532, Train_Loss: 4864.24755859375, Val_Loss: 5142.099609375\n",
      "Epoch 3533, Train_Loss: 4864.052734375, Val_Loss: 5141.85400390625\n",
      "Epoch 3534, Train_Loss: 4863.85107421875, Val_Loss: 5141.61865234375\n",
      "Epoch 3535, Train_Loss: 4863.64990234375, Val_Loss: 5141.380859375\n",
      "Epoch 3536, Train_Loss: 4863.45361328125, Val_Loss: 5141.14111328125\n",
      "Epoch 3537, Train_Loss: 4863.23095703125, Val_Loss: 5140.91943359375\n",
      "Epoch 3538, Train_Loss: 4863.0341796875, Val_Loss: 5140.6884765625\n",
      "Epoch 3539, Train_Loss: 4862.8212890625, Val_Loss: 5140.45458984375\n",
      "Epoch 3540, Train_Loss: 4862.630859375, Val_Loss: 5140.2138671875\n",
      "Epoch 3541, Train_Loss: 4862.4345703125, Val_Loss: 5139.96826171875\n",
      "Epoch 3542, Train_Loss: 4862.1708984375, Val_Loss: 5139.73779296875\n",
      "Epoch 3543, Train_Loss: 4861.91064453125, Val_Loss: 5139.52734375\n",
      "Epoch 3544, Train_Loss: 4861.71630859375, Val_Loss: 5139.29248046875\n",
      "Epoch 3545, Train_Loss: 4861.51904296875, Val_Loss: 5139.05517578125\n",
      "Epoch 3546, Train_Loss: 4861.3173828125, Val_Loss: 5138.81689453125\n",
      "Epoch 3547, Train_Loss: 4861.1455078125, Val_Loss: 5138.5673828125\n",
      "Epoch 3548, Train_Loss: 4860.95068359375, Val_Loss: 5138.3505859375\n",
      "Epoch 3549, Train_Loss: 4860.7568359375, Val_Loss: 5138.11328125\n",
      "Epoch 3550, Train_Loss: 4860.55712890625, Val_Loss: 5137.873046875\n",
      "Epoch 3551, Train_Loss: 4860.2841796875, Val_Loss: 5137.642578125\n",
      "Epoch 3552, Train_Loss: 4860.07080078125, Val_Loss: 5137.404296875\n",
      "Epoch 3553, Train_Loss: 4859.86962890625, Val_Loss: 5137.16845703125\n",
      "Epoch 3554, Train_Loss: 4859.65966796875, Val_Loss: 5136.93310546875\n",
      "Epoch 3555, Train_Loss: 4859.46484375, Val_Loss: 5136.697265625\n",
      "Epoch 3556, Train_Loss: 4859.26708984375, Val_Loss: 5136.4677734375\n",
      "Epoch 3557, Train_Loss: 4859.02099609375, Val_Loss: 5136.2509765625\n",
      "Epoch 3558, Train_Loss: 4858.82421875, Val_Loss: 5136.01611328125\n",
      "Epoch 3559, Train_Loss: 4858.6123046875, Val_Loss: 5135.77294921875\n",
      "Epoch 3560, Train_Loss: 4858.40771484375, Val_Loss: 5135.52197265625\n",
      "Epoch 3561, Train_Loss: 4858.2158203125, Val_Loss: 5135.2861328125\n",
      "Epoch 3562, Train_Loss: 4857.98876953125, Val_Loss: 5135.04833984375\n",
      "Epoch 3563, Train_Loss: 4857.78173828125, Val_Loss: 5134.82958984375\n",
      "Epoch 3564, Train_Loss: 4857.57958984375, Val_Loss: 5134.6005859375\n",
      "Epoch 3565, Train_Loss: 4857.37744140625, Val_Loss: 5134.3564453125\n",
      "Epoch 3566, Train_Loss: 4857.1748046875, Val_Loss: 5134.1025390625\n",
      "Epoch 3567, Train_Loss: 4856.9609375, Val_Loss: 5133.873046875\n",
      "Epoch 3568, Train_Loss: 4856.74560546875, Val_Loss: 5133.65283203125\n",
      "Epoch 3569, Train_Loss: 4856.544921875, Val_Loss: 5133.41259765625\n",
      "Epoch 3570, Train_Loss: 4856.2978515625, Val_Loss: 5133.1572265625\n",
      "Epoch 3571, Train_Loss: 4856.10595703125, Val_Loss: 5132.9345703125\n",
      "Epoch 3572, Train_Loss: 4855.89599609375, Val_Loss: 5132.689453125\n",
      "Epoch 3573, Train_Loss: 4855.68212890625, Val_Loss: 5132.44775390625\n",
      "Epoch 3574, Train_Loss: 4855.48583984375, Val_Loss: 5132.20849609375\n",
      "Epoch 3575, Train_Loss: 4855.27490234375, Val_Loss: 5131.974609375\n",
      "Epoch 3576, Train_Loss: 4855.06591796875, Val_Loss: 5131.74169921875\n",
      "Epoch 3577, Train_Loss: 4854.8466796875, Val_Loss: 5131.52197265625\n",
      "Epoch 3578, Train_Loss: 4854.63916015625, Val_Loss: 5131.28662109375\n",
      "Epoch 3579, Train_Loss: 4854.42333984375, Val_Loss: 5131.046875\n",
      "Epoch 3580, Train_Loss: 4854.23583984375, Val_Loss: 5130.802734375\n",
      "Epoch 3581, Train_Loss: 4854.02978515625, Val_Loss: 5130.5830078125\n",
      "Epoch 3582, Train_Loss: 4853.83154296875, Val_Loss: 5130.34814453125\n",
      "Epoch 3583, Train_Loss: 4853.583984375, Val_Loss: 5130.1201171875\n",
      "Epoch 3584, Train_Loss: 4853.392578125, Val_Loss: 5129.88818359375\n",
      "Epoch 3585, Train_Loss: 4853.20751953125, Val_Loss: 5129.65673828125\n",
      "Epoch 3586, Train_Loss: 4853.013671875, Val_Loss: 5129.39892578125\n",
      "Epoch 3587, Train_Loss: 4852.8017578125, Val_Loss: 5129.16845703125\n",
      "Epoch 3588, Train_Loss: 4852.60498046875, Val_Loss: 5128.93994140625\n",
      "Epoch 3589, Train_Loss: 4852.404296875, Val_Loss: 5128.7177734375\n",
      "Epoch 3590, Train_Loss: 4852.19873046875, Val_Loss: 5128.48583984375\n",
      "Epoch 3591, Train_Loss: 4851.98876953125, Val_Loss: 5128.2529296875\n",
      "Epoch 3592, Train_Loss: 4851.763671875, Val_Loss: 5128.025390625\n",
      "Epoch 3593, Train_Loss: 4851.56396484375, Val_Loss: 5127.7841796875\n",
      "Epoch 3594, Train_Loss: 4851.373046875, Val_Loss: 5127.5400390625\n",
      "Epoch 3595, Train_Loss: 4851.1708984375, Val_Loss: 5127.3154296875\n",
      "Epoch 3596, Train_Loss: 4850.9423828125, Val_Loss: 5127.10009765625\n",
      "Epoch 3597, Train_Loss: 4850.71630859375, Val_Loss: 5126.8515625\n",
      "Epoch 3598, Train_Loss: 4850.52685546875, Val_Loss: 5126.619140625\n",
      "Epoch 3599, Train_Loss: 4850.33447265625, Val_Loss: 5126.38720703125\n",
      "Epoch 3600, Train_Loss: 4850.18115234375, Val_Loss: 5126.1318359375\n",
      "Epoch 3601, Train_Loss: 4849.984375, Val_Loss: 5125.9091796875\n",
      "Epoch 3602, Train_Loss: 4849.759765625, Val_Loss: 5125.673828125\n",
      "Epoch 3603, Train_Loss: 4849.56103515625, Val_Loss: 5125.44580078125\n",
      "Epoch 3604, Train_Loss: 4849.34716796875, Val_Loss: 5125.21923828125\n",
      "Epoch 3605, Train_Loss: 4849.1630859375, Val_Loss: 5124.9951171875\n",
      "Epoch 3606, Train_Loss: 4848.9677734375, Val_Loss: 5124.75048828125\n",
      "Epoch 3607, Train_Loss: 4848.7421875, Val_Loss: 5124.47705078125\n",
      "Epoch 3608, Train_Loss: 4848.5302734375, Val_Loss: 5124.25244140625\n",
      "Epoch 3609, Train_Loss: 4848.279296875, Val_Loss: 5124.02734375\n",
      "Epoch 3610, Train_Loss: 4848.08349609375, Val_Loss: 5123.79638671875\n",
      "Epoch 3611, Train_Loss: 4847.89111328125, Val_Loss: 5123.56689453125\n",
      "Epoch 3612, Train_Loss: 4847.66748046875, Val_Loss: 5123.3330078125\n",
      "Epoch 3613, Train_Loss: 4847.4716796875, Val_Loss: 5123.09326171875\n",
      "Epoch 3614, Train_Loss: 4847.27392578125, Val_Loss: 5122.85498046875\n",
      "Epoch 3615, Train_Loss: 4847.07568359375, Val_Loss: 5122.6298828125\n",
      "Epoch 3616, Train_Loss: 4846.853515625, Val_Loss: 5122.40087890625\n",
      "Epoch 3617, Train_Loss: 4846.64111328125, Val_Loss: 5122.16796875\n",
      "Epoch 3618, Train_Loss: 4846.44189453125, Val_Loss: 5121.939453125\n",
      "Epoch 3619, Train_Loss: 4846.255859375, Val_Loss: 5121.69580078125\n",
      "Epoch 3620, Train_Loss: 4846.0625, Val_Loss: 5121.4677734375\n",
      "Epoch 3621, Train_Loss: 4845.86376953125, Val_Loss: 5121.224609375\n",
      "Epoch 3622, Train_Loss: 4845.6552734375, Val_Loss: 5121.00830078125\n",
      "Epoch 3623, Train_Loss: 4845.4306640625, Val_Loss: 5120.78271484375\n",
      "Epoch 3624, Train_Loss: 4845.22900390625, Val_Loss: 5120.54052734375\n",
      "Epoch 3625, Train_Loss: 4845.021484375, Val_Loss: 5120.3193359375\n",
      "Epoch 3626, Train_Loss: 4844.82568359375, Val_Loss: 5120.07080078125\n",
      "Epoch 3627, Train_Loss: 4844.619140625, Val_Loss: 5119.841796875\n",
      "Epoch 3628, Train_Loss: 4844.42626953125, Val_Loss: 5119.6123046875\n",
      "Epoch 3629, Train_Loss: 4844.2265625, Val_Loss: 5119.38671875\n",
      "Epoch 3630, Train_Loss: 4844.02392578125, Val_Loss: 5119.16015625\n",
      "Epoch 3631, Train_Loss: 4843.82275390625, Val_Loss: 5118.9287109375\n",
      "Epoch 3632, Train_Loss: 4843.5927734375, Val_Loss: 5118.6982421875\n",
      "Epoch 3633, Train_Loss: 4843.38720703125, Val_Loss: 5118.4501953125\n",
      "Epoch 3634, Train_Loss: 4843.17626953125, Val_Loss: 5118.22900390625\n",
      "Epoch 3635, Train_Loss: 4842.96826171875, Val_Loss: 5117.97509765625\n",
      "Epoch 3636, Train_Loss: 4842.7587890625, Val_Loss: 5117.7626953125\n",
      "Epoch 3637, Train_Loss: 4842.548828125, Val_Loss: 5117.5234375\n",
      "Epoch 3638, Train_Loss: 4842.3798828125, Val_Loss: 5117.29541015625\n",
      "Epoch 3639, Train_Loss: 4842.20166015625, Val_Loss: 5117.044921875\n",
      "Epoch 3640, Train_Loss: 4841.99853515625, Val_Loss: 5116.8193359375\n",
      "Epoch 3641, Train_Loss: 4841.80126953125, Val_Loss: 5116.58740234375\n",
      "Epoch 3642, Train_Loss: 4841.59033203125, Val_Loss: 5116.3671875\n",
      "Epoch 3643, Train_Loss: 4841.38671875, Val_Loss: 5116.13818359375\n",
      "Epoch 3644, Train_Loss: 4841.16357421875, Val_Loss: 5115.85986328125\n",
      "Epoch 3645, Train_Loss: 4840.97265625, Val_Loss: 5115.64208984375\n",
      "Epoch 3646, Train_Loss: 4840.78076171875, Val_Loss: 5115.3994140625\n",
      "Epoch 3647, Train_Loss: 4840.568359375, Val_Loss: 5115.1796875\n",
      "Epoch 3648, Train_Loss: 4840.3623046875, Val_Loss: 5114.9482421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3649, Train_Loss: 4840.119140625, Val_Loss: 5114.72119140625\n",
      "Epoch 3650, Train_Loss: 4839.923828125, Val_Loss: 5114.48583984375\n",
      "Epoch 3651, Train_Loss: 4839.73876953125, Val_Loss: 5114.25341796875\n",
      "Epoch 3652, Train_Loss: 4839.52392578125, Val_Loss: 5114.02783203125\n",
      "Epoch 3653, Train_Loss: 4839.375, Val_Loss: 5113.77685546875\n",
      "Epoch 3654, Train_Loss: 4839.17578125, Val_Loss: 5113.55126953125\n",
      "Epoch 3655, Train_Loss: 4838.9814453125, Val_Loss: 5113.31591796875\n",
      "Epoch 3656, Train_Loss: 4838.76806640625, Val_Loss: 5113.091796875\n",
      "Epoch 3657, Train_Loss: 4838.56103515625, Val_Loss: 5112.865234375\n",
      "Epoch 3658, Train_Loss: 4838.36181640625, Val_Loss: 5112.63916015625\n",
      "Epoch 3659, Train_Loss: 4838.15478515625, Val_Loss: 5112.400390625\n",
      "Epoch 3660, Train_Loss: 4837.95361328125, Val_Loss: 5112.16943359375\n",
      "Epoch 3661, Train_Loss: 4837.76708984375, Val_Loss: 5111.93896484375\n",
      "Epoch 3662, Train_Loss: 4837.50830078125, Val_Loss: 5111.70947265625\n",
      "Epoch 3663, Train_Loss: 4837.30419921875, Val_Loss: 5111.47998046875\n",
      "Epoch 3664, Train_Loss: 4837.11669921875, Val_Loss: 5111.2421875\n",
      "Epoch 3665, Train_Loss: 4836.92236328125, Val_Loss: 5111.00927734375\n",
      "Epoch 3666, Train_Loss: 4836.73828125, Val_Loss: 5110.7724609375\n",
      "Epoch 3667, Train_Loss: 4836.546875, Val_Loss: 5110.54931640625\n",
      "Epoch 3668, Train_Loss: 4836.33935546875, Val_Loss: 5110.32421875\n",
      "Epoch 3669, Train_Loss: 4836.1357421875, Val_Loss: 5110.10302734375\n",
      "Epoch 3670, Train_Loss: 4835.9580078125, Val_Loss: 5109.87646484375\n",
      "Epoch 3671, Train_Loss: 4835.76708984375, Val_Loss: 5109.642578125\n",
      "Epoch 3672, Train_Loss: 4835.548828125, Val_Loss: 5109.37548828125\n",
      "Epoch 3673, Train_Loss: 4835.35009765625, Val_Loss: 5109.16015625\n",
      "Epoch 3674, Train_Loss: 4835.14599609375, Val_Loss: 5108.92236328125\n",
      "Epoch 3675, Train_Loss: 4834.94970703125, Val_Loss: 5108.693359375\n",
      "Epoch 3676, Train_Loss: 4834.71923828125, Val_Loss: 5108.47265625\n",
      "Epoch 3677, Train_Loss: 4834.51123046875, Val_Loss: 5108.2509765625\n",
      "Epoch 3678, Train_Loss: 4834.3095703125, Val_Loss: 5108.0224609375\n",
      "Epoch 3679, Train_Loss: 4834.103515625, Val_Loss: 5107.78759765625\n",
      "Epoch 3680, Train_Loss: 4833.89892578125, Val_Loss: 5107.54443359375\n",
      "Epoch 3681, Train_Loss: 4833.705078125, Val_Loss: 5107.3154296875\n",
      "Epoch 3682, Train_Loss: 4833.47802734375, Val_Loss: 5107.05908203125\n",
      "Epoch 3683, Train_Loss: 4833.28466796875, Val_Loss: 5106.83740234375\n",
      "Epoch 3684, Train_Loss: 4833.07958984375, Val_Loss: 5106.6162109375\n",
      "Epoch 3685, Train_Loss: 4832.8720703125, Val_Loss: 5106.37939453125\n",
      "Epoch 3686, Train_Loss: 4832.66357421875, Val_Loss: 5106.12890625\n",
      "Epoch 3687, Train_Loss: 4832.4345703125, Val_Loss: 5105.91259765625\n",
      "Epoch 3688, Train_Loss: 4832.23291015625, Val_Loss: 5105.6845703125\n",
      "Epoch 3689, Train_Loss: 4831.99462890625, Val_Loss: 5105.45947265625\n",
      "Epoch 3690, Train_Loss: 4831.80810546875, Val_Loss: 5105.23193359375\n",
      "Epoch 3691, Train_Loss: 4831.53662109375, Val_Loss: 5104.998046875\n",
      "Epoch 3692, Train_Loss: 4831.3330078125, Val_Loss: 5104.76220703125\n",
      "Epoch 3693, Train_Loss: 4831.12841796875, Val_Loss: 5104.51953125\n",
      "Epoch 3694, Train_Loss: 4830.9365234375, Val_Loss: 5104.30322265625\n",
      "Epoch 3695, Train_Loss: 4830.75, Val_Loss: 5104.0703125\n",
      "Epoch 3696, Train_Loss: 4830.546875, Val_Loss: 5103.84912109375\n",
      "Epoch 3697, Train_Loss: 4830.3427734375, Val_Loss: 5103.61767578125\n",
      "Epoch 3698, Train_Loss: 4830.13427734375, Val_Loss: 5103.39501953125\n",
      "Epoch 3699, Train_Loss: 4829.9384765625, Val_Loss: 5103.1669921875\n",
      "Epoch 3700, Train_Loss: 4829.67578125, Val_Loss: 5102.93359375\n",
      "Epoch 3701, Train_Loss: 4829.474609375, Val_Loss: 5102.70654296875\n",
      "Epoch 3702, Train_Loss: 4829.2451171875, Val_Loss: 5102.47265625\n",
      "Epoch 3703, Train_Loss: 4829.048828125, Val_Loss: 5102.25048828125\n",
      "Epoch 3704, Train_Loss: 4828.8662109375, Val_Loss: 5102.015625\n",
      "Epoch 3705, Train_Loss: 4828.67724609375, Val_Loss: 5101.78515625\n",
      "Epoch 3706, Train_Loss: 4828.50732421875, Val_Loss: 5101.54248046875\n",
      "Epoch 3707, Train_Loss: 4828.310546875, Val_Loss: 5101.3115234375\n",
      "Epoch 3708, Train_Loss: 4828.0947265625, Val_Loss: 5101.09423828125\n",
      "Epoch 3709, Train_Loss: 4827.8935546875, Val_Loss: 5100.875\n",
      "Epoch 3710, Train_Loss: 4827.68994140625, Val_Loss: 5100.62744140625\n",
      "Epoch 3711, Train_Loss: 4827.4873046875, Val_Loss: 5100.41455078125\n",
      "Epoch 3712, Train_Loss: 4827.29248046875, Val_Loss: 5100.1669921875\n",
      "Epoch 3713, Train_Loss: 4827.0810546875, Val_Loss: 5099.94580078125\n",
      "Epoch 3714, Train_Loss: 4826.89697265625, Val_Loss: 5099.70703125\n",
      "Epoch 3715, Train_Loss: 4826.64892578125, Val_Loss: 5099.4912109375\n",
      "Epoch 3716, Train_Loss: 4826.45849609375, Val_Loss: 5099.2626953125\n",
      "Epoch 3717, Train_Loss: 4826.2724609375, Val_Loss: 5099.03857421875\n",
      "Epoch 3718, Train_Loss: 4826.052734375, Val_Loss: 5098.80419921875\n",
      "Epoch 3719, Train_Loss: 4825.857421875, Val_Loss: 5098.525390625\n",
      "Epoch 3720, Train_Loss: 4825.66162109375, Val_Loss: 5098.30712890625\n",
      "Epoch 3721, Train_Loss: 4825.4638671875, Val_Loss: 5098.076171875\n",
      "Epoch 3722, Train_Loss: 4825.26123046875, Val_Loss: 5097.86083984375\n",
      "Epoch 3723, Train_Loss: 4825.0380859375, Val_Loss: 5097.626953125\n",
      "Epoch 3724, Train_Loss: 4824.83447265625, Val_Loss: 5097.3994140625\n",
      "Epoch 3725, Train_Loss: 4824.64013671875, Val_Loss: 5097.16015625\n",
      "Epoch 3726, Train_Loss: 4824.44921875, Val_Loss: 5096.943359375\n",
      "Epoch 3727, Train_Loss: 4824.24609375, Val_Loss: 5096.716796875\n",
      "Epoch 3728, Train_Loss: 4824.04638671875, Val_Loss: 5096.48681640625\n",
      "Epoch 3729, Train_Loss: 4823.8271484375, Val_Loss: 5096.25830078125\n",
      "Epoch 3730, Train_Loss: 4823.62939453125, Val_Loss: 5096.04296875\n",
      "Epoch 3731, Train_Loss: 4823.43017578125, Val_Loss: 5095.81103515625\n",
      "Epoch 3732, Train_Loss: 4823.27392578125, Val_Loss: 5095.5634765625\n",
      "Epoch 3733, Train_Loss: 4823.05712890625, Val_Loss: 5095.3388671875\n",
      "Epoch 3734, Train_Loss: 4822.8681640625, Val_Loss: 5095.115234375\n",
      "Epoch 3735, Train_Loss: 4822.677734375, Val_Loss: 5094.896484375\n",
      "Epoch 3736, Train_Loss: 4822.4755859375, Val_Loss: 5094.6650390625\n",
      "Epoch 3737, Train_Loss: 4822.28173828125, Val_Loss: 5094.4501953125\n",
      "Epoch 3738, Train_Loss: 4822.05859375, Val_Loss: 5094.21142578125\n",
      "Epoch 3739, Train_Loss: 4821.87255859375, Val_Loss: 5093.9609375\n",
      "Epoch 3740, Train_Loss: 4821.65771484375, Val_Loss: 5093.73828125\n",
      "Epoch 3741, Train_Loss: 4821.43896484375, Val_Loss: 5093.52099609375\n",
      "Epoch 3742, Train_Loss: 4821.21484375, Val_Loss: 5093.306640625\n",
      "Epoch 3743, Train_Loss: 4821.0166015625, Val_Loss: 5093.07275390625\n",
      "Epoch 3744, Train_Loss: 4820.8408203125, Val_Loss: 5092.83935546875\n",
      "Epoch 3745, Train_Loss: 4820.67529296875, Val_Loss: 5092.60205078125\n",
      "Epoch 3746, Train_Loss: 4820.47998046875, Val_Loss: 5092.38330078125\n",
      "Epoch 3747, Train_Loss: 4820.2880859375, Val_Loss: 5092.13720703125\n",
      "Epoch 3748, Train_Loss: 4820.0908203125, Val_Loss: 5091.9130859375\n",
      "Epoch 3749, Train_Loss: 4819.8935546875, Val_Loss: 5091.70263671875\n",
      "Epoch 3750, Train_Loss: 4819.69775390625, Val_Loss: 5091.466796875\n",
      "Epoch 3751, Train_Loss: 4819.52685546875, Val_Loss: 5091.23681640625\n",
      "Epoch 3752, Train_Loss: 4819.318359375, Val_Loss: 5091.0048828125\n",
      "Epoch 3753, Train_Loss: 4819.09521484375, Val_Loss: 5090.7822265625\n",
      "Epoch 3754, Train_Loss: 4818.890625, Val_Loss: 5090.5537109375\n",
      "Epoch 3755, Train_Loss: 4818.65771484375, Val_Loss: 5090.3330078125\n",
      "Epoch 3756, Train_Loss: 4818.45751953125, Val_Loss: 5090.0732421875\n",
      "Epoch 3757, Train_Loss: 4818.27099609375, Val_Loss: 5089.8427734375\n",
      "Epoch 3758, Train_Loss: 4818.06884765625, Val_Loss: 5089.60888671875\n",
      "Epoch 3759, Train_Loss: 4817.90625, Val_Loss: 5089.37744140625\n",
      "Epoch 3760, Train_Loss: 4817.7158203125, Val_Loss: 5089.15380859375\n",
      "Epoch 3761, Train_Loss: 4817.5234375, Val_Loss: 5088.92138671875\n",
      "Epoch 3762, Train_Loss: 4817.33349609375, Val_Loss: 5088.7041015625\n",
      "Epoch 3763, Train_Loss: 4817.11669921875, Val_Loss: 5088.470703125\n",
      "Epoch 3764, Train_Loss: 4816.92138671875, Val_Loss: 5088.2451171875\n",
      "Epoch 3765, Train_Loss: 4816.732421875, Val_Loss: 5088.0107421875\n",
      "Epoch 3766, Train_Loss: 4816.51904296875, Val_Loss: 5087.7841796875\n",
      "Epoch 3767, Train_Loss: 4816.31298828125, Val_Loss: 5087.55859375\n",
      "Epoch 3768, Train_Loss: 4816.0576171875, Val_Loss: 5087.33642578125\n",
      "Epoch 3769, Train_Loss: 4815.86181640625, Val_Loss: 5087.1083984375\n",
      "Epoch 3770, Train_Loss: 4815.68115234375, Val_Loss: 5086.88818359375\n",
      "Epoch 3771, Train_Loss: 4815.47900390625, Val_Loss: 5086.6650390625\n",
      "Epoch 3772, Train_Loss: 4815.27783203125, Val_Loss: 5086.4150390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3773, Train_Loss: 4815.0654296875, Val_Loss: 5086.19580078125\n",
      "Epoch 3774, Train_Loss: 4814.8642578125, Val_Loss: 5085.9677734375\n",
      "Epoch 3775, Train_Loss: 4814.56298828125, Val_Loss: 5085.74365234375\n",
      "Epoch 3776, Train_Loss: 4814.36865234375, Val_Loss: 5085.52294921875\n",
      "Epoch 3777, Train_Loss: 4814.17578125, Val_Loss: 5085.28076171875\n",
      "Epoch 3778, Train_Loss: 4813.9619140625, Val_Loss: 5085.05712890625\n",
      "Epoch 3779, Train_Loss: 4813.76806640625, Val_Loss: 5084.82763671875\n",
      "Epoch 3780, Train_Loss: 4813.57666015625, Val_Loss: 5084.6083984375\n",
      "Epoch 3781, Train_Loss: 4813.384765625, Val_Loss: 5084.375\n",
      "Epoch 3782, Train_Loss: 4813.15283203125, Val_Loss: 5084.16015625\n",
      "Epoch 3783, Train_Loss: 4812.95654296875, Val_Loss: 5083.92724609375\n",
      "Epoch 3784, Train_Loss: 4812.7666015625, Val_Loss: 5083.68212890625\n",
      "Epoch 3785, Train_Loss: 4812.5556640625, Val_Loss: 5083.45458984375\n",
      "Epoch 3786, Train_Loss: 4812.357421875, Val_Loss: 5083.2294921875\n",
      "Epoch 3787, Train_Loss: 4812.16650390625, Val_Loss: 5083.0048828125\n",
      "Epoch 3788, Train_Loss: 4811.9697265625, Val_Loss: 5082.78564453125\n",
      "Epoch 3789, Train_Loss: 4811.77880859375, Val_Loss: 5082.56005859375\n",
      "Epoch 3790, Train_Loss: 4811.59423828125, Val_Loss: 5082.32177734375\n",
      "Epoch 3791, Train_Loss: 4811.4013671875, Val_Loss: 5082.09326171875\n",
      "Epoch 3792, Train_Loss: 4811.2001953125, Val_Loss: 5081.873046875\n",
      "Epoch 3793, Train_Loss: 4810.9658203125, Val_Loss: 5081.6416015625\n",
      "Epoch 3794, Train_Loss: 4810.7265625, Val_Loss: 5081.3857421875\n",
      "Epoch 3795, Train_Loss: 4810.48583984375, Val_Loss: 5081.1689453125\n",
      "Epoch 3796, Train_Loss: 4810.28369140625, Val_Loss: 5080.943359375\n",
      "Epoch 3797, Train_Loss: 4810.0966796875, Val_Loss: 5080.71435546875\n",
      "Epoch 3798, Train_Loss: 4809.89892578125, Val_Loss: 5080.47216796875\n",
      "Epoch 3799, Train_Loss: 4809.70654296875, Val_Loss: 5080.25390625\n",
      "Epoch 3800, Train_Loss: 4809.50244140625, Val_Loss: 5080.01904296875\n",
      "Epoch 3801, Train_Loss: 4809.2939453125, Val_Loss: 5079.7890625\n",
      "Epoch 3802, Train_Loss: 4809.0947265625, Val_Loss: 5079.5849609375\n",
      "Epoch 3803, Train_Loss: 4808.88525390625, Val_Loss: 5079.3486328125\n",
      "Epoch 3804, Train_Loss: 4808.693359375, Val_Loss: 5079.1318359375\n",
      "Epoch 3805, Train_Loss: 4808.505859375, Val_Loss: 5078.87744140625\n",
      "Epoch 3806, Train_Loss: 4808.3203125, Val_Loss: 5078.65576171875\n",
      "Epoch 3807, Train_Loss: 4808.13525390625, Val_Loss: 5078.4326171875\n",
      "Epoch 3808, Train_Loss: 4807.90380859375, Val_Loss: 5078.2177734375\n",
      "Epoch 3809, Train_Loss: 4807.71435546875, Val_Loss: 5077.98779296875\n",
      "Epoch 3810, Train_Loss: 4807.5263671875, Val_Loss: 5077.759765625\n",
      "Epoch 3811, Train_Loss: 4807.345703125, Val_Loss: 5077.52978515625\n",
      "Epoch 3812, Train_Loss: 4807.14697265625, Val_Loss: 5077.2841796875\n",
      "Epoch 3813, Train_Loss: 4806.935546875, Val_Loss: 5077.0625\n",
      "Epoch 3814, Train_Loss: 4806.7412109375, Val_Loss: 5076.82958984375\n",
      "Epoch 3815, Train_Loss: 4806.5615234375, Val_Loss: 5076.62060546875\n",
      "Epoch 3816, Train_Loss: 4806.36572265625, Val_Loss: 5076.390625\n",
      "Epoch 3817, Train_Loss: 4806.18798828125, Val_Loss: 5076.17041015625\n",
      "Epoch 3818, Train_Loss: 4805.98095703125, Val_Loss: 5075.93603515625\n",
      "Epoch 3819, Train_Loss: 4805.79638671875, Val_Loss: 5075.70263671875\n",
      "Epoch 3820, Train_Loss: 4805.5830078125, Val_Loss: 5075.478515625\n",
      "Epoch 3821, Train_Loss: 4805.3408203125, Val_Loss: 5075.25390625\n",
      "Epoch 3822, Train_Loss: 4805.150390625, Val_Loss: 5075.0185546875\n",
      "Epoch 3823, Train_Loss: 4804.935546875, Val_Loss: 5074.8017578125\n",
      "Epoch 3824, Train_Loss: 4804.73681640625, Val_Loss: 5074.56396484375\n",
      "Epoch 3825, Train_Loss: 4804.5478515625, Val_Loss: 5074.3330078125\n",
      "Epoch 3826, Train_Loss: 4804.35009765625, Val_Loss: 5074.0986328125\n",
      "Epoch 3827, Train_Loss: 4804.142578125, Val_Loss: 5073.87744140625\n",
      "Epoch 3828, Train_Loss: 4803.93115234375, Val_Loss: 5073.65576171875\n",
      "Epoch 3829, Train_Loss: 4803.744140625, Val_Loss: 5073.43798828125\n",
      "Epoch 3830, Train_Loss: 4803.5458984375, Val_Loss: 5073.212890625\n",
      "Epoch 3831, Train_Loss: 4803.34521484375, Val_Loss: 5072.94580078125\n",
      "Epoch 3832, Train_Loss: 4803.15478515625, Val_Loss: 5072.70947265625\n",
      "Epoch 3833, Train_Loss: 4802.92138671875, Val_Loss: 5072.48974609375\n",
      "Epoch 3834, Train_Loss: 4802.73779296875, Val_Loss: 5072.2646484375\n",
      "Epoch 3835, Train_Loss: 4802.50439453125, Val_Loss: 5072.0419921875\n",
      "Epoch 3836, Train_Loss: 4802.32080078125, Val_Loss: 5071.82177734375\n",
      "Epoch 3837, Train_Loss: 4802.1328125, Val_Loss: 5071.59521484375\n",
      "Epoch 3838, Train_Loss: 4801.96630859375, Val_Loss: 5071.36181640625\n",
      "Epoch 3839, Train_Loss: 4801.78369140625, Val_Loss: 5071.1328125\n",
      "Epoch 3840, Train_Loss: 4801.58056640625, Val_Loss: 5070.90234375\n",
      "Epoch 3841, Train_Loss: 4801.39453125, Val_Loss: 5070.6845703125\n",
      "Epoch 3842, Train_Loss: 4801.2001953125, Val_Loss: 5070.4638671875\n",
      "Epoch 3843, Train_Loss: 4800.98876953125, Val_Loss: 5070.23388671875\n",
      "Epoch 3844, Train_Loss: 4800.79296875, Val_Loss: 5070.005859375\n",
      "Epoch 3845, Train_Loss: 4800.60986328125, Val_Loss: 5069.775390625\n",
      "Epoch 3846, Train_Loss: 4800.421875, Val_Loss: 5069.55322265625\n",
      "Epoch 3847, Train_Loss: 4800.22509765625, Val_Loss: 5069.3271484375\n",
      "Epoch 3848, Train_Loss: 4800.0, Val_Loss: 5069.1162109375\n",
      "Epoch 3849, Train_Loss: 4799.8076171875, Val_Loss: 5068.87939453125\n",
      "Epoch 3850, Train_Loss: 4799.544921875, Val_Loss: 5068.6513671875\n",
      "Epoch 3851, Train_Loss: 4799.37255859375, Val_Loss: 5068.4111328125\n",
      "Epoch 3852, Train_Loss: 4799.17919921875, Val_Loss: 5068.189453125\n",
      "Epoch 3853, Train_Loss: 4798.9736328125, Val_Loss: 5067.9638671875\n",
      "Epoch 3854, Train_Loss: 4798.783203125, Val_Loss: 5067.73291015625\n",
      "Epoch 3855, Train_Loss: 4798.59814453125, Val_Loss: 5067.5283203125\n",
      "Epoch 3856, Train_Loss: 4798.40771484375, Val_Loss: 5067.3017578125\n",
      "Epoch 3857, Train_Loss: 4798.212890625, Val_Loss: 5067.07275390625\n",
      "Epoch 3858, Train_Loss: 4797.98828125, Val_Loss: 5066.83349609375\n",
      "Epoch 3859, Train_Loss: 4797.7939453125, Val_Loss: 5066.5986328125\n",
      "Epoch 3860, Train_Loss: 4797.5986328125, Val_Loss: 5066.3779296875\n",
      "Epoch 3861, Train_Loss: 4797.36572265625, Val_Loss: 5066.154296875\n",
      "Epoch 3862, Train_Loss: 4797.171875, Val_Loss: 5065.94140625\n",
      "Epoch 3863, Train_Loss: 4796.97705078125, Val_Loss: 5065.7138671875\n",
      "Epoch 3864, Train_Loss: 4796.8212890625, Val_Loss: 5065.46728515625\n",
      "Epoch 3865, Train_Loss: 4796.61474609375, Val_Loss: 5065.24072265625\n",
      "Epoch 3866, Train_Loss: 4796.43798828125, Val_Loss: 5065.02099609375\n",
      "Epoch 3867, Train_Loss: 4796.24609375, Val_Loss: 5064.798828125\n",
      "Epoch 3868, Train_Loss: 4796.03466796875, Val_Loss: 5064.54931640625\n",
      "Epoch 3869, Train_Loss: 4795.83740234375, Val_Loss: 5064.31982421875\n",
      "Epoch 3870, Train_Loss: 4795.6533203125, Val_Loss: 5064.09765625\n",
      "Epoch 3871, Train_Loss: 4795.45751953125, Val_Loss: 5063.85546875\n",
      "Epoch 3872, Train_Loss: 4795.2626953125, Val_Loss: 5063.63232421875\n",
      "Epoch 3873, Train_Loss: 4795.04248046875, Val_Loss: 5063.40478515625\n",
      "Epoch 3874, Train_Loss: 4794.80322265625, Val_Loss: 5063.1982421875\n",
      "Epoch 3875, Train_Loss: 4794.6083984375, Val_Loss: 5062.9765625\n",
      "Epoch 3876, Train_Loss: 4794.42919921875, Val_Loss: 5062.74755859375\n",
      "Epoch 3877, Train_Loss: 4794.2412109375, Val_Loss: 5062.52490234375\n",
      "Epoch 3878, Train_Loss: 4794.04443359375, Val_Loss: 5062.2802734375\n",
      "Epoch 3879, Train_Loss: 4793.84130859375, Val_Loss: 5062.05712890625\n",
      "Epoch 3880, Train_Loss: 4793.64306640625, Val_Loss: 5061.83740234375\n",
      "Epoch 3881, Train_Loss: 4793.45556640625, Val_Loss: 5061.62548828125\n",
      "Epoch 3882, Train_Loss: 4793.27001953125, Val_Loss: 5061.39111328125\n",
      "Epoch 3883, Train_Loss: 4793.04248046875, Val_Loss: 5061.1708984375\n",
      "Epoch 3884, Train_Loss: 4792.849609375, Val_Loss: 5060.9345703125\n",
      "Epoch 3885, Train_Loss: 4792.6591796875, Val_Loss: 5060.7080078125\n",
      "Epoch 3886, Train_Loss: 4792.455078125, Val_Loss: 5060.4833984375\n",
      "Epoch 3887, Train_Loss: 4792.2529296875, Val_Loss: 5060.259765625\n",
      "Epoch 3888, Train_Loss: 4792.02001953125, Val_Loss: 5060.04296875\n",
      "Epoch 3889, Train_Loss: 4791.82763671875, Val_Loss: 5059.82373046875\n",
      "Epoch 3890, Train_Loss: 4791.6337890625, Val_Loss: 5059.58984375\n",
      "Epoch 3891, Train_Loss: 4791.4326171875, Val_Loss: 5059.3505859375\n",
      "Epoch 3892, Train_Loss: 4791.24560546875, Val_Loss: 5059.1220703125\n",
      "Epoch 3893, Train_Loss: 4791.01953125, Val_Loss: 5058.9072265625\n",
      "Epoch 3894, Train_Loss: 4790.82421875, Val_Loss: 5058.685546875\n",
      "Epoch 3895, Train_Loss: 4790.62744140625, Val_Loss: 5058.46533203125\n",
      "Epoch 3896, Train_Loss: 4790.43798828125, Val_Loss: 5058.22900390625\n",
      "Epoch 3897, Train_Loss: 4790.24951171875, Val_Loss: 5057.9990234375\n",
      "Epoch 3898, Train_Loss: 4790.03173828125, Val_Loss: 5057.7666015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3899, Train_Loss: 4789.82958984375, Val_Loss: 5057.54248046875\n",
      "Epoch 3900, Train_Loss: 4789.63134765625, Val_Loss: 5057.326171875\n",
      "Epoch 3901, Train_Loss: 4789.39697265625, Val_Loss: 5057.10693359375\n",
      "Epoch 3902, Train_Loss: 4789.216796875, Val_Loss: 5056.880859375\n",
      "Epoch 3903, Train_Loss: 4789.021484375, Val_Loss: 5056.65869140625\n",
      "Epoch 3904, Train_Loss: 4788.84521484375, Val_Loss: 5056.42529296875\n",
      "Epoch 3905, Train_Loss: 4788.64453125, Val_Loss: 5056.2001953125\n",
      "Epoch 3906, Train_Loss: 4788.45263671875, Val_Loss: 5055.94384765625\n",
      "Epoch 3907, Train_Loss: 4788.255859375, Val_Loss: 5055.7265625\n",
      "Epoch 3908, Train_Loss: 4788.0537109375, Val_Loss: 5055.5107421875\n",
      "Epoch 3909, Train_Loss: 4787.87451171875, Val_Loss: 5055.29248046875\n",
      "Epoch 3910, Train_Loss: 4787.70263671875, Val_Loss: 5055.06298828125\n",
      "Epoch 3911, Train_Loss: 4787.51953125, Val_Loss: 5054.82177734375\n",
      "Epoch 3912, Train_Loss: 4787.32275390625, Val_Loss: 5054.5986328125\n",
      "Epoch 3913, Train_Loss: 4787.10205078125, Val_Loss: 5054.37890625\n",
      "Epoch 3914, Train_Loss: 4786.8857421875, Val_Loss: 5054.15478515625\n",
      "Epoch 3915, Train_Loss: 4786.67626953125, Val_Loss: 5053.94189453125\n",
      "Epoch 3916, Train_Loss: 4786.4814453125, Val_Loss: 5053.71142578125\n",
      "Epoch 3917, Train_Loss: 4786.3134765625, Val_Loss: 5053.484375\n",
      "Epoch 3918, Train_Loss: 4786.11669921875, Val_Loss: 5053.267578125\n",
      "Epoch 3919, Train_Loss: 4785.93359375, Val_Loss: 5053.03759765625\n",
      "Epoch 3920, Train_Loss: 4785.728515625, Val_Loss: 5052.80908203125\n",
      "Epoch 3921, Train_Loss: 4785.53076171875, Val_Loss: 5052.58203125\n",
      "Epoch 3922, Train_Loss: 4785.33349609375, Val_Loss: 5052.3681640625\n",
      "Epoch 3923, Train_Loss: 4785.11572265625, Val_Loss: 5052.15380859375\n",
      "Epoch 3924, Train_Loss: 4784.7490234375, Val_Loss: 5051.9013671875\n",
      "Epoch 3925, Train_Loss: 4784.56640625, Val_Loss: 5051.6787109375\n",
      "Epoch 3926, Train_Loss: 4784.361328125, Val_Loss: 5051.458984375\n",
      "Epoch 3927, Train_Loss: 4784.11572265625, Val_Loss: 5051.23779296875\n",
      "Epoch 3928, Train_Loss: 4783.904296875, Val_Loss: 5051.0244140625\n",
      "Epoch 3929, Train_Loss: 4783.728515625, Val_Loss: 5050.794921875\n",
      "Epoch 3930, Train_Loss: 4783.55224609375, Val_Loss: 5050.5830078125\n",
      "Epoch 3931, Train_Loss: 4783.37060546875, Val_Loss: 5050.333984375\n",
      "Epoch 3932, Train_Loss: 4783.16845703125, Val_Loss: 5050.1162109375\n",
      "Epoch 3933, Train_Loss: 4782.9560546875, Val_Loss: 5049.88623046875\n",
      "Epoch 3934, Train_Loss: 4782.7548828125, Val_Loss: 5049.6669921875\n",
      "Epoch 3935, Train_Loss: 4782.57177734375, Val_Loss: 5049.443359375\n",
      "Epoch 3936, Train_Loss: 4782.380859375, Val_Loss: 5049.21923828125\n",
      "Epoch 3937, Train_Loss: 4782.19384765625, Val_Loss: 5048.98486328125\n",
      "Epoch 3938, Train_Loss: 4781.97607421875, Val_Loss: 5048.75439453125\n",
      "Epoch 3939, Train_Loss: 4781.78662109375, Val_Loss: 5048.5390625\n",
      "Epoch 3940, Train_Loss: 4781.544921875, Val_Loss: 5048.32373046875\n",
      "Epoch 3941, Train_Loss: 4781.35400390625, Val_Loss: 5048.09228515625\n",
      "Epoch 3942, Train_Loss: 4781.158203125, Val_Loss: 5047.8701171875\n",
      "Epoch 3943, Train_Loss: 4780.935546875, Val_Loss: 5047.61376953125\n",
      "Epoch 3944, Train_Loss: 4780.78515625, Val_Loss: 5047.37744140625\n",
      "Epoch 3945, Train_Loss: 4780.5908203125, Val_Loss: 5047.15380859375\n",
      "Epoch 3946, Train_Loss: 4780.4091796875, Val_Loss: 5046.9345703125\n",
      "Epoch 3947, Train_Loss: 4780.21142578125, Val_Loss: 5046.7158203125\n",
      "Epoch 3948, Train_Loss: 4780.01513671875, Val_Loss: 5046.5029296875\n",
      "Epoch 3949, Train_Loss: 4779.818359375, Val_Loss: 5046.2763671875\n",
      "Epoch 3950, Train_Loss: 4779.6318359375, Val_Loss: 5046.05419921875\n",
      "Epoch 3951, Train_Loss: 4779.43603515625, Val_Loss: 5045.8134765625\n",
      "Epoch 3952, Train_Loss: 4779.24462890625, Val_Loss: 5045.5908203125\n",
      "Epoch 3953, Train_Loss: 4779.04150390625, Val_Loss: 5045.375\n",
      "Epoch 3954, Train_Loss: 4778.8310546875, Val_Loss: 5045.16015625\n",
      "Epoch 3955, Train_Loss: 4778.63916015625, Val_Loss: 5044.93359375\n",
      "Epoch 3956, Train_Loss: 4778.4501953125, Val_Loss: 5044.71875\n",
      "Epoch 3957, Train_Loss: 4778.271484375, Val_Loss: 5044.4794921875\n",
      "Epoch 3958, Train_Loss: 4778.07275390625, Val_Loss: 5044.25732421875\n",
      "Epoch 3959, Train_Loss: 4777.8828125, Val_Loss: 5044.0322265625\n",
      "Epoch 3960, Train_Loss: 4777.69677734375, Val_Loss: 5043.82177734375\n",
      "Epoch 3961, Train_Loss: 4777.47802734375, Val_Loss: 5043.5966796875\n",
      "Epoch 3962, Train_Loss: 4777.2861328125, Val_Loss: 5043.36669921875\n",
      "Epoch 3963, Train_Loss: 4777.0732421875, Val_Loss: 5043.14501953125\n",
      "Epoch 3964, Train_Loss: 4776.8671875, Val_Loss: 5042.9140625\n",
      "Epoch 3965, Train_Loss: 4776.67919921875, Val_Loss: 5042.68408203125\n",
      "Epoch 3966, Train_Loss: 4776.4931640625, Val_Loss: 5042.4677734375\n",
      "Epoch 3967, Train_Loss: 4776.26513671875, Val_Loss: 5042.251953125\n",
      "Epoch 3968, Train_Loss: 4776.0693359375, Val_Loss: 5042.0361328125\n",
      "Epoch 3969, Train_Loss: 4775.87939453125, Val_Loss: 5041.79736328125\n",
      "Epoch 3970, Train_Loss: 4775.70654296875, Val_Loss: 5041.55419921875\n",
      "Epoch 3971, Train_Loss: 4775.53173828125, Val_Loss: 5041.33544921875\n",
      "Epoch 3972, Train_Loss: 4775.337890625, Val_Loss: 5041.1162109375\n",
      "Epoch 3973, Train_Loss: 4775.123046875, Val_Loss: 5040.88671875\n",
      "Epoch 3974, Train_Loss: 4774.92919921875, Val_Loss: 5040.66845703125\n",
      "Epoch 3975, Train_Loss: 4774.74072265625, Val_Loss: 5040.44677734375\n",
      "Epoch 3976, Train_Loss: 4774.5615234375, Val_Loss: 5040.22021484375\n",
      "Epoch 3977, Train_Loss: 4774.3583984375, Val_Loss: 5039.99072265625\n",
      "Epoch 3978, Train_Loss: 4774.14794921875, Val_Loss: 5039.77294921875\n",
      "Epoch 3979, Train_Loss: 4773.9677734375, Val_Loss: 5039.54541015625\n",
      "Epoch 3980, Train_Loss: 4773.71435546875, Val_Loss: 5039.31298828125\n",
      "Epoch 3981, Train_Loss: 4773.52587890625, Val_Loss: 5039.076171875\n",
      "Epoch 3982, Train_Loss: 4773.34375, Val_Loss: 5038.857421875\n",
      "Epoch 3983, Train_Loss: 4773.158203125, Val_Loss: 5038.6171875\n",
      "Epoch 3984, Train_Loss: 4772.978515625, Val_Loss: 5038.38818359375\n",
      "Epoch 3985, Train_Loss: 4772.78955078125, Val_Loss: 5038.171875\n",
      "Epoch 3986, Train_Loss: 4772.6005859375, Val_Loss: 5037.9501953125\n",
      "Epoch 3987, Train_Loss: 4772.41259765625, Val_Loss: 5037.74609375\n",
      "Epoch 3988, Train_Loss: 4772.22021484375, Val_Loss: 5037.5185546875\n",
      "Epoch 3989, Train_Loss: 4771.958984375, Val_Loss: 5037.28857421875\n",
      "Epoch 3990, Train_Loss: 4771.76318359375, Val_Loss: 5037.05517578125\n",
      "Epoch 3991, Train_Loss: 4771.5693359375, Val_Loss: 5036.8232421875\n",
      "Epoch 3992, Train_Loss: 4771.3671875, Val_Loss: 5036.61376953125\n",
      "Epoch 3993, Train_Loss: 4771.115234375, Val_Loss: 5036.4033203125\n",
      "Epoch 3994, Train_Loss: 4770.91455078125, Val_Loss: 5036.173828125\n",
      "Epoch 3995, Train_Loss: 4770.7255859375, Val_Loss: 5035.94091796875\n",
      "Epoch 3996, Train_Loss: 4770.5283203125, Val_Loss: 5035.7275390625\n",
      "Epoch 3997, Train_Loss: 4770.322265625, Val_Loss: 5035.5048828125\n",
      "Epoch 3998, Train_Loss: 4770.123046875, Val_Loss: 5035.2666015625\n",
      "Epoch 3999, Train_Loss: 4769.89013671875, Val_Loss: 5035.0380859375\n",
      "Epoch 4000, Train_Loss: 4769.70166015625, Val_Loss: 5034.83203125\n",
      "Epoch 4001, Train_Loss: 4769.5166015625, Val_Loss: 5034.61181640625\n",
      "Epoch 4002, Train_Loss: 4769.33544921875, Val_Loss: 5034.392578125\n",
      "Epoch 4003, Train_Loss: 4769.11865234375, Val_Loss: 5034.14794921875\n",
      "Epoch 4004, Train_Loss: 4768.923828125, Val_Loss: 5033.9326171875\n",
      "Epoch 4005, Train_Loss: 4768.720703125, Val_Loss: 5033.71044921875\n",
      "Epoch 4006, Train_Loss: 4768.529296875, Val_Loss: 5033.474609375\n",
      "Epoch 4007, Train_Loss: 4768.28271484375, Val_Loss: 5033.26416015625\n",
      "Epoch 4008, Train_Loss: 4768.07861328125, Val_Loss: 5033.0341796875\n",
      "Epoch 4009, Train_Loss: 4767.8828125, Val_Loss: 5032.81396484375\n",
      "Epoch 4010, Train_Loss: 4767.70458984375, Val_Loss: 5032.57763671875\n",
      "Epoch 4011, Train_Loss: 4767.50390625, Val_Loss: 5032.35205078125\n",
      "Epoch 4012, Train_Loss: 4767.33203125, Val_Loss: 5032.1455078125\n",
      "Epoch 4013, Train_Loss: 4767.11865234375, Val_Loss: 5031.91748046875\n",
      "Epoch 4014, Train_Loss: 4766.92431640625, Val_Loss: 5031.693359375\n",
      "Epoch 4015, Train_Loss: 4766.75146484375, Val_Loss: 5031.47607421875\n",
      "Epoch 4016, Train_Loss: 4766.55810546875, Val_Loss: 5031.2333984375\n",
      "Epoch 4017, Train_Loss: 4766.34619140625, Val_Loss: 5030.9853515625\n",
      "Epoch 4018, Train_Loss: 4766.13525390625, Val_Loss: 5030.7783203125\n",
      "Epoch 4019, Train_Loss: 4765.94921875, Val_Loss: 5030.541015625\n",
      "Epoch 4020, Train_Loss: 4765.73876953125, Val_Loss: 5030.333984375\n",
      "Epoch 4021, Train_Loss: 4765.548828125, Val_Loss: 5030.0986328125\n",
      "Epoch 4022, Train_Loss: 4765.36865234375, Val_Loss: 5029.87841796875\n",
      "Epoch 4023, Train_Loss: 4765.1982421875, Val_Loss: 5029.65185546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4024, Train_Loss: 4765.0146484375, Val_Loss: 5029.421875\n",
      "Epoch 4025, Train_Loss: 4764.82568359375, Val_Loss: 5029.212890625\n",
      "Epoch 4026, Train_Loss: 4764.63525390625, Val_Loss: 5028.990234375\n",
      "Epoch 4027, Train_Loss: 4764.4482421875, Val_Loss: 5028.76220703125\n",
      "Epoch 4028, Train_Loss: 4764.25634765625, Val_Loss: 5028.544921875\n",
      "Epoch 4029, Train_Loss: 4764.0791015625, Val_Loss: 5028.32421875\n",
      "Epoch 4030, Train_Loss: 4763.88330078125, Val_Loss: 5028.09228515625\n",
      "Epoch 4031, Train_Loss: 4763.66357421875, Val_Loss: 5027.8662109375\n",
      "Epoch 4032, Train_Loss: 4763.48193359375, Val_Loss: 5027.6494140625\n",
      "Epoch 4033, Train_Loss: 4763.21044921875, Val_Loss: 5027.43896484375\n",
      "Epoch 4034, Train_Loss: 4763.02587890625, Val_Loss: 5027.2138671875\n",
      "Epoch 4035, Train_Loss: 4762.837890625, Val_Loss: 5026.98583984375\n",
      "Epoch 4036, Train_Loss: 4762.63427734375, Val_Loss: 5026.7607421875\n",
      "Epoch 4037, Train_Loss: 4762.44482421875, Val_Loss: 5026.54296875\n",
      "Epoch 4038, Train_Loss: 4762.259765625, Val_Loss: 5026.310546875\n",
      "Epoch 4039, Train_Loss: 4762.05224609375, Val_Loss: 5026.09423828125\n",
      "Epoch 4040, Train_Loss: 4761.86181640625, Val_Loss: 5025.87060546875\n",
      "Epoch 4041, Train_Loss: 4761.68408203125, Val_Loss: 5025.65576171875\n",
      "Epoch 4042, Train_Loss: 4761.47314453125, Val_Loss: 5025.43701171875\n",
      "Epoch 4043, Train_Loss: 4761.29833984375, Val_Loss: 5025.19384765625\n",
      "Epoch 4044, Train_Loss: 4761.0947265625, Val_Loss: 5024.978515625\n",
      "Epoch 4045, Train_Loss: 4760.8916015625, Val_Loss: 5024.74560546875\n",
      "Epoch 4046, Train_Loss: 4760.65869140625, Val_Loss: 5024.5341796875\n",
      "Epoch 4047, Train_Loss: 4760.45703125, Val_Loss: 5024.306640625\n",
      "Epoch 4048, Train_Loss: 4760.27734375, Val_Loss: 5024.08984375\n",
      "Epoch 4049, Train_Loss: 4760.10546875, Val_Loss: 5023.85302734375\n",
      "Epoch 4050, Train_Loss: 4759.91357421875, Val_Loss: 5023.63525390625\n",
      "Epoch 4051, Train_Loss: 4759.740234375, Val_Loss: 5023.41259765625\n",
      "Epoch 4052, Train_Loss: 4759.53759765625, Val_Loss: 5023.1982421875\n",
      "Epoch 4053, Train_Loss: 4759.357421875, Val_Loss: 5022.99658203125\n",
      "Epoch 4054, Train_Loss: 4759.154296875, Val_Loss: 5022.73046875\n",
      "Epoch 4055, Train_Loss: 4758.97119140625, Val_Loss: 5022.5107421875\n",
      "Epoch 4056, Train_Loss: 4758.77783203125, Val_Loss: 5022.27783203125\n",
      "Epoch 4057, Train_Loss: 4758.57080078125, Val_Loss: 5022.0595703125\n",
      "Epoch 4058, Train_Loss: 4758.388671875, Val_Loss: 5021.8349609375\n",
      "Epoch 4059, Train_Loss: 4758.185546875, Val_Loss: 5021.62060546875\n",
      "Epoch 4060, Train_Loss: 4757.99365234375, Val_Loss: 5021.3984375\n",
      "Epoch 4061, Train_Loss: 4757.81640625, Val_Loss: 5021.1689453125\n",
      "Epoch 4062, Train_Loss: 4757.615234375, Val_Loss: 5020.955078125\n",
      "Epoch 4063, Train_Loss: 4757.4365234375, Val_Loss: 5020.72900390625\n",
      "Epoch 4064, Train_Loss: 4757.21484375, Val_Loss: 5020.50634765625\n",
      "Epoch 4065, Train_Loss: 4757.02880859375, Val_Loss: 5020.2900390625\n",
      "Epoch 4066, Train_Loss: 4756.85107421875, Val_Loss: 5020.06298828125\n",
      "Epoch 4067, Train_Loss: 4756.65966796875, Val_Loss: 5019.8544921875\n",
      "Epoch 4068, Train_Loss: 4756.4736328125, Val_Loss: 5019.62646484375\n",
      "Epoch 4069, Train_Loss: 4756.28466796875, Val_Loss: 5019.392578125\n",
      "Epoch 4070, Train_Loss: 4756.0908203125, Val_Loss: 5019.17529296875\n",
      "Epoch 4071, Train_Loss: 4755.90283203125, Val_Loss: 5018.9521484375\n",
      "Epoch 4072, Train_Loss: 4755.6474609375, Val_Loss: 5018.74169921875\n",
      "Epoch 4073, Train_Loss: 4755.31201171875, Val_Loss: 5018.521484375\n",
      "Epoch 4074, Train_Loss: 4755.12255859375, Val_Loss: 5018.28515625\n",
      "Epoch 4075, Train_Loss: 4754.9375, Val_Loss: 5018.07861328125\n",
      "Epoch 4076, Train_Loss: 4754.78125, Val_Loss: 5017.85498046875\n",
      "Epoch 4077, Train_Loss: 4754.58740234375, Val_Loss: 5017.62939453125\n",
      "Epoch 4078, Train_Loss: 4754.39794921875, Val_Loss: 5017.39990234375\n",
      "Epoch 4079, Train_Loss: 4754.21240234375, Val_Loss: 5017.18212890625\n",
      "Epoch 4080, Train_Loss: 4754.017578125, Val_Loss: 5016.958984375\n",
      "Epoch 4081, Train_Loss: 4753.8251953125, Val_Loss: 5016.7412109375\n",
      "Epoch 4082, Train_Loss: 4753.59521484375, Val_Loss: 5016.50146484375\n",
      "Epoch 4083, Train_Loss: 4753.41845703125, Val_Loss: 5016.2822265625\n",
      "Epoch 4084, Train_Loss: 4753.228515625, Val_Loss: 5016.05615234375\n",
      "Epoch 4085, Train_Loss: 4753.05224609375, Val_Loss: 5015.828125\n",
      "Epoch 4086, Train_Loss: 4752.80029296875, Val_Loss: 5015.625\n",
      "Epoch 4087, Train_Loss: 4752.60107421875, Val_Loss: 5015.40087890625\n",
      "Epoch 4088, Train_Loss: 4752.41796875, Val_Loss: 5015.1845703125\n",
      "Epoch 4089, Train_Loss: 4752.2236328125, Val_Loss: 5014.9462890625\n",
      "Epoch 4090, Train_Loss: 4752.0546875, Val_Loss: 5014.73583984375\n",
      "Epoch 4091, Train_Loss: 4751.86376953125, Val_Loss: 5014.51318359375\n",
      "Epoch 4092, Train_Loss: 4751.65576171875, Val_Loss: 5014.26513671875\n",
      "Epoch 4093, Train_Loss: 4751.458984375, Val_Loss: 5014.0419921875\n",
      "Epoch 4094, Train_Loss: 4751.2607421875, Val_Loss: 5013.82421875\n",
      "Epoch 4095, Train_Loss: 4751.0712890625, Val_Loss: 5013.6015625\n",
      "Epoch 4096, Train_Loss: 4750.888671875, Val_Loss: 5013.3662109375\n",
      "Epoch 4097, Train_Loss: 4750.69873046875, Val_Loss: 5013.1494140625\n",
      "Epoch 4098, Train_Loss: 4750.5107421875, Val_Loss: 5012.9384765625\n",
      "Epoch 4099, Train_Loss: 4750.28173828125, Val_Loss: 5012.73046875\n",
      "Epoch 4100, Train_Loss: 4750.0908203125, Val_Loss: 5012.5068359375\n",
      "Epoch 4101, Train_Loss: 4749.8935546875, Val_Loss: 5012.28466796875\n",
      "Epoch 4102, Train_Loss: 4749.67626953125, Val_Loss: 5012.05078125\n",
      "Epoch 4103, Train_Loss: 4749.4765625, Val_Loss: 5011.828125\n",
      "Epoch 4104, Train_Loss: 4749.29541015625, Val_Loss: 5011.60498046875\n",
      "Epoch 4105, Train_Loss: 4749.10595703125, Val_Loss: 5011.3876953125\n",
      "Epoch 4106, Train_Loss: 4748.91796875, Val_Loss: 5011.185546875\n",
      "Epoch 4107, Train_Loss: 4748.72412109375, Val_Loss: 5010.95703125\n",
      "Epoch 4108, Train_Loss: 4748.5302734375, Val_Loss: 5010.74169921875\n",
      "Epoch 4109, Train_Loss: 4748.3408203125, Val_Loss: 5010.5009765625\n",
      "Epoch 4110, Train_Loss: 4748.15869140625, Val_Loss: 5010.2763671875\n",
      "Epoch 4111, Train_Loss: 4747.9775390625, Val_Loss: 5010.05517578125\n",
      "Epoch 4112, Train_Loss: 4747.7119140625, Val_Loss: 5009.84765625\n",
      "Epoch 4113, Train_Loss: 4747.51611328125, Val_Loss: 5009.62255859375\n",
      "Epoch 4114, Train_Loss: 4747.33642578125, Val_Loss: 5009.40380859375\n",
      "Epoch 4115, Train_Loss: 4747.14501953125, Val_Loss: 5009.16943359375\n",
      "Epoch 4116, Train_Loss: 4746.96630859375, Val_Loss: 5008.9443359375\n",
      "Epoch 4117, Train_Loss: 4746.7626953125, Val_Loss: 5008.73486328125\n",
      "Epoch 4118, Train_Loss: 4746.5810546875, Val_Loss: 5008.51953125\n",
      "Epoch 4119, Train_Loss: 4746.3828125, Val_Loss: 5008.30859375\n",
      "Epoch 4120, Train_Loss: 4746.19287109375, Val_Loss: 5008.0703125\n",
      "Epoch 4121, Train_Loss: 4746.00390625, Val_Loss: 5007.8466796875\n",
      "Epoch 4122, Train_Loss: 4745.78759765625, Val_Loss: 5007.625\n",
      "Epoch 4123, Train_Loss: 4745.59765625, Val_Loss: 5007.40673828125\n",
      "Epoch 4124, Train_Loss: 4745.40869140625, Val_Loss: 5007.18701171875\n",
      "Epoch 4125, Train_Loss: 4745.2001953125, Val_Loss: 5006.96728515625\n",
      "Epoch 4126, Train_Loss: 4745.01806640625, Val_Loss: 5006.7421875\n",
      "Epoch 4127, Train_Loss: 4744.82666015625, Val_Loss: 5006.52197265625\n",
      "Epoch 4128, Train_Loss: 4744.63623046875, Val_Loss: 5006.30419921875\n",
      "Epoch 4129, Train_Loss: 4744.46728515625, Val_Loss: 5006.02880859375\n",
      "Epoch 4130, Train_Loss: 4744.27587890625, Val_Loss: 5005.8232421875\n",
      "Epoch 4131, Train_Loss: 4744.07958984375, Val_Loss: 5005.5986328125\n",
      "Epoch 4132, Train_Loss: 4743.86279296875, Val_Loss: 5005.39306640625\n",
      "Epoch 4133, Train_Loss: 4743.68212890625, Val_Loss: 5005.158203125\n",
      "Epoch 4134, Train_Loss: 4743.5126953125, Val_Loss: 5004.95068359375\n",
      "Epoch 4135, Train_Loss: 4743.3271484375, Val_Loss: 5004.7236328125\n",
      "Epoch 4136, Train_Loss: 4743.15576171875, Val_Loss: 5004.49462890625\n",
      "Epoch 4137, Train_Loss: 4742.95458984375, Val_Loss: 5004.28759765625\n",
      "Epoch 4138, Train_Loss: 4742.650390625, Val_Loss: 5004.07275390625\n",
      "Epoch 4139, Train_Loss: 4742.45458984375, Val_Loss: 5003.857421875\n",
      "Epoch 4140, Train_Loss: 4742.2529296875, Val_Loss: 5003.626953125\n",
      "Epoch 4141, Train_Loss: 4742.07080078125, Val_Loss: 5003.40966796875\n",
      "Epoch 4142, Train_Loss: 4741.88134765625, Val_Loss: 5003.1748046875\n",
      "Epoch 4143, Train_Loss: 4741.6953125, Val_Loss: 5002.96142578125\n",
      "Epoch 4144, Train_Loss: 4741.486328125, Val_Loss: 5002.73583984375\n",
      "Epoch 4145, Train_Loss: 4741.31689453125, Val_Loss: 5002.517578125\n",
      "Epoch 4146, Train_Loss: 4741.14208984375, Val_Loss: 5002.3046875\n",
      "Epoch 4147, Train_Loss: 4740.8828125, Val_Loss: 5002.080078125\n",
      "Epoch 4148, Train_Loss: 4740.6904296875, Val_Loss: 5001.8515625\n",
      "Epoch 4149, Train_Loss: 4740.49951171875, Val_Loss: 5001.64404296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4150, Train_Loss: 4740.3154296875, Val_Loss: 5001.41015625\n",
      "Epoch 4151, Train_Loss: 4740.12158203125, Val_Loss: 5001.18408203125\n",
      "Epoch 4152, Train_Loss: 4739.8681640625, Val_Loss: 5000.984375\n",
      "Epoch 4153, Train_Loss: 4739.68310546875, Val_Loss: 5000.75439453125\n",
      "Epoch 4154, Train_Loss: 4739.4912109375, Val_Loss: 5000.54541015625\n",
      "Epoch 4155, Train_Loss: 4739.34619140625, Val_Loss: 5000.30859375\n",
      "Epoch 4156, Train_Loss: 4739.17041015625, Val_Loss: 5000.08984375\n",
      "Epoch 4157, Train_Loss: 4738.96826171875, Val_Loss: 4999.8515625\n",
      "Epoch 4158, Train_Loss: 4738.775390625, Val_Loss: 4999.6474609375\n",
      "Epoch 4159, Train_Loss: 4738.57958984375, Val_Loss: 4999.435546875\n",
      "Epoch 4160, Train_Loss: 4738.3955078125, Val_Loss: 4999.21240234375\n",
      "Epoch 4161, Train_Loss: 4738.2060546875, Val_Loss: 4998.9892578125\n",
      "Epoch 4162, Train_Loss: 4737.98828125, Val_Loss: 4998.759765625\n",
      "Epoch 4163, Train_Loss: 4737.81298828125, Val_Loss: 4998.537109375\n",
      "Epoch 4164, Train_Loss: 4737.64013671875, Val_Loss: 4998.32177734375\n",
      "Epoch 4165, Train_Loss: 4737.4443359375, Val_Loss: 4998.11279296875\n",
      "Epoch 4166, Train_Loss: 4737.2578125, Val_Loss: 4997.857421875\n",
      "Epoch 4167, Train_Loss: 4737.0498046875, Val_Loss: 4997.6337890625\n",
      "Epoch 4168, Train_Loss: 4736.857421875, Val_Loss: 4997.40625\n",
      "Epoch 4169, Train_Loss: 4736.6845703125, Val_Loss: 4997.18408203125\n",
      "Epoch 4170, Train_Loss: 4736.501953125, Val_Loss: 4996.97119140625\n",
      "Epoch 4171, Train_Loss: 4736.32421875, Val_Loss: 4996.763671875\n",
      "Epoch 4172, Train_Loss: 4736.126953125, Val_Loss: 4996.54541015625\n",
      "Epoch 4173, Train_Loss: 4735.9375, Val_Loss: 4996.32666015625\n",
      "Epoch 4174, Train_Loss: 4735.748046875, Val_Loss: 4996.1064453125\n",
      "Epoch 4175, Train_Loss: 4735.55322265625, Val_Loss: 4995.875\n",
      "Epoch 4176, Train_Loss: 4735.37255859375, Val_Loss: 4995.65576171875\n",
      "Epoch 4177, Train_Loss: 4735.18701171875, Val_Loss: 4995.4404296875\n",
      "Epoch 4178, Train_Loss: 4734.94970703125, Val_Loss: 4995.22900390625\n",
      "Epoch 4179, Train_Loss: 4734.74951171875, Val_Loss: 4995.0009765625\n",
      "Epoch 4180, Train_Loss: 4734.56298828125, Val_Loss: 4994.783203125\n",
      "Epoch 4181, Train_Loss: 4734.4072265625, Val_Loss: 4994.56396484375\n",
      "Epoch 4182, Train_Loss: 4734.20751953125, Val_Loss: 4994.34619140625\n",
      "Epoch 4183, Train_Loss: 4734.03369140625, Val_Loss: 4994.13232421875\n",
      "Epoch 4184, Train_Loss: 4733.84765625, Val_Loss: 4993.9150390625\n",
      "Epoch 4185, Train_Loss: 4733.65673828125, Val_Loss: 4993.6953125\n",
      "Epoch 4186, Train_Loss: 4733.470703125, Val_Loss: 4993.4716796875\n",
      "Epoch 4187, Train_Loss: 4733.27978515625, Val_Loss: 4993.25\n",
      "Epoch 4188, Train_Loss: 4733.10205078125, Val_Loss: 4993.02734375\n",
      "Epoch 4189, Train_Loss: 4732.9111328125, Val_Loss: 4992.8154296875\n",
      "Epoch 4190, Train_Loss: 4732.734375, Val_Loss: 4992.60009765625\n",
      "Epoch 4191, Train_Loss: 4732.48388671875, Val_Loss: 4992.384765625\n",
      "Epoch 4192, Train_Loss: 4732.265625, Val_Loss: 4992.16259765625\n",
      "Epoch 4193, Train_Loss: 4732.08740234375, Val_Loss: 4991.9384765625\n",
      "Epoch 4194, Train_Loss: 4731.9130859375, Val_Loss: 4991.6962890625\n",
      "Epoch 4195, Train_Loss: 4731.72509765625, Val_Loss: 4991.48095703125\n",
      "Epoch 4196, Train_Loss: 4731.5361328125, Val_Loss: 4991.2626953125\n",
      "Epoch 4197, Train_Loss: 4731.34033203125, Val_Loss: 4991.048828125\n",
      "Epoch 4198, Train_Loss: 4731.150390625, Val_Loss: 4990.83447265625\n",
      "Epoch 4199, Train_Loss: 4730.96142578125, Val_Loss: 4990.6279296875\n",
      "Epoch 4200, Train_Loss: 4730.77001953125, Val_Loss: 4990.404296875\n",
      "Epoch 4201, Train_Loss: 4730.5810546875, Val_Loss: 4990.1689453125\n",
      "Epoch 4202, Train_Loss: 4730.37841796875, Val_Loss: 4989.95068359375\n",
      "Epoch 4203, Train_Loss: 4730.169921875, Val_Loss: 4989.70654296875\n",
      "Epoch 4204, Train_Loss: 4729.93798828125, Val_Loss: 4989.50146484375\n",
      "Epoch 4205, Train_Loss: 4729.7509765625, Val_Loss: 4989.27587890625\n",
      "Epoch 4206, Train_Loss: 4729.57373046875, Val_Loss: 4989.06103515625\n",
      "Epoch 4207, Train_Loss: 4729.36865234375, Val_Loss: 4988.84521484375\n",
      "Epoch 4208, Train_Loss: 4729.17529296875, Val_Loss: 4988.60986328125\n",
      "Epoch 4209, Train_Loss: 4728.99951171875, Val_Loss: 4988.39892578125\n",
      "Epoch 4210, Train_Loss: 4728.8125, Val_Loss: 4988.185546875\n",
      "Epoch 4211, Train_Loss: 4728.619140625, Val_Loss: 4987.9697265625\n",
      "Epoch 4212, Train_Loss: 4728.39501953125, Val_Loss: 4987.7529296875\n",
      "Epoch 4213, Train_Loss: 4728.21875, Val_Loss: 4987.52880859375\n",
      "Epoch 4214, Train_Loss: 4728.03564453125, Val_Loss: 4987.3125\n",
      "Epoch 4215, Train_Loss: 4727.8544921875, Val_Loss: 4987.09619140625\n",
      "Epoch 4216, Train_Loss: 4727.6591796875, Val_Loss: 4986.87841796875\n",
      "Epoch 4217, Train_Loss: 4727.4541015625, Val_Loss: 4986.6630859375\n",
      "Epoch 4218, Train_Loss: 4727.23046875, Val_Loss: 4986.4619140625\n",
      "Epoch 4219, Train_Loss: 4727.05712890625, Val_Loss: 4986.2412109375\n",
      "Epoch 4220, Train_Loss: 4726.8740234375, Val_Loss: 4986.029296875\n",
      "Epoch 4221, Train_Loss: 4726.6826171875, Val_Loss: 4985.7939453125\n",
      "Epoch 4222, Train_Loss: 4726.44482421875, Val_Loss: 4985.57958984375\n",
      "Epoch 4223, Train_Loss: 4726.2568359375, Val_Loss: 4985.35107421875\n",
      "Epoch 4224, Train_Loss: 4726.0751953125, Val_Loss: 4985.15576171875\n",
      "Epoch 4225, Train_Loss: 4725.89892578125, Val_Loss: 4984.9404296875\n",
      "Epoch 4226, Train_Loss: 4725.7021484375, Val_Loss: 4984.724609375\n",
      "Epoch 4227, Train_Loss: 4725.51025390625, Val_Loss: 4984.4921875\n",
      "Epoch 4228, Train_Loss: 4725.333984375, Val_Loss: 4984.27734375\n",
      "Epoch 4229, Train_Loss: 4725.13818359375, Val_Loss: 4984.0576171875\n",
      "Epoch 4230, Train_Loss: 4724.95703125, Val_Loss: 4983.83740234375\n",
      "Epoch 4231, Train_Loss: 4724.7373046875, Val_Loss: 4983.63134765625\n",
      "Epoch 4232, Train_Loss: 4724.5390625, Val_Loss: 4983.39697265625\n",
      "Epoch 4233, Train_Loss: 4724.3564453125, Val_Loss: 4983.18017578125\n",
      "Epoch 4234, Train_Loss: 4724.18310546875, Val_Loss: 4982.96533203125\n",
      "Epoch 4235, Train_Loss: 4724.00634765625, Val_Loss: 4982.7490234375\n",
      "Epoch 4236, Train_Loss: 4723.82470703125, Val_Loss: 4982.53857421875\n",
      "Epoch 4237, Train_Loss: 4723.6298828125, Val_Loss: 4982.32373046875\n",
      "Epoch 4238, Train_Loss: 4723.45068359375, Val_Loss: 4982.10595703125\n",
      "Epoch 4239, Train_Loss: 4723.25830078125, Val_Loss: 4981.8916015625\n",
      "Epoch 4240, Train_Loss: 4723.0537109375, Val_Loss: 4981.6455078125\n",
      "Epoch 4241, Train_Loss: 4722.87109375, Val_Loss: 4981.41357421875\n",
      "Epoch 4242, Train_Loss: 4722.6611328125, Val_Loss: 4981.201171875\n",
      "Epoch 4243, Train_Loss: 4722.47802734375, Val_Loss: 4980.97802734375\n",
      "Epoch 4244, Train_Loss: 4722.2392578125, Val_Loss: 4980.77978515625\n",
      "Epoch 4245, Train_Loss: 4722.04736328125, Val_Loss: 4980.56201171875\n",
      "Epoch 4246, Train_Loss: 4721.8642578125, Val_Loss: 4980.33984375\n",
      "Epoch 4247, Train_Loss: 4721.6650390625, Val_Loss: 4980.111328125\n",
      "Epoch 4248, Train_Loss: 4721.48828125, Val_Loss: 4979.8994140625\n",
      "Epoch 4249, Train_Loss: 4721.30224609375, Val_Loss: 4979.6982421875\n",
      "Epoch 4250, Train_Loss: 4721.1064453125, Val_Loss: 4979.48291015625\n",
      "Epoch 4251, Train_Loss: 4720.919921875, Val_Loss: 4979.26708984375\n",
      "Epoch 4252, Train_Loss: 4720.72265625, Val_Loss: 4979.04443359375\n",
      "Epoch 4253, Train_Loss: 4720.5341796875, Val_Loss: 4978.82421875\n",
      "Epoch 4254, Train_Loss: 4720.361328125, Val_Loss: 4978.599609375\n",
      "Epoch 4255, Train_Loss: 4720.177734375, Val_Loss: 4978.39404296875\n",
      "Epoch 4256, Train_Loss: 4719.98046875, Val_Loss: 4978.18505859375\n",
      "Epoch 4257, Train_Loss: 4719.76025390625, Val_Loss: 4977.9716796875\n",
      "Epoch 4258, Train_Loss: 4719.58349609375, Val_Loss: 4977.75634765625\n",
      "Epoch 4259, Train_Loss: 4719.39013671875, Val_Loss: 4977.52783203125\n",
      "Epoch 4260, Train_Loss: 4719.21240234375, Val_Loss: 4977.31396484375\n",
      "Epoch 4261, Train_Loss: 4719.06787109375, Val_Loss: 4977.0869140625\n",
      "Epoch 4262, Train_Loss: 4718.8642578125, Val_Loss: 4976.8740234375\n",
      "Epoch 4263, Train_Loss: 4718.6806640625, Val_Loss: 4976.6669921875\n",
      "Epoch 4264, Train_Loss: 4718.50390625, Val_Loss: 4976.45263671875\n",
      "Epoch 4265, Train_Loss: 4718.30810546875, Val_Loss: 4976.2333984375\n",
      "Epoch 4266, Train_Loss: 4718.11767578125, Val_Loss: 4976.02587890625\n",
      "Epoch 4267, Train_Loss: 4717.93212890625, Val_Loss: 4975.8095703125\n",
      "Epoch 4268, Train_Loss: 4717.7392578125, Val_Loss: 4975.5703125\n",
      "Epoch 4269, Train_Loss: 4717.55712890625, Val_Loss: 4975.34814453125\n",
      "Epoch 4270, Train_Loss: 4717.3603515625, Val_Loss: 4975.1513671875\n",
      "Epoch 4271, Train_Loss: 4717.18408203125, Val_Loss: 4974.93310546875\n",
      "Epoch 4272, Train_Loss: 4716.98681640625, Val_Loss: 4974.724609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4273, Train_Loss: 4716.80078125, Val_Loss: 4974.505859375\n",
      "Epoch 4274, Train_Loss: 4716.6240234375, Val_Loss: 4974.27880859375\n",
      "Epoch 4275, Train_Loss: 4716.44287109375, Val_Loss: 4974.06982421875\n",
      "Epoch 4276, Train_Loss: 4716.2548828125, Val_Loss: 4973.8505859375\n",
      "Epoch 4277, Train_Loss: 4716.0537109375, Val_Loss: 4973.65869140625\n",
      "Epoch 4278, Train_Loss: 4715.8701171875, Val_Loss: 4973.40283203125\n",
      "Epoch 4279, Train_Loss: 4715.701171875, Val_Loss: 4973.1865234375\n",
      "Epoch 4280, Train_Loss: 4715.53173828125, Val_Loss: 4972.95263671875\n",
      "Epoch 4281, Train_Loss: 4715.33203125, Val_Loss: 4972.7509765625\n",
      "Epoch 4282, Train_Loss: 4715.1220703125, Val_Loss: 4972.5244140625\n",
      "Epoch 4283, Train_Loss: 4714.9208984375, Val_Loss: 4972.31494140625\n",
      "Epoch 4284, Train_Loss: 4714.69873046875, Val_Loss: 4972.103515625\n",
      "Epoch 4285, Train_Loss: 4714.5205078125, Val_Loss: 4971.8994140625\n",
      "Epoch 4286, Train_Loss: 4714.341796875, Val_Loss: 4971.6923828125\n",
      "Epoch 4287, Train_Loss: 4714.08740234375, Val_Loss: 4971.46728515625\n",
      "Epoch 4288, Train_Loss: 4713.91259765625, Val_Loss: 4971.24609375\n",
      "Epoch 4289, Train_Loss: 4713.73095703125, Val_Loss: 4971.0341796875\n",
      "Epoch 4290, Train_Loss: 4713.54248046875, Val_Loss: 4970.8349609375\n",
      "Epoch 4291, Train_Loss: 4713.369140625, Val_Loss: 4970.62548828125\n",
      "Epoch 4292, Train_Loss: 4713.15771484375, Val_Loss: 4970.392578125\n",
      "Epoch 4293, Train_Loss: 4712.9765625, Val_Loss: 4970.193359375\n",
      "Epoch 4294, Train_Loss: 4712.79345703125, Val_Loss: 4969.9755859375\n",
      "Epoch 4295, Train_Loss: 4712.6201171875, Val_Loss: 4969.75390625\n",
      "Epoch 4296, Train_Loss: 4712.3525390625, Val_Loss: 4969.5322265625\n",
      "Epoch 4297, Train_Loss: 4712.10400390625, Val_Loss: 4969.34033203125\n",
      "Epoch 4298, Train_Loss: 4711.91650390625, Val_Loss: 4969.1181640625\n",
      "Epoch 4299, Train_Loss: 4711.7353515625, Val_Loss: 4968.90869140625\n",
      "Epoch 4300, Train_Loss: 4711.55810546875, Val_Loss: 4968.6904296875\n",
      "Epoch 4301, Train_Loss: 4711.37939453125, Val_Loss: 4968.4599609375\n",
      "Epoch 4302, Train_Loss: 4711.18896484375, Val_Loss: 4968.251953125\n",
      "Epoch 4303, Train_Loss: 4711.013671875, Val_Loss: 4968.0458984375\n",
      "Epoch 4304, Train_Loss: 4710.83251953125, Val_Loss: 4967.83056640625\n",
      "Epoch 4305, Train_Loss: 4710.65771484375, Val_Loss: 4967.61572265625\n",
      "Epoch 4306, Train_Loss: 4710.47021484375, Val_Loss: 4967.39794921875\n",
      "Epoch 4307, Train_Loss: 4710.2763671875, Val_Loss: 4967.16552734375\n",
      "Epoch 4308, Train_Loss: 4710.0888671875, Val_Loss: 4966.96240234375\n",
      "Epoch 4309, Train_Loss: 4709.91845703125, Val_Loss: 4966.74560546875\n",
      "Epoch 4310, Train_Loss: 4709.708984375, Val_Loss: 4966.5400390625\n",
      "Epoch 4311, Train_Loss: 4709.51806640625, Val_Loss: 4966.33349609375\n",
      "Epoch 4312, Train_Loss: 4709.32080078125, Val_Loss: 4966.1083984375\n",
      "Epoch 4313, Train_Loss: 4709.12255859375, Val_Loss: 4965.89404296875\n",
      "Epoch 4314, Train_Loss: 4708.94677734375, Val_Loss: 4965.6806640625\n",
      "Epoch 4315, Train_Loss: 4708.75537109375, Val_Loss: 4965.42578125\n",
      "Epoch 4316, Train_Loss: 4708.5673828125, Val_Loss: 4965.2109375\n",
      "Epoch 4317, Train_Loss: 4708.35888671875, Val_Loss: 4965.0185546875\n",
      "Epoch 4318, Train_Loss: 4708.1875, Val_Loss: 4964.7958984375\n",
      "Epoch 4319, Train_Loss: 4707.99853515625, Val_Loss: 4964.58984375\n",
      "Epoch 4320, Train_Loss: 4707.8203125, Val_Loss: 4964.35400390625\n",
      "Epoch 4321, Train_Loss: 4707.64404296875, Val_Loss: 4964.15380859375\n",
      "Epoch 4322, Train_Loss: 4707.43115234375, Val_Loss: 4963.9384765625\n",
      "Epoch 4323, Train_Loss: 4707.19140625, Val_Loss: 4963.73779296875\n",
      "Epoch 4324, Train_Loss: 4707.02099609375, Val_Loss: 4963.525390625\n",
      "Epoch 4325, Train_Loss: 4706.8427734375, Val_Loss: 4963.306640625\n",
      "Epoch 4326, Train_Loss: 4706.6572265625, Val_Loss: 4963.08056640625\n",
      "Epoch 4327, Train_Loss: 4706.46435546875, Val_Loss: 4962.865234375\n",
      "Epoch 4328, Train_Loss: 4706.28369140625, Val_Loss: 4962.64990234375\n",
      "Epoch 4329, Train_Loss: 4706.0966796875, Val_Loss: 4962.44921875\n",
      "Epoch 4330, Train_Loss: 4705.92138671875, Val_Loss: 4962.23779296875\n",
      "Epoch 4331, Train_Loss: 4705.73193359375, Val_Loss: 4962.0302734375\n",
      "Epoch 4332, Train_Loss: 4705.5361328125, Val_Loss: 4961.82421875\n",
      "Epoch 4333, Train_Loss: 4705.349609375, Val_Loss: 4961.6064453125\n",
      "Epoch 4334, Train_Loss: 4705.1611328125, Val_Loss: 4961.3818359375\n",
      "Epoch 4335, Train_Loss: 4704.9716796875, Val_Loss: 4961.16162109375\n",
      "Epoch 4336, Train_Loss: 4704.783203125, Val_Loss: 4960.9677734375\n",
      "Epoch 4337, Train_Loss: 4704.5771484375, Val_Loss: 4960.75048828125\n",
      "Epoch 4338, Train_Loss: 4704.4033203125, Val_Loss: 4960.54150390625\n",
      "Epoch 4339, Train_Loss: 4704.21533203125, Val_Loss: 4960.33203125\n",
      "Epoch 4340, Train_Loss: 4704.056640625, Val_Loss: 4960.10205078125\n",
      "Epoch 4341, Train_Loss: 4703.8759765625, Val_Loss: 4959.88623046875\n",
      "Epoch 4342, Train_Loss: 4703.693359375, Val_Loss: 4959.66259765625\n",
      "Epoch 4343, Train_Loss: 4703.50634765625, Val_Loss: 4959.4599609375\n",
      "Epoch 4344, Train_Loss: 4703.3369140625, Val_Loss: 4959.2451171875\n",
      "Epoch 4345, Train_Loss: 4703.1552734375, Val_Loss: 4959.03662109375\n",
      "Epoch 4346, Train_Loss: 4702.97900390625, Val_Loss: 4958.8095703125\n",
      "Epoch 4347, Train_Loss: 4702.77880859375, Val_Loss: 4958.59619140625\n",
      "Epoch 4348, Train_Loss: 4702.60400390625, Val_Loss: 4958.388671875\n",
      "Epoch 4349, Train_Loss: 4702.4208984375, Val_Loss: 4958.1845703125\n",
      "Epoch 4350, Train_Loss: 4702.18994140625, Val_Loss: 4957.98291015625\n",
      "Epoch 4351, Train_Loss: 4702.00341796875, Val_Loss: 4957.759765625\n",
      "Epoch 4352, Train_Loss: 4701.79443359375, Val_Loss: 4957.51171875\n",
      "Epoch 4353, Train_Loss: 4701.60693359375, Val_Loss: 4957.29345703125\n",
      "Epoch 4354, Train_Loss: 4701.44140625, Val_Loss: 4957.08251953125\n",
      "Epoch 4355, Train_Loss: 4701.25927734375, Val_Loss: 4956.8662109375\n",
      "Epoch 4356, Train_Loss: 4701.064453125, Val_Loss: 4956.66162109375\n",
      "Epoch 4357, Train_Loss: 4700.87158203125, Val_Loss: 4956.4482421875\n",
      "Epoch 4358, Train_Loss: 4700.69189453125, Val_Loss: 4956.2421875\n",
      "Epoch 4359, Train_Loss: 4700.51611328125, Val_Loss: 4956.03369140625\n",
      "Epoch 4360, Train_Loss: 4700.330078125, Val_Loss: 4955.79736328125\n",
      "Epoch 4361, Train_Loss: 4700.12109375, Val_Loss: 4955.59619140625\n",
      "Epoch 4362, Train_Loss: 4699.91943359375, Val_Loss: 4955.3857421875\n",
      "Epoch 4363, Train_Loss: 4699.69384765625, Val_Loss: 4955.18359375\n",
      "Epoch 4364, Train_Loss: 4699.51611328125, Val_Loss: 4954.9697265625\n",
      "Epoch 4365, Train_Loss: 4699.3427734375, Val_Loss: 4954.7666015625\n",
      "Epoch 4366, Train_Loss: 4699.18603515625, Val_Loss: 4954.54296875\n",
      "Epoch 4367, Train_Loss: 4698.9931640625, Val_Loss: 4954.3193359375\n",
      "Epoch 4368, Train_Loss: 4698.81494140625, Val_Loss: 4954.11181640625\n",
      "Epoch 4369, Train_Loss: 4698.638671875, Val_Loss: 4953.88916015625\n",
      "Epoch 4370, Train_Loss: 4698.4560546875, Val_Loss: 4953.69091796875\n",
      "Epoch 4371, Train_Loss: 4698.154296875, Val_Loss: 4953.4765625\n",
      "Epoch 4372, Train_Loss: 4697.96240234375, Val_Loss: 4953.267578125\n",
      "Epoch 4373, Train_Loss: 4697.7744140625, Val_Loss: 4953.0380859375\n",
      "Epoch 4374, Train_Loss: 4697.59912109375, Val_Loss: 4952.83642578125\n",
      "Epoch 4375, Train_Loss: 4697.41845703125, Val_Loss: 4952.6376953125\n",
      "Epoch 4376, Train_Loss: 4697.22509765625, Val_Loss: 4952.4306640625\n",
      "Epoch 4377, Train_Loss: 4697.01904296875, Val_Loss: 4952.20458984375\n",
      "Epoch 4378, Train_Loss: 4696.84912109375, Val_Loss: 4951.99365234375\n",
      "Epoch 4379, Train_Loss: 4696.67333984375, Val_Loss: 4951.77734375\n",
      "Epoch 4380, Train_Loss: 4696.48486328125, Val_Loss: 4951.56005859375\n",
      "Epoch 4381, Train_Loss: 4696.298828125, Val_Loss: 4951.345703125\n",
      "Epoch 4382, Train_Loss: 4696.1201171875, Val_Loss: 4951.13134765625\n",
      "Epoch 4383, Train_Loss: 4695.9423828125, Val_Loss: 4950.92236328125\n",
      "Epoch 4384, Train_Loss: 4695.75537109375, Val_Loss: 4950.7109375\n",
      "Epoch 4385, Train_Loss: 4695.5869140625, Val_Loss: 4950.5048828125\n",
      "Epoch 4386, Train_Loss: 4695.421875, Val_Loss: 4950.2890625\n",
      "Epoch 4387, Train_Loss: 4695.24267578125, Val_Loss: 4950.07421875\n",
      "Epoch 4388, Train_Loss: 4695.05224609375, Val_Loss: 4949.85595703125\n",
      "Epoch 4389, Train_Loss: 4694.79296875, Val_Loss: 4949.63623046875\n",
      "Epoch 4390, Train_Loss: 4694.619140625, Val_Loss: 4949.423828125\n",
      "Epoch 4391, Train_Loss: 4694.42822265625, Val_Loss: 4949.208984375\n",
      "Epoch 4392, Train_Loss: 4694.2021484375, Val_Loss: 4948.98876953125\n",
      "Epoch 4393, Train_Loss: 4694.06005859375, Val_Loss: 4948.77880859375\n",
      "Epoch 4394, Train_Loss: 4693.8828125, Val_Loss: 4948.56982421875\n",
      "Epoch 4395, Train_Loss: 4693.697265625, Val_Loss: 4948.3515625\n",
      "Epoch 4396, Train_Loss: 4693.52880859375, Val_Loss: 4948.158203125\n",
      "Epoch 4397, Train_Loss: 4693.32763671875, Val_Loss: 4947.95166015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4398, Train_Loss: 4693.1474609375, Val_Loss: 4947.73779296875\n",
      "Epoch 4399, Train_Loss: 4692.94775390625, Val_Loss: 4947.50390625\n",
      "Epoch 4400, Train_Loss: 4692.779296875, Val_Loss: 4947.29150390625\n",
      "Epoch 4401, Train_Loss: 4692.59716796875, Val_Loss: 4947.0849609375\n",
      "Epoch 4402, Train_Loss: 4692.40234375, Val_Loss: 4946.87939453125\n",
      "Epoch 4403, Train_Loss: 4692.1591796875, Val_Loss: 4946.6767578125\n",
      "Epoch 4404, Train_Loss: 4691.9833984375, Val_Loss: 4946.46728515625\n",
      "Epoch 4405, Train_Loss: 4691.8173828125, Val_Loss: 4946.25341796875\n",
      "Epoch 4406, Train_Loss: 4691.64013671875, Val_Loss: 4946.02099609375\n",
      "Epoch 4407, Train_Loss: 4691.44921875, Val_Loss: 4945.8154296875\n",
      "Epoch 4408, Train_Loss: 4691.25732421875, Val_Loss: 4945.607421875\n",
      "Epoch 4409, Train_Loss: 4691.07275390625, Val_Loss: 4945.40234375\n",
      "Epoch 4410, Train_Loss: 4690.89013671875, Val_Loss: 4945.1884765625\n",
      "Epoch 4411, Train_Loss: 4690.70458984375, Val_Loss: 4944.98828125\n",
      "Epoch 4412, Train_Loss: 4690.50537109375, Val_Loss: 4944.77490234375\n",
      "Epoch 4413, Train_Loss: 4690.32958984375, Val_Loss: 4944.5625\n",
      "Epoch 4414, Train_Loss: 4690.14794921875, Val_Loss: 4944.341796875\n",
      "Epoch 4415, Train_Loss: 4689.97314453125, Val_Loss: 4944.140625\n",
      "Epoch 4416, Train_Loss: 4689.75830078125, Val_Loss: 4943.9384765625\n",
      "Epoch 4417, Train_Loss: 4689.560546875, Val_Loss: 4943.708984375\n",
      "Epoch 4418, Train_Loss: 4689.38037109375, Val_Loss: 4943.5\n",
      "Epoch 4419, Train_Loss: 4689.18994140625, Val_Loss: 4943.27978515625\n",
      "Epoch 4420, Train_Loss: 4689.013671875, Val_Loss: 4943.0654296875\n",
      "Epoch 4421, Train_Loss: 4688.82568359375, Val_Loss: 4942.84619140625\n",
      "Epoch 4422, Train_Loss: 4688.61865234375, Val_Loss: 4942.66064453125\n",
      "Epoch 4423, Train_Loss: 4688.4609375, Val_Loss: 4942.45068359375\n",
      "Epoch 4424, Train_Loss: 4688.2880859375, Val_Loss: 4942.22607421875\n",
      "Epoch 4425, Train_Loss: 4688.10595703125, Val_Loss: 4942.03125\n",
      "Epoch 4426, Train_Loss: 4687.92578125, Val_Loss: 4941.80322265625\n",
      "Epoch 4427, Train_Loss: 4687.744140625, Val_Loss: 4941.5576171875\n",
      "Epoch 4428, Train_Loss: 4687.564453125, Val_Loss: 4941.34521484375\n",
      "Epoch 4429, Train_Loss: 4687.34326171875, Val_Loss: 4941.15087890625\n",
      "Epoch 4430, Train_Loss: 4687.1650390625, Val_Loss: 4940.94873046875\n",
      "Epoch 4431, Train_Loss: 4686.99267578125, Val_Loss: 4940.7294921875\n",
      "Epoch 4432, Train_Loss: 4686.8017578125, Val_Loss: 4940.5048828125\n",
      "Epoch 4433, Train_Loss: 4686.6337890625, Val_Loss: 4940.2958984375\n",
      "Epoch 4434, Train_Loss: 4686.4462890625, Val_Loss: 4940.09814453125\n",
      "Epoch 4435, Train_Loss: 4686.26171875, Val_Loss: 4939.8837890625\n",
      "Epoch 4436, Train_Loss: 4686.0673828125, Val_Loss: 4939.67919921875\n",
      "Epoch 4437, Train_Loss: 4685.87255859375, Val_Loss: 4939.46337890625\n",
      "Epoch 4438, Train_Loss: 4685.70947265625, Val_Loss: 4939.2578125\n",
      "Epoch 4439, Train_Loss: 4685.5234375, Val_Loss: 4939.03857421875\n",
      "Epoch 4440, Train_Loss: 4685.33251953125, Val_Loss: 4938.83837890625\n",
      "Epoch 4441, Train_Loss: 4685.13232421875, Val_Loss: 4938.62890625\n",
      "Epoch 4442, Train_Loss: 4684.89697265625, Val_Loss: 4938.42138671875\n",
      "Epoch 4443, Train_Loss: 4684.7197265625, Val_Loss: 4938.208984375\n",
      "Epoch 4444, Train_Loss: 4684.560546875, Val_Loss: 4937.998046875\n",
      "Epoch 4445, Train_Loss: 4684.2939453125, Val_Loss: 4937.7900390625\n",
      "Epoch 4446, Train_Loss: 4684.14453125, Val_Loss: 4937.56689453125\n",
      "Epoch 4447, Train_Loss: 4683.95263671875, Val_Loss: 4937.35302734375\n",
      "Epoch 4448, Train_Loss: 4683.7744140625, Val_Loss: 4937.140625\n",
      "Epoch 4449, Train_Loss: 4683.58740234375, Val_Loss: 4936.95458984375\n",
      "Epoch 4450, Train_Loss: 4683.41748046875, Val_Loss: 4936.74609375\n",
      "Epoch 4451, Train_Loss: 4683.2392578125, Val_Loss: 4936.52099609375\n",
      "Epoch 4452, Train_Loss: 4683.03955078125, Val_Loss: 4936.306640625\n",
      "Epoch 4453, Train_Loss: 4682.8623046875, Val_Loss: 4936.10595703125\n",
      "Epoch 4454, Train_Loss: 4682.6630859375, Val_Loss: 4935.900390625\n",
      "Epoch 4455, Train_Loss: 4682.4248046875, Val_Loss: 4935.68017578125\n",
      "Epoch 4456, Train_Loss: 4682.25146484375, Val_Loss: 4935.46240234375\n",
      "Epoch 4457, Train_Loss: 4682.05615234375, Val_Loss: 4935.26171875\n",
      "Epoch 4458, Train_Loss: 4681.86767578125, Val_Loss: 4935.05810546875\n",
      "Epoch 4459, Train_Loss: 4681.70556640625, Val_Loss: 4934.8330078125\n",
      "Epoch 4460, Train_Loss: 4681.54443359375, Val_Loss: 4934.62060546875\n",
      "Epoch 4461, Train_Loss: 4681.36279296875, Val_Loss: 4934.408203125\n",
      "Epoch 4462, Train_Loss: 4681.18115234375, Val_Loss: 4934.2080078125\n",
      "Epoch 4463, Train_Loss: 4681.0029296875, Val_Loss: 4934.00341796875\n",
      "Epoch 4464, Train_Loss: 4680.81982421875, Val_Loss: 4933.76220703125\n",
      "Epoch 4465, Train_Loss: 4680.6328125, Val_Loss: 4933.541015625\n",
      "Epoch 4466, Train_Loss: 4680.46044921875, Val_Loss: 4933.3330078125\n",
      "Epoch 4467, Train_Loss: 4680.2666015625, Val_Loss: 4933.130859375\n",
      "Epoch 4468, Train_Loss: 4680.09619140625, Val_Loss: 4932.91650390625\n",
      "Epoch 4469, Train_Loss: 4679.8896484375, Val_Loss: 4932.7109375\n",
      "Epoch 4470, Train_Loss: 4679.71435546875, Val_Loss: 4932.5009765625\n",
      "Epoch 4471, Train_Loss: 4679.5390625, Val_Loss: 4932.2841796875\n",
      "Epoch 4472, Train_Loss: 4679.3818359375, Val_Loss: 4932.078125\n",
      "Epoch 4473, Train_Loss: 4679.20068359375, Val_Loss: 4931.87158203125\n",
      "Epoch 4474, Train_Loss: 4679.01953125, Val_Loss: 4931.66259765625\n",
      "Epoch 4475, Train_Loss: 4678.84130859375, Val_Loss: 4931.46044921875\n",
      "Epoch 4476, Train_Loss: 4678.6640625, Val_Loss: 4931.24658203125\n",
      "Epoch 4477, Train_Loss: 4678.46875, Val_Loss: 4931.04443359375\n",
      "Epoch 4478, Train_Loss: 4678.287109375, Val_Loss: 4930.82861328125\n",
      "Epoch 4479, Train_Loss: 4678.10009765625, Val_Loss: 4930.6162109375\n",
      "Epoch 4480, Train_Loss: 4677.93408203125, Val_Loss: 4930.4111328125\n",
      "Epoch 4481, Train_Loss: 4677.75439453125, Val_Loss: 4930.203125\n",
      "Epoch 4482, Train_Loss: 4677.54345703125, Val_Loss: 4930.00390625\n",
      "Epoch 4483, Train_Loss: 4677.3427734375, Val_Loss: 4929.783203125\n",
      "Epoch 4484, Train_Loss: 4677.17919921875, Val_Loss: 4929.57568359375\n",
      "Epoch 4485, Train_Loss: 4677.01904296875, Val_Loss: 4929.37158203125\n",
      "Epoch 4486, Train_Loss: 4676.84716796875, Val_Loss: 4929.1484375\n",
      "Epoch 4487, Train_Loss: 4676.65087890625, Val_Loss: 4928.94921875\n",
      "Epoch 4488, Train_Loss: 4676.482421875, Val_Loss: 4928.7412109375\n",
      "Epoch 4489, Train_Loss: 4676.29052734375, Val_Loss: 4928.5419921875\n",
      "Epoch 4490, Train_Loss: 4676.11083984375, Val_Loss: 4928.330078125\n",
      "Epoch 4491, Train_Loss: 4675.93603515625, Val_Loss: 4928.12646484375\n",
      "Epoch 4492, Train_Loss: 4675.73681640625, Val_Loss: 4927.890625\n",
      "Epoch 4493, Train_Loss: 4675.55810546875, Val_Loss: 4927.6845703125\n",
      "Epoch 4494, Train_Loss: 4675.38134765625, Val_Loss: 4927.4755859375\n",
      "Epoch 4495, Train_Loss: 4675.15478515625, Val_Loss: 4927.28369140625\n",
      "Epoch 4496, Train_Loss: 4674.974609375, Val_Loss: 4927.08056640625\n",
      "Epoch 4497, Train_Loss: 4674.8017578125, Val_Loss: 4926.87451171875\n",
      "Epoch 4498, Train_Loss: 4674.61279296875, Val_Loss: 4926.65185546875\n",
      "Epoch 4499, Train_Loss: 4674.47265625, Val_Loss: 4926.43798828125\n",
      "Epoch 4500, Train_Loss: 4674.287109375, Val_Loss: 4926.22802734375\n",
      "Epoch 4501, Train_Loss: 4674.10107421875, Val_Loss: 4925.98583984375\n",
      "Epoch 4502, Train_Loss: 4673.91748046875, Val_Loss: 4925.77880859375\n",
      "Epoch 4503, Train_Loss: 4673.74609375, Val_Loss: 4925.576171875\n",
      "Epoch 4504, Train_Loss: 4673.5732421875, Val_Loss: 4925.3798828125\n",
      "Epoch 4505, Train_Loss: 4673.392578125, Val_Loss: 4925.154296875\n",
      "Epoch 4506, Train_Loss: 4673.21923828125, Val_Loss: 4924.94873046875\n",
      "Epoch 4507, Train_Loss: 4673.013671875, Val_Loss: 4924.74072265625\n",
      "Epoch 4508, Train_Loss: 4672.79052734375, Val_Loss: 4924.5419921875\n",
      "Epoch 4509, Train_Loss: 4672.6142578125, Val_Loss: 4924.33740234375\n",
      "Epoch 4510, Train_Loss: 4672.45458984375, Val_Loss: 4924.125\n",
      "Epoch 4511, Train_Loss: 4672.2451171875, Val_Loss: 4923.92724609375\n",
      "Epoch 4512, Train_Loss: 4672.0556640625, Val_Loss: 4923.69677734375\n",
      "Epoch 4513, Train_Loss: 4671.892578125, Val_Loss: 4923.490234375\n",
      "Epoch 4514, Train_Loss: 4671.7236328125, Val_Loss: 4923.29443359375\n",
      "Epoch 4515, Train_Loss: 4671.5263671875, Val_Loss: 4923.09228515625\n",
      "Epoch 4516, Train_Loss: 4671.35693359375, Val_Loss: 4922.87255859375\n",
      "Epoch 4517, Train_Loss: 4671.1552734375, Val_Loss: 4922.666015625\n",
      "Epoch 4518, Train_Loss: 4670.97802734375, Val_Loss: 4922.4521484375\n",
      "Epoch 4519, Train_Loss: 4670.80859375, Val_Loss: 4922.24072265625\n",
      "Epoch 4520, Train_Loss: 4670.45703125, Val_Loss: 4922.033203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4521, Train_Loss: 4670.279296875, Val_Loss: 4921.8330078125\n",
      "Epoch 4522, Train_Loss: 4670.04638671875, Val_Loss: 4921.63330078125\n",
      "Epoch 4523, Train_Loss: 4669.85009765625, Val_Loss: 4921.4169921875\n",
      "Epoch 4524, Train_Loss: 4669.66455078125, Val_Loss: 4921.22412109375\n",
      "Epoch 4525, Train_Loss: 4669.4755859375, Val_Loss: 4921.0068359375\n",
      "Epoch 4526, Train_Loss: 4669.30908203125, Val_Loss: 4920.78857421875\n",
      "Epoch 4527, Train_Loss: 4669.1083984375, Val_Loss: 4920.5859375\n",
      "Epoch 4528, Train_Loss: 4668.927734375, Val_Loss: 4920.39208984375\n",
      "Epoch 4529, Train_Loss: 4668.74267578125, Val_Loss: 4920.16748046875\n",
      "Epoch 4530, Train_Loss: 4668.5791015625, Val_Loss: 4919.9765625\n",
      "Epoch 4531, Train_Loss: 4668.41943359375, Val_Loss: 4919.7587890625\n",
      "Epoch 4532, Train_Loss: 4668.2255859375, Val_Loss: 4919.541015625\n",
      "Epoch 4533, Train_Loss: 4668.03955078125, Val_Loss: 4919.32958984375\n",
      "Epoch 4534, Train_Loss: 4667.8671875, Val_Loss: 4919.1181640625\n",
      "Epoch 4535, Train_Loss: 4667.64990234375, Val_Loss: 4918.923828125\n",
      "Epoch 4536, Train_Loss: 4667.4775390625, Val_Loss: 4918.73095703125\n",
      "Epoch 4537, Train_Loss: 4667.2939453125, Val_Loss: 4918.5166015625\n",
      "Epoch 4538, Train_Loss: 4667.1279296875, Val_Loss: 4918.29296875\n",
      "Epoch 4539, Train_Loss: 4666.92431640625, Val_Loss: 4918.0595703125\n",
      "Epoch 4540, Train_Loss: 4666.76025390625, Val_Loss: 4917.8505859375\n",
      "Epoch 4541, Train_Loss: 4666.58056640625, Val_Loss: 4917.64501953125\n",
      "Epoch 4542, Train_Loss: 4666.38623046875, Val_Loss: 4917.43359375\n",
      "Epoch 4543, Train_Loss: 4666.20556640625, Val_Loss: 4917.23291015625\n",
      "Epoch 4544, Train_Loss: 4666.04248046875, Val_Loss: 4917.03271484375\n",
      "Epoch 4545, Train_Loss: 4665.86474609375, Val_Loss: 4916.81103515625\n",
      "Epoch 4546, Train_Loss: 4665.68701171875, Val_Loss: 4916.6103515625\n",
      "Epoch 4547, Train_Loss: 4665.49267578125, Val_Loss: 4916.40478515625\n",
      "Epoch 4548, Train_Loss: 4665.27587890625, Val_Loss: 4916.20166015625\n",
      "Epoch 4549, Train_Loss: 4665.11083984375, Val_Loss: 4915.98828125\n",
      "Epoch 4550, Train_Loss: 4664.9375, Val_Loss: 4915.7861328125\n",
      "Epoch 4551, Train_Loss: 4664.7861328125, Val_Loss: 4915.57177734375\n",
      "Epoch 4552, Train_Loss: 4664.58740234375, Val_Loss: 4915.359375\n",
      "Epoch 4553, Train_Loss: 4664.41015625, Val_Loss: 4915.14794921875\n",
      "Epoch 4554, Train_Loss: 4664.22705078125, Val_Loss: 4914.94189453125\n",
      "Epoch 4555, Train_Loss: 4664.060546875, Val_Loss: 4914.755859375\n",
      "Epoch 4556, Train_Loss: 4663.88330078125, Val_Loss: 4914.54296875\n",
      "Epoch 4557, Train_Loss: 4663.689453125, Val_Loss: 4914.32177734375\n",
      "Epoch 4558, Train_Loss: 4663.4970703125, Val_Loss: 4914.10498046875\n",
      "Epoch 4559, Train_Loss: 4663.33349609375, Val_Loss: 4913.90380859375\n",
      "Epoch 4560, Train_Loss: 4663.15185546875, Val_Loss: 4913.70263671875\n",
      "Epoch 4561, Train_Loss: 4662.92138671875, Val_Loss: 4913.50341796875\n",
      "Epoch 4562, Train_Loss: 4662.72314453125, Val_Loss: 4913.2978515625\n",
      "Epoch 4563, Train_Loss: 4662.54296875, Val_Loss: 4913.08984375\n",
      "Epoch 4564, Train_Loss: 4662.37451171875, Val_Loss: 4912.87841796875\n",
      "Epoch 4565, Train_Loss: 4662.19921875, Val_Loss: 4912.65283203125\n",
      "Epoch 4566, Train_Loss: 4662.03125, Val_Loss: 4912.45849609375\n",
      "Epoch 4567, Train_Loss: 4661.833984375, Val_Loss: 4912.2314453125\n",
      "Epoch 4568, Train_Loss: 4661.65087890625, Val_Loss: 4912.03662109375\n",
      "Epoch 4569, Train_Loss: 4661.47412109375, Val_Loss: 4911.8349609375\n",
      "Epoch 4570, Train_Loss: 4661.2939453125, Val_Loss: 4911.625\n",
      "Epoch 4571, Train_Loss: 4661.119140625, Val_Loss: 4911.40576171875\n",
      "Epoch 4572, Train_Loss: 4660.93115234375, Val_Loss: 4911.2001953125\n",
      "Epoch 4573, Train_Loss: 4660.75341796875, Val_Loss: 4911.00537109375\n",
      "Epoch 4574, Train_Loss: 4660.58740234375, Val_Loss: 4910.796875\n",
      "Epoch 4575, Train_Loss: 4660.373046875, Val_Loss: 4910.59423828125\n",
      "Epoch 4576, Train_Loss: 4660.17431640625, Val_Loss: 4910.35888671875\n",
      "Epoch 4577, Train_Loss: 4659.9921875, Val_Loss: 4910.13818359375\n",
      "Epoch 4578, Train_Loss: 4659.86083984375, Val_Loss: 4909.92578125\n",
      "Epoch 4579, Train_Loss: 4659.6953125, Val_Loss: 4909.72607421875\n",
      "Epoch 4580, Train_Loss: 4659.5263671875, Val_Loss: 4909.52978515625\n",
      "Epoch 4581, Train_Loss: 4659.35498046875, Val_Loss: 4909.32080078125\n",
      "Epoch 4582, Train_Loss: 4659.16796875, Val_Loss: 4909.1162109375\n",
      "Epoch 4583, Train_Loss: 4658.9765625, Val_Loss: 4908.90283203125\n",
      "Epoch 4584, Train_Loss: 4658.806640625, Val_Loss: 4908.71337890625\n",
      "Epoch 4585, Train_Loss: 4658.54638671875, Val_Loss: 4908.4873046875\n",
      "Epoch 4586, Train_Loss: 4658.3720703125, Val_Loss: 4908.28564453125\n",
      "Epoch 4587, Train_Loss: 4658.1826171875, Val_Loss: 4908.0849609375\n",
      "Epoch 4588, Train_Loss: 4657.9921875, Val_Loss: 4907.8837890625\n",
      "Epoch 4589, Train_Loss: 4657.822265625, Val_Loss: 4907.67724609375\n",
      "Epoch 4590, Train_Loss: 4657.6513671875, Val_Loss: 4907.470703125\n",
      "Epoch 4591, Train_Loss: 4657.47314453125, Val_Loss: 4907.24658203125\n",
      "Epoch 4592, Train_Loss: 4657.26611328125, Val_Loss: 4907.04833984375\n",
      "Epoch 4593, Train_Loss: 4657.0888671875, Val_Loss: 4906.83984375\n",
      "Epoch 4594, Train_Loss: 4656.9208984375, Val_Loss: 4906.64404296875\n",
      "Epoch 4595, Train_Loss: 4656.6982421875, Val_Loss: 4906.44775390625\n",
      "Epoch 4596, Train_Loss: 4656.5078125, Val_Loss: 4906.21826171875\n",
      "Epoch 4597, Train_Loss: 4656.337890625, Val_Loss: 4906.01708984375\n",
      "Epoch 4598, Train_Loss: 4656.1728515625, Val_Loss: 4905.79052734375\n",
      "Epoch 4599, Train_Loss: 4655.99609375, Val_Loss: 4905.59814453125\n",
      "Epoch 4600, Train_Loss: 4655.83154296875, Val_Loss: 4905.396484375\n",
      "Epoch 4601, Train_Loss: 4655.60498046875, Val_Loss: 4905.189453125\n",
      "Epoch 4602, Train_Loss: 4655.40576171875, Val_Loss: 4904.98486328125\n",
      "Epoch 4603, Train_Loss: 4655.2294921875, Val_Loss: 4904.783203125\n",
      "Epoch 4604, Train_Loss: 4655.08056640625, Val_Loss: 4904.55078125\n",
      "Epoch 4605, Train_Loss: 4654.91650390625, Val_Loss: 4904.34521484375\n",
      "Epoch 4606, Train_Loss: 4654.75, Val_Loss: 4904.1494140625\n",
      "Epoch 4607, Train_Loss: 4654.5595703125, Val_Loss: 4903.93896484375\n",
      "Epoch 4608, Train_Loss: 4654.38134765625, Val_Loss: 4903.74169921875\n",
      "Epoch 4609, Train_Loss: 4654.20458984375, Val_Loss: 4903.5341796875\n",
      "Epoch 4610, Train_Loss: 4654.0244140625, Val_Loss: 4903.3388671875\n",
      "Epoch 4611, Train_Loss: 4653.86181640625, Val_Loss: 4903.11181640625\n",
      "Epoch 4612, Train_Loss: 4653.67333984375, Val_Loss: 4902.89501953125\n",
      "Epoch 4613, Train_Loss: 4653.4921875, Val_Loss: 4902.66943359375\n",
      "Epoch 4614, Train_Loss: 4653.2724609375, Val_Loss: 4902.474609375\n",
      "Epoch 4615, Train_Loss: 4653.087890625, Val_Loss: 4902.26513671875\n",
      "Epoch 4616, Train_Loss: 4652.92431640625, Val_Loss: 4902.056640625\n",
      "Epoch 4617, Train_Loss: 4652.74609375, Val_Loss: 4901.85498046875\n",
      "Epoch 4618, Train_Loss: 4652.56884765625, Val_Loss: 4901.6318359375\n",
      "Epoch 4619, Train_Loss: 4652.380859375, Val_Loss: 4901.42822265625\n",
      "Epoch 4620, Train_Loss: 4652.19384765625, Val_Loss: 4901.22607421875\n",
      "Epoch 4621, Train_Loss: 4652.0048828125, Val_Loss: 4901.02734375\n",
      "Epoch 4622, Train_Loss: 4651.8291015625, Val_Loss: 4900.8330078125\n",
      "Epoch 4623, Train_Loss: 4651.64599609375, Val_Loss: 4900.62060546875\n",
      "Epoch 4624, Train_Loss: 4651.48046875, Val_Loss: 4900.40478515625\n",
      "Epoch 4625, Train_Loss: 4651.30224609375, Val_Loss: 4900.19970703125\n",
      "Epoch 4626, Train_Loss: 4651.12890625, Val_Loss: 4899.98388671875\n",
      "Epoch 4627, Train_Loss: 4650.9453125, Val_Loss: 4899.77978515625\n",
      "Epoch 4628, Train_Loss: 4650.73828125, Val_Loss: 4899.580078125\n",
      "Epoch 4629, Train_Loss: 4650.56494140625, Val_Loss: 4899.388671875\n",
      "Epoch 4630, Train_Loss: 4650.3984375, Val_Loss: 4899.18017578125\n",
      "Epoch 4631, Train_Loss: 4650.216796875, Val_Loss: 4898.9619140625\n",
      "Epoch 4632, Train_Loss: 4650.017578125, Val_Loss: 4898.7451171875\n",
      "Epoch 4633, Train_Loss: 4649.8369140625, Val_Loss: 4898.5517578125\n",
      "Epoch 4634, Train_Loss: 4649.662109375, Val_Loss: 4898.34375\n",
      "Epoch 4635, Train_Loss: 4649.4970703125, Val_Loss: 4898.1552734375\n",
      "Epoch 4636, Train_Loss: 4649.31396484375, Val_Loss: 4897.94140625\n",
      "Epoch 4637, Train_Loss: 4649.1396484375, Val_Loss: 4897.7451171875\n",
      "Epoch 4638, Train_Loss: 4648.97021484375, Val_Loss: 4897.5234375\n",
      "Epoch 4639, Train_Loss: 4648.77880859375, Val_Loss: 4897.3125\n",
      "Epoch 4640, Train_Loss: 4648.59912109375, Val_Loss: 4897.10986328125\n",
      "Epoch 4641, Train_Loss: 4648.36669921875, Val_Loss: 4896.89990234375\n",
      "Epoch 4642, Train_Loss: 4648.1796875, Val_Loss: 4896.6943359375\n",
      "Epoch 4643, Train_Loss: 4648.00830078125, Val_Loss: 4896.4892578125\n",
      "Epoch 4644, Train_Loss: 4647.8359375, Val_Loss: 4896.27880859375\n",
      "Epoch 4645, Train_Loss: 4647.6728515625, Val_Loss: 4896.076171875\n",
      "Epoch 4646, Train_Loss: 4647.49462890625, Val_Loss: 4895.86083984375\n",
      "Epoch 4647, Train_Loss: 4647.30615234375, Val_Loss: 4895.67724609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4648, Train_Loss: 4647.13330078125, Val_Loss: 4895.4609375\n",
      "Epoch 4649, Train_Loss: 4646.97119140625, Val_Loss: 4895.2666015625\n",
      "Epoch 4650, Train_Loss: 4646.78515625, Val_Loss: 4895.0693359375\n",
      "Epoch 4651, Train_Loss: 4646.60595703125, Val_Loss: 4894.8134765625\n",
      "Epoch 4652, Train_Loss: 4646.419921875, Val_Loss: 4894.6103515625\n",
      "Epoch 4653, Train_Loss: 4646.2392578125, Val_Loss: 4894.4033203125\n",
      "Epoch 4654, Train_Loss: 4646.0361328125, Val_Loss: 4894.19775390625\n",
      "Epoch 4655, Train_Loss: 4645.86865234375, Val_Loss: 4894.0107421875\n",
      "Epoch 4656, Train_Loss: 4645.69677734375, Val_Loss: 4893.79296875\n",
      "Epoch 4657, Train_Loss: 4645.548828125, Val_Loss: 4893.57470703125\n",
      "Epoch 4658, Train_Loss: 4645.36572265625, Val_Loss: 4893.3720703125\n",
      "Epoch 4659, Train_Loss: 4645.17919921875, Val_Loss: 4893.1689453125\n",
      "Epoch 4660, Train_Loss: 4644.982421875, Val_Loss: 4892.96728515625\n",
      "Epoch 4661, Train_Loss: 4644.8154296875, Val_Loss: 4892.76904296875\n",
      "Epoch 4662, Train_Loss: 4644.64111328125, Val_Loss: 4892.56396484375\n",
      "Epoch 4663, Train_Loss: 4644.46240234375, Val_Loss: 4892.36376953125\n",
      "Epoch 4664, Train_Loss: 4644.294921875, Val_Loss: 4892.13818359375\n",
      "Epoch 4665, Train_Loss: 4644.11767578125, Val_Loss: 4891.94921875\n",
      "Epoch 4666, Train_Loss: 4643.9453125, Val_Loss: 4891.74658203125\n",
      "Epoch 4667, Train_Loss: 4643.716796875, Val_Loss: 4891.54736328125\n",
      "Epoch 4668, Train_Loss: 4643.55908203125, Val_Loss: 4891.34375\n",
      "Epoch 4669, Train_Loss: 4643.2412109375, Val_Loss: 4891.13818359375\n",
      "Epoch 4670, Train_Loss: 4643.06640625, Val_Loss: 4890.9306640625\n",
      "Epoch 4671, Train_Loss: 4642.89599609375, Val_Loss: 4890.70849609375\n",
      "Epoch 4672, Train_Loss: 4642.69873046875, Val_Loss: 4890.51025390625\n",
      "Epoch 4673, Train_Loss: 4642.52392578125, Val_Loss: 4890.302734375\n",
      "Epoch 4674, Train_Loss: 4642.35400390625, Val_Loss: 4890.1162109375\n",
      "Epoch 4675, Train_Loss: 4642.1728515625, Val_Loss: 4889.9208984375\n",
      "Epoch 4676, Train_Loss: 4641.99560546875, Val_Loss: 4889.71142578125\n",
      "Epoch 4677, Train_Loss: 4641.8115234375, Val_Loss: 4889.49658203125\n",
      "Epoch 4678, Train_Loss: 4641.64013671875, Val_Loss: 4889.2998046875\n",
      "Epoch 4679, Train_Loss: 4641.46875, Val_Loss: 4889.083984375\n",
      "Epoch 4680, Train_Loss: 4641.29541015625, Val_Loss: 4888.8876953125\n",
      "Epoch 4681, Train_Loss: 4641.08984375, Val_Loss: 4888.68115234375\n",
      "Epoch 4682, Train_Loss: 4640.8955078125, Val_Loss: 4888.47021484375\n",
      "Epoch 4683, Train_Loss: 4640.69677734375, Val_Loss: 4888.265625\n",
      "Epoch 4684, Train_Loss: 4640.556640625, Val_Loss: 4888.05810546875\n",
      "Epoch 4685, Train_Loss: 4640.3916015625, Val_Loss: 4887.8564453125\n",
      "Epoch 4686, Train_Loss: 4640.2314453125, Val_Loss: 4887.6572265625\n",
      "Epoch 4687, Train_Loss: 4640.04736328125, Val_Loss: 4887.462890625\n",
      "Epoch 4688, Train_Loss: 4639.86181640625, Val_Loss: 4887.21875\n",
      "Epoch 4689, Train_Loss: 4639.69091796875, Val_Loss: 4887.01953125\n",
      "Epoch 4690, Train_Loss: 4639.51708984375, Val_Loss: 4886.7998046875\n",
      "Epoch 4691, Train_Loss: 4639.3349609375, Val_Loss: 4886.60009765625\n",
      "Epoch 4692, Train_Loss: 4639.13916015625, Val_Loss: 4886.39990234375\n",
      "Epoch 4693, Train_Loss: 4638.9609375, Val_Loss: 4886.1787109375\n",
      "Epoch 4694, Train_Loss: 4638.7607421875, Val_Loss: 4886.0029296875\n",
      "Epoch 4695, Train_Loss: 4638.60546875, Val_Loss: 4885.7978515625\n",
      "Epoch 4696, Train_Loss: 4638.43115234375, Val_Loss: 4885.59423828125\n",
      "Epoch 4697, Train_Loss: 4638.2578125, Val_Loss: 4885.37841796875\n",
      "Epoch 4698, Train_Loss: 4638.07958984375, Val_Loss: 4885.18359375\n",
      "Epoch 4699, Train_Loss: 4637.8955078125, Val_Loss: 4884.97119140625\n",
      "Epoch 4700, Train_Loss: 4637.73486328125, Val_Loss: 4884.7822265625\n",
      "Epoch 4701, Train_Loss: 4637.56298828125, Val_Loss: 4884.57861328125\n",
      "Epoch 4702, Train_Loss: 4637.3837890625, Val_Loss: 4884.3779296875\n",
      "Epoch 4703, Train_Loss: 4637.21435546875, Val_Loss: 4884.17138671875\n",
      "Epoch 4704, Train_Loss: 4637.03369140625, Val_Loss: 4883.958984375\n",
      "Epoch 4705, Train_Loss: 4636.8603515625, Val_Loss: 4883.75341796875\n",
      "Epoch 4706, Train_Loss: 4636.69580078125, Val_Loss: 4883.5517578125\n",
      "Epoch 4707, Train_Loss: 4636.447265625, Val_Loss: 4883.3427734375\n",
      "Epoch 4708, Train_Loss: 4636.283203125, Val_Loss: 4883.14794921875\n",
      "Epoch 4709, Train_Loss: 4636.111328125, Val_Loss: 4882.94140625\n",
      "Epoch 4710, Train_Loss: 4635.962890625, Val_Loss: 4882.7236328125\n",
      "Epoch 4711, Train_Loss: 4635.78662109375, Val_Loss: 4882.5224609375\n",
      "Epoch 4712, Train_Loss: 4635.58447265625, Val_Loss: 4882.3310546875\n",
      "Epoch 4713, Train_Loss: 4635.4140625, Val_Loss: 4882.12158203125\n",
      "Epoch 4714, Train_Loss: 4635.24072265625, Val_Loss: 4881.93798828125\n",
      "Epoch 4715, Train_Loss: 4635.06591796875, Val_Loss: 4881.736328125\n",
      "Epoch 4716, Train_Loss: 4634.8876953125, Val_Loss: 4881.52001953125\n",
      "Epoch 4717, Train_Loss: 4634.70166015625, Val_Loss: 4881.29345703125\n",
      "Epoch 4718, Train_Loss: 4634.5341796875, Val_Loss: 4881.09423828125\n",
      "Epoch 4719, Train_Loss: 4634.37744140625, Val_Loss: 4880.89501953125\n",
      "Epoch 4720, Train_Loss: 4634.16748046875, Val_Loss: 4880.7021484375\n",
      "Epoch 4721, Train_Loss: 4633.99658203125, Val_Loss: 4880.4931640625\n",
      "Epoch 4722, Train_Loss: 4633.802734375, Val_Loss: 4880.283203125\n",
      "Epoch 4723, Train_Loss: 4633.6328125, Val_Loss: 4880.08984375\n",
      "Epoch 4724, Train_Loss: 4633.46875, Val_Loss: 4879.8818359375\n",
      "Epoch 4725, Train_Loss: 4633.2998046875, Val_Loss: 4879.64892578125\n",
      "Epoch 4726, Train_Loss: 4633.13330078125, Val_Loss: 4879.44580078125\n",
      "Epoch 4727, Train_Loss: 4632.93017578125, Val_Loss: 4879.25048828125\n",
      "Epoch 4728, Train_Loss: 4632.75048828125, Val_Loss: 4879.04736328125\n",
      "Epoch 4729, Train_Loss: 4632.5771484375, Val_Loss: 4878.84423828125\n",
      "Epoch 4730, Train_Loss: 4632.40625, Val_Loss: 4878.625\n",
      "Epoch 4731, Train_Loss: 4632.2373046875, Val_Loss: 4878.4248046875\n",
      "Epoch 4732, Train_Loss: 4632.05029296875, Val_Loss: 4878.22216796875\n",
      "Epoch 4733, Train_Loss: 4631.8759765625, Val_Loss: 4878.02783203125\n",
      "Epoch 4734, Train_Loss: 4631.671875, Val_Loss: 4877.83984375\n",
      "Epoch 4735, Train_Loss: 4631.45654296875, Val_Loss: 4877.6279296875\n",
      "Epoch 4736, Train_Loss: 4631.2890625, Val_Loss: 4877.423828125\n",
      "Epoch 4737, Train_Loss: 4631.09326171875, Val_Loss: 4877.20751953125\n",
      "Epoch 4738, Train_Loss: 4630.91748046875, Val_Loss: 4877.01220703125\n",
      "Epoch 4739, Train_Loss: 4630.74462890625, Val_Loss: 4876.8193359375\n",
      "Epoch 4740, Train_Loss: 4630.5791015625, Val_Loss: 4876.60986328125\n",
      "Epoch 4741, Train_Loss: 4630.4072265625, Val_Loss: 4876.4130859375\n",
      "Epoch 4742, Train_Loss: 4630.21484375, Val_Loss: 4876.2080078125\n",
      "Epoch 4743, Train_Loss: 4630.05224609375, Val_Loss: 4875.9951171875\n",
      "Epoch 4744, Train_Loss: 4629.810546875, Val_Loss: 4875.79296875\n",
      "Epoch 4745, Train_Loss: 4629.63818359375, Val_Loss: 4875.58984375\n",
      "Epoch 4746, Train_Loss: 4629.45654296875, Val_Loss: 4875.38134765625\n",
      "Epoch 4747, Train_Loss: 4629.232421875, Val_Loss: 4875.18896484375\n",
      "Epoch 4748, Train_Loss: 4629.06005859375, Val_Loss: 4874.97705078125\n",
      "Epoch 4749, Train_Loss: 4628.89453125, Val_Loss: 4874.783203125\n",
      "Epoch 4750, Train_Loss: 4628.72021484375, Val_Loss: 4874.57177734375\n",
      "Epoch 4751, Train_Loss: 4628.54248046875, Val_Loss: 4874.365234375\n",
      "Epoch 4752, Train_Loss: 4628.35107421875, Val_Loss: 4874.17236328125\n",
      "Epoch 4753, Train_Loss: 4628.1708984375, Val_Loss: 4873.962890625\n",
      "Epoch 4754, Train_Loss: 4627.99365234375, Val_Loss: 4873.76220703125\n",
      "Epoch 4755, Train_Loss: 4627.8212890625, Val_Loss: 4873.55859375\n",
      "Epoch 4756, Train_Loss: 4627.6435546875, Val_Loss: 4873.36181640625\n",
      "Epoch 4757, Train_Loss: 4627.47021484375, Val_Loss: 4873.1376953125\n",
      "Epoch 4758, Train_Loss: 4627.30126953125, Val_Loss: 4872.93701171875\n",
      "Epoch 4759, Train_Loss: 4627.13671875, Val_Loss: 4872.74267578125\n",
      "Epoch 4760, Train_Loss: 4626.94384765625, Val_Loss: 4872.56396484375\n",
      "Epoch 4761, Train_Loss: 4626.76611328125, Val_Loss: 4872.36181640625\n",
      "Epoch 4762, Train_Loss: 4626.58642578125, Val_Loss: 4872.14599609375\n",
      "Epoch 4763, Train_Loss: 4626.416015625, Val_Loss: 4871.8916015625\n",
      "Epoch 4764, Train_Loss: 4626.240234375, Val_Loss: 4871.7001953125\n",
      "Epoch 4765, Train_Loss: 4626.0703125, Val_Loss: 4871.49169921875\n",
      "Epoch 4766, Train_Loss: 4625.89599609375, Val_Loss: 4871.30126953125\n",
      "Epoch 4767, Train_Loss: 4625.70947265625, Val_Loss: 4871.107421875\n",
      "Epoch 4768, Train_Loss: 4625.54150390625, Val_Loss: 4870.88671875\n",
      "Epoch 4769, Train_Loss: 4625.3798828125, Val_Loss: 4870.7001953125\n",
      "Epoch 4770, Train_Loss: 4625.203125, Val_Loss: 4870.49560546875\n",
      "Epoch 4771, Train_Loss: 4625.02587890625, Val_Loss: 4870.291015625\n",
      "Epoch 4772, Train_Loss: 4624.830078125, Val_Loss: 4870.0908203125\n",
      "Epoch 4773, Train_Loss: 4624.603515625, Val_Loss: 4869.89501953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4774, Train_Loss: 4624.427734375, Val_Loss: 4869.6953125\n",
      "Epoch 4775, Train_Loss: 4624.26220703125, Val_Loss: 4869.49365234375\n",
      "Epoch 4776, Train_Loss: 4624.09912109375, Val_Loss: 4869.2998046875\n",
      "Epoch 4777, Train_Loss: 4623.91259765625, Val_Loss: 4869.08154296875\n",
      "Epoch 4778, Train_Loss: 4623.74267578125, Val_Loss: 4868.8779296875\n",
      "Epoch 4779, Train_Loss: 4623.56005859375, Val_Loss: 4868.67236328125\n",
      "Epoch 4780, Train_Loss: 4623.3955078125, Val_Loss: 4868.47802734375\n",
      "Epoch 4781, Train_Loss: 4623.21435546875, Val_Loss: 4868.287109375\n",
      "Epoch 4782, Train_Loss: 4623.0419921875, Val_Loss: 4868.07861328125\n",
      "Epoch 4783, Train_Loss: 4622.861328125, Val_Loss: 4867.875\n",
      "Epoch 4784, Train_Loss: 4622.69970703125, Val_Loss: 4867.66943359375\n",
      "Epoch 4785, Train_Loss: 4622.5185546875, Val_Loss: 4867.478515625\n",
      "Epoch 4786, Train_Loss: 4622.31201171875, Val_Loss: 4867.28466796875\n",
      "Epoch 4787, Train_Loss: 4622.126953125, Val_Loss: 4867.064453125\n",
      "Epoch 4788, Train_Loss: 4621.94921875, Val_Loss: 4866.86376953125\n",
      "Epoch 4789, Train_Loss: 4621.78564453125, Val_Loss: 4866.65771484375\n",
      "Epoch 4790, Train_Loss: 4621.64501953125, Val_Loss: 4866.45166015625\n",
      "Epoch 4791, Train_Loss: 4621.48583984375, Val_Loss: 4866.2421875\n",
      "Epoch 4792, Train_Loss: 4621.298828125, Val_Loss: 4866.04443359375\n",
      "Epoch 4793, Train_Loss: 4621.12451171875, Val_Loss: 4865.84375\n",
      "Epoch 4794, Train_Loss: 4620.943359375, Val_Loss: 4865.64697265625\n",
      "Epoch 4795, Train_Loss: 4620.7685546875, Val_Loss: 4865.43896484375\n",
      "Epoch 4796, Train_Loss: 4620.6103515625, Val_Loss: 4865.22802734375\n",
      "Epoch 4797, Train_Loss: 4620.42919921875, Val_Loss: 4865.02685546875\n",
      "Epoch 4798, Train_Loss: 4620.2578125, Val_Loss: 4864.8251953125\n",
      "Epoch 4799, Train_Loss: 4620.07958984375, Val_Loss: 4864.63671875\n",
      "Epoch 4800, Train_Loss: 4619.89111328125, Val_Loss: 4864.41357421875\n",
      "Epoch 4801, Train_Loss: 4619.72900390625, Val_Loss: 4864.2158203125\n",
      "Epoch 4802, Train_Loss: 4619.52783203125, Val_Loss: 4864.00537109375\n",
      "Epoch 4803, Train_Loss: 4619.35791015625, Val_Loss: 4863.796875\n",
      "Epoch 4804, Train_Loss: 4619.1923828125, Val_Loss: 4863.60498046875\n",
      "Epoch 4805, Train_Loss: 4619.0224609375, Val_Loss: 4863.39794921875\n",
      "Epoch 4806, Train_Loss: 4618.85546875, Val_Loss: 4863.19580078125\n",
      "Epoch 4807, Train_Loss: 4618.671875, Val_Loss: 4863.0078125\n",
      "Epoch 4808, Train_Loss: 4618.501953125, Val_Loss: 4862.80224609375\n",
      "Epoch 4809, Train_Loss: 4618.30908203125, Val_Loss: 4862.595703125\n",
      "Epoch 4810, Train_Loss: 4618.12841796875, Val_Loss: 4862.38525390625\n",
      "Epoch 4811, Train_Loss: 4617.95556640625, Val_Loss: 4862.17529296875\n",
      "Epoch 4812, Train_Loss: 4617.76318359375, Val_Loss: 4861.974609375\n",
      "Epoch 4813, Train_Loss: 4617.5400390625, Val_Loss: 4861.79296875\n",
      "Epoch 4814, Train_Loss: 4617.37255859375, Val_Loss: 4861.60595703125\n",
      "Epoch 4815, Train_Loss: 4617.20068359375, Val_Loss: 4861.39404296875\n",
      "Epoch 4816, Train_Loss: 4617.0712890625, Val_Loss: 4861.17724609375\n",
      "Epoch 4817, Train_Loss: 4616.89453125, Val_Loss: 4860.974609375\n",
      "Epoch 4818, Train_Loss: 4616.72509765625, Val_Loss: 4860.77734375\n",
      "Epoch 4819, Train_Loss: 4616.51806640625, Val_Loss: 4860.56591796875\n",
      "Epoch 4820, Train_Loss: 4616.34765625, Val_Loss: 4860.380859375\n",
      "Epoch 4821, Train_Loss: 4616.17578125, Val_Loss: 4860.1787109375\n",
      "Epoch 4822, Train_Loss: 4616.00927734375, Val_Loss: 4859.97021484375\n",
      "Epoch 4823, Train_Loss: 4615.837890625, Val_Loss: 4859.75927734375\n",
      "Epoch 4824, Train_Loss: 4615.6611328125, Val_Loss: 4859.56591796875\n",
      "Epoch 4825, Train_Loss: 4615.49609375, Val_Loss: 4859.37060546875\n",
      "Epoch 4826, Train_Loss: 4615.271484375, Val_Loss: 4859.18017578125\n",
      "Epoch 4827, Train_Loss: 4615.0791015625, Val_Loss: 4858.9892578125\n",
      "Epoch 4828, Train_Loss: 4614.91259765625, Val_Loss: 4858.76611328125\n",
      "Epoch 4829, Train_Loss: 4614.7421875, Val_Loss: 4858.58056640625\n",
      "Epoch 4830, Train_Loss: 4614.578125, Val_Loss: 4858.36279296875\n",
      "Epoch 4831, Train_Loss: 4614.412109375, Val_Loss: 4858.15771484375\n",
      "Epoch 4832, Train_Loss: 4614.21240234375, Val_Loss: 4857.95263671875\n",
      "Epoch 4833, Train_Loss: 4614.03515625, Val_Loss: 4857.7646484375\n",
      "Epoch 4834, Train_Loss: 4613.8857421875, Val_Loss: 4857.5625\n",
      "Epoch 4835, Train_Loss: 4613.71435546875, Val_Loss: 4857.38134765625\n",
      "Epoch 4836, Train_Loss: 4613.5390625, Val_Loss: 4857.15673828125\n",
      "Epoch 4837, Train_Loss: 4613.35009765625, Val_Loss: 4856.92138671875\n",
      "Epoch 4838, Train_Loss: 4613.16015625, Val_Loss: 4856.72509765625\n",
      "Epoch 4839, Train_Loss: 4612.95068359375, Val_Loss: 4856.541015625\n",
      "Epoch 4840, Train_Loss: 4612.77978515625, Val_Loss: 4856.341796875\n",
      "Epoch 4841, Train_Loss: 4612.6083984375, Val_Loss: 4856.1416015625\n",
      "Epoch 4842, Train_Loss: 4612.4267578125, Val_Loss: 4855.94140625\n",
      "Epoch 4843, Train_Loss: 4612.25341796875, Val_Loss: 4855.73486328125\n",
      "Epoch 4844, Train_Loss: 4612.076171875, Val_Loss: 4855.53662109375\n",
      "Epoch 4845, Train_Loss: 4611.92333984375, Val_Loss: 4855.33056640625\n",
      "Epoch 4846, Train_Loss: 4611.759765625, Val_Loss: 4855.13671875\n",
      "Epoch 4847, Train_Loss: 4611.57666015625, Val_Loss: 4854.9306640625\n",
      "Epoch 4848, Train_Loss: 4611.41259765625, Val_Loss: 4854.7333984375\n",
      "Epoch 4849, Train_Loss: 4611.251953125, Val_Loss: 4854.525390625\n",
      "Epoch 4850, Train_Loss: 4611.0869140625, Val_Loss: 4854.32861328125\n",
      "Epoch 4851, Train_Loss: 4610.90869140625, Val_Loss: 4854.1259765625\n",
      "Epoch 4852, Train_Loss: 4610.71630859375, Val_Loss: 4853.92919921875\n",
      "Epoch 4853, Train_Loss: 4610.4951171875, Val_Loss: 4853.7373046875\n",
      "Epoch 4854, Train_Loss: 4610.32421875, Val_Loss: 4853.5400390625\n",
      "Epoch 4855, Train_Loss: 4610.15966796875, Val_Loss: 4853.34326171875\n",
      "Epoch 4856, Train_Loss: 4610.0, Val_Loss: 4853.12255859375\n",
      "Epoch 4857, Train_Loss: 4609.81298828125, Val_Loss: 4852.927734375\n",
      "Epoch 4858, Train_Loss: 4609.6572265625, Val_Loss: 4852.7333984375\n",
      "Epoch 4859, Train_Loss: 4609.4951171875, Val_Loss: 4852.53857421875\n",
      "Epoch 4860, Train_Loss: 4609.31982421875, Val_Loss: 4852.34033203125\n",
      "Epoch 4861, Train_Loss: 4609.158203125, Val_Loss: 4852.14013671875\n",
      "Epoch 4862, Train_Loss: 4608.978515625, Val_Loss: 4851.9404296875\n",
      "Epoch 4863, Train_Loss: 4608.818359375, Val_Loss: 4851.72900390625\n",
      "Epoch 4864, Train_Loss: 4608.64501953125, Val_Loss: 4851.53662109375\n",
      "Epoch 4865, Train_Loss: 4608.4736328125, Val_Loss: 4851.32470703125\n",
      "Epoch 4866, Train_Loss: 4608.2900390625, Val_Loss: 4851.13232421875\n",
      "Epoch 4867, Train_Loss: 4608.12646484375, Val_Loss: 4850.94775390625\n",
      "Epoch 4868, Train_Loss: 4607.9580078125, Val_Loss: 4850.740234375\n",
      "Epoch 4869, Train_Loss: 4607.81787109375, Val_Loss: 4850.5322265625\n",
      "Epoch 4870, Train_Loss: 4607.63671875, Val_Loss: 4850.33203125\n",
      "Epoch 4871, Train_Loss: 4607.46484375, Val_Loss: 4850.13330078125\n",
      "Epoch 4872, Train_Loss: 4607.2724609375, Val_Loss: 4849.92822265625\n",
      "Epoch 4873, Train_Loss: 4607.1083984375, Val_Loss: 4849.74755859375\n",
      "Epoch 4874, Train_Loss: 4606.94091796875, Val_Loss: 4849.55126953125\n",
      "Epoch 4875, Train_Loss: 4606.77392578125, Val_Loss: 4849.30419921875\n",
      "Epoch 4876, Train_Loss: 4606.607421875, Val_Loss: 4849.10546875\n",
      "Epoch 4877, Train_Loss: 4606.435546875, Val_Loss: 4848.9072265625\n",
      "Epoch 4878, Train_Loss: 4606.25927734375, Val_Loss: 4848.70068359375\n",
      "Epoch 4879, Train_Loss: 4606.03955078125, Val_Loss: 4848.515625\n",
      "Epoch 4880, Train_Loss: 4605.861328125, Val_Loss: 4848.326171875\n",
      "Epoch 4881, Train_Loss: 4605.6962890625, Val_Loss: 4848.13037109375\n",
      "Epoch 4882, Train_Loss: 4605.50537109375, Val_Loss: 4847.9169921875\n",
      "Epoch 4883, Train_Loss: 4605.34814453125, Val_Loss: 4847.7158203125\n",
      "Epoch 4884, Train_Loss: 4605.08984375, Val_Loss: 4847.51220703125\n",
      "Epoch 4885, Train_Loss: 4604.9296875, Val_Loss: 4847.32568359375\n",
      "Epoch 4886, Train_Loss: 4604.76220703125, Val_Loss: 4847.126953125\n",
      "Epoch 4887, Train_Loss: 4604.5810546875, Val_Loss: 4846.935546875\n",
      "Epoch 4888, Train_Loss: 4604.40966796875, Val_Loss: 4846.73583984375\n",
      "Epoch 4889, Train_Loss: 4604.2314453125, Val_Loss: 4846.5283203125\n",
      "Epoch 4890, Train_Loss: 4604.0625, Val_Loss: 4846.333984375\n",
      "Epoch 4891, Train_Loss: 4603.89599609375, Val_Loss: 4846.13330078125\n",
      "Epoch 4892, Train_Loss: 4603.70263671875, Val_Loss: 4845.9326171875\n",
      "Epoch 4893, Train_Loss: 4603.42919921875, Val_Loss: 4845.75\n",
      "Epoch 4894, Train_Loss: 4603.2685546875, Val_Loss: 4845.5400390625\n",
      "Epoch 4895, Train_Loss: 4603.10498046875, Val_Loss: 4845.34130859375\n",
      "Epoch 4896, Train_Loss: 4602.97802734375, Val_Loss: 4845.12744140625\n",
      "Epoch 4897, Train_Loss: 4602.796875, Val_Loss: 4844.93408203125\n",
      "Epoch 4898, Train_Loss: 4602.62548828125, Val_Loss: 4844.7451171875\n",
      "Epoch 4899, Train_Loss: 4602.45458984375, Val_Loss: 4844.54931640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4900, Train_Loss: 4602.287109375, Val_Loss: 4844.3447265625\n",
      "Epoch 4901, Train_Loss: 4602.111328125, Val_Loss: 4844.1513671875\n",
      "Epoch 4902, Train_Loss: 4601.93603515625, Val_Loss: 4843.94287109375\n",
      "Epoch 4903, Train_Loss: 4601.7685546875, Val_Loss: 4843.7451171875\n",
      "Epoch 4904, Train_Loss: 4601.58935546875, Val_Loss: 4843.54052734375\n",
      "Epoch 4905, Train_Loss: 4601.4248046875, Val_Loss: 4843.3408203125\n",
      "Epoch 4906, Train_Loss: 4601.2373046875, Val_Loss: 4843.1484375\n",
      "Epoch 4907, Train_Loss: 4601.08056640625, Val_Loss: 4842.943359375\n",
      "Epoch 4908, Train_Loss: 4600.9091796875, Val_Loss: 4842.74658203125\n",
      "Epoch 4909, Train_Loss: 4600.75390625, Val_Loss: 4842.53662109375\n",
      "Epoch 4910, Train_Loss: 4600.59130859375, Val_Loss: 4842.33251953125\n",
      "Epoch 4911, Train_Loss: 4600.42236328125, Val_Loss: 4842.1416015625\n",
      "Epoch 4912, Train_Loss: 4600.22412109375, Val_Loss: 4841.91845703125\n",
      "Epoch 4913, Train_Loss: 4600.06201171875, Val_Loss: 4841.71240234375\n",
      "Epoch 4914, Train_Loss: 4599.896484375, Val_Loss: 4841.53173828125\n",
      "Epoch 4915, Train_Loss: 4599.7314453125, Val_Loss: 4841.333984375\n",
      "Epoch 4916, Train_Loss: 4599.5693359375, Val_Loss: 4841.12451171875\n",
      "Epoch 4917, Train_Loss: 4599.37841796875, Val_Loss: 4840.91845703125\n",
      "Epoch 4918, Train_Loss: 4599.21826171875, Val_Loss: 4840.720703125\n",
      "Epoch 4919, Train_Loss: 4598.99658203125, Val_Loss: 4840.529296875\n",
      "Epoch 4920, Train_Loss: 4598.828125, Val_Loss: 4840.33984375\n",
      "Epoch 4921, Train_Loss: 4598.6630859375, Val_Loss: 4840.13671875\n",
      "Epoch 4922, Train_Loss: 4598.5205078125, Val_Loss: 4839.935546875\n",
      "Epoch 4923, Train_Loss: 4598.33251953125, Val_Loss: 4839.7314453125\n",
      "Epoch 4924, Train_Loss: 4598.1611328125, Val_Loss: 4839.53662109375\n",
      "Epoch 4925, Train_Loss: 4597.998046875, Val_Loss: 4839.33740234375\n",
      "Epoch 4926, Train_Loss: 4597.82666015625, Val_Loss: 4839.15087890625\n",
      "Epoch 4927, Train_Loss: 4597.6533203125, Val_Loss: 4838.9501953125\n",
      "Epoch 4928, Train_Loss: 4597.482421875, Val_Loss: 4838.74755859375\n",
      "Epoch 4929, Train_Loss: 4597.30810546875, Val_Loss: 4838.53662109375\n",
      "Epoch 4930, Train_Loss: 4597.1435546875, Val_Loss: 4838.3515625\n",
      "Epoch 4931, Train_Loss: 4596.97119140625, Val_Loss: 4838.1328125\n",
      "Epoch 4932, Train_Loss: 4596.7490234375, Val_Loss: 4837.95166015625\n",
      "Epoch 4933, Train_Loss: 4596.55419921875, Val_Loss: 4837.7587890625\n",
      "Epoch 4934, Train_Loss: 4596.39990234375, Val_Loss: 4837.556640625\n",
      "Epoch 4935, Train_Loss: 4596.23291015625, Val_Loss: 4837.357421875\n",
      "Epoch 4936, Train_Loss: 4596.076171875, Val_Loss: 4837.1474609375\n",
      "Epoch 4937, Train_Loss: 4595.90673828125, Val_Loss: 4836.9482421875\n",
      "Epoch 4938, Train_Loss: 4595.7265625, Val_Loss: 4836.75244140625\n",
      "Epoch 4939, Train_Loss: 4595.56103515625, Val_Loss: 4836.55908203125\n",
      "Epoch 4940, Train_Loss: 4595.39501953125, Val_Loss: 4836.35205078125\n",
      "Epoch 4941, Train_Loss: 4595.22900390625, Val_Loss: 4836.1630859375\n",
      "Epoch 4942, Train_Loss: 4595.06787109375, Val_Loss: 4835.96337890625\n",
      "Epoch 4943, Train_Loss: 4594.89892578125, Val_Loss: 4835.75244140625\n",
      "Epoch 4944, Train_Loss: 4594.72705078125, Val_Loss: 4835.5517578125\n",
      "Epoch 4945, Train_Loss: 4594.58154296875, Val_Loss: 4835.35791015625\n",
      "Epoch 4946, Train_Loss: 4594.37841796875, Val_Loss: 4835.1748046875\n",
      "Epoch 4947, Train_Loss: 4594.21923828125, Val_Loss: 4834.97900390625\n",
      "Epoch 4948, Train_Loss: 4594.0478515625, Val_Loss: 4834.775390625\n",
      "Epoch 4949, Train_Loss: 4593.87255859375, Val_Loss: 4834.54541015625\n",
      "Epoch 4950, Train_Loss: 4593.6884765625, Val_Loss: 4834.33447265625\n",
      "Epoch 4951, Train_Loss: 4593.52783203125, Val_Loss: 4834.14208984375\n",
      "Epoch 4952, Train_Loss: 4593.365234375, Val_Loss: 4833.95361328125\n",
      "Epoch 4953, Train_Loss: 4593.18212890625, Val_Loss: 4833.76025390625\n",
      "Epoch 4954, Train_Loss: 4593.00146484375, Val_Loss: 4833.556640625\n",
      "Epoch 4955, Train_Loss: 4592.83203125, Val_Loss: 4833.37255859375\n",
      "Epoch 4956, Train_Loss: 4592.66650390625, Val_Loss: 4833.158203125\n",
      "Epoch 4957, Train_Loss: 4592.498046875, Val_Loss: 4832.951171875\n",
      "Epoch 4958, Train_Loss: 4592.3212890625, Val_Loss: 4832.7666015625\n",
      "Epoch 4959, Train_Loss: 4592.083984375, Val_Loss: 4832.5732421875\n",
      "Epoch 4960, Train_Loss: 4591.919921875, Val_Loss: 4832.37939453125\n",
      "Epoch 4961, Train_Loss: 4591.76123046875, Val_Loss: 4832.16748046875\n",
      "Epoch 4962, Train_Loss: 4591.6005859375, Val_Loss: 4831.96337890625\n",
      "Epoch 4963, Train_Loss: 4591.40576171875, Val_Loss: 4831.763671875\n",
      "Epoch 4964, Train_Loss: 4591.2353515625, Val_Loss: 4831.572265625\n",
      "Epoch 4965, Train_Loss: 4591.06787109375, Val_Loss: 4831.39013671875\n",
      "Epoch 4966, Train_Loss: 4590.90771484375, Val_Loss: 4831.18212890625\n",
      "Epoch 4967, Train_Loss: 4590.75048828125, Val_Loss: 4830.982421875\n",
      "Epoch 4968, Train_Loss: 4590.44580078125, Val_Loss: 4830.78515625\n",
      "Epoch 4969, Train_Loss: 4590.2744140625, Val_Loss: 4830.58056640625\n",
      "Epoch 4970, Train_Loss: 4590.10009765625, Val_Loss: 4830.3916015625\n",
      "Epoch 4971, Train_Loss: 4589.93017578125, Val_Loss: 4830.189453125\n",
      "Epoch 4972, Train_Loss: 4589.7353515625, Val_Loss: 4830.01123046875\n",
      "Epoch 4973, Train_Loss: 4589.55126953125, Val_Loss: 4829.822265625\n",
      "Epoch 4974, Train_Loss: 4589.3876953125, Val_Loss: 4829.60791015625\n",
      "Epoch 4975, Train_Loss: 4589.25927734375, Val_Loss: 4829.4091796875\n",
      "Epoch 4976, Train_Loss: 4589.09228515625, Val_Loss: 4829.21337890625\n",
      "Epoch 4977, Train_Loss: 4588.9150390625, Val_Loss: 4829.0166015625\n",
      "Epoch 4978, Train_Loss: 4588.73974609375, Val_Loss: 4828.80615234375\n",
      "Epoch 4979, Train_Loss: 4588.58984375, Val_Loss: 4828.619140625\n",
      "Epoch 4980, Train_Loss: 4588.4443359375, Val_Loss: 4828.40966796875\n",
      "Epoch 4981, Train_Loss: 4588.2744140625, Val_Loss: 4828.22607421875\n",
      "Epoch 4982, Train_Loss: 4588.11767578125, Val_Loss: 4828.0205078125\n",
      "Epoch 4983, Train_Loss: 4587.9462890625, Val_Loss: 4827.81982421875\n",
      "Epoch 4984, Train_Loss: 4587.7685546875, Val_Loss: 4827.6259765625\n",
      "Epoch 4985, Train_Loss: 4587.55810546875, Val_Loss: 4827.4443359375\n",
      "Epoch 4986, Train_Loss: 4587.3876953125, Val_Loss: 4827.2490234375\n",
      "Epoch 4987, Train_Loss: 4587.18115234375, Val_Loss: 4827.01416015625\n",
      "Epoch 4988, Train_Loss: 4587.00732421875, Val_Loss: 4826.826171875\n",
      "Epoch 4989, Train_Loss: 4586.83251953125, Val_Loss: 4826.615234375\n",
      "Epoch 4990, Train_Loss: 4586.6650390625, Val_Loss: 4826.41796875\n",
      "Epoch 4991, Train_Loss: 4586.50341796875, Val_Loss: 4826.220703125\n",
      "Epoch 4992, Train_Loss: 4586.35009765625, Val_Loss: 4826.0439453125\n",
      "Epoch 4993, Train_Loss: 4586.1650390625, Val_Loss: 4825.84814453125\n",
      "Epoch 4994, Train_Loss: 4585.9951171875, Val_Loss: 4825.6416015625\n",
      "Epoch 4995, Train_Loss: 4585.8408203125, Val_Loss: 4825.443359375\n",
      "Epoch 4996, Train_Loss: 4585.66796875, Val_Loss: 4825.23779296875\n",
      "Epoch 4997, Train_Loss: 4585.49169921875, Val_Loss: 4825.04296875\n",
      "Epoch 4998, Train_Loss: 4585.3173828125, Val_Loss: 4824.85302734375\n",
      "Epoch 4999, Train_Loss: 4585.13037109375, Val_Loss: 4824.6650390625\n",
      "Epoch 5000, Train_Loss: 4584.9658203125, Val_Loss: 4824.47802734375\n",
      "Epoch 5001, Train_Loss: 4584.77099609375, Val_Loss: 4824.2822265625\n",
      "Epoch 5002, Train_Loss: 4584.64453125, Val_Loss: 4824.0703125\n",
      "Epoch 5003, Train_Loss: 4584.44970703125, Val_Loss: 4823.88623046875\n",
      "Epoch 5004, Train_Loss: 4584.27490234375, Val_Loss: 4823.681640625\n",
      "Epoch 5005, Train_Loss: 4584.12109375, Val_Loss: 4823.49462890625\n",
      "Epoch 5006, Train_Loss: 4583.95263671875, Val_Loss: 4823.2861328125\n",
      "Epoch 5007, Train_Loss: 4583.80078125, Val_Loss: 4823.09423828125\n",
      "Epoch 5008, Train_Loss: 4583.61767578125, Val_Loss: 4822.90234375\n",
      "Epoch 5009, Train_Loss: 4583.4609375, Val_Loss: 4822.70361328125\n",
      "Epoch 5010, Train_Loss: 4583.2939453125, Val_Loss: 4822.5\n",
      "Epoch 5011, Train_Loss: 4583.12646484375, Val_Loss: 4822.2998046875\n",
      "Epoch 5012, Train_Loss: 4582.9521484375, Val_Loss: 4822.1123046875\n",
      "Epoch 5013, Train_Loss: 4582.7705078125, Val_Loss: 4821.9306640625\n",
      "Epoch 5014, Train_Loss: 4582.6015625, Val_Loss: 4821.72607421875\n",
      "Epoch 5015, Train_Loss: 4582.44775390625, Val_Loss: 4821.51953125\n",
      "Epoch 5016, Train_Loss: 4582.2724609375, Val_Loss: 4821.32421875\n",
      "Epoch 5017, Train_Loss: 4582.10498046875, Val_Loss: 4821.1240234375\n",
      "Epoch 5018, Train_Loss: 4581.93017578125, Val_Loss: 4820.92431640625\n",
      "Epoch 5019, Train_Loss: 4581.76416015625, Val_Loss: 4820.74658203125\n",
      "Epoch 5020, Train_Loss: 4581.59228515625, Val_Loss: 4820.55126953125\n",
      "Epoch 5021, Train_Loss: 4581.4306640625, Val_Loss: 4820.357421875\n",
      "Epoch 5022, Train_Loss: 4581.2724609375, Val_Loss: 4820.15576171875\n",
      "Epoch 5023, Train_Loss: 4581.1181640625, Val_Loss: 4819.9619140625\n",
      "Epoch 5024, Train_Loss: 4580.9521484375, Val_Loss: 4819.71923828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5025, Train_Loss: 4580.74462890625, Val_Loss: 4819.53857421875\n",
      "Epoch 5026, Train_Loss: 4580.5830078125, Val_Loss: 4819.34130859375\n",
      "Epoch 5027, Train_Loss: 4580.412109375, Val_Loss: 4819.146484375\n",
      "Epoch 5028, Train_Loss: 4580.23046875, Val_Loss: 4818.9462890625\n",
      "Epoch 5029, Train_Loss: 4580.1005859375, Val_Loss: 4818.7392578125\n",
      "Epoch 5030, Train_Loss: 4579.93505859375, Val_Loss: 4818.5498046875\n",
      "Epoch 5031, Train_Loss: 4579.76318359375, Val_Loss: 4818.3544921875\n",
      "Epoch 5032, Train_Loss: 4579.59814453125, Val_Loss: 4818.17333984375\n",
      "Epoch 5033, Train_Loss: 4579.41357421875, Val_Loss: 4817.9765625\n",
      "Epoch 5034, Train_Loss: 4579.220703125, Val_Loss: 4817.77880859375\n",
      "Epoch 5035, Train_Loss: 4579.07177734375, Val_Loss: 4817.57177734375\n",
      "Epoch 5036, Train_Loss: 4578.90673828125, Val_Loss: 4817.37841796875\n",
      "Epoch 5037, Train_Loss: 4578.74169921875, Val_Loss: 4817.1845703125\n",
      "Epoch 5038, Train_Loss: 4578.51904296875, Val_Loss: 4817.00341796875\n",
      "Epoch 5039, Train_Loss: 4578.35498046875, Val_Loss: 4816.81005859375\n",
      "Epoch 5040, Train_Loss: 4578.1953125, Val_Loss: 4816.61669921875\n",
      "Epoch 5041, Train_Loss: 4578.04296875, Val_Loss: 4816.4228515625\n",
      "Epoch 5042, Train_Loss: 4577.880859375, Val_Loss: 4816.21337890625\n",
      "Epoch 5043, Train_Loss: 4577.61328125, Val_Loss: 4816.0234375\n",
      "Epoch 5044, Train_Loss: 4577.44775390625, Val_Loss: 4815.82080078125\n",
      "Epoch 5045, Train_Loss: 4577.28857421875, Val_Loss: 4815.6318359375\n",
      "Epoch 5046, Train_Loss: 4577.12646484375, Val_Loss: 4815.4453125\n",
      "Epoch 5047, Train_Loss: 4576.96826171875, Val_Loss: 4815.2451171875\n",
      "Epoch 5048, Train_Loss: 4576.79296875, Val_Loss: 4815.05078125\n",
      "Epoch 5049, Train_Loss: 4576.62841796875, Val_Loss: 4814.8427734375\n",
      "Epoch 5050, Train_Loss: 4576.46044921875, Val_Loss: 4814.6396484375\n",
      "Epoch 5051, Train_Loss: 4576.283203125, Val_Loss: 4814.46240234375\n",
      "Epoch 5052, Train_Loss: 4576.07421875, Val_Loss: 4814.2705078125\n",
      "Epoch 5053, Train_Loss: 4575.8818359375, Val_Loss: 4814.0615234375\n",
      "Epoch 5054, Train_Loss: 4575.70947265625, Val_Loss: 4813.869140625\n",
      "Epoch 5055, Train_Loss: 4575.556640625, Val_Loss: 4813.673828125\n",
      "Epoch 5056, Train_Loss: 4575.38916015625, Val_Loss: 4813.4775390625\n",
      "Epoch 5057, Train_Loss: 4575.2265625, Val_Loss: 4813.28125\n",
      "Epoch 5058, Train_Loss: 4575.052734375, Val_Loss: 4813.09130859375\n",
      "Epoch 5059, Train_Loss: 4574.88916015625, Val_Loss: 4812.88818359375\n",
      "Epoch 5060, Train_Loss: 4574.73583984375, Val_Loss: 4812.6962890625\n",
      "Epoch 5061, Train_Loss: 4574.57958984375, Val_Loss: 4812.505859375\n",
      "Epoch 5062, Train_Loss: 4574.41015625, Val_Loss: 4812.2646484375\n",
      "Epoch 5063, Train_Loss: 4574.232421875, Val_Loss: 4812.06787109375\n",
      "Epoch 5064, Train_Loss: 4574.0556640625, Val_Loss: 4811.8818359375\n",
      "Epoch 5065, Train_Loss: 4573.8232421875, Val_Loss: 4811.69580078125\n",
      "Epoch 5066, Train_Loss: 4573.65576171875, Val_Loss: 4811.505859375\n",
      "Epoch 5067, Train_Loss: 4573.50830078125, Val_Loss: 4811.2998046875\n",
      "Epoch 5068, Train_Loss: 4573.3515625, Val_Loss: 4811.10302734375\n",
      "Epoch 5069, Train_Loss: 4573.19580078125, Val_Loss: 4810.9052734375\n",
      "Epoch 5070, Train_Loss: 4573.0302734375, Val_Loss: 4810.70703125\n",
      "Epoch 5071, Train_Loss: 4572.853515625, Val_Loss: 4810.52490234375\n",
      "Epoch 5072, Train_Loss: 4572.70068359375, Val_Loss: 4810.33544921875\n",
      "Epoch 5073, Train_Loss: 4572.51513671875, Val_Loss: 4810.146484375\n",
      "Epoch 5074, Train_Loss: 4572.34716796875, Val_Loss: 4809.95068359375\n",
      "Epoch 5075, Train_Loss: 4572.18212890625, Val_Loss: 4809.74755859375\n",
      "Epoch 5076, Train_Loss: 4572.00830078125, Val_Loss: 4809.556640625\n",
      "Epoch 5077, Train_Loss: 4571.84423828125, Val_Loss: 4809.357421875\n",
      "Epoch 5078, Train_Loss: 4571.64111328125, Val_Loss: 4809.17333984375\n",
      "Epoch 5079, Train_Loss: 4571.4794921875, Val_Loss: 4808.9736328125\n",
      "Epoch 5080, Train_Loss: 4571.32373046875, Val_Loss: 4808.775390625\n",
      "Epoch 5081, Train_Loss: 4571.16357421875, Val_Loss: 4808.5908203125\n",
      "Epoch 5082, Train_Loss: 4571.0302734375, Val_Loss: 4808.388671875\n",
      "Epoch 5083, Train_Loss: 4570.84619140625, Val_Loss: 4808.19580078125\n",
      "Epoch 5084, Train_Loss: 4570.67626953125, Val_Loss: 4807.9892578125\n",
      "Epoch 5085, Train_Loss: 4570.50244140625, Val_Loss: 4807.80322265625\n",
      "Epoch 5086, Train_Loss: 4570.34033203125, Val_Loss: 4807.6162109375\n",
      "Epoch 5087, Train_Loss: 4570.1796875, Val_Loss: 4807.4169921875\n",
      "Epoch 5088, Train_Loss: 4570.00048828125, Val_Loss: 4807.2236328125\n",
      "Epoch 5089, Train_Loss: 4569.83740234375, Val_Loss: 4807.03125\n",
      "Epoch 5090, Train_Loss: 4569.66162109375, Val_Loss: 4806.82373046875\n",
      "Epoch 5091, Train_Loss: 4569.501953125, Val_Loss: 4806.630859375\n",
      "Epoch 5092, Train_Loss: 4569.2861328125, Val_Loss: 4806.447265625\n",
      "Epoch 5093, Train_Loss: 4569.09716796875, Val_Loss: 4806.2421875\n",
      "Epoch 5094, Train_Loss: 4568.9404296875, Val_Loss: 4806.04736328125\n",
      "Epoch 5095, Train_Loss: 4568.78759765625, Val_Loss: 4805.85205078125\n",
      "Epoch 5096, Train_Loss: 4568.62841796875, Val_Loss: 4805.66162109375\n",
      "Epoch 5097, Train_Loss: 4568.4658203125, Val_Loss: 4805.46533203125\n",
      "Epoch 5098, Train_Loss: 4568.29296875, Val_Loss: 4805.287109375\n",
      "Epoch 5099, Train_Loss: 4568.1123046875, Val_Loss: 4805.0595703125\n",
      "Epoch 5100, Train_Loss: 4567.95166015625, Val_Loss: 4804.8681640625\n",
      "Epoch 5101, Train_Loss: 4567.78369140625, Val_Loss: 4804.6669921875\n",
      "Epoch 5102, Train_Loss: 4567.61474609375, Val_Loss: 4804.4716796875\n",
      "Epoch 5103, Train_Loss: 4567.45361328125, Val_Loss: 4804.2861328125\n",
      "Epoch 5104, Train_Loss: 4567.29541015625, Val_Loss: 4804.08642578125\n",
      "Epoch 5105, Train_Loss: 4567.1005859375, Val_Loss: 4803.8916015625\n",
      "Epoch 5106, Train_Loss: 4566.95068359375, Val_Loss: 4803.7001953125\n",
      "Epoch 5107, Train_Loss: 4566.7724609375, Val_Loss: 4803.51318359375\n",
      "Epoch 5108, Train_Loss: 4566.62646484375, Val_Loss: 4803.3056640625\n",
      "Epoch 5109, Train_Loss: 4566.45947265625, Val_Loss: 4803.11083984375\n",
      "Epoch 5110, Train_Loss: 4566.30224609375, Val_Loss: 4802.92822265625\n",
      "Epoch 5111, Train_Loss: 4566.15234375, Val_Loss: 4802.72802734375\n",
      "Epoch 5112, Train_Loss: 4565.98779296875, Val_Loss: 4802.53955078125\n",
      "Epoch 5113, Train_Loss: 4565.7919921875, Val_Loss: 4802.3447265625\n",
      "Epoch 5114, Train_Loss: 4565.62548828125, Val_Loss: 4802.16015625\n",
      "Epoch 5115, Train_Loss: 4565.46826171875, Val_Loss: 4801.955078125\n",
      "Epoch 5116, Train_Loss: 4565.31787109375, Val_Loss: 4801.7734375\n",
      "Epoch 5117, Train_Loss: 4565.15771484375, Val_Loss: 4801.57421875\n",
      "Epoch 5118, Train_Loss: 4564.7998046875, Val_Loss: 4801.38623046875\n",
      "Epoch 5119, Train_Loss: 4564.62744140625, Val_Loss: 4801.189453125\n",
      "Epoch 5120, Train_Loss: 4564.47119140625, Val_Loss: 4801.0\n",
      "Epoch 5121, Train_Loss: 4564.31787109375, Val_Loss: 4800.791015625\n",
      "Epoch 5122, Train_Loss: 4564.14892578125, Val_Loss: 4800.6005859375\n",
      "Epoch 5123, Train_Loss: 4563.97119140625, Val_Loss: 4800.40380859375\n",
      "Epoch 5124, Train_Loss: 4563.7958984375, Val_Loss: 4800.201171875\n",
      "Epoch 5125, Train_Loss: 4563.63232421875, Val_Loss: 4800.02685546875\n",
      "Epoch 5126, Train_Loss: 4563.47412109375, Val_Loss: 4799.83740234375\n",
      "Epoch 5127, Train_Loss: 4563.32080078125, Val_Loss: 4799.630859375\n",
      "Epoch 5128, Train_Loss: 4563.14404296875, Val_Loss: 4799.4248046875\n",
      "Epoch 5129, Train_Loss: 4562.97412109375, Val_Loss: 4799.23681640625\n",
      "Epoch 5130, Train_Loss: 4562.80908203125, Val_Loss: 4799.05419921875\n",
      "Epoch 5131, Train_Loss: 4562.6103515625, Val_Loss: 4798.87158203125\n",
      "Epoch 5132, Train_Loss: 4562.4521484375, Val_Loss: 4798.6845703125\n",
      "Epoch 5133, Train_Loss: 4562.28271484375, Val_Loss: 4798.47802734375\n",
      "Epoch 5134, Train_Loss: 4562.11669921875, Val_Loss: 4798.2822265625\n",
      "Epoch 5135, Train_Loss: 4561.9755859375, Val_Loss: 4798.09423828125\n",
      "Epoch 5136, Train_Loss: 4561.7890625, Val_Loss: 4797.857421875\n",
      "Epoch 5137, Train_Loss: 4561.62890625, Val_Loss: 4797.67138671875\n",
      "Epoch 5138, Train_Loss: 4561.45166015625, Val_Loss: 4797.49462890625\n",
      "Epoch 5139, Train_Loss: 4561.2861328125, Val_Loss: 4797.30078125\n",
      "Epoch 5140, Train_Loss: 4561.12939453125, Val_Loss: 4797.10693359375\n",
      "Epoch 5141, Train_Loss: 4560.97509765625, Val_Loss: 4796.8984375\n",
      "Epoch 5142, Train_Loss: 4560.810546875, Val_Loss: 4796.6943359375\n",
      "Epoch 5143, Train_Loss: 4560.6396484375, Val_Loss: 4796.51416015625\n",
      "Epoch 5144, Train_Loss: 4560.47509765625, Val_Loss: 4796.32421875\n",
      "Epoch 5145, Train_Loss: 4560.2548828125, Val_Loss: 4796.13623046875\n",
      "Epoch 5146, Train_Loss: 4560.095703125, Val_Loss: 4795.94140625\n",
      "Epoch 5147, Train_Loss: 4559.9375, Val_Loss: 4795.74658203125\n",
      "Epoch 5148, Train_Loss: 4559.763671875, Val_Loss: 4795.54443359375\n",
      "Epoch 5149, Train_Loss: 4559.6064453125, Val_Loss: 4795.35791015625\n",
      "Epoch 5150, Train_Loss: 4559.4462890625, Val_Loss: 4795.1640625\n",
      "Epoch 5151, Train_Loss: 4559.2841796875, Val_Loss: 4794.97607421875\n",
      "Epoch 5152, Train_Loss: 4559.11962890625, Val_Loss: 4794.7890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5153, Train_Loss: 4558.9248046875, Val_Loss: 4794.60546875\n",
      "Epoch 5154, Train_Loss: 4558.771484375, Val_Loss: 4794.40576171875\n",
      "Epoch 5155, Train_Loss: 4558.61376953125, Val_Loss: 4794.19970703125\n",
      "Epoch 5156, Train_Loss: 4558.4501953125, Val_Loss: 4794.01123046875\n",
      "Epoch 5157, Train_Loss: 4558.29296875, Val_Loss: 4793.80810546875\n",
      "Epoch 5158, Train_Loss: 4558.08984375, Val_Loss: 4793.634765625\n",
      "Epoch 5159, Train_Loss: 4557.9189453125, Val_Loss: 4793.45263671875\n",
      "Epoch 5160, Train_Loss: 4557.7529296875, Val_Loss: 4793.2509765625\n",
      "Epoch 5161, Train_Loss: 4557.58935546875, Val_Loss: 4793.05078125\n",
      "Epoch 5162, Train_Loss: 4557.4228515625, Val_Loss: 4792.86767578125\n",
      "Epoch 5163, Train_Loss: 4557.240234375, Val_Loss: 4792.67431640625\n",
      "Epoch 5164, Train_Loss: 4557.08935546875, Val_Loss: 4792.4814453125\n",
      "Epoch 5165, Train_Loss: 4556.92724609375, Val_Loss: 4792.27685546875\n",
      "Epoch 5166, Train_Loss: 4556.7724609375, Val_Loss: 4792.08447265625\n",
      "Epoch 5167, Train_Loss: 4556.60986328125, Val_Loss: 4791.89990234375\n",
      "Epoch 5168, Train_Loss: 4556.4345703125, Val_Loss: 4791.70654296875\n",
      "Epoch 5169, Train_Loss: 4556.28271484375, Val_Loss: 4791.5048828125\n",
      "Epoch 5170, Train_Loss: 4556.115234375, Val_Loss: 4791.31494140625\n",
      "Epoch 5171, Train_Loss: 4555.91748046875, Val_Loss: 4791.123046875\n",
      "Epoch 5172, Train_Loss: 4555.759765625, Val_Loss: 4790.93310546875\n",
      "Epoch 5173, Train_Loss: 4555.5859375, Val_Loss: 4790.74462890625\n",
      "Epoch 5174, Train_Loss: 4555.4248046875, Val_Loss: 4790.5107421875\n",
      "Epoch 5175, Train_Loss: 4555.2646484375, Val_Loss: 4790.3212890625\n",
      "Epoch 5176, Train_Loss: 4555.09423828125, Val_Loss: 4790.12646484375\n",
      "Epoch 5177, Train_Loss: 4554.93017578125, Val_Loss: 4789.93896484375\n",
      "Epoch 5178, Train_Loss: 4554.75244140625, Val_Loss: 4789.75146484375\n",
      "Epoch 5179, Train_Loss: 4554.58740234375, Val_Loss: 4789.56787109375\n",
      "Epoch 5180, Train_Loss: 4554.4375, Val_Loss: 4789.37744140625\n",
      "Epoch 5181, Train_Loss: 4554.2705078125, Val_Loss: 4789.17626953125\n",
      "Epoch 5182, Train_Loss: 4554.10986328125, Val_Loss: 4788.96923828125\n",
      "Epoch 5183, Train_Loss: 4553.87939453125, Val_Loss: 4788.78662109375\n",
      "Epoch 5184, Train_Loss: 4553.69921875, Val_Loss: 4788.6015625\n",
      "Epoch 5185, Train_Loss: 4553.544921875, Val_Loss: 4788.41943359375\n",
      "Epoch 5186, Train_Loss: 4553.39599609375, Val_Loss: 4788.23095703125\n",
      "Epoch 5187, Train_Loss: 4553.228515625, Val_Loss: 4788.03466796875\n",
      "Epoch 5188, Train_Loss: 4553.09716796875, Val_Loss: 4787.83056640625\n",
      "Epoch 5189, Train_Loss: 4552.92626953125, Val_Loss: 4787.6416015625\n",
      "Epoch 5190, Train_Loss: 4552.76904296875, Val_Loss: 4787.439453125\n",
      "Epoch 5191, Train_Loss: 4552.59326171875, Val_Loss: 4787.26513671875\n",
      "Epoch 5192, Train_Loss: 4552.43603515625, Val_Loss: 4787.0693359375\n",
      "Epoch 5193, Train_Loss: 4552.20703125, Val_Loss: 4786.87744140625\n",
      "Epoch 5194, Train_Loss: 4552.04541015625, Val_Loss: 4786.6845703125\n",
      "Epoch 5195, Train_Loss: 4551.87744140625, Val_Loss: 4786.4833984375\n",
      "Epoch 5196, Train_Loss: 4551.70263671875, Val_Loss: 4786.30859375\n",
      "Epoch 5197, Train_Loss: 4551.548828125, Val_Loss: 4786.09716796875\n",
      "Epoch 5198, Train_Loss: 4551.3134765625, Val_Loss: 4785.9326171875\n",
      "Epoch 5199, Train_Loss: 4551.15185546875, Val_Loss: 4785.73779296875\n",
      "Epoch 5200, Train_Loss: 4550.98974609375, Val_Loss: 4785.54248046875\n",
      "Epoch 5201, Train_Loss: 4550.8359375, Val_Loss: 4785.34765625\n",
      "Epoch 5202, Train_Loss: 4550.65966796875, Val_Loss: 4785.13134765625\n",
      "Epoch 5203, Train_Loss: 4550.48388671875, Val_Loss: 4784.94873046875\n",
      "Epoch 5204, Train_Loss: 4550.32177734375, Val_Loss: 4784.765625\n",
      "Epoch 5205, Train_Loss: 4550.16064453125, Val_Loss: 4784.5703125\n",
      "Epoch 5206, Train_Loss: 4550.00244140625, Val_Loss: 4784.3837890625\n",
      "Epoch 5207, Train_Loss: 4549.83740234375, Val_Loss: 4784.19482421875\n",
      "Epoch 5208, Train_Loss: 4549.666015625, Val_Loss: 4783.98583984375\n",
      "Epoch 5209, Train_Loss: 4549.482421875, Val_Loss: 4783.80419921875\n",
      "Epoch 5210, Train_Loss: 4549.32763671875, Val_Loss: 4783.60693359375\n",
      "Epoch 5211, Train_Loss: 4549.111328125, Val_Loss: 4783.39697265625\n",
      "Epoch 5212, Train_Loss: 4548.9365234375, Val_Loss: 4783.20849609375\n",
      "Epoch 5213, Train_Loss: 4548.763671875, Val_Loss: 4783.01611328125\n",
      "Epoch 5214, Train_Loss: 4548.64501953125, Val_Loss: 4782.80419921875\n",
      "Epoch 5215, Train_Loss: 4548.49609375, Val_Loss: 4782.62158203125\n",
      "Epoch 5216, Train_Loss: 4548.333984375, Val_Loss: 4782.43359375\n",
      "Epoch 5217, Train_Loss: 4548.17578125, Val_Loss: 4782.24267578125\n",
      "Epoch 5218, Train_Loss: 4548.02490234375, Val_Loss: 4782.06298828125\n",
      "Epoch 5219, Train_Loss: 4547.8515625, Val_Loss: 4781.87744140625\n",
      "Epoch 5220, Train_Loss: 4547.697265625, Val_Loss: 4781.6806640625\n",
      "Epoch 5221, Train_Loss: 4547.54443359375, Val_Loss: 4781.46435546875\n",
      "Epoch 5222, Train_Loss: 4547.3720703125, Val_Loss: 4781.27880859375\n",
      "Epoch 5223, Train_Loss: 4547.20849609375, Val_Loss: 4781.09228515625\n",
      "Epoch 5224, Train_Loss: 4547.02685546875, Val_Loss: 4780.92919921875\n",
      "Epoch 5225, Train_Loss: 4546.8720703125, Val_Loss: 4780.7158203125\n",
      "Epoch 5226, Train_Loss: 4546.74267578125, Val_Loss: 4780.5322265625\n",
      "Epoch 5227, Train_Loss: 4546.587890625, Val_Loss: 4780.33642578125\n",
      "Epoch 5228, Train_Loss: 4546.427734375, Val_Loss: 4780.134765625\n",
      "Epoch 5229, Train_Loss: 4546.2626953125, Val_Loss: 4779.947265625\n",
      "Epoch 5230, Train_Loss: 4546.095703125, Val_Loss: 4779.7421875\n",
      "Epoch 5231, Train_Loss: 4545.93896484375, Val_Loss: 4779.5634765625\n",
      "Epoch 5232, Train_Loss: 4545.77880859375, Val_Loss: 4779.37109375\n",
      "Epoch 5233, Train_Loss: 4545.6162109375, Val_Loss: 4779.18115234375\n",
      "Epoch 5234, Train_Loss: 4545.4375, Val_Loss: 4778.98291015625\n",
      "Epoch 5235, Train_Loss: 4545.28857421875, Val_Loss: 4778.787109375\n",
      "Epoch 5236, Train_Loss: 4545.11865234375, Val_Loss: 4778.59765625\n",
      "Epoch 5237, Train_Loss: 4544.95166015625, Val_Loss: 4778.4033203125\n",
      "Epoch 5238, Train_Loss: 4544.734375, Val_Loss: 4778.224609375\n",
      "Epoch 5239, Train_Loss: 4544.5576171875, Val_Loss: 4778.02099609375\n",
      "Epoch 5240, Train_Loss: 4544.39599609375, Val_Loss: 4777.83447265625\n",
      "Epoch 5241, Train_Loss: 4544.26953125, Val_Loss: 4777.6259765625\n",
      "Epoch 5242, Train_Loss: 4544.11279296875, Val_Loss: 4777.44091796875\n",
      "Epoch 5243, Train_Loss: 4543.94921875, Val_Loss: 4777.2529296875\n",
      "Epoch 5244, Train_Loss: 4543.759765625, Val_Loss: 4777.07861328125\n",
      "Epoch 5245, Train_Loss: 4543.59716796875, Val_Loss: 4776.87939453125\n",
      "Epoch 5246, Train_Loss: 4543.4501953125, Val_Loss: 4776.6845703125\n",
      "Epoch 5247, Train_Loss: 4543.29833984375, Val_Loss: 4776.4873046875\n",
      "Epoch 5248, Train_Loss: 4543.14208984375, Val_Loss: 4776.29345703125\n",
      "Epoch 5249, Train_Loss: 4542.96435546875, Val_Loss: 4776.06103515625\n",
      "Epoch 5250, Train_Loss: 4542.79736328125, Val_Loss: 4775.86767578125\n",
      "Epoch 5251, Train_Loss: 4542.59619140625, Val_Loss: 4775.69970703125\n",
      "Epoch 5252, Train_Loss: 4542.4521484375, Val_Loss: 4775.50634765625\n",
      "Epoch 5253, Train_Loss: 4542.28955078125, Val_Loss: 4775.3232421875\n",
      "Epoch 5254, Train_Loss: 4542.11767578125, Val_Loss: 4775.11669921875\n",
      "Epoch 5255, Train_Loss: 4541.9609375, Val_Loss: 4774.92822265625\n",
      "Epoch 5256, Train_Loss: 4541.80224609375, Val_Loss: 4774.74658203125\n",
      "Epoch 5257, Train_Loss: 4541.64404296875, Val_Loss: 4774.5634765625\n",
      "Epoch 5258, Train_Loss: 4541.458984375, Val_Loss: 4774.36474609375\n",
      "Epoch 5259, Train_Loss: 4541.28076171875, Val_Loss: 4774.17578125\n",
      "Epoch 5260, Train_Loss: 4541.111328125, Val_Loss: 4773.97412109375\n",
      "Epoch 5261, Train_Loss: 4540.94970703125, Val_Loss: 4773.7841796875\n",
      "Epoch 5262, Train_Loss: 4540.7734375, Val_Loss: 4773.58740234375\n",
      "Epoch 5263, Train_Loss: 4540.6142578125, Val_Loss: 4773.40380859375\n",
      "Epoch 5264, Train_Loss: 4540.39794921875, Val_Loss: 4773.22412109375\n",
      "Epoch 5265, Train_Loss: 4540.24169921875, Val_Loss: 4773.02685546875\n",
      "Epoch 5266, Train_Loss: 4540.0908203125, Val_Loss: 4772.8388671875\n",
      "Epoch 5267, Train_Loss: 4539.7841796875, Val_Loss: 4772.626953125\n",
      "Epoch 5268, Train_Loss: 4539.62939453125, Val_Loss: 4772.44970703125\n",
      "Epoch 5269, Train_Loss: 4539.466796875, Val_Loss: 4772.2578125\n",
      "Epoch 5270, Train_Loss: 4539.3046875, Val_Loss: 4772.06982421875\n",
      "Epoch 5271, Train_Loss: 4539.15869140625, Val_Loss: 4771.87060546875\n",
      "Epoch 5272, Train_Loss: 4538.99169921875, Val_Loss: 4771.6865234375\n",
      "Epoch 5273, Train_Loss: 4538.828125, Val_Loss: 4771.50537109375\n",
      "Epoch 5274, Train_Loss: 4538.634765625, Val_Loss: 4771.3095703125\n",
      "Epoch 5275, Train_Loss: 4538.47216796875, Val_Loss: 4771.115234375\n",
      "Epoch 5276, Train_Loss: 4538.3173828125, Val_Loss: 4770.93115234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5277, Train_Loss: 4538.1162109375, Val_Loss: 4770.7314453125\n",
      "Epoch 5278, Train_Loss: 4537.9521484375, Val_Loss: 4770.5498046875\n",
      "Epoch 5279, Train_Loss: 4537.7939453125, Val_Loss: 4770.3525390625\n",
      "Epoch 5280, Train_Loss: 4537.63720703125, Val_Loss: 4770.16015625\n",
      "Epoch 5281, Train_Loss: 4537.48876953125, Val_Loss: 4769.95947265625\n",
      "Epoch 5282, Train_Loss: 4537.3271484375, Val_Loss: 4769.77783203125\n",
      "Epoch 5283, Train_Loss: 4537.1669921875, Val_Loss: 4769.5986328125\n",
      "Epoch 5284, Train_Loss: 4536.98095703125, Val_Loss: 4769.41796875\n",
      "Epoch 5285, Train_Loss: 4536.81640625, Val_Loss: 4769.2236328125\n",
      "Epoch 5286, Train_Loss: 4536.6533203125, Val_Loss: 4768.98828125\n",
      "Epoch 5287, Train_Loss: 4536.48486328125, Val_Loss: 4768.796875\n",
      "Epoch 5288, Train_Loss: 4536.33349609375, Val_Loss: 4768.607421875\n",
      "Epoch 5289, Train_Loss: 4536.15185546875, Val_Loss: 4768.4150390625\n",
      "Epoch 5290, Train_Loss: 4535.998046875, Val_Loss: 4768.224609375\n",
      "Epoch 5291, Train_Loss: 4535.8193359375, Val_Loss: 4768.044921875\n",
      "Epoch 5292, Train_Loss: 4535.6552734375, Val_Loss: 4767.8564453125\n",
      "Epoch 5293, Train_Loss: 4535.5107421875, Val_Loss: 4767.67041015625\n",
      "Epoch 5294, Train_Loss: 4535.37255859375, Val_Loss: 4767.46875\n",
      "Epoch 5295, Train_Loss: 4535.21142578125, Val_Loss: 4767.27880859375\n",
      "Epoch 5296, Train_Loss: 4535.04833984375, Val_Loss: 4767.0927734375\n",
      "Epoch 5297, Train_Loss: 4534.8896484375, Val_Loss: 4766.9130859375\n",
      "Epoch 5298, Train_Loss: 4534.71875, Val_Loss: 4766.7119140625\n",
      "Epoch 5299, Train_Loss: 4534.55517578125, Val_Loss: 4766.53271484375\n",
      "Epoch 5300, Train_Loss: 4534.38134765625, Val_Loss: 4766.3427734375\n",
      "Epoch 5301, Train_Loss: 4534.22607421875, Val_Loss: 4766.140625\n",
      "Epoch 5302, Train_Loss: 4534.0673828125, Val_Loss: 4765.9521484375\n",
      "Epoch 5303, Train_Loss: 4533.9072265625, Val_Loss: 4765.76220703125\n",
      "Epoch 5304, Train_Loss: 4533.701171875, Val_Loss: 4765.58251953125\n",
      "Epoch 5305, Train_Loss: 4533.54296875, Val_Loss: 4765.3740234375\n",
      "Epoch 5306, Train_Loss: 4533.3876953125, Val_Loss: 4765.19140625\n",
      "Epoch 5307, Train_Loss: 4533.23681640625, Val_Loss: 4764.9990234375\n",
      "Epoch 5308, Train_Loss: 4533.08154296875, Val_Loss: 4764.80859375\n",
      "Epoch 5309, Train_Loss: 4532.92041015625, Val_Loss: 4764.6240234375\n",
      "Epoch 5310, Train_Loss: 4532.76025390625, Val_Loss: 4764.43310546875\n",
      "Epoch 5311, Train_Loss: 4532.6123046875, Val_Loss: 4764.25341796875\n",
      "Epoch 5312, Train_Loss: 4532.453125, Val_Loss: 4764.05419921875\n",
      "Epoch 5313, Train_Loss: 4532.29541015625, Val_Loss: 4763.86181640625\n",
      "Epoch 5314, Train_Loss: 4532.1142578125, Val_Loss: 4763.64794921875\n",
      "Epoch 5315, Train_Loss: 4531.9462890625, Val_Loss: 4763.45751953125\n",
      "Epoch 5316, Train_Loss: 4531.7880859375, Val_Loss: 4763.26904296875\n",
      "Epoch 5317, Train_Loss: 4531.6064453125, Val_Loss: 4763.09423828125\n",
      "Epoch 5318, Train_Loss: 4531.4580078125, Val_Loss: 4762.91650390625\n",
      "Epoch 5319, Train_Loss: 4531.287109375, Val_Loss: 4762.72509765625\n",
      "Epoch 5320, Train_Loss: 4531.126953125, Val_Loss: 4762.53662109375\n",
      "Epoch 5321, Train_Loss: 4531.0185546875, Val_Loss: 4762.3369140625\n",
      "Epoch 5322, Train_Loss: 4530.8564453125, Val_Loss: 4762.14306640625\n",
      "Epoch 5323, Train_Loss: 4530.6982421875, Val_Loss: 4761.953125\n",
      "Epoch 5324, Train_Loss: 4530.51904296875, Val_Loss: 4761.74169921875\n",
      "Epoch 5325, Train_Loss: 4530.34130859375, Val_Loss: 4761.5546875\n",
      "Epoch 5326, Train_Loss: 4530.19775390625, Val_Loss: 4761.3642578125\n",
      "Epoch 5327, Train_Loss: 4530.04296875, Val_Loss: 4761.16943359375\n",
      "Epoch 5328, Train_Loss: 4529.87841796875, Val_Loss: 4760.98583984375\n",
      "Epoch 5329, Train_Loss: 4529.69873046875, Val_Loss: 4760.79248046875\n",
      "Epoch 5330, Train_Loss: 4529.51611328125, Val_Loss: 4760.60498046875\n",
      "Epoch 5331, Train_Loss: 4529.3515625, Val_Loss: 4760.4169921875\n",
      "Epoch 5332, Train_Loss: 4529.21142578125, Val_Loss: 4760.23681640625\n",
      "Epoch 5333, Train_Loss: 4528.99755859375, Val_Loss: 4760.04248046875\n",
      "Epoch 5334, Train_Loss: 4528.841796875, Val_Loss: 4759.8486328125\n",
      "Epoch 5335, Train_Loss: 4528.67431640625, Val_Loss: 4759.66259765625\n",
      "Epoch 5336, Train_Loss: 4528.515625, Val_Loss: 4759.47216796875\n",
      "Epoch 5337, Train_Loss: 4528.36572265625, Val_Loss: 4759.29736328125\n",
      "Epoch 5338, Train_Loss: 4528.2099609375, Val_Loss: 4759.0966796875\n",
      "Epoch 5339, Train_Loss: 4528.033203125, Val_Loss: 4758.9169921875\n",
      "Epoch 5340, Train_Loss: 4527.865234375, Val_Loss: 4758.71923828125\n",
      "Epoch 5341, Train_Loss: 4527.71728515625, Val_Loss: 4758.53369140625\n",
      "Epoch 5342, Train_Loss: 4527.48583984375, Val_Loss: 4758.3310546875\n",
      "Epoch 5343, Train_Loss: 4527.32958984375, Val_Loss: 4758.14794921875\n",
      "Epoch 5344, Train_Loss: 4527.12109375, Val_Loss: 4757.970703125\n",
      "Epoch 5345, Train_Loss: 4526.95751953125, Val_Loss: 4757.77294921875\n",
      "Epoch 5346, Train_Loss: 4526.79931640625, Val_Loss: 4757.58056640625\n",
      "Epoch 5347, Train_Loss: 4526.67578125, Val_Loss: 4757.38623046875\n",
      "Epoch 5348, Train_Loss: 4526.5224609375, Val_Loss: 4757.19921875\n",
      "Epoch 5349, Train_Loss: 4526.36669921875, Val_Loss: 4757.0146484375\n",
      "Epoch 5350, Train_Loss: 4526.21142578125, Val_Loss: 4756.837890625\n",
      "Epoch 5351, Train_Loss: 4526.06787109375, Val_Loss: 4756.6552734375\n",
      "Epoch 5352, Train_Loss: 4525.91259765625, Val_Loss: 4756.44580078125\n",
      "Epoch 5353, Train_Loss: 4525.73681640625, Val_Loss: 4756.2626953125\n",
      "Epoch 5354, Train_Loss: 4525.548828125, Val_Loss: 4756.05615234375\n",
      "Epoch 5355, Train_Loss: 4525.40673828125, Val_Loss: 4755.8759765625\n",
      "Epoch 5356, Train_Loss: 4525.251953125, Val_Loss: 4755.6904296875\n",
      "Epoch 5357, Train_Loss: 4525.03955078125, Val_Loss: 4755.50146484375\n",
      "Epoch 5358, Train_Loss: 4524.873046875, Val_Loss: 4755.32763671875\n",
      "Epoch 5359, Train_Loss: 4524.69921875, Val_Loss: 4755.1328125\n",
      "Epoch 5360, Train_Loss: 4524.55322265625, Val_Loss: 4754.923828125\n",
      "Epoch 5361, Train_Loss: 4524.392578125, Val_Loss: 4754.708984375\n",
      "Epoch 5362, Train_Loss: 4524.24072265625, Val_Loss: 4754.51806640625\n",
      "Epoch 5363, Train_Loss: 4524.08349609375, Val_Loss: 4754.33056640625\n",
      "Epoch 5364, Train_Loss: 4523.9013671875, Val_Loss: 4754.1591796875\n",
      "Epoch 5365, Train_Loss: 4523.72802734375, Val_Loss: 4753.9609375\n",
      "Epoch 5366, Train_Loss: 4523.576171875, Val_Loss: 4753.77587890625\n",
      "Epoch 5367, Train_Loss: 4523.42578125, Val_Loss: 4753.58203125\n",
      "Epoch 5368, Train_Loss: 4523.27734375, Val_Loss: 4753.384765625\n",
      "Epoch 5369, Train_Loss: 4523.10693359375, Val_Loss: 4753.19873046875\n",
      "Epoch 5370, Train_Loss: 4522.904296875, Val_Loss: 4753.02099609375\n",
      "Epoch 5371, Train_Loss: 4522.76220703125, Val_Loss: 4752.84130859375\n",
      "Epoch 5372, Train_Loss: 4522.6103515625, Val_Loss: 4752.64697265625\n",
      "Epoch 5373, Train_Loss: 4522.4501953125, Val_Loss: 4752.46044921875\n",
      "Epoch 5374, Train_Loss: 4522.2666015625, Val_Loss: 4752.2705078125\n",
      "Epoch 5375, Train_Loss: 4522.09814453125, Val_Loss: 4752.068359375\n",
      "Epoch 5376, Train_Loss: 4521.935546875, Val_Loss: 4751.89111328125\n",
      "Epoch 5377, Train_Loss: 4521.7958984375, Val_Loss: 4751.7099609375\n",
      "Epoch 5378, Train_Loss: 4521.6376953125, Val_Loss: 4751.52978515625\n",
      "Epoch 5379, Train_Loss: 4521.474609375, Val_Loss: 4751.337890625\n",
      "Epoch 5380, Train_Loss: 4521.3193359375, Val_Loss: 4751.1318359375\n",
      "Epoch 5381, Train_Loss: 4521.16064453125, Val_Loss: 4750.9423828125\n",
      "Epoch 5382, Train_Loss: 4521.00927734375, Val_Loss: 4750.75048828125\n",
      "Epoch 5383, Train_Loss: 4520.84765625, Val_Loss: 4750.560546875\n",
      "Epoch 5384, Train_Loss: 4520.64306640625, Val_Loss: 4750.3837890625\n",
      "Epoch 5385, Train_Loss: 4520.4912109375, Val_Loss: 4750.20263671875\n",
      "Epoch 5386, Train_Loss: 4520.34130859375, Val_Loss: 4750.0205078125\n",
      "Epoch 5387, Train_Loss: 4520.1904296875, Val_Loss: 4749.81298828125\n",
      "Epoch 5388, Train_Loss: 4520.03564453125, Val_Loss: 4749.62646484375\n",
      "Epoch 5389, Train_Loss: 4519.859375, Val_Loss: 4749.42138671875\n",
      "Epoch 5390, Train_Loss: 4519.68798828125, Val_Loss: 4749.2412109375\n",
      "Epoch 5391, Train_Loss: 4519.53662109375, Val_Loss: 4749.06103515625\n",
      "Epoch 5392, Train_Loss: 4519.388671875, Val_Loss: 4748.87939453125\n",
      "Epoch 5393, Train_Loss: 4519.23779296875, Val_Loss: 4748.6845703125\n",
      "Epoch 5394, Train_Loss: 4519.05029296875, Val_Loss: 4748.48193359375\n",
      "Epoch 5395, Train_Loss: 4518.89892578125, Val_Loss: 4748.29638671875\n",
      "Epoch 5396, Train_Loss: 4518.74755859375, Val_Loss: 4748.11767578125\n",
      "Epoch 5397, Train_Loss: 4518.55517578125, Val_Loss: 4747.93798828125\n",
      "Epoch 5398, Train_Loss: 4518.39013671875, Val_Loss: 4747.7509765625\n",
      "Epoch 5399, Train_Loss: 4518.21826171875, Val_Loss: 4747.52685546875\n",
      "Epoch 5400, Train_Loss: 4518.103515625, Val_Loss: 4747.32861328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5401, Train_Loss: 4517.94189453125, Val_Loss: 4747.140625\n",
      "Epoch 5402, Train_Loss: 4517.79248046875, Val_Loss: 4746.94921875\n",
      "Epoch 5403, Train_Loss: 4517.6494140625, Val_Loss: 4746.76904296875\n",
      "Epoch 5404, Train_Loss: 4517.4697265625, Val_Loss: 4746.59033203125\n",
      "Epoch 5405, Train_Loss: 4517.31640625, Val_Loss: 4746.408203125\n",
      "Epoch 5406, Train_Loss: 4517.16162109375, Val_Loss: 4746.22509765625\n",
      "Epoch 5407, Train_Loss: 4517.0107421875, Val_Loss: 4746.02783203125\n",
      "Epoch 5408, Train_Loss: 4516.82958984375, Val_Loss: 4745.82958984375\n",
      "Epoch 5409, Train_Loss: 4516.66064453125, Val_Loss: 4745.65380859375\n",
      "Epoch 5410, Train_Loss: 4516.45263671875, Val_Loss: 4745.47705078125\n",
      "Epoch 5411, Train_Loss: 4516.29345703125, Val_Loss: 4745.287109375\n",
      "Epoch 5412, Train_Loss: 4516.14306640625, Val_Loss: 4745.1015625\n",
      "Epoch 5413, Train_Loss: 4515.99658203125, Val_Loss: 4744.919921875\n",
      "Epoch 5414, Train_Loss: 4515.82666015625, Val_Loss: 4744.70751953125\n",
      "Epoch 5415, Train_Loss: 4515.66650390625, Val_Loss: 4744.54052734375\n",
      "Epoch 5416, Train_Loss: 4515.50830078125, Val_Loss: 4744.34228515625\n",
      "Epoch 5417, Train_Loss: 4515.2998046875, Val_Loss: 4744.1591796875\n",
      "Epoch 5418, Train_Loss: 4515.140625, Val_Loss: 4743.9794921875\n",
      "Epoch 5419, Train_Loss: 4514.96728515625, Val_Loss: 4743.78515625\n",
      "Epoch 5420, Train_Loss: 4514.81396484375, Val_Loss: 4743.59228515625\n",
      "Epoch 5421, Train_Loss: 4514.65087890625, Val_Loss: 4743.41015625\n",
      "Epoch 5422, Train_Loss: 4514.49560546875, Val_Loss: 4743.224609375\n",
      "Epoch 5423, Train_Loss: 4514.34375, Val_Loss: 4743.037109375\n",
      "Epoch 5424, Train_Loss: 4514.1416015625, Val_Loss: 4742.85400390625\n",
      "Epoch 5425, Train_Loss: 4513.99609375, Val_Loss: 4742.66357421875\n",
      "Epoch 5426, Train_Loss: 4513.84423828125, Val_Loss: 4742.48193359375\n",
      "Epoch 5427, Train_Loss: 4513.71923828125, Val_Loss: 4742.26416015625\n",
      "Epoch 5428, Train_Loss: 4513.56689453125, Val_Loss: 4742.08251953125\n",
      "Epoch 5429, Train_Loss: 4513.400390625, Val_Loss: 4741.8974609375\n",
      "Epoch 5430, Train_Loss: 4513.25732421875, Val_Loss: 4741.724609375\n",
      "Epoch 5431, Train_Loss: 4513.09521484375, Val_Loss: 4741.5322265625\n",
      "Epoch 5432, Train_Loss: 4512.939453125, Val_Loss: 4741.341796875\n",
      "Epoch 5433, Train_Loss: 4512.77734375, Val_Loss: 4741.150390625\n",
      "Epoch 5434, Train_Loss: 4512.58544921875, Val_Loss: 4740.95703125\n",
      "Epoch 5435, Train_Loss: 4512.42578125, Val_Loss: 4740.78173828125\n",
      "Epoch 5436, Train_Loss: 4512.2529296875, Val_Loss: 4740.55859375\n",
      "Epoch 5437, Train_Loss: 4512.076171875, Val_Loss: 4740.38818359375\n",
      "Epoch 5438, Train_Loss: 4511.939453125, Val_Loss: 4740.19970703125\n",
      "Epoch 5439, Train_Loss: 4511.77099609375, Val_Loss: 4740.02099609375\n",
      "Epoch 5440, Train_Loss: 4511.6298828125, Val_Loss: 4739.81982421875\n",
      "Epoch 5441, Train_Loss: 4511.46875, Val_Loss: 4739.63037109375\n",
      "Epoch 5442, Train_Loss: 4511.32568359375, Val_Loss: 4739.44384765625\n",
      "Epoch 5443, Train_Loss: 4511.17822265625, Val_Loss: 4739.26806640625\n",
      "Epoch 5444, Train_Loss: 4511.00830078125, Val_Loss: 4739.0849609375\n",
      "Epoch 5445, Train_Loss: 4510.86376953125, Val_Loss: 4738.8935546875\n",
      "Epoch 5446, Train_Loss: 4510.693359375, Val_Loss: 4738.70263671875\n",
      "Epoch 5447, Train_Loss: 4510.5400390625, Val_Loss: 4738.51123046875\n",
      "Epoch 5448, Train_Loss: 4510.38330078125, Val_Loss: 4738.318359375\n",
      "Epoch 5449, Train_Loss: 4510.224609375, Val_Loss: 4738.14306640625\n",
      "Epoch 5450, Train_Loss: 4510.0166015625, Val_Loss: 4737.9697265625\n",
      "Epoch 5451, Train_Loss: 4509.86279296875, Val_Loss: 4737.78515625\n",
      "Epoch 5452, Train_Loss: 4509.720703125, Val_Loss: 4737.60009765625\n",
      "Epoch 5453, Train_Loss: 4509.60302734375, Val_Loss: 4737.39501953125\n",
      "Epoch 5454, Train_Loss: 4509.4404296875, Val_Loss: 4737.20849609375\n",
      "Epoch 5455, Train_Loss: 4509.26123046875, Val_Loss: 4737.01708984375\n",
      "Epoch 5456, Train_Loss: 4509.1083984375, Val_Loss: 4736.82275390625\n",
      "Epoch 5457, Train_Loss: 4508.94921875, Val_Loss: 4736.6572265625\n",
      "Epoch 5458, Train_Loss: 4508.80908203125, Val_Loss: 4736.462890625\n",
      "Epoch 5459, Train_Loss: 4508.65625, Val_Loss: 4736.27783203125\n",
      "Epoch 5460, Train_Loss: 4508.484375, Val_Loss: 4736.0830078125\n",
      "Epoch 5461, Train_Loss: 4508.337890625, Val_Loss: 4735.90185546875\n",
      "Epoch 5462, Train_Loss: 4508.173828125, Val_Loss: 4735.72216796875\n",
      "Epoch 5463, Train_Loss: 4507.94970703125, Val_Loss: 4735.5498046875\n",
      "Epoch 5464, Train_Loss: 4507.802734375, Val_Loss: 4735.34326171875\n",
      "Epoch 5465, Train_Loss: 4507.65087890625, Val_Loss: 4735.162109375\n",
      "Epoch 5466, Train_Loss: 4507.498046875, Val_Loss: 4734.9658203125\n",
      "Epoch 5467, Train_Loss: 4507.3505859375, Val_Loss: 4734.77734375\n",
      "Epoch 5468, Train_Loss: 4507.18798828125, Val_Loss: 4734.59228515625\n",
      "Epoch 5469, Train_Loss: 4507.0263671875, Val_Loss: 4734.41455078125\n",
      "Epoch 5470, Train_Loss: 4506.86572265625, Val_Loss: 4734.22607421875\n",
      "Epoch 5471, Train_Loss: 4506.71337890625, Val_Loss: 4734.05419921875\n",
      "Epoch 5472, Train_Loss: 4506.5576171875, Val_Loss: 4733.86767578125\n",
      "Epoch 5473, Train_Loss: 4506.4033203125, Val_Loss: 4733.66259765625\n",
      "Epoch 5474, Train_Loss: 4506.240234375, Val_Loss: 4733.443359375\n",
      "Epoch 5475, Train_Loss: 4506.06298828125, Val_Loss: 4733.25634765625\n",
      "Epoch 5476, Train_Loss: 4505.90966796875, Val_Loss: 4733.0732421875\n",
      "Epoch 5477, Train_Loss: 4505.7158203125, Val_Loss: 4732.90185546875\n",
      "Epoch 5478, Train_Loss: 4505.57470703125, Val_Loss: 4732.70556640625\n",
      "Epoch 5479, Train_Loss: 4505.40771484375, Val_Loss: 4732.5244140625\n",
      "Epoch 5480, Train_Loss: 4505.26318359375, Val_Loss: 4732.33154296875\n",
      "Epoch 5481, Train_Loss: 4505.1162109375, Val_Loss: 4732.1455078125\n",
      "Epoch 5482, Train_Loss: 4504.962890625, Val_Loss: 4731.958984375\n",
      "Epoch 5483, Train_Loss: 4504.73046875, Val_Loss: 4731.77734375\n",
      "Epoch 5484, Train_Loss: 4504.57763671875, Val_Loss: 4731.58203125\n",
      "Epoch 5485, Train_Loss: 4504.40869140625, Val_Loss: 4731.40380859375\n",
      "Epoch 5486, Train_Loss: 4504.2490234375, Val_Loss: 4731.23095703125\n",
      "Epoch 5487, Train_Loss: 4504.10693359375, Val_Loss: 4731.02880859375\n",
      "Epoch 5488, Train_Loss: 4503.94970703125, Val_Loss: 4730.84375\n",
      "Epoch 5489, Train_Loss: 4503.80126953125, Val_Loss: 4730.6552734375\n",
      "Epoch 5490, Train_Loss: 4503.58984375, Val_Loss: 4730.47509765625\n",
      "Epoch 5491, Train_Loss: 4503.43017578125, Val_Loss: 4730.2958984375\n",
      "Epoch 5492, Train_Loss: 4503.22119140625, Val_Loss: 4730.115234375\n",
      "Epoch 5493, Train_Loss: 4503.07958984375, Val_Loss: 4729.91552734375\n",
      "Epoch 5494, Train_Loss: 4502.93798828125, Val_Loss: 4729.72607421875\n",
      "Epoch 5495, Train_Loss: 4502.75830078125, Val_Loss: 4729.53564453125\n",
      "Epoch 5496, Train_Loss: 4502.603515625, Val_Loss: 4729.359375\n",
      "Epoch 5497, Train_Loss: 4502.453125, Val_Loss: 4729.18896484375\n",
      "Epoch 5498, Train_Loss: 4502.302734375, Val_Loss: 4728.99658203125\n",
      "Epoch 5499, Train_Loss: 4502.15771484375, Val_Loss: 4728.8095703125\n",
      "Epoch 5500, Train_Loss: 4501.99658203125, Val_Loss: 4728.62255859375\n",
      "Epoch 5501, Train_Loss: 4501.84521484375, Val_Loss: 4728.43408203125\n",
      "Epoch 5502, Train_Loss: 4501.6796875, Val_Loss: 4728.23291015625\n",
      "Epoch 5503, Train_Loss: 4501.50244140625, Val_Loss: 4728.0673828125\n",
      "Epoch 5504, Train_Loss: 4501.34814453125, Val_Loss: 4727.88818359375\n",
      "Epoch 5505, Train_Loss: 4501.17626953125, Val_Loss: 4727.69482421875\n",
      "Epoch 5506, Train_Loss: 4501.01416015625, Val_Loss: 4727.49951171875\n",
      "Epoch 5507, Train_Loss: 4500.892578125, Val_Loss: 4727.30126953125\n",
      "Epoch 5508, Train_Loss: 4500.73681640625, Val_Loss: 4727.126953125\n",
      "Epoch 5509, Train_Loss: 4500.5869140625, Val_Loss: 4726.9384765625\n",
      "Epoch 5510, Train_Loss: 4500.44140625, Val_Loss: 4726.765625\n",
      "Epoch 5511, Train_Loss: 4500.2783203125, Val_Loss: 4726.55419921875\n",
      "Epoch 5512, Train_Loss: 4500.12060546875, Val_Loss: 4726.36328125\n",
      "Epoch 5513, Train_Loss: 4499.974609375, Val_Loss: 4726.17578125\n",
      "Epoch 5514, Train_Loss: 4499.8154296875, Val_Loss: 4725.99072265625\n",
      "Epoch 5515, Train_Loss: 4499.63232421875, Val_Loss: 4725.79833984375\n",
      "Epoch 5516, Train_Loss: 4499.48095703125, Val_Loss: 4725.6171875\n",
      "Epoch 5517, Train_Loss: 4499.271484375, Val_Loss: 4725.44482421875\n",
      "Epoch 5518, Train_Loss: 4499.1171875, Val_Loss: 4725.2578125\n",
      "Epoch 5519, Train_Loss: 4498.96337890625, Val_Loss: 4725.06982421875\n",
      "Epoch 5520, Train_Loss: 4498.7998046875, Val_Loss: 4724.876953125\n",
      "Epoch 5521, Train_Loss: 4498.65234375, Val_Loss: 4724.7001953125\n",
      "Epoch 5522, Train_Loss: 4498.5087890625, Val_Loss: 4724.51025390625\n",
      "Epoch 5523, Train_Loss: 4498.34912109375, Val_Loss: 4724.333984375\n",
      "Epoch 5524, Train_Loss: 4498.20263671875, Val_Loss: 4724.1455078125\n",
      "Epoch 5525, Train_Loss: 4498.0126953125, Val_Loss: 4723.9619140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5526, Train_Loss: 4497.85595703125, Val_Loss: 4723.77978515625\n",
      "Epoch 5527, Train_Loss: 4497.70068359375, Val_Loss: 4723.58056640625\n",
      "Epoch 5528, Train_Loss: 4497.552734375, Val_Loss: 4723.4072265625\n",
      "Epoch 5529, Train_Loss: 4497.41552734375, Val_Loss: 4723.21337890625\n",
      "Epoch 5530, Train_Loss: 4497.21923828125, Val_Loss: 4723.04248046875\n",
      "Epoch 5531, Train_Loss: 4497.064453125, Val_Loss: 4722.8505859375\n",
      "Epoch 5532, Train_Loss: 4496.921875, Val_Loss: 4722.66650390625\n",
      "Epoch 5533, Train_Loss: 4496.7998046875, Val_Loss: 4722.46923828125\n",
      "Epoch 5534, Train_Loss: 4496.63525390625, Val_Loss: 4722.2734375\n",
      "Epoch 5535, Train_Loss: 4496.46484375, Val_Loss: 4722.09423828125\n",
      "Epoch 5536, Train_Loss: 4496.31640625, Val_Loss: 4721.91015625\n",
      "Epoch 5537, Train_Loss: 4496.1650390625, Val_Loss: 4721.740234375\n",
      "Epoch 5538, Train_Loss: 4496.01611328125, Val_Loss: 4721.55126953125\n",
      "Epoch 5539, Train_Loss: 4495.8642578125, Val_Loss: 4721.3623046875\n",
      "Epoch 5540, Train_Loss: 4495.69970703125, Val_Loss: 4721.16015625\n",
      "Epoch 5541, Train_Loss: 4495.5546875, Val_Loss: 4720.9873046875\n",
      "Epoch 5542, Train_Loss: 4495.3974609375, Val_Loss: 4720.802734375\n",
      "Epoch 5543, Train_Loss: 4495.22705078125, Val_Loss: 4720.63037109375\n",
      "Epoch 5544, Train_Loss: 4495.08837890625, Val_Loss: 4720.44775390625\n",
      "Epoch 5545, Train_Loss: 4494.93359375, Val_Loss: 4720.25048828125\n",
      "Epoch 5546, Train_Loss: 4494.77783203125, Val_Loss: 4720.0751953125\n",
      "Epoch 5547, Train_Loss: 4494.640625, Val_Loss: 4719.8828125\n",
      "Epoch 5548, Train_Loss: 4494.4970703125, Val_Loss: 4719.6982421875\n",
      "Epoch 5549, Train_Loss: 4494.3447265625, Val_Loss: 4719.47900390625\n",
      "Epoch 5550, Train_Loss: 4494.18017578125, Val_Loss: 4719.2998046875\n",
      "Epoch 5551, Train_Loss: 4494.03564453125, Val_Loss: 4719.11181640625\n",
      "Epoch 5552, Train_Loss: 4493.8828125, Val_Loss: 4718.94189453125\n",
      "Epoch 5553, Train_Loss: 4493.7353515625, Val_Loss: 4718.74462890625\n",
      "Epoch 5554, Train_Loss: 4493.59375, Val_Loss: 4718.5517578125\n",
      "Epoch 5555, Train_Loss: 4493.40283203125, Val_Loss: 4718.3798828125\n",
      "Epoch 5556, Train_Loss: 4493.24365234375, Val_Loss: 4718.18701171875\n",
      "Epoch 5557, Train_Loss: 4493.0556640625, Val_Loss: 4718.00439453125\n",
      "Epoch 5558, Train_Loss: 4492.880859375, Val_Loss: 4717.830078125\n",
      "Epoch 5559, Train_Loss: 4492.71435546875, Val_Loss: 4717.646484375\n",
      "Epoch 5560, Train_Loss: 4492.58349609375, Val_Loss: 4717.45703125\n",
      "Epoch 5561, Train_Loss: 4492.4267578125, Val_Loss: 4717.2783203125\n",
      "Epoch 5562, Train_Loss: 4492.263671875, Val_Loss: 4717.087890625\n",
      "Epoch 5563, Train_Loss: 4492.12255859375, Val_Loss: 4716.91796875\n",
      "Epoch 5564, Train_Loss: 4491.98095703125, Val_Loss: 4716.72900390625\n",
      "Epoch 5565, Train_Loss: 4491.8095703125, Val_Loss: 4716.548828125\n",
      "Epoch 5566, Train_Loss: 4491.64404296875, Val_Loss: 4716.3603515625\n",
      "Epoch 5567, Train_Loss: 4491.4814453125, Val_Loss: 4716.1689453125\n",
      "Epoch 5568, Train_Loss: 4491.2060546875, Val_Loss: 4715.98046875\n",
      "Epoch 5569, Train_Loss: 4491.06396484375, Val_Loss: 4715.79296875\n",
      "Epoch 5570, Train_Loss: 4490.8349609375, Val_Loss: 4715.625\n",
      "Epoch 5571, Train_Loss: 4490.69384765625, Val_Loss: 4715.44580078125\n",
      "Epoch 5572, Train_Loss: 4490.55078125, Val_Loss: 4715.2578125\n",
      "Epoch 5573, Train_Loss: 4490.4072265625, Val_Loss: 4715.06494140625\n",
      "Epoch 5574, Train_Loss: 4490.279296875, Val_Loss: 4714.875\n",
      "Epoch 5575, Train_Loss: 4490.1279296875, Val_Loss: 4714.69970703125\n",
      "Epoch 5576, Train_Loss: 4489.958984375, Val_Loss: 4714.51806640625\n",
      "Epoch 5577, Train_Loss: 4489.8017578125, Val_Loss: 4714.328125\n",
      "Epoch 5578, Train_Loss: 4489.65234375, Val_Loss: 4714.1484375\n",
      "Epoch 5579, Train_Loss: 4489.494140625, Val_Loss: 4713.95703125\n",
      "Epoch 5580, Train_Loss: 4489.3427734375, Val_Loss: 4713.759765625\n",
      "Epoch 5581, Train_Loss: 4489.171875, Val_Loss: 4713.580078125\n",
      "Epoch 5582, Train_Loss: 4489.0048828125, Val_Loss: 4713.400390625\n",
      "Epoch 5583, Train_Loss: 4488.8359375, Val_Loss: 4713.23046875\n",
      "Epoch 5584, Train_Loss: 4488.6767578125, Val_Loss: 4713.04296875\n",
      "Epoch 5585, Train_Loss: 4488.5380859375, Val_Loss: 4712.85693359375\n",
      "Epoch 5586, Train_Loss: 4488.33984375, Val_Loss: 4712.63818359375\n",
      "Epoch 5587, Train_Loss: 4488.1875, Val_Loss: 4712.43994140625\n",
      "Epoch 5588, Train_Loss: 4488.04150390625, Val_Loss: 4712.26416015625\n",
      "Epoch 5589, Train_Loss: 4487.89208984375, Val_Loss: 4712.078125\n",
      "Epoch 5590, Train_Loss: 4487.748046875, Val_Loss: 4711.91357421875\n",
      "Epoch 5591, Train_Loss: 4487.59521484375, Val_Loss: 4711.73291015625\n",
      "Epoch 5592, Train_Loss: 4487.4404296875, Val_Loss: 4711.541015625\n",
      "Epoch 5593, Train_Loss: 4487.29638671875, Val_Loss: 4711.34765625\n",
      "Epoch 5594, Train_Loss: 4487.1435546875, Val_Loss: 4711.15576171875\n",
      "Epoch 5595, Train_Loss: 4486.98828125, Val_Loss: 4710.98486328125\n",
      "Epoch 5596, Train_Loss: 4486.79931640625, Val_Loss: 4710.7939453125\n",
      "Epoch 5597, Train_Loss: 4486.60791015625, Val_Loss: 4710.62255859375\n",
      "Epoch 5598, Train_Loss: 4486.47021484375, Val_Loss: 4710.435546875\n",
      "Epoch 5599, Train_Loss: 4486.31982421875, Val_Loss: 4710.25927734375\n",
      "Epoch 5600, Train_Loss: 4486.18017578125, Val_Loss: 4710.06103515625\n",
      "Epoch 5601, Train_Loss: 4486.0126953125, Val_Loss: 4709.8818359375\n",
      "Epoch 5602, Train_Loss: 4485.861328125, Val_Loss: 4709.71484375\n",
      "Epoch 5603, Train_Loss: 4485.705078125, Val_Loss: 4709.53076171875\n",
      "Epoch 5604, Train_Loss: 4485.56103515625, Val_Loss: 4709.3486328125\n",
      "Epoch 5605, Train_Loss: 4485.404296875, Val_Loss: 4709.16455078125\n",
      "Epoch 5606, Train_Loss: 4485.23193359375, Val_Loss: 4708.974609375\n",
      "Epoch 5607, Train_Loss: 4485.07958984375, Val_Loss: 4708.7724609375\n",
      "Epoch 5608, Train_Loss: 4484.9375, Val_Loss: 4708.60009765625\n",
      "Epoch 5609, Train_Loss: 4484.802734375, Val_Loss: 4708.42578125\n",
      "Epoch 5610, Train_Loss: 4484.62939453125, Val_Loss: 4708.23974609375\n",
      "Epoch 5611, Train_Loss: 4484.44921875, Val_Loss: 4708.0537109375\n",
      "Epoch 5612, Train_Loss: 4484.298828125, Val_Loss: 4707.87158203125\n",
      "Epoch 5613, Train_Loss: 4484.17919921875, Val_Loss: 4707.6748046875\n",
      "Epoch 5614, Train_Loss: 4484.02880859375, Val_Loss: 4707.48583984375\n",
      "Epoch 5615, Train_Loss: 4483.87646484375, Val_Loss: 4707.30322265625\n",
      "Epoch 5616, Train_Loss: 4483.69482421875, Val_Loss: 4707.12353515625\n",
      "Epoch 5617, Train_Loss: 4483.544921875, Val_Loss: 4706.9423828125\n",
      "Epoch 5618, Train_Loss: 4483.39697265625, Val_Loss: 4706.75439453125\n",
      "Epoch 5619, Train_Loss: 4483.26513671875, Val_Loss: 4706.5771484375\n",
      "Epoch 5620, Train_Loss: 4483.13134765625, Val_Loss: 4706.3828125\n",
      "Epoch 5621, Train_Loss: 4482.96728515625, Val_Loss: 4706.19677734375\n",
      "Epoch 5622, Train_Loss: 4482.81298828125, Val_Loss: 4706.009765625\n",
      "Epoch 5623, Train_Loss: 4482.619140625, Val_Loss: 4705.85498046875\n",
      "Epoch 5624, Train_Loss: 4482.46484375, Val_Loss: 4705.6357421875\n",
      "Epoch 5625, Train_Loss: 4482.3134765625, Val_Loss: 4705.4501953125\n",
      "Epoch 5626, Train_Loss: 4482.1572265625, Val_Loss: 4705.27099609375\n",
      "Epoch 5627, Train_Loss: 4482.00537109375, Val_Loss: 4705.0712890625\n",
      "Epoch 5628, Train_Loss: 4481.86865234375, Val_Loss: 4704.8896484375\n",
      "Epoch 5629, Train_Loss: 4481.72119140625, Val_Loss: 4704.7060546875\n",
      "Epoch 5630, Train_Loss: 4481.556640625, Val_Loss: 4704.53271484375\n",
      "Epoch 5631, Train_Loss: 4481.39990234375, Val_Loss: 4704.35693359375\n",
      "Epoch 5632, Train_Loss: 4481.25830078125, Val_Loss: 4704.16796875\n",
      "Epoch 5633, Train_Loss: 4481.1044921875, Val_Loss: 4703.97412109375\n",
      "Epoch 5634, Train_Loss: 4480.958984375, Val_Loss: 4703.79541015625\n",
      "Epoch 5635, Train_Loss: 4480.79931640625, Val_Loss: 4703.61083984375\n",
      "Epoch 5636, Train_Loss: 4480.6220703125, Val_Loss: 4703.43896484375\n",
      "Epoch 5637, Train_Loss: 4480.42919921875, Val_Loss: 4703.26025390625\n",
      "Epoch 5638, Train_Loss: 4480.283203125, Val_Loss: 4703.0771484375\n",
      "Epoch 5639, Train_Loss: 4480.12939453125, Val_Loss: 4702.8984375\n",
      "Epoch 5640, Train_Loss: 4480.01708984375, Val_Loss: 4702.70263671875\n",
      "Epoch 5641, Train_Loss: 4479.85400390625, Val_Loss: 4702.51416015625\n",
      "Epoch 5642, Train_Loss: 4479.69677734375, Val_Loss: 4702.3369140625\n",
      "Epoch 5643, Train_Loss: 4479.47802734375, Val_Loss: 4702.15380859375\n",
      "Epoch 5644, Train_Loss: 4479.33203125, Val_Loss: 4701.98046875\n",
      "Epoch 5645, Train_Loss: 4479.18505859375, Val_Loss: 4701.79931640625\n",
      "Epoch 5646, Train_Loss: 4479.0185546875, Val_Loss: 4701.6083984375\n",
      "Epoch 5647, Train_Loss: 4478.85693359375, Val_Loss: 4701.423828125\n",
      "Epoch 5648, Train_Loss: 4478.6953125, Val_Loss: 4701.2490234375\n",
      "Epoch 5649, Train_Loss: 4478.54833984375, Val_Loss: 4701.0615234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5650, Train_Loss: 4478.380859375, Val_Loss: 4700.90087890625\n",
      "Epoch 5651, Train_Loss: 4478.2294921875, Val_Loss: 4700.71533203125\n",
      "Epoch 5652, Train_Loss: 4478.0966796875, Val_Loss: 4700.50830078125\n",
      "Epoch 5653, Train_Loss: 4477.96044921875, Val_Loss: 4700.30712890625\n",
      "Epoch 5654, Train_Loss: 4477.8017578125, Val_Loss: 4700.13232421875\n",
      "Epoch 5655, Train_Loss: 4477.65771484375, Val_Loss: 4699.95556640625\n",
      "Epoch 5656, Train_Loss: 4477.48046875, Val_Loss: 4699.77099609375\n",
      "Epoch 5657, Train_Loss: 4477.3349609375, Val_Loss: 4699.6064453125\n",
      "Epoch 5658, Train_Loss: 4477.189453125, Val_Loss: 4699.42236328125\n",
      "Epoch 5659, Train_Loss: 4477.04296875, Val_Loss: 4699.248046875\n",
      "Epoch 5660, Train_Loss: 4476.90283203125, Val_Loss: 4699.044921875\n",
      "Epoch 5661, Train_Loss: 4476.73046875, Val_Loss: 4698.87060546875\n",
      "Epoch 5662, Train_Loss: 4476.57666015625, Val_Loss: 4698.6484375\n",
      "Epoch 5663, Train_Loss: 4476.3818359375, Val_Loss: 4698.4677734375\n",
      "Epoch 5664, Train_Loss: 4476.2353515625, Val_Loss: 4698.28955078125\n",
      "Epoch 5665, Train_Loss: 4476.08544921875, Val_Loss: 4698.11572265625\n",
      "Epoch 5666, Train_Loss: 4475.9248046875, Val_Loss: 4697.93603515625\n",
      "Epoch 5667, Train_Loss: 4475.82958984375, Val_Loss: 4697.7509765625\n",
      "Epoch 5668, Train_Loss: 4475.67822265625, Val_Loss: 4697.55712890625\n",
      "Epoch 5669, Train_Loss: 4475.5341796875, Val_Loss: 4697.384765625\n",
      "Epoch 5670, Train_Loss: 4475.37548828125, Val_Loss: 4697.2021484375\n",
      "Epoch 5671, Train_Loss: 4475.220703125, Val_Loss: 4697.02099609375\n",
      "Epoch 5672, Train_Loss: 4475.05908203125, Val_Loss: 4696.84619140625\n",
      "Epoch 5673, Train_Loss: 4474.8984375, Val_Loss: 4696.64306640625\n",
      "Epoch 5674, Train_Loss: 4474.763671875, Val_Loss: 4696.462890625\n",
      "Epoch 5675, Train_Loss: 4474.6240234375, Val_Loss: 4696.27978515625\n",
      "Epoch 5676, Train_Loss: 4474.44677734375, Val_Loss: 4696.09912109375\n",
      "Epoch 5677, Train_Loss: 4474.2470703125, Val_Loss: 4695.935546875\n",
      "Epoch 5678, Train_Loss: 4474.095703125, Val_Loss: 4695.73486328125\n",
      "Epoch 5679, Train_Loss: 4473.955078125, Val_Loss: 4695.56689453125\n",
      "Epoch 5680, Train_Loss: 4473.81689453125, Val_Loss: 4695.357421875\n",
      "Epoch 5681, Train_Loss: 4473.6455078125, Val_Loss: 4695.19091796875\n",
      "Epoch 5682, Train_Loss: 4473.48046875, Val_Loss: 4695.00341796875\n",
      "Epoch 5683, Train_Loss: 4473.3388671875, Val_Loss: 4694.82958984375\n",
      "Epoch 5684, Train_Loss: 4473.1923828125, Val_Loss: 4694.64794921875\n",
      "Epoch 5685, Train_Loss: 4473.04248046875, Val_Loss: 4694.45849609375\n",
      "Epoch 5686, Train_Loss: 4472.8681640625, Val_Loss: 4694.2822265625\n",
      "Epoch 5687, Train_Loss: 4472.7216796875, Val_Loss: 4694.091796875\n",
      "Epoch 5688, Train_Loss: 4472.56982421875, Val_Loss: 4693.9150390625\n",
      "Epoch 5689, Train_Loss: 4472.423828125, Val_Loss: 4693.73876953125\n",
      "Epoch 5690, Train_Loss: 4472.2255859375, Val_Loss: 4693.5400390625\n",
      "Epoch 5691, Train_Loss: 4472.06396484375, Val_Loss: 4693.3623046875\n",
      "Epoch 5692, Train_Loss: 4471.93212890625, Val_Loss: 4693.18359375\n",
      "Epoch 5693, Train_Loss: 4471.78466796875, Val_Loss: 4693.0068359375\n",
      "Epoch 5694, Train_Loss: 4471.63623046875, Val_Loss: 4692.8154296875\n",
      "Epoch 5695, Train_Loss: 4471.47705078125, Val_Loss: 4692.623046875\n",
      "Epoch 5696, Train_Loss: 4471.32958984375, Val_Loss: 4692.45703125\n",
      "Epoch 5697, Train_Loss: 4471.16650390625, Val_Loss: 4692.27587890625\n",
      "Epoch 5698, Train_Loss: 4471.015625, Val_Loss: 4692.107421875\n",
      "Epoch 5699, Train_Loss: 4470.869140625, Val_Loss: 4691.87939453125\n",
      "Epoch 5700, Train_Loss: 4470.7265625, Val_Loss: 4691.6826171875\n",
      "Epoch 5701, Train_Loss: 4470.58837890625, Val_Loss: 4691.50537109375\n",
      "Epoch 5702, Train_Loss: 4470.42333984375, Val_Loss: 4691.33251953125\n",
      "Epoch 5703, Train_Loss: 4470.23486328125, Val_Loss: 4691.1611328125\n",
      "Epoch 5704, Train_Loss: 4470.07763671875, Val_Loss: 4690.98486328125\n",
      "Epoch 5705, Train_Loss: 4469.93212890625, Val_Loss: 4690.7939453125\n",
      "Epoch 5706, Train_Loss: 4469.78857421875, Val_Loss: 4690.61181640625\n",
      "Epoch 5707, Train_Loss: 4469.646484375, Val_Loss: 4690.42626953125\n",
      "Epoch 5708, Train_Loss: 4469.4873046875, Val_Loss: 4690.2490234375\n",
      "Epoch 5709, Train_Loss: 4469.3212890625, Val_Loss: 4690.0654296875\n",
      "Epoch 5710, Train_Loss: 4469.1796875, Val_Loss: 4689.88671875\n",
      "Epoch 5711, Train_Loss: 4469.0283203125, Val_Loss: 4689.71875\n",
      "Epoch 5712, Train_Loss: 4468.86181640625, Val_Loss: 4689.53857421875\n",
      "Epoch 5713, Train_Loss: 4468.71044921875, Val_Loss: 4689.3564453125\n",
      "Epoch 5714, Train_Loss: 4468.55712890625, Val_Loss: 4689.1669921875\n",
      "Epoch 5715, Train_Loss: 4468.40478515625, Val_Loss: 4688.990234375\n",
      "Epoch 5716, Train_Loss: 4468.2529296875, Val_Loss: 4688.79931640625\n",
      "Epoch 5717, Train_Loss: 4468.06298828125, Val_Loss: 4688.625\n",
      "Epoch 5718, Train_Loss: 4467.78759765625, Val_Loss: 4688.44384765625\n",
      "Epoch 5719, Train_Loss: 4467.64892578125, Val_Loss: 4688.2734375\n",
      "Epoch 5720, Train_Loss: 4467.5400390625, Val_Loss: 4688.07568359375\n",
      "Epoch 5721, Train_Loss: 4467.39794921875, Val_Loss: 4687.8955078125\n",
      "Epoch 5722, Train_Loss: 4467.24755859375, Val_Loss: 4687.71337890625\n",
      "Epoch 5723, Train_Loss: 4467.099609375, Val_Loss: 4687.5419921875\n",
      "Epoch 5724, Train_Loss: 4466.95458984375, Val_Loss: 4687.36328125\n",
      "Epoch 5725, Train_Loss: 4466.8037109375, Val_Loss: 4687.18017578125\n",
      "Epoch 5726, Train_Loss: 4466.6533203125, Val_Loss: 4687.0048828125\n",
      "Epoch 5727, Train_Loss: 4466.4912109375, Val_Loss: 4686.7998046875\n",
      "Epoch 5728, Train_Loss: 4466.341796875, Val_Loss: 4686.61376953125\n",
      "Epoch 5729, Train_Loss: 4466.18994140625, Val_Loss: 4686.43212890625\n",
      "Epoch 5730, Train_Loss: 4465.97998046875, Val_Loss: 4686.27392578125\n",
      "Epoch 5731, Train_Loss: 4465.82568359375, Val_Loss: 4686.08984375\n",
      "Epoch 5732, Train_Loss: 4465.673828125, Val_Loss: 4685.90087890625\n",
      "Epoch 5733, Train_Loss: 4465.5322265625, Val_Loss: 4685.7236328125\n",
      "Epoch 5734, Train_Loss: 4465.39453125, Val_Loss: 4685.53662109375\n",
      "Epoch 5735, Train_Loss: 4465.25048828125, Val_Loss: 4685.36328125\n",
      "Epoch 5736, Train_Loss: 4465.11181640625, Val_Loss: 4685.1728515625\n",
      "Epoch 5737, Train_Loss: 4464.92138671875, Val_Loss: 4684.96923828125\n",
      "Epoch 5738, Train_Loss: 4464.771484375, Val_Loss: 4684.80078125\n",
      "Epoch 5739, Train_Loss: 4464.6259765625, Val_Loss: 4684.61181640625\n",
      "Epoch 5740, Train_Loss: 4464.47607421875, Val_Loss: 4684.4296875\n",
      "Epoch 5741, Train_Loss: 4464.31884765625, Val_Loss: 4684.23779296875\n",
      "Epoch 5742, Train_Loss: 4464.15673828125, Val_Loss: 4684.0654296875\n",
      "Epoch 5743, Train_Loss: 4464.00244140625, Val_Loss: 4683.888671875\n",
      "Epoch 5744, Train_Loss: 4463.82080078125, Val_Loss: 4683.7158203125\n",
      "Epoch 5745, Train_Loss: 4463.68115234375, Val_Loss: 4683.5302734375\n",
      "Epoch 5746, Train_Loss: 4463.5263671875, Val_Loss: 4683.34521484375\n",
      "Epoch 5747, Train_Loss: 4463.3935546875, Val_Loss: 4683.15966796875\n",
      "Epoch 5748, Train_Loss: 4463.25830078125, Val_Loss: 4682.970703125\n",
      "Epoch 5749, Train_Loss: 4463.1240234375, Val_Loss: 4682.7998046875\n",
      "Epoch 5750, Train_Loss: 4462.99072265625, Val_Loss: 4682.6318359375\n",
      "Epoch 5751, Train_Loss: 4462.830078125, Val_Loss: 4682.4501953125\n",
      "Epoch 5752, Train_Loss: 4462.6796875, Val_Loss: 4682.275390625\n",
      "Epoch 5753, Train_Loss: 4462.52294921875, Val_Loss: 4682.09619140625\n",
      "Epoch 5754, Train_Loss: 4462.3818359375, Val_Loss: 4681.9140625\n",
      "Epoch 5755, Train_Loss: 4462.22900390625, Val_Loss: 4681.7265625\n",
      "Epoch 5756, Train_Loss: 4462.0703125, Val_Loss: 4681.52880859375\n",
      "Epoch 5757, Train_Loss: 4461.8857421875, Val_Loss: 4681.365234375\n",
      "Epoch 5758, Train_Loss: 4461.73486328125, Val_Loss: 4681.19189453125\n",
      "Epoch 5759, Train_Loss: 4461.59375, Val_Loss: 4681.01123046875\n",
      "Epoch 5760, Train_Loss: 4461.46044921875, Val_Loss: 4680.82177734375\n",
      "Epoch 5761, Train_Loss: 4461.30810546875, Val_Loss: 4680.64208984375\n",
      "Epoch 5762, Train_Loss: 4461.15087890625, Val_Loss: 4680.46142578125\n",
      "Epoch 5763, Train_Loss: 4460.99951171875, Val_Loss: 4680.28759765625\n",
      "Epoch 5764, Train_Loss: 4460.85888671875, Val_Loss: 4680.11767578125\n",
      "Epoch 5765, Train_Loss: 4460.7109375, Val_Loss: 4679.919921875\n",
      "Epoch 5766, Train_Loss: 4460.56884765625, Val_Loss: 4679.74169921875\n",
      "Epoch 5767, Train_Loss: 4460.41064453125, Val_Loss: 4679.55029296875\n",
      "Epoch 5768, Train_Loss: 4460.26513671875, Val_Loss: 4679.37255859375\n",
      "Epoch 5769, Train_Loss: 4460.12255859375, Val_Loss: 4679.20263671875\n",
      "Epoch 5770, Train_Loss: 4459.9423828125, Val_Loss: 4679.03857421875\n",
      "Epoch 5771, Train_Loss: 4459.78564453125, Val_Loss: 4678.8486328125\n",
      "Epoch 5772, Train_Loss: 4459.64208984375, Val_Loss: 4678.6640625\n",
      "Epoch 5773, Train_Loss: 4459.498046875, Val_Loss: 4678.48486328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5774, Train_Loss: 4459.38427734375, Val_Loss: 4678.27294921875\n",
      "Epoch 5775, Train_Loss: 4459.23876953125, Val_Loss: 4678.08740234375\n",
      "Epoch 5776, Train_Loss: 4459.10009765625, Val_Loss: 4677.91552734375\n",
      "Epoch 5777, Train_Loss: 4458.93310546875, Val_Loss: 4677.744140625\n",
      "Epoch 5778, Train_Loss: 4458.78369140625, Val_Loss: 4677.564453125\n",
      "Epoch 5779, Train_Loss: 4458.6328125, Val_Loss: 4677.39697265625\n",
      "Epoch 5780, Train_Loss: 4458.513671875, Val_Loss: 4677.20458984375\n",
      "Epoch 5781, Train_Loss: 4458.37060546875, Val_Loss: 4677.01904296875\n",
      "Epoch 5782, Train_Loss: 4458.197265625, Val_Loss: 4676.82666015625\n",
      "Epoch 5783, Train_Loss: 4458.0556640625, Val_Loss: 4676.66064453125\n",
      "Epoch 5784, Train_Loss: 4457.7685546875, Val_Loss: 4676.48828125\n",
      "Epoch 5785, Train_Loss: 4457.62451171875, Val_Loss: 4676.310546875\n",
      "Epoch 5786, Train_Loss: 4457.47314453125, Val_Loss: 4676.11865234375\n",
      "Epoch 5787, Train_Loss: 4457.31982421875, Val_Loss: 4675.92822265625\n",
      "Epoch 5788, Train_Loss: 4457.1748046875, Val_Loss: 4675.7607421875\n",
      "Epoch 5789, Train_Loss: 4457.02392578125, Val_Loss: 4675.583984375\n",
      "Epoch 5790, Train_Loss: 4456.8662109375, Val_Loss: 4675.41015625\n",
      "Epoch 5791, Train_Loss: 4456.7275390625, Val_Loss: 4675.23095703125\n",
      "Epoch 5792, Train_Loss: 4456.58447265625, Val_Loss: 4675.04541015625\n",
      "Epoch 5793, Train_Loss: 4456.39404296875, Val_Loss: 4674.86181640625\n",
      "Epoch 5794, Train_Loss: 4456.24755859375, Val_Loss: 4674.67431640625\n",
      "Epoch 5795, Train_Loss: 4456.09033203125, Val_Loss: 4674.50244140625\n",
      "Epoch 5796, Train_Loss: 4455.94921875, Val_Loss: 4674.3173828125\n",
      "Epoch 5797, Train_Loss: 4455.73876953125, Val_Loss: 4674.158203125\n",
      "Epoch 5798, Train_Loss: 4455.595703125, Val_Loss: 4673.9755859375\n",
      "Epoch 5799, Train_Loss: 4455.453125, Val_Loss: 4673.796875\n",
      "Epoch 5800, Train_Loss: 4455.2939453125, Val_Loss: 4673.61279296875\n",
      "Epoch 5801, Train_Loss: 4455.1416015625, Val_Loss: 4673.42822265625\n",
      "Epoch 5802, Train_Loss: 4454.9873046875, Val_Loss: 4673.248046875\n",
      "Epoch 5803, Train_Loss: 4454.84130859375, Val_Loss: 4673.0654296875\n",
      "Epoch 5804, Train_Loss: 4454.6982421875, Val_Loss: 4672.88623046875\n",
      "Epoch 5805, Train_Loss: 4454.5546875, Val_Loss: 4672.6982421875\n",
      "Epoch 5806, Train_Loss: 4454.4072265625, Val_Loss: 4672.5224609375\n",
      "Epoch 5807, Train_Loss: 4454.27099609375, Val_Loss: 4672.341796875\n",
      "Epoch 5808, Train_Loss: 4454.103515625, Val_Loss: 4672.17578125\n",
      "Epoch 5809, Train_Loss: 4453.947265625, Val_Loss: 4671.9931640625\n",
      "Epoch 5810, Train_Loss: 4453.76611328125, Val_Loss: 4671.822265625\n",
      "Epoch 5811, Train_Loss: 4453.625, Val_Loss: 4671.65380859375\n",
      "Epoch 5812, Train_Loss: 4453.474609375, Val_Loss: 4671.42529296875\n",
      "Epoch 5813, Train_Loss: 4453.33740234375, Val_Loss: 4671.25341796875\n",
      "Epoch 5814, Train_Loss: 4453.2021484375, Val_Loss: 4671.060546875\n",
      "Epoch 5815, Train_Loss: 4453.06005859375, Val_Loss: 4670.8701171875\n",
      "Epoch 5816, Train_Loss: 4452.90673828125, Val_Loss: 4670.693359375\n",
      "Epoch 5817, Train_Loss: 4452.77734375, Val_Loss: 4670.5341796875\n",
      "Epoch 5818, Train_Loss: 4452.61474609375, Val_Loss: 4670.34912109375\n",
      "Epoch 5819, Train_Loss: 4452.45947265625, Val_Loss: 4670.1826171875\n",
      "Epoch 5820, Train_Loss: 4452.32470703125, Val_Loss: 4669.99658203125\n",
      "Epoch 5821, Train_Loss: 4452.1767578125, Val_Loss: 4669.80517578125\n",
      "Epoch 5822, Train_Loss: 4452.02490234375, Val_Loss: 4669.63623046875\n",
      "Epoch 5823, Train_Loss: 4451.8623046875, Val_Loss: 4669.4541015625\n",
      "Epoch 5824, Train_Loss: 4451.6943359375, Val_Loss: 4669.29052734375\n",
      "Epoch 5825, Train_Loss: 4451.5537109375, Val_Loss: 4669.107421875\n",
      "Epoch 5826, Train_Loss: 4451.40869140625, Val_Loss: 4668.93115234375\n",
      "Epoch 5827, Train_Loss: 4451.30029296875, Val_Loss: 4668.74462890625\n",
      "Epoch 5828, Train_Loss: 4451.14404296875, Val_Loss: 4668.56298828125\n",
      "Epoch 5829, Train_Loss: 4450.98974609375, Val_Loss: 4668.384765625\n",
      "Epoch 5830, Train_Loss: 4450.87109375, Val_Loss: 4668.2138671875\n",
      "Epoch 5831, Train_Loss: 4450.7255859375, Val_Loss: 4668.02490234375\n",
      "Epoch 5832, Train_Loss: 4450.5791015625, Val_Loss: 4667.84521484375\n",
      "Epoch 5833, Train_Loss: 4450.4169921875, Val_Loss: 4667.67529296875\n",
      "Epoch 5834, Train_Loss: 4450.27783203125, Val_Loss: 4667.4814453125\n",
      "Epoch 5835, Train_Loss: 4450.12060546875, Val_Loss: 4667.31298828125\n",
      "Epoch 5836, Train_Loss: 4449.97509765625, Val_Loss: 4667.12890625\n",
      "Epoch 5837, Train_Loss: 4449.771484375, Val_Loss: 4666.9658203125\n",
      "Epoch 5838, Train_Loss: 4449.60107421875, Val_Loss: 4666.79833984375\n",
      "Epoch 5839, Train_Loss: 4449.458984375, Val_Loss: 4666.60107421875\n",
      "Epoch 5840, Train_Loss: 4449.31787109375, Val_Loss: 4666.408203125\n",
      "Epoch 5841, Train_Loss: 4449.18603515625, Val_Loss: 4666.22412109375\n",
      "Epoch 5842, Train_Loss: 4449.04541015625, Val_Loss: 4666.0478515625\n",
      "Epoch 5843, Train_Loss: 4448.8935546875, Val_Loss: 4665.875\n",
      "Epoch 5844, Train_Loss: 4448.7451171875, Val_Loss: 4665.708984375\n",
      "Epoch 5845, Train_Loss: 4448.5986328125, Val_Loss: 4665.51708984375\n",
      "Epoch 5846, Train_Loss: 4448.45166015625, Val_Loss: 4665.3486328125\n",
      "Epoch 5847, Train_Loss: 4448.30224609375, Val_Loss: 4665.16015625\n",
      "Epoch 5848, Train_Loss: 4448.12939453125, Val_Loss: 4664.98046875\n",
      "Epoch 5849, Train_Loss: 4447.974609375, Val_Loss: 4664.81005859375\n",
      "Epoch 5850, Train_Loss: 4447.78662109375, Val_Loss: 4664.5986328125\n",
      "Epoch 5851, Train_Loss: 4447.63427734375, Val_Loss: 4664.42822265625\n",
      "Epoch 5852, Train_Loss: 4447.50634765625, Val_Loss: 4664.244140625\n",
      "Epoch 5853, Train_Loss: 4447.3564453125, Val_Loss: 4664.06982421875\n",
      "Epoch 5854, Train_Loss: 4447.236328125, Val_Loss: 4663.8828125\n",
      "Epoch 5855, Train_Loss: 4447.08837890625, Val_Loss: 4663.71728515625\n",
      "Epoch 5856, Train_Loss: 4446.94384765625, Val_Loss: 4663.53759765625\n",
      "Epoch 5857, Train_Loss: 4446.8173828125, Val_Loss: 4663.357421875\n",
      "Epoch 5858, Train_Loss: 4446.64697265625, Val_Loss: 4663.1826171875\n",
      "Epoch 5859, Train_Loss: 4446.4794921875, Val_Loss: 4663.0009765625\n",
      "Epoch 5860, Train_Loss: 4446.349609375, Val_Loss: 4662.82275390625\n",
      "Epoch 5861, Train_Loss: 4446.2138671875, Val_Loss: 4662.6494140625\n",
      "Epoch 5862, Train_Loss: 4446.046875, Val_Loss: 4662.470703125\n",
      "Epoch 5863, Train_Loss: 4445.8828125, Val_Loss: 4662.2939453125\n",
      "Epoch 5864, Train_Loss: 4445.72509765625, Val_Loss: 4662.1259765625\n",
      "Epoch 5865, Train_Loss: 4445.5810546875, Val_Loss: 4661.95361328125\n",
      "Epoch 5866, Train_Loss: 4445.4345703125, Val_Loss: 4661.7685546875\n",
      "Epoch 5867, Train_Loss: 4445.29443359375, Val_Loss: 4661.58740234375\n",
      "Epoch 5868, Train_Loss: 4444.98046875, Val_Loss: 4661.41015625\n",
      "Epoch 5869, Train_Loss: 4444.83984375, Val_Loss: 4661.22998046875\n",
      "Epoch 5870, Train_Loss: 4444.7001953125, Val_Loss: 4661.06005859375\n",
      "Epoch 5871, Train_Loss: 4444.55029296875, Val_Loss: 4660.89111328125\n",
      "Epoch 5872, Train_Loss: 4444.40673828125, Val_Loss: 4660.7109375\n",
      "Epoch 5873, Train_Loss: 4444.2685546875, Val_Loss: 4660.53076171875\n",
      "Epoch 5874, Train_Loss: 4444.11181640625, Val_Loss: 4660.34619140625\n",
      "Epoch 5875, Train_Loss: 4443.9599609375, Val_Loss: 4660.17578125\n",
      "Epoch 5876, Train_Loss: 4443.82275390625, Val_Loss: 4659.99951171875\n",
      "Epoch 5877, Train_Loss: 4443.63720703125, Val_Loss: 4659.833984375\n",
      "Epoch 5878, Train_Loss: 4443.45556640625, Val_Loss: 4659.64501953125\n",
      "Epoch 5879, Train_Loss: 4443.30712890625, Val_Loss: 4659.45849609375\n",
      "Epoch 5880, Train_Loss: 4443.20458984375, Val_Loss: 4659.2802734375\n",
      "Epoch 5881, Train_Loss: 4443.0576171875, Val_Loss: 4659.1005859375\n",
      "Epoch 5882, Train_Loss: 4442.9189453125, Val_Loss: 4658.92529296875\n",
      "Epoch 5883, Train_Loss: 4442.76416015625, Val_Loss: 4658.74755859375\n",
      "Epoch 5884, Train_Loss: 4442.61865234375, Val_Loss: 4658.583984375\n",
      "Epoch 5885, Train_Loss: 4442.47802734375, Val_Loss: 4658.4033203125\n",
      "Epoch 5886, Train_Loss: 4442.326171875, Val_Loss: 4658.22265625\n",
      "Epoch 5887, Train_Loss: 4442.1630859375, Val_Loss: 4658.009765625\n",
      "Epoch 5888, Train_Loss: 4442.00390625, Val_Loss: 4657.837890625\n",
      "Epoch 5889, Train_Loss: 4441.8623046875, Val_Loss: 4657.662109375\n",
      "Epoch 5890, Train_Loss: 4441.6572265625, Val_Loss: 4657.49462890625\n",
      "Epoch 5891, Train_Loss: 4441.525390625, Val_Loss: 4657.32861328125\n",
      "Epoch 5892, Train_Loss: 4441.3857421875, Val_Loss: 4657.14697265625\n",
      "Epoch 5893, Train_Loss: 4441.23779296875, Val_Loss: 4656.96337890625\n",
      "Epoch 5894, Train_Loss: 4441.0966796875, Val_Loss: 4656.7841796875\n",
      "Epoch 5895, Train_Loss: 4440.9599609375, Val_Loss: 4656.61328125\n",
      "Epoch 5896, Train_Loss: 4440.80908203125, Val_Loss: 4656.43017578125\n",
      "Epoch 5897, Train_Loss: 4440.66845703125, Val_Loss: 4656.26220703125\n",
      "Epoch 5898, Train_Loss: 4440.51123046875, Val_Loss: 4656.08837890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5899, Train_Loss: 4440.373046875, Val_Loss: 4655.912109375\n",
      "Epoch 5900, Train_Loss: 4440.23974609375, Val_Loss: 4655.7255859375\n",
      "Epoch 5901, Train_Loss: 4440.107421875, Val_Loss: 4655.54541015625\n",
      "Epoch 5902, Train_Loss: 4439.94873046875, Val_Loss: 4655.37744140625\n",
      "Epoch 5903, Train_Loss: 4439.79248046875, Val_Loss: 4655.19677734375\n",
      "Epoch 5904, Train_Loss: 4439.59716796875, Val_Loss: 4655.029296875\n",
      "Epoch 5905, Train_Loss: 4439.4638671875, Val_Loss: 4654.857421875\n",
      "Epoch 5906, Train_Loss: 4439.306640625, Val_Loss: 4654.669921875\n",
      "Epoch 5907, Train_Loss: 4439.1533203125, Val_Loss: 4654.49853515625\n",
      "Epoch 5908, Train_Loss: 4438.98681640625, Val_Loss: 4654.31787109375\n",
      "Epoch 5909, Train_Loss: 4438.8427734375, Val_Loss: 4654.142578125\n",
      "Epoch 5910, Train_Loss: 4438.71435546875, Val_Loss: 4653.9775390625\n",
      "Epoch 5911, Train_Loss: 4438.5849609375, Val_Loss: 4653.79931640625\n",
      "Epoch 5912, Train_Loss: 4438.4482421875, Val_Loss: 4653.61865234375\n",
      "Epoch 5913, Train_Loss: 4438.31591796875, Val_Loss: 4653.447265625\n",
      "Epoch 5914, Train_Loss: 4438.1904296875, Val_Loss: 4653.26220703125\n",
      "Epoch 5915, Train_Loss: 4438.03955078125, Val_Loss: 4653.080078125\n",
      "Epoch 5916, Train_Loss: 4437.896484375, Val_Loss: 4652.89892578125\n",
      "Epoch 5917, Train_Loss: 4437.7109375, Val_Loss: 4652.73583984375\n",
      "Epoch 5918, Train_Loss: 4437.56201171875, Val_Loss: 4652.55859375\n",
      "Epoch 5919, Train_Loss: 4437.3994140625, Val_Loss: 4652.37744140625\n",
      "Epoch 5920, Train_Loss: 4437.2646484375, Val_Loss: 4652.19140625\n",
      "Epoch 5921, Train_Loss: 4437.1220703125, Val_Loss: 4652.021484375\n",
      "Epoch 5922, Train_Loss: 4436.97314453125, Val_Loss: 4651.84375\n",
      "Epoch 5923, Train_Loss: 4436.83154296875, Val_Loss: 4651.66943359375\n",
      "Epoch 5924, Train_Loss: 4436.6689453125, Val_Loss: 4651.50341796875\n",
      "Epoch 5925, Train_Loss: 4436.51171875, Val_Loss: 4651.2958984375\n",
      "Epoch 5926, Train_Loss: 4436.37451171875, Val_Loss: 4651.12255859375\n",
      "Epoch 5927, Train_Loss: 4436.22998046875, Val_Loss: 4650.93115234375\n",
      "Epoch 5928, Train_Loss: 4436.09228515625, Val_Loss: 4650.76611328125\n",
      "Epoch 5929, Train_Loss: 4435.93505859375, Val_Loss: 4650.59228515625\n",
      "Epoch 5930, Train_Loss: 4435.76123046875, Val_Loss: 4650.43017578125\n",
      "Epoch 5931, Train_Loss: 4435.63134765625, Val_Loss: 4650.255859375\n",
      "Epoch 5932, Train_Loss: 4435.4970703125, Val_Loss: 4650.0712890625\n",
      "Epoch 5933, Train_Loss: 4435.3623046875, Val_Loss: 4649.89794921875\n",
      "Epoch 5934, Train_Loss: 4435.21875, Val_Loss: 4649.71240234375\n",
      "Epoch 5935, Train_Loss: 4435.076171875, Val_Loss: 4649.5419921875\n",
      "Epoch 5936, Train_Loss: 4434.94091796875, Val_Loss: 4649.3603515625\n",
      "Epoch 5937, Train_Loss: 4434.79541015625, Val_Loss: 4649.19287109375\n",
      "Epoch 5938, Train_Loss: 4434.64892578125, Val_Loss: 4649.02587890625\n",
      "Epoch 5939, Train_Loss: 4434.48291015625, Val_Loss: 4648.84326171875\n",
      "Epoch 5940, Train_Loss: 4434.34033203125, Val_Loss: 4648.6591796875\n",
      "Epoch 5941, Train_Loss: 4434.1982421875, Val_Loss: 4648.49072265625\n",
      "Epoch 5942, Train_Loss: 4434.05322265625, Val_Loss: 4648.31982421875\n",
      "Epoch 5943, Train_Loss: 4433.9033203125, Val_Loss: 4648.15771484375\n",
      "Epoch 5944, Train_Loss: 4433.60205078125, Val_Loss: 4647.97998046875\n",
      "Epoch 5945, Train_Loss: 4433.46923828125, Val_Loss: 4647.8037109375\n",
      "Epoch 5946, Train_Loss: 4433.3369140625, Val_Loss: 4647.63427734375\n",
      "Epoch 5947, Train_Loss: 4433.2080078125, Val_Loss: 4647.439453125\n",
      "Epoch 5948, Train_Loss: 4433.04296875, Val_Loss: 4647.2568359375\n",
      "Epoch 5949, Train_Loss: 4432.892578125, Val_Loss: 4647.10302734375\n",
      "Epoch 5950, Train_Loss: 4432.755859375, Val_Loss: 4646.9267578125\n",
      "Epoch 5951, Train_Loss: 4432.6064453125, Val_Loss: 4646.755859375\n",
      "Epoch 5952, Train_Loss: 4432.47265625, Val_Loss: 4646.57861328125\n",
      "Epoch 5953, Train_Loss: 4432.31201171875, Val_Loss: 4646.39453125\n",
      "Epoch 5954, Train_Loss: 4432.1708984375, Val_Loss: 4646.20361328125\n",
      "Epoch 5955, Train_Loss: 4432.0322265625, Val_Loss: 4646.0361328125\n",
      "Epoch 5956, Train_Loss: 4431.89111328125, Val_Loss: 4645.85498046875\n",
      "Epoch 5957, Train_Loss: 4431.7236328125, Val_Loss: 4645.69873046875\n",
      "Epoch 5958, Train_Loss: 4431.56689453125, Val_Loss: 4645.51904296875\n",
      "Epoch 5959, Train_Loss: 4431.39794921875, Val_Loss: 4645.34521484375\n",
      "Epoch 5960, Train_Loss: 4431.28125, Val_Loss: 4645.15283203125\n",
      "Epoch 5961, Train_Loss: 4431.14599609375, Val_Loss: 4644.986328125\n",
      "Epoch 5962, Train_Loss: 4430.99951171875, Val_Loss: 4644.8046875\n",
      "Epoch 5963, Train_Loss: 4430.857421875, Val_Loss: 4644.611328125\n",
      "Epoch 5964, Train_Loss: 4430.71044921875, Val_Loss: 4644.43603515625\n",
      "Epoch 5965, Train_Loss: 4430.56787109375, Val_Loss: 4644.2626953125\n",
      "Epoch 5966, Train_Loss: 4430.43798828125, Val_Loss: 4644.0986328125\n",
      "Epoch 5967, Train_Loss: 4430.29833984375, Val_Loss: 4643.90576171875\n",
      "Epoch 5968, Train_Loss: 4430.13720703125, Val_Loss: 4643.7392578125\n",
      "Epoch 5969, Train_Loss: 4429.9736328125, Val_Loss: 4643.57080078125\n",
      "Epoch 5970, Train_Loss: 4429.79931640625, Val_Loss: 4643.39208984375\n",
      "Epoch 5971, Train_Loss: 4429.66064453125, Val_Loss: 4643.22900390625\n",
      "Epoch 5972, Train_Loss: 4429.5322265625, Val_Loss: 4643.05859375\n",
      "Epoch 5973, Train_Loss: 4429.39501953125, Val_Loss: 4642.88232421875\n",
      "Epoch 5974, Train_Loss: 4429.24365234375, Val_Loss: 4642.69287109375\n",
      "Epoch 5975, Train_Loss: 4429.09521484375, Val_Loss: 4642.5205078125\n",
      "Epoch 5976, Train_Loss: 4428.95703125, Val_Loss: 4642.3505859375\n",
      "Epoch 5977, Train_Loss: 4428.82470703125, Val_Loss: 4642.18359375\n",
      "Epoch 5978, Train_Loss: 4428.68017578125, Val_Loss: 4642.0068359375\n",
      "Epoch 5979, Train_Loss: 4428.5224609375, Val_Loss: 4641.8330078125\n",
      "Epoch 5980, Train_Loss: 4428.37646484375, Val_Loss: 4641.6533203125\n",
      "Epoch 5981, Train_Loss: 4428.22705078125, Val_Loss: 4641.48193359375\n",
      "Epoch 5982, Train_Loss: 4428.07763671875, Val_Loss: 4641.30224609375\n",
      "Epoch 5983, Train_Loss: 4427.9443359375, Val_Loss: 4641.1328125\n",
      "Epoch 5984, Train_Loss: 4427.74853515625, Val_Loss: 4640.97021484375\n",
      "Epoch 5985, Train_Loss: 4427.61083984375, Val_Loss: 4640.79638671875\n",
      "Epoch 5986, Train_Loss: 4427.4541015625, Val_Loss: 4640.615234375\n",
      "Epoch 5987, Train_Loss: 4427.35791015625, Val_Loss: 4640.43798828125\n",
      "Epoch 5988, Train_Loss: 4427.2216796875, Val_Loss: 4640.27197265625\n",
      "Epoch 5989, Train_Loss: 4427.05859375, Val_Loss: 4640.091796875\n",
      "Epoch 5990, Train_Loss: 4426.92724609375, Val_Loss: 4639.93017578125\n",
      "Epoch 5991, Train_Loss: 4426.779296875, Val_Loss: 4639.73974609375\n",
      "Epoch 5992, Train_Loss: 4426.63818359375, Val_Loss: 4639.572265625\n",
      "Epoch 5993, Train_Loss: 4426.48974609375, Val_Loss: 4639.39599609375\n",
      "Epoch 5994, Train_Loss: 4426.34814453125, Val_Loss: 4639.2177734375\n",
      "Epoch 5995, Train_Loss: 4426.22119140625, Val_Loss: 4639.044921875\n",
      "Epoch 5996, Train_Loss: 4426.09033203125, Val_Loss: 4638.86962890625\n",
      "Epoch 5997, Train_Loss: 4425.892578125, Val_Loss: 4638.7021484375\n",
      "Epoch 5998, Train_Loss: 4425.74853515625, Val_Loss: 4638.52978515625\n",
      "Epoch 5999, Train_Loss: 4425.58349609375, Val_Loss: 4638.3642578125\n",
      "Epoch 6000, Train_Loss: 4425.43896484375, Val_Loss: 4638.1572265625\n",
      "Epoch 6001, Train_Loss: 4425.296875, Val_Loss: 4637.96875\n",
      "Epoch 6002, Train_Loss: 4425.14111328125, Val_Loss: 4637.8056640625\n",
      "Epoch 6003, Train_Loss: 4425.0068359375, Val_Loss: 4637.6259765625\n",
      "Epoch 6004, Train_Loss: 4424.869140625, Val_Loss: 4637.46484375\n",
      "Epoch 6005, Train_Loss: 4424.72705078125, Val_Loss: 4637.29345703125\n",
      "Epoch 6006, Train_Loss: 4424.5859375, Val_Loss: 4637.1181640625\n",
      "Epoch 6007, Train_Loss: 4424.45751953125, Val_Loss: 4636.93310546875\n",
      "Epoch 6008, Train_Loss: 4424.30859375, Val_Loss: 4636.76513671875\n",
      "Epoch 6009, Train_Loss: 4424.1572265625, Val_Loss: 4636.58544921875\n",
      "Epoch 6010, Train_Loss: 4423.94140625, Val_Loss: 4636.41943359375\n",
      "Epoch 6011, Train_Loss: 4423.79150390625, Val_Loss: 4636.25634765625\n",
      "Epoch 6012, Train_Loss: 4423.66064453125, Val_Loss: 4636.0859375\n",
      "Epoch 6013, Train_Loss: 4423.517578125, Val_Loss: 4635.91015625\n",
      "Epoch 6014, Train_Loss: 4423.35791015625, Val_Loss: 4635.72900390625\n",
      "Epoch 6015, Train_Loss: 4423.22314453125, Val_Loss: 4635.55419921875\n",
      "Epoch 6016, Train_Loss: 4423.07373046875, Val_Loss: 4635.37744140625\n",
      "Epoch 6017, Train_Loss: 4422.93701171875, Val_Loss: 4635.232421875\n",
      "Epoch 6018, Train_Loss: 4422.78955078125, Val_Loss: 4635.05615234375\n",
      "Epoch 6019, Train_Loss: 4422.58447265625, Val_Loss: 4634.8671875\n",
      "Epoch 6020, Train_Loss: 4422.4453125, Val_Loss: 4634.68603515625\n",
      "Epoch 6021, Train_Loss: 4422.3046875, Val_Loss: 4634.5126953125\n",
      "Epoch 6022, Train_Loss: 4422.15966796875, Val_Loss: 4634.3388671875\n",
      "Epoch 6023, Train_Loss: 4422.03369140625, Val_Loss: 4634.171875\n",
      "Epoch 6024, Train_Loss: 4421.84375, Val_Loss: 4634.0078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6025, Train_Loss: 4421.70263671875, Val_Loss: 4633.8330078125\n",
      "Epoch 6026, Train_Loss: 4421.5537109375, Val_Loss: 4633.65966796875\n",
      "Epoch 6027, Train_Loss: 4421.4228515625, Val_Loss: 4633.4833984375\n",
      "Epoch 6028, Train_Loss: 4421.271484375, Val_Loss: 4633.296875\n",
      "Epoch 6029, Train_Loss: 4421.140625, Val_Loss: 4633.1328125\n",
      "Epoch 6030, Train_Loss: 4420.9931640625, Val_Loss: 4632.96240234375\n",
      "Epoch 6031, Train_Loss: 4420.85595703125, Val_Loss: 4632.794921875\n",
      "Epoch 6032, Train_Loss: 4420.72216796875, Val_Loss: 4632.6064453125\n",
      "Epoch 6033, Train_Loss: 4420.576171875, Val_Loss: 4632.4345703125\n",
      "Epoch 6034, Train_Loss: 4420.43359375, Val_Loss: 4632.2568359375\n",
      "Epoch 6035, Train_Loss: 4420.28466796875, Val_Loss: 4632.0849609375\n",
      "Epoch 6036, Train_Loss: 4420.15576171875, Val_Loss: 4631.91943359375\n",
      "Epoch 6037, Train_Loss: 4419.9990234375, Val_Loss: 4631.7509765625\n",
      "Epoch 6038, Train_Loss: 4419.8291015625, Val_Loss: 4631.541015625\n",
      "Epoch 6039, Train_Loss: 4419.685546875, Val_Loss: 4631.37841796875\n",
      "Epoch 6040, Train_Loss: 4419.5517578125, Val_Loss: 4631.18896484375\n",
      "Epoch 6041, Train_Loss: 4419.40234375, Val_Loss: 4631.015625\n",
      "Epoch 6042, Train_Loss: 4419.26611328125, Val_Loss: 4630.84619140625\n",
      "Epoch 6043, Train_Loss: 4419.12841796875, Val_Loss: 4630.67578125\n",
      "Epoch 6044, Train_Loss: 4419.0029296875, Val_Loss: 4630.515625\n",
      "Epoch 6045, Train_Loss: 4418.85107421875, Val_Loss: 4630.33935546875\n",
      "Epoch 6046, Train_Loss: 4418.7099609375, Val_Loss: 4630.1650390625\n",
      "Epoch 6047, Train_Loss: 4418.5595703125, Val_Loss: 4630.0\n",
      "Epoch 6048, Train_Loss: 4418.43603515625, Val_Loss: 4629.81640625\n",
      "Epoch 6049, Train_Loss: 4418.2890625, Val_Loss: 4629.6318359375\n",
      "Epoch 6050, Train_Loss: 4418.05810546875, Val_Loss: 4629.4794921875\n",
      "Epoch 6051, Train_Loss: 4417.9248046875, Val_Loss: 4629.30517578125\n",
      "Epoch 6052, Train_Loss: 4417.79638671875, Val_Loss: 4629.1396484375\n",
      "Epoch 6053, Train_Loss: 4417.64404296875, Val_Loss: 4628.96875\n",
      "Epoch 6054, Train_Loss: 4417.5107421875, Val_Loss: 4628.7822265625\n",
      "Epoch 6055, Train_Loss: 4417.36376953125, Val_Loss: 4628.60498046875\n",
      "Epoch 6056, Train_Loss: 4417.21240234375, Val_Loss: 4628.4404296875\n",
      "Epoch 6057, Train_Loss: 4417.0654296875, Val_Loss: 4628.275390625\n",
      "Epoch 6058, Train_Loss: 4416.92724609375, Val_Loss: 4628.09619140625\n",
      "Epoch 6059, Train_Loss: 4416.7861328125, Val_Loss: 4627.919921875\n",
      "Epoch 6060, Train_Loss: 4416.62255859375, Val_Loss: 4627.75244140625\n",
      "Epoch 6061, Train_Loss: 4416.48486328125, Val_Loss: 4627.57421875\n",
      "Epoch 6062, Train_Loss: 4416.3544921875, Val_Loss: 4627.40576171875\n",
      "Epoch 6063, Train_Loss: 4416.20751953125, Val_Loss: 4627.23193359375\n",
      "Epoch 6064, Train_Loss: 4416.033203125, Val_Loss: 4627.05908203125\n",
      "Epoch 6065, Train_Loss: 4415.87451171875, Val_Loss: 4626.89453125\n",
      "Epoch 6066, Train_Loss: 4415.74169921875, Val_Loss: 4626.71240234375\n",
      "Epoch 6067, Train_Loss: 4415.62353515625, Val_Loss: 4626.52783203125\n",
      "Epoch 6068, Train_Loss: 4415.47998046875, Val_Loss: 4626.35302734375\n",
      "Epoch 6069, Train_Loss: 4415.34521484375, Val_Loss: 4626.17822265625\n",
      "Epoch 6070, Train_Loss: 4415.208984375, Val_Loss: 4626.02099609375\n",
      "Epoch 6071, Train_Loss: 4415.0595703125, Val_Loss: 4625.8525390625\n",
      "Epoch 6072, Train_Loss: 4414.92529296875, Val_Loss: 4625.67822265625\n",
      "Epoch 6073, Train_Loss: 4414.78466796875, Val_Loss: 4625.4990234375\n",
      "Epoch 6074, Train_Loss: 4414.6474609375, Val_Loss: 4625.32177734375\n",
      "Epoch 6075, Train_Loss: 4414.48779296875, Val_Loss: 4625.12353515625\n",
      "Epoch 6076, Train_Loss: 4414.3486328125, Val_Loss: 4624.953125\n",
      "Epoch 6077, Train_Loss: 4414.1943359375, Val_Loss: 4624.78662109375\n",
      "Epoch 6078, Train_Loss: 4414.052734375, Val_Loss: 4624.61376953125\n",
      "Epoch 6079, Train_Loss: 4413.91748046875, Val_Loss: 4624.4384765625\n",
      "Epoch 6080, Train_Loss: 4413.75, Val_Loss: 4624.26171875\n",
      "Epoch 6081, Train_Loss: 4413.615234375, Val_Loss: 4624.09130859375\n",
      "Epoch 6082, Train_Loss: 4413.47998046875, Val_Loss: 4623.91552734375\n",
      "Epoch 6083, Train_Loss: 4413.333984375, Val_Loss: 4623.74462890625\n",
      "Epoch 6084, Train_Loss: 4413.19091796875, Val_Loss: 4623.58740234375\n",
      "Epoch 6085, Train_Loss: 4412.95068359375, Val_Loss: 4623.416015625\n",
      "Epoch 6086, Train_Loss: 4412.8173828125, Val_Loss: 4623.23583984375\n",
      "Epoch 6087, Train_Loss: 4412.6767578125, Val_Loss: 4623.05517578125\n",
      "Epoch 6088, Train_Loss: 4412.53271484375, Val_Loss: 4622.88720703125\n",
      "Epoch 6089, Train_Loss: 4412.39990234375, Val_Loss: 4622.71533203125\n",
      "Epoch 6090, Train_Loss: 4412.2001953125, Val_Loss: 4622.55810546875\n",
      "Epoch 6091, Train_Loss: 4412.06396484375, Val_Loss: 4622.38720703125\n",
      "Epoch 6092, Train_Loss: 4411.92529296875, Val_Loss: 4622.21826171875\n",
      "Epoch 6093, Train_Loss: 4411.7958984375, Val_Loss: 4622.04541015625\n",
      "Epoch 6094, Train_Loss: 4411.630859375, Val_Loss: 4621.85986328125\n",
      "Epoch 6095, Train_Loss: 4411.48974609375, Val_Loss: 4621.69287109375\n",
      "Epoch 6096, Train_Loss: 4411.3369140625, Val_Loss: 4621.51611328125\n",
      "Epoch 6097, Train_Loss: 4411.19384765625, Val_Loss: 4621.36181640625\n",
      "Epoch 6098, Train_Loss: 4411.05712890625, Val_Loss: 4621.1875\n",
      "Epoch 6099, Train_Loss: 4410.91650390625, Val_Loss: 4621.01904296875\n",
      "Epoch 6100, Train_Loss: 4410.76171875, Val_Loss: 4620.841796875\n",
      "Epoch 6101, Train_Loss: 4410.61962890625, Val_Loss: 4620.66015625\n",
      "Epoch 6102, Train_Loss: 4410.47705078125, Val_Loss: 4620.49853515625\n",
      "Epoch 6103, Train_Loss: 4410.33447265625, Val_Loss: 4620.32861328125\n",
      "Epoch 6104, Train_Loss: 4410.12841796875, Val_Loss: 4620.1552734375\n",
      "Epoch 6105, Train_Loss: 4409.97265625, Val_Loss: 4619.98095703125\n",
      "Epoch 6106, Train_Loss: 4409.84521484375, Val_Loss: 4619.80419921875\n",
      "Epoch 6107, Train_Loss: 4409.70263671875, Val_Loss: 4619.62548828125\n",
      "Epoch 6108, Train_Loss: 4409.5703125, Val_Loss: 4619.453125\n",
      "Epoch 6109, Train_Loss: 4409.43017578125, Val_Loss: 4619.28369140625\n",
      "Epoch 6110, Train_Loss: 4409.28662109375, Val_Loss: 4619.1162109375\n",
      "Epoch 6111, Train_Loss: 4409.13818359375, Val_Loss: 4618.95068359375\n",
      "Epoch 6112, Train_Loss: 4409.0, Val_Loss: 4618.7822265625\n",
      "Epoch 6113, Train_Loss: 4408.8544921875, Val_Loss: 4618.5791015625\n",
      "Epoch 6114, Train_Loss: 4408.71533203125, Val_Loss: 4618.39501953125\n",
      "Epoch 6115, Train_Loss: 4408.5673828125, Val_Loss: 4618.23095703125\n",
      "Epoch 6116, Train_Loss: 4408.42529296875, Val_Loss: 4618.056640625\n",
      "Epoch 6117, Train_Loss: 4408.24658203125, Val_Loss: 4617.88623046875\n",
      "Epoch 6118, Train_Loss: 4408.10791015625, Val_Loss: 4617.72216796875\n",
      "Epoch 6119, Train_Loss: 4407.9736328125, Val_Loss: 4617.55517578125\n",
      "Epoch 6120, Train_Loss: 4407.81298828125, Val_Loss: 4617.37451171875\n",
      "Epoch 6121, Train_Loss: 4407.6650390625, Val_Loss: 4617.2041015625\n",
      "Epoch 6122, Train_Loss: 4407.5302734375, Val_Loss: 4617.0302734375\n",
      "Epoch 6123, Train_Loss: 4407.38427734375, Val_Loss: 4616.8623046875\n",
      "Epoch 6124, Train_Loss: 4407.25390625, Val_Loss: 4616.708984375\n",
      "Epoch 6125, Train_Loss: 4407.1044921875, Val_Loss: 4616.54052734375\n",
      "Epoch 6126, Train_Loss: 4406.9736328125, Val_Loss: 4616.3720703125\n",
      "Epoch 6127, Train_Loss: 4406.8369140625, Val_Loss: 4616.18359375\n",
      "Epoch 6128, Train_Loss: 4406.7109375, Val_Loss: 4616.0126953125\n",
      "Epoch 6129, Train_Loss: 4406.56396484375, Val_Loss: 4615.849609375\n",
      "Epoch 6130, Train_Loss: 4406.38623046875, Val_Loss: 4615.69140625\n",
      "Epoch 6131, Train_Loss: 4406.2451171875, Val_Loss: 4615.52197265625\n",
      "Epoch 6132, Train_Loss: 4406.11328125, Val_Loss: 4615.33203125\n",
      "Epoch 6133, Train_Loss: 4405.9775390625, Val_Loss: 4615.15771484375\n",
      "Epoch 6134, Train_Loss: 4405.8349609375, Val_Loss: 4614.9892578125\n",
      "Epoch 6135, Train_Loss: 4405.6845703125, Val_Loss: 4614.81298828125\n",
      "Epoch 6136, Train_Loss: 4405.54931640625, Val_Loss: 4614.64208984375\n",
      "Epoch 6137, Train_Loss: 4405.4052734375, Val_Loss: 4614.4921875\n",
      "Epoch 6138, Train_Loss: 4405.275390625, Val_Loss: 4614.32470703125\n",
      "Epoch 6139, Train_Loss: 4405.13818359375, Val_Loss: 4614.13623046875\n",
      "Epoch 6140, Train_Loss: 4404.9755859375, Val_Loss: 4613.97265625\n",
      "Epoch 6141, Train_Loss: 4404.837890625, Val_Loss: 4613.7890625\n",
      "Epoch 6142, Train_Loss: 4404.69873046875, Val_Loss: 4613.6142578125\n",
      "Epoch 6143, Train_Loss: 4404.5546875, Val_Loss: 4613.4453125\n",
      "Epoch 6144, Train_Loss: 4404.3798828125, Val_Loss: 4613.27392578125\n",
      "Epoch 6145, Train_Loss: 4404.2392578125, Val_Loss: 4613.107421875\n",
      "Epoch 6146, Train_Loss: 4404.111328125, Val_Loss: 4612.9482421875\n",
      "Epoch 6147, Train_Loss: 4404.0205078125, Val_Loss: 4612.7666015625\n",
      "Epoch 6148, Train_Loss: 4403.873046875, Val_Loss: 4612.5888671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6149, Train_Loss: 4403.71142578125, Val_Loss: 4612.42578125\n",
      "Epoch 6150, Train_Loss: 4403.58203125, Val_Loss: 4612.24853515625\n",
      "Epoch 6151, Train_Loss: 4403.42529296875, Val_Loss: 4612.0751953125\n",
      "Epoch 6152, Train_Loss: 4403.27978515625, Val_Loss: 4611.90673828125\n",
      "Epoch 6153, Train_Loss: 4403.15087890625, Val_Loss: 4611.7265625\n",
      "Epoch 6154, Train_Loss: 4403.0283203125, Val_Loss: 4611.546875\n",
      "Epoch 6155, Train_Loss: 4402.89599609375, Val_Loss: 4611.37890625\n",
      "Epoch 6156, Train_Loss: 4402.7490234375, Val_Loss: 4611.2109375\n",
      "Epoch 6157, Train_Loss: 4402.556640625, Val_Loss: 4611.05419921875\n",
      "Epoch 6158, Train_Loss: 4402.41943359375, Val_Loss: 4610.88427734375\n",
      "Epoch 6159, Train_Loss: 4402.287109375, Val_Loss: 4610.7158203125\n",
      "Epoch 6160, Train_Loss: 4402.13916015625, Val_Loss: 4610.537109375\n",
      "Epoch 6161, Train_Loss: 4401.99560546875, Val_Loss: 4610.35498046875\n",
      "Epoch 6162, Train_Loss: 4401.85498046875, Val_Loss: 4610.1904296875\n",
      "Epoch 6163, Train_Loss: 4401.71875, Val_Loss: 4610.01806640625\n",
      "Epoch 6164, Train_Loss: 4401.57421875, Val_Loss: 4609.8603515625\n",
      "Epoch 6165, Train_Loss: 4401.4375, Val_Loss: 4609.6982421875\n",
      "Epoch 6166, Train_Loss: 4401.27099609375, Val_Loss: 4609.525390625\n",
      "Epoch 6167, Train_Loss: 4401.14111328125, Val_Loss: 4609.34375\n",
      "Epoch 6168, Train_Loss: 4400.99755859375, Val_Loss: 4609.171875\n",
      "Epoch 6169, Train_Loss: 4400.70654296875, Val_Loss: 4608.99951171875\n",
      "Epoch 6170, Train_Loss: 4400.5556640625, Val_Loss: 4608.82373046875\n",
      "Epoch 6171, Train_Loss: 4400.3720703125, Val_Loss: 4608.67138671875\n",
      "Epoch 6172, Train_Loss: 4400.22314453125, Val_Loss: 4608.50732421875\n",
      "Epoch 6173, Train_Loss: 4400.1064453125, Val_Loss: 4608.3359375\n",
      "Epoch 6174, Train_Loss: 4399.97998046875, Val_Loss: 4608.1494140625\n",
      "Epoch 6175, Train_Loss: 4399.84912109375, Val_Loss: 4607.98388671875\n",
      "Epoch 6176, Train_Loss: 4399.705078125, Val_Loss: 4607.81396484375\n",
      "Epoch 6177, Train_Loss: 4399.5732421875, Val_Loss: 4607.65478515625\n",
      "Epoch 6178, Train_Loss: 4399.4443359375, Val_Loss: 4607.4765625\n",
      "Epoch 6179, Train_Loss: 4399.29541015625, Val_Loss: 4607.3056640625\n",
      "Epoch 6180, Train_Loss: 4399.1533203125, Val_Loss: 4607.14111328125\n",
      "Epoch 6181, Train_Loss: 4398.99853515625, Val_Loss: 4606.95751953125\n",
      "Epoch 6182, Train_Loss: 4398.853515625, Val_Loss: 4606.79296875\n",
      "Epoch 6183, Train_Loss: 4398.712890625, Val_Loss: 4606.6240234375\n",
      "Epoch 6184, Train_Loss: 4398.5654296875, Val_Loss: 4606.46630859375\n",
      "Epoch 6185, Train_Loss: 4398.42333984375, Val_Loss: 4606.298828125\n",
      "Epoch 6186, Train_Loss: 4398.267578125, Val_Loss: 4606.13037109375\n",
      "Epoch 6187, Train_Loss: 4398.13818359375, Val_Loss: 4605.9560546875\n",
      "Epoch 6188, Train_Loss: 4397.9697265625, Val_Loss: 4605.74365234375\n",
      "Epoch 6189, Train_Loss: 4397.8466796875, Val_Loss: 4605.576171875\n",
      "Epoch 6190, Train_Loss: 4397.70263671875, Val_Loss: 4605.41064453125\n",
      "Epoch 6191, Train_Loss: 4397.55078125, Val_Loss: 4605.25146484375\n",
      "Epoch 6192, Train_Loss: 4397.42431640625, Val_Loss: 4605.0830078125\n",
      "Epoch 6193, Train_Loss: 4397.2861328125, Val_Loss: 4604.9150390625\n",
      "Epoch 6194, Train_Loss: 4397.15966796875, Val_Loss: 4604.73291015625\n",
      "Epoch 6195, Train_Loss: 4397.02880859375, Val_Loss: 4604.5634765625\n",
      "Epoch 6196, Train_Loss: 4396.88720703125, Val_Loss: 4604.40087890625\n",
      "Epoch 6197, Train_Loss: 4396.70166015625, Val_Loss: 4604.24609375\n",
      "Epoch 6198, Train_Loss: 4396.556640625, Val_Loss: 4604.07080078125\n",
      "Epoch 6199, Train_Loss: 4396.42822265625, Val_Loss: 4603.8984375\n",
      "Epoch 6200, Train_Loss: 4396.28759765625, Val_Loss: 4603.7265625\n",
      "Epoch 6201, Train_Loss: 4396.177734375, Val_Loss: 4603.556640625\n",
      "Epoch 6202, Train_Loss: 4396.04150390625, Val_Loss: 4603.39404296875\n",
      "Epoch 6203, Train_Loss: 4395.9111328125, Val_Loss: 4603.2275390625\n",
      "Epoch 6204, Train_Loss: 4395.7724609375, Val_Loss: 4603.05908203125\n",
      "Epoch 6205, Train_Loss: 4395.63916015625, Val_Loss: 4602.88427734375\n",
      "Epoch 6206, Train_Loss: 4395.48388671875, Val_Loss: 4602.7158203125\n",
      "Epoch 6207, Train_Loss: 4395.33251953125, Val_Loss: 4602.52734375\n",
      "Epoch 6208, Train_Loss: 4395.20654296875, Val_Loss: 4602.35986328125\n",
      "Epoch 6209, Train_Loss: 4395.0732421875, Val_Loss: 4602.2001953125\n",
      "Epoch 6210, Train_Loss: 4394.9326171875, Val_Loss: 4602.02197265625\n",
      "Epoch 6211, Train_Loss: 4394.7216796875, Val_Loss: 4601.86474609375\n",
      "Epoch 6212, Train_Loss: 4394.57568359375, Val_Loss: 4601.7001953125\n",
      "Epoch 6213, Train_Loss: 4394.4482421875, Val_Loss: 4601.53466796875\n",
      "Epoch 6214, Train_Loss: 4394.3232421875, Val_Loss: 4601.359375\n",
      "Epoch 6215, Train_Loss: 4394.1728515625, Val_Loss: 4601.18798828125\n",
      "Epoch 6216, Train_Loss: 4394.02197265625, Val_Loss: 4601.01904296875\n",
      "Epoch 6217, Train_Loss: 4393.8828125, Val_Loss: 4600.8486328125\n",
      "Epoch 6218, Train_Loss: 4393.73974609375, Val_Loss: 4600.68359375\n",
      "Epoch 6219, Train_Loss: 4393.607421875, Val_Loss: 4600.5224609375\n",
      "Epoch 6220, Train_Loss: 4393.46533203125, Val_Loss: 4600.34619140625\n",
      "Epoch 6221, Train_Loss: 4393.30810546875, Val_Loss: 4600.16943359375\n",
      "Epoch 6222, Train_Loss: 4393.162109375, Val_Loss: 4600.00390625\n",
      "Epoch 6223, Train_Loss: 4393.02001953125, Val_Loss: 4599.8388671875\n",
      "Epoch 6224, Train_Loss: 4392.85009765625, Val_Loss: 4599.68701171875\n",
      "Epoch 6225, Train_Loss: 4392.7197265625, Val_Loss: 4599.51953125\n",
      "Epoch 6226, Train_Loss: 4392.5693359375, Val_Loss: 4599.30615234375\n",
      "Epoch 6227, Train_Loss: 4392.41259765625, Val_Loss: 4599.13232421875\n",
      "Epoch 6228, Train_Loss: 4392.27587890625, Val_Loss: 4598.970703125\n",
      "Epoch 6229, Train_Loss: 4392.14404296875, Val_Loss: 4598.8046875\n",
      "Epoch 6230, Train_Loss: 4392.01416015625, Val_Loss: 4598.6337890625\n",
      "Epoch 6231, Train_Loss: 4391.884765625, Val_Loss: 4598.47509765625\n",
      "Epoch 6232, Train_Loss: 4391.748046875, Val_Loss: 4598.31103515625\n",
      "Epoch 6233, Train_Loss: 4391.615234375, Val_Loss: 4598.1494140625\n",
      "Epoch 6234, Train_Loss: 4391.48291015625, Val_Loss: 4597.96826171875\n",
      "Epoch 6235, Train_Loss: 4391.3369140625, Val_Loss: 4597.7998046875\n",
      "Epoch 6236, Train_Loss: 4391.19921875, Val_Loss: 4597.62939453125\n",
      "Epoch 6237, Train_Loss: 4391.03369140625, Val_Loss: 4597.4765625\n",
      "Epoch 6238, Train_Loss: 4390.9072265625, Val_Loss: 4597.318359375\n",
      "Epoch 6239, Train_Loss: 4390.7802734375, Val_Loss: 4597.14453125\n",
      "Epoch 6240, Train_Loss: 4390.646484375, Val_Loss: 4596.9755859375\n",
      "Epoch 6241, Train_Loss: 4390.48388671875, Val_Loss: 4596.7978515625\n",
      "Epoch 6242, Train_Loss: 4390.3642578125, Val_Loss: 4596.634765625\n",
      "Epoch 6243, Train_Loss: 4390.2275390625, Val_Loss: 4596.46337890625\n",
      "Epoch 6244, Train_Loss: 4390.0947265625, Val_Loss: 4596.310546875\n",
      "Epoch 6245, Train_Loss: 4389.87841796875, Val_Loss: 4596.1337890625\n",
      "Epoch 6246, Train_Loss: 4389.71826171875, Val_Loss: 4595.974609375\n",
      "Epoch 6247, Train_Loss: 4389.587890625, Val_Loss: 4595.79638671875\n",
      "Epoch 6248, Train_Loss: 4389.44091796875, Val_Loss: 4595.63623046875\n",
      "Epoch 6249, Train_Loss: 4389.30810546875, Val_Loss: 4595.4599609375\n",
      "Epoch 6250, Train_Loss: 4389.1708984375, Val_Loss: 4595.30810546875\n",
      "Epoch 6251, Train_Loss: 4388.98828125, Val_Loss: 4595.14697265625\n",
      "Epoch 6252, Train_Loss: 4388.861328125, Val_Loss: 4594.97705078125\n",
      "Epoch 6253, Train_Loss: 4388.7119140625, Val_Loss: 4594.8095703125\n",
      "Epoch 6254, Train_Loss: 4388.6171875, Val_Loss: 4594.62353515625\n",
      "Epoch 6255, Train_Loss: 4388.48779296875, Val_Loss: 4594.45556640625\n",
      "Epoch 6256, Train_Loss: 4388.3310546875, Val_Loss: 4594.28271484375\n",
      "Epoch 6257, Train_Loss: 4388.19580078125, Val_Loss: 4594.1318359375\n",
      "Epoch 6258, Train_Loss: 4388.05126953125, Val_Loss: 4593.974609375\n",
      "Epoch 6259, Train_Loss: 4387.9072265625, Val_Loss: 4593.80322265625\n",
      "Epoch 6260, Train_Loss: 4387.77880859375, Val_Loss: 4593.63623046875\n",
      "Epoch 6261, Train_Loss: 4387.6298828125, Val_Loss: 4593.46533203125\n",
      "Epoch 6262, Train_Loss: 4387.48876953125, Val_Loss: 4593.310546875\n",
      "Epoch 6263, Train_Loss: 4387.35888671875, Val_Loss: 4593.1015625\n",
      "Epoch 6264, Train_Loss: 4387.169921875, Val_Loss: 4592.94921875\n",
      "Epoch 6265, Train_Loss: 4387.041015625, Val_Loss: 4592.77685546875\n",
      "Epoch 6266, Train_Loss: 4386.9052734375, Val_Loss: 4592.6044921875\n",
      "Epoch 6267, Train_Loss: 4386.7548828125, Val_Loss: 4592.43017578125\n",
      "Epoch 6268, Train_Loss: 4386.609375, Val_Loss: 4592.2607421875\n",
      "Epoch 6269, Train_Loss: 4386.48095703125, Val_Loss: 4592.09814453125\n",
      "Epoch 6270, Train_Loss: 4386.35498046875, Val_Loss: 4591.93505859375\n",
      "Epoch 6271, Train_Loss: 4386.20947265625, Val_Loss: 4591.77197265625\n",
      "Epoch 6272, Train_Loss: 4386.06640625, Val_Loss: 4591.6083984375\n",
      "Epoch 6273, Train_Loss: 4385.92578125, Val_Loss: 4591.44384765625\n",
      "Epoch 6274, Train_Loss: 4385.8056640625, Val_Loss: 4591.2734375\n",
      "Epoch 6275, Train_Loss: 4385.6845703125, Val_Loss: 4591.109375\n",
      "Epoch 6276, Train_Loss: 4385.53955078125, Val_Loss: 4590.947265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6277, Train_Loss: 4385.36767578125, Val_Loss: 4590.79296875\n",
      "Epoch 6278, Train_Loss: 4385.216796875, Val_Loss: 4590.61328125\n",
      "Epoch 6279, Train_Loss: 4385.0859375, Val_Loss: 4590.4482421875\n",
      "Epoch 6280, Train_Loss: 4384.94921875, Val_Loss: 4590.2841796875\n",
      "Epoch 6281, Train_Loss: 4384.8310546875, Val_Loss: 4590.107421875\n",
      "Epoch 6282, Train_Loss: 4384.662109375, Val_Loss: 4589.9375\n",
      "Epoch 6283, Train_Loss: 4384.52685546875, Val_Loss: 4589.76123046875\n",
      "Epoch 6284, Train_Loss: 4384.39306640625, Val_Loss: 4589.6201171875\n",
      "Epoch 6285, Train_Loss: 4384.25341796875, Val_Loss: 4589.45458984375\n",
      "Epoch 6286, Train_Loss: 4384.12548828125, Val_Loss: 4589.2802734375\n",
      "Epoch 6287, Train_Loss: 4383.98583984375, Val_Loss: 4589.11572265625\n",
      "Epoch 6288, Train_Loss: 4383.8603515625, Val_Loss: 4588.947265625\n",
      "Epoch 6289, Train_Loss: 4383.71484375, Val_Loss: 4588.7666015625\n",
      "Epoch 6290, Train_Loss: 4383.5703125, Val_Loss: 4588.6103515625\n",
      "Epoch 6291, Train_Loss: 4383.43359375, Val_Loss: 4588.458984375\n",
      "Epoch 6292, Train_Loss: 4383.28857421875, Val_Loss: 4588.2822265625\n",
      "Epoch 6293, Train_Loss: 4383.14697265625, Val_Loss: 4588.12060546875\n",
      "Epoch 6294, Train_Loss: 4383.02001953125, Val_Loss: 4587.9345703125\n",
      "Epoch 6295, Train_Loss: 4382.900390625, Val_Loss: 4587.7783203125\n",
      "Epoch 6296, Train_Loss: 4382.7646484375, Val_Loss: 4587.611328125\n",
      "Epoch 6297, Train_Loss: 4382.61962890625, Val_Loss: 4587.458984375\n",
      "Epoch 6298, Train_Loss: 4382.49560546875, Val_Loss: 4587.29541015625\n",
      "Epoch 6299, Train_Loss: 4382.35107421875, Val_Loss: 4587.13330078125\n",
      "Epoch 6300, Train_Loss: 4382.2158203125, Val_Loss: 4586.9638671875\n",
      "Epoch 6301, Train_Loss: 4382.07763671875, Val_Loss: 4586.75146484375\n",
      "Epoch 6302, Train_Loss: 4381.9287109375, Val_Loss: 4586.5869140625\n",
      "Epoch 6303, Train_Loss: 4381.7890625, Val_Loss: 4586.42529296875\n",
      "Epoch 6304, Train_Loss: 4381.62646484375, Val_Loss: 4586.26904296875\n",
      "Epoch 6305, Train_Loss: 4381.4873046875, Val_Loss: 4586.10205078125\n",
      "Epoch 6306, Train_Loss: 4381.35107421875, Val_Loss: 4585.93994140625\n",
      "Epoch 6307, Train_Loss: 4381.25, Val_Loss: 4585.75830078125\n",
      "Epoch 6308, Train_Loss: 4381.11083984375, Val_Loss: 4585.6005859375\n",
      "Epoch 6309, Train_Loss: 4380.96923828125, Val_Loss: 4585.44384765625\n",
      "Epoch 6310, Train_Loss: 4380.82080078125, Val_Loss: 4585.26953125\n",
      "Epoch 6311, Train_Loss: 4380.69482421875, Val_Loss: 4585.11767578125\n",
      "Epoch 6312, Train_Loss: 4380.564453125, Val_Loss: 4584.96630859375\n",
      "Epoch 6313, Train_Loss: 4380.4228515625, Val_Loss: 4584.787109375\n",
      "Epoch 6314, Train_Loss: 4380.29052734375, Val_Loss: 4584.607421875\n",
      "Epoch 6315, Train_Loss: 4380.16357421875, Val_Loss: 4584.44677734375\n",
      "Epoch 6316, Train_Loss: 4380.03564453125, Val_Loss: 4584.291015625\n",
      "Epoch 6317, Train_Loss: 4379.83935546875, Val_Loss: 4584.13623046875\n",
      "Epoch 6318, Train_Loss: 4379.703125, Val_Loss: 4583.96826171875\n",
      "Epoch 6319, Train_Loss: 4379.572265625, Val_Loss: 4583.80517578125\n",
      "Epoch 6320, Train_Loss: 4379.3212890625, Val_Loss: 4583.6298828125\n",
      "Epoch 6321, Train_Loss: 4379.19287109375, Val_Loss: 4583.466796875\n",
      "Epoch 6322, Train_Loss: 4379.0361328125, Val_Loss: 4583.29833984375\n",
      "Epoch 6323, Train_Loss: 4378.8935546875, Val_Loss: 4583.1337890625\n",
      "Epoch 6324, Train_Loss: 4378.74755859375, Val_Loss: 4582.9892578125\n",
      "Epoch 6325, Train_Loss: 4378.6142578125, Val_Loss: 4582.806640625\n",
      "Epoch 6326, Train_Loss: 4378.478515625, Val_Loss: 4582.640625\n",
      "Epoch 6327, Train_Loss: 4378.330078125, Val_Loss: 4582.48291015625\n",
      "Epoch 6328, Train_Loss: 4378.18994140625, Val_Loss: 4582.31298828125\n",
      "Epoch 6329, Train_Loss: 4378.05126953125, Val_Loss: 4582.1357421875\n",
      "Epoch 6330, Train_Loss: 4377.92138671875, Val_Loss: 4581.99169921875\n",
      "Epoch 6331, Train_Loss: 4377.74560546875, Val_Loss: 4581.818359375\n",
      "Epoch 6332, Train_Loss: 4377.6025390625, Val_Loss: 4581.65283203125\n",
      "Epoch 6333, Train_Loss: 4377.4755859375, Val_Loss: 4581.4853515625\n",
      "Epoch 6334, Train_Loss: 4377.33837890625, Val_Loss: 4581.31787109375\n",
      "Epoch 6335, Train_Loss: 4377.2138671875, Val_Loss: 4581.15478515625\n",
      "Epoch 6336, Train_Loss: 4377.06640625, Val_Loss: 4580.99560546875\n",
      "Epoch 6337, Train_Loss: 4376.9267578125, Val_Loss: 4580.8349609375\n",
      "Epoch 6338, Train_Loss: 4376.79150390625, Val_Loss: 4580.66943359375\n",
      "Epoch 6339, Train_Loss: 4376.6279296875, Val_Loss: 4580.48046875\n",
      "Epoch 6340, Train_Loss: 4376.4990234375, Val_Loss: 4580.306640625\n",
      "Epoch 6341, Train_Loss: 4376.375, Val_Loss: 4580.138671875\n",
      "Epoch 6342, Train_Loss: 4376.216796875, Val_Loss: 4579.9853515625\n",
      "Epoch 6343, Train_Loss: 4376.09033203125, Val_Loss: 4579.81494140625\n",
      "Epoch 6344, Train_Loss: 4375.9228515625, Val_Loss: 4579.6513671875\n",
      "Epoch 6345, Train_Loss: 4375.78857421875, Val_Loss: 4579.49560546875\n",
      "Epoch 6346, Train_Loss: 4375.68115234375, Val_Loss: 4579.32080078125\n",
      "Epoch 6347, Train_Loss: 4375.5361328125, Val_Loss: 4579.15283203125\n",
      "Epoch 6348, Train_Loss: 4375.408203125, Val_Loss: 4578.99560546875\n",
      "Epoch 6349, Train_Loss: 4375.263671875, Val_Loss: 4578.837890625\n",
      "Epoch 6350, Train_Loss: 4375.12939453125, Val_Loss: 4578.66552734375\n",
      "Epoch 6351, Train_Loss: 4374.990234375, Val_Loss: 4578.50537109375\n",
      "Epoch 6352, Train_Loss: 4374.83056640625, Val_Loss: 4578.3466796875\n",
      "Epoch 6353, Train_Loss: 4374.70556640625, Val_Loss: 4578.18798828125\n",
      "Epoch 6354, Train_Loss: 4374.57177734375, Val_Loss: 4578.02197265625\n",
      "Epoch 6355, Train_Loss: 4374.44921875, Val_Loss: 4577.8642578125\n",
      "Epoch 6356, Train_Loss: 4374.3203125, Val_Loss: 4577.7021484375\n",
      "Epoch 6357, Train_Loss: 4374.1474609375, Val_Loss: 4577.54736328125\n",
      "Epoch 6358, Train_Loss: 4374.00927734375, Val_Loss: 4577.37060546875\n",
      "Epoch 6359, Train_Loss: 4373.86279296875, Val_Loss: 4577.2109375\n",
      "Epoch 6360, Train_Loss: 4373.73828125, Val_Loss: 4577.0478515625\n",
      "Epoch 6361, Train_Loss: 4373.64501953125, Val_Loss: 4576.8720703125\n",
      "Epoch 6362, Train_Loss: 4373.51904296875, Val_Loss: 4576.70947265625\n",
      "Epoch 6363, Train_Loss: 4373.3662109375, Val_Loss: 4576.55615234375\n",
      "Epoch 6364, Train_Loss: 4373.236328125, Val_Loss: 4576.40625\n",
      "Epoch 6365, Train_Loss: 4373.1025390625, Val_Loss: 4576.23876953125\n",
      "Epoch 6366, Train_Loss: 4372.966796875, Val_Loss: 4576.064453125\n",
      "Epoch 6367, Train_Loss: 4372.82861328125, Val_Loss: 4575.89013671875\n",
      "Epoch 6368, Train_Loss: 4372.69384765625, Val_Loss: 4575.72119140625\n",
      "Epoch 6369, Train_Loss: 4372.560546875, Val_Loss: 4575.56005859375\n",
      "Epoch 6370, Train_Loss: 4372.43310546875, Val_Loss: 4575.39990234375\n",
      "Epoch 6371, Train_Loss: 4372.24951171875, Val_Loss: 4575.24560546875\n",
      "Epoch 6372, Train_Loss: 4372.1142578125, Val_Loss: 4575.0859375\n",
      "Epoch 6373, Train_Loss: 4371.95166015625, Val_Loss: 4574.92041015625\n",
      "Epoch 6374, Train_Loss: 4371.82421875, Val_Loss: 4574.7431640625\n",
      "Epoch 6375, Train_Loss: 4371.6923828125, Val_Loss: 4574.58740234375\n",
      "Epoch 6376, Train_Loss: 4371.56005859375, Val_Loss: 4574.39306640625\n",
      "Epoch 6377, Train_Loss: 4371.4228515625, Val_Loss: 4574.23291015625\n",
      "Epoch 6378, Train_Loss: 4371.29345703125, Val_Loss: 4574.0947265625\n",
      "Epoch 6379, Train_Loss: 4371.1650390625, Val_Loss: 4573.93310546875\n",
      "Epoch 6380, Train_Loss: 4371.01953125, Val_Loss: 4573.76513671875\n",
      "Epoch 6381, Train_Loss: 4370.888671875, Val_Loss: 4573.5888671875\n",
      "Epoch 6382, Train_Loss: 4370.751953125, Val_Loss: 4573.4306640625\n",
      "Epoch 6383, Train_Loss: 4370.59814453125, Val_Loss: 4573.26904296875\n",
      "Epoch 6384, Train_Loss: 4370.43017578125, Val_Loss: 4573.1171875\n",
      "Epoch 6385, Train_Loss: 4370.29833984375, Val_Loss: 4572.94580078125\n",
      "Epoch 6386, Train_Loss: 4370.083984375, Val_Loss: 4572.78857421875\n",
      "Epoch 6387, Train_Loss: 4369.95263671875, Val_Loss: 4572.63818359375\n",
      "Epoch 6388, Train_Loss: 4369.826171875, Val_Loss: 4572.46044921875\n",
      "Epoch 6389, Train_Loss: 4369.673828125, Val_Loss: 4572.298828125\n",
      "Epoch 6390, Train_Loss: 4369.556640625, Val_Loss: 4572.1337890625\n",
      "Epoch 6391, Train_Loss: 4369.4375, Val_Loss: 4571.9794921875\n",
      "Epoch 6392, Train_Loss: 4369.3046875, Val_Loss: 4571.82666015625\n",
      "Epoch 6393, Train_Loss: 4369.16455078125, Val_Loss: 4571.6650390625\n",
      "Epoch 6394, Train_Loss: 4369.03125, Val_Loss: 4571.4794921875\n",
      "Epoch 6395, Train_Loss: 4368.84521484375, Val_Loss: 4571.32666015625\n",
      "Epoch 6396, Train_Loss: 4368.7197265625, Val_Loss: 4571.16357421875\n",
      "Epoch 6397, Train_Loss: 4368.59130859375, Val_Loss: 4571.00341796875\n",
      "Epoch 6398, Train_Loss: 4368.4443359375, Val_Loss: 4570.857421875\n",
      "Epoch 6399, Train_Loss: 4368.3134765625, Val_Loss: 4570.697265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6400, Train_Loss: 4368.18505859375, Val_Loss: 4570.53466796875\n",
      "Epoch 6401, Train_Loss: 4368.05615234375, Val_Loss: 4570.35693359375\n",
      "Epoch 6402, Train_Loss: 4367.90966796875, Val_Loss: 4570.19140625\n",
      "Epoch 6403, Train_Loss: 4367.75390625, Val_Loss: 4570.05078125\n",
      "Epoch 6404, Train_Loss: 4367.62109375, Val_Loss: 4569.89208984375\n",
      "Epoch 6405, Train_Loss: 4367.4814453125, Val_Loss: 4569.71728515625\n",
      "Epoch 6406, Train_Loss: 4367.34912109375, Val_Loss: 4569.5595703125\n",
      "Epoch 6407, Train_Loss: 4367.2099609375, Val_Loss: 4569.39990234375\n",
      "Epoch 6408, Train_Loss: 4367.072265625, Val_Loss: 4569.2392578125\n",
      "Epoch 6409, Train_Loss: 4366.9482421875, Val_Loss: 4569.076171875\n",
      "Epoch 6410, Train_Loss: 4366.82373046875, Val_Loss: 4568.9150390625\n",
      "Epoch 6411, Train_Loss: 4366.673828125, Val_Loss: 4568.7646484375\n",
      "Epoch 6412, Train_Loss: 4366.5546875, Val_Loss: 4568.60498046875\n",
      "Epoch 6413, Train_Loss: 4366.40380859375, Val_Loss: 4568.43798828125\n",
      "Epoch 6414, Train_Loss: 4366.31005859375, Val_Loss: 4568.24560546875\n",
      "Epoch 6415, Train_Loss: 4366.18798828125, Val_Loss: 4568.0927734375\n",
      "Epoch 6416, Train_Loss: 4366.0654296875, Val_Loss: 4567.92626953125\n",
      "Epoch 6417, Train_Loss: 4365.93115234375, Val_Loss: 4567.76953125\n",
      "Epoch 6418, Train_Loss: 4365.78515625, Val_Loss: 4567.623046875\n",
      "Epoch 6419, Train_Loss: 4365.6484375, Val_Loss: 4567.46337890625\n",
      "Epoch 6420, Train_Loss: 4365.509765625, Val_Loss: 4567.3017578125\n",
      "Epoch 6421, Train_Loss: 4365.37841796875, Val_Loss: 4567.14501953125\n",
      "Epoch 6422, Train_Loss: 4365.25341796875, Val_Loss: 4566.98193359375\n",
      "Epoch 6423, Train_Loss: 4365.107421875, Val_Loss: 4566.8212890625\n",
      "Epoch 6424, Train_Loss: 4364.9150390625, Val_Loss: 4566.6669921875\n",
      "Epoch 6425, Train_Loss: 4364.77587890625, Val_Loss: 4566.50537109375\n",
      "Epoch 6426, Train_Loss: 4364.6513671875, Val_Loss: 4566.34912109375\n",
      "Epoch 6427, Train_Loss: 4364.51953125, Val_Loss: 4566.19140625\n",
      "Epoch 6428, Train_Loss: 4364.38427734375, Val_Loss: 4566.01953125\n",
      "Epoch 6429, Train_Loss: 4364.26025390625, Val_Loss: 4565.875\n",
      "Epoch 6430, Train_Loss: 4364.1376953125, Val_Loss: 4565.70654296875\n",
      "Epoch 6431, Train_Loss: 4364.0048828125, Val_Loss: 4565.5634765625\n",
      "Epoch 6432, Train_Loss: 4363.873046875, Val_Loss: 4565.40673828125\n",
      "Epoch 6433, Train_Loss: 4363.7177734375, Val_Loss: 4565.23779296875\n",
      "Epoch 6434, Train_Loss: 4363.58984375, Val_Loss: 4565.08056640625\n",
      "Epoch 6435, Train_Loss: 4363.46044921875, Val_Loss: 4564.9150390625\n",
      "Epoch 6436, Train_Loss: 4363.34375, Val_Loss: 4564.75390625\n",
      "Epoch 6437, Train_Loss: 4363.2060546875, Val_Loss: 4564.603515625\n",
      "Epoch 6438, Train_Loss: 4363.03564453125, Val_Loss: 4564.44775390625\n",
      "Epoch 6439, Train_Loss: 4362.89990234375, Val_Loss: 4564.28662109375\n",
      "Epoch 6440, Train_Loss: 4362.76953125, Val_Loss: 4564.1337890625\n",
      "Epoch 6441, Train_Loss: 4362.6396484375, Val_Loss: 4563.96142578125\n",
      "Epoch 6442, Train_Loss: 4362.51123046875, Val_Loss: 4563.79541015625\n",
      "Epoch 6443, Train_Loss: 4362.38671875, Val_Loss: 4563.62255859375\n",
      "Epoch 6444, Train_Loss: 4362.234375, Val_Loss: 4563.47900390625\n",
      "Epoch 6445, Train_Loss: 4362.119140625, Val_Loss: 4563.32373046875\n",
      "Epoch 6446, Train_Loss: 4361.99169921875, Val_Loss: 4563.1669921875\n",
      "Epoch 6447, Train_Loss: 4361.857421875, Val_Loss: 4563.01220703125\n",
      "Epoch 6448, Train_Loss: 4361.7216796875, Val_Loss: 4562.83984375\n",
      "Epoch 6449, Train_Loss: 4361.58349609375, Val_Loss: 4562.68408203125\n",
      "Epoch 6450, Train_Loss: 4361.439453125, Val_Loss: 4562.52099609375\n",
      "Epoch 6451, Train_Loss: 4361.2802734375, Val_Loss: 4562.37841796875\n",
      "Epoch 6452, Train_Loss: 4361.146484375, Val_Loss: 4562.1767578125\n",
      "Epoch 6453, Train_Loss: 4361.021484375, Val_Loss: 4562.02001953125\n",
      "Epoch 6454, Train_Loss: 4360.873046875, Val_Loss: 4561.86376953125\n",
      "Epoch 6455, Train_Loss: 4360.75927734375, Val_Loss: 4561.69189453125\n",
      "Epoch 6456, Train_Loss: 4360.6337890625, Val_Loss: 4561.5302734375\n",
      "Epoch 6457, Train_Loss: 4360.4892578125, Val_Loss: 4561.37255859375\n",
      "Epoch 6458, Train_Loss: 4360.36767578125, Val_Loss: 4561.22802734375\n",
      "Epoch 6459, Train_Loss: 4360.2373046875, Val_Loss: 4561.06396484375\n",
      "Epoch 6460, Train_Loss: 4360.111328125, Val_Loss: 4560.90283203125\n",
      "Epoch 6461, Train_Loss: 4359.9892578125, Val_Loss: 4560.74462890625\n",
      "Epoch 6462, Train_Loss: 4359.8291015625, Val_Loss: 4560.58154296875\n",
      "Epoch 6463, Train_Loss: 4359.697265625, Val_Loss: 4560.42724609375\n",
      "Epoch 6464, Train_Loss: 4359.548828125, Val_Loss: 4560.26806640625\n",
      "Epoch 6465, Train_Loss: 4359.3779296875, Val_Loss: 4560.1240234375\n",
      "Epoch 6466, Train_Loss: 4359.2509765625, Val_Loss: 4559.96435546875\n",
      "Epoch 6467, Train_Loss: 4359.12939453125, Val_Loss: 4559.80908203125\n",
      "Epoch 6468, Train_Loss: 4359.05029296875, Val_Loss: 4559.64599609375\n",
      "Epoch 6469, Train_Loss: 4358.92236328125, Val_Loss: 4559.494140625\n",
      "Epoch 6470, Train_Loss: 4358.78759765625, Val_Loss: 4559.33447265625\n",
      "Epoch 6471, Train_Loss: 4358.5, Val_Loss: 4559.16455078125\n",
      "Epoch 6472, Train_Loss: 4358.37451171875, Val_Loss: 4559.02490234375\n",
      "Epoch 6473, Train_Loss: 4358.23681640625, Val_Loss: 4558.857421875\n",
      "Epoch 6474, Train_Loss: 4358.1005859375, Val_Loss: 4558.69873046875\n",
      "Epoch 6475, Train_Loss: 4357.974609375, Val_Loss: 4558.53662109375\n",
      "Epoch 6476, Train_Loss: 4357.84423828125, Val_Loss: 4558.36962890625\n",
      "Epoch 6477, Train_Loss: 4357.7197265625, Val_Loss: 4558.224609375\n",
      "Epoch 6478, Train_Loss: 4357.5234375, Val_Loss: 4558.07177734375\n",
      "Epoch 6479, Train_Loss: 4357.3974609375, Val_Loss: 4557.9111328125\n",
      "Epoch 6480, Train_Loss: 4357.25341796875, Val_Loss: 4557.74853515625\n",
      "Epoch 6481, Train_Loss: 4357.115234375, Val_Loss: 4557.59228515625\n",
      "Epoch 6482, Train_Loss: 4357.0, Val_Loss: 4557.416015625\n",
      "Epoch 6483, Train_Loss: 4356.880859375, Val_Loss: 4557.25537109375\n",
      "Epoch 6484, Train_Loss: 4356.736328125, Val_Loss: 4557.10595703125\n",
      "Epoch 6485, Train_Loss: 4356.6025390625, Val_Loss: 4556.96337890625\n",
      "Epoch 6486, Train_Loss: 4356.4765625, Val_Loss: 4556.79736328125\n",
      "Epoch 6487, Train_Loss: 4356.33984375, Val_Loss: 4556.642578125\n",
      "Epoch 6488, Train_Loss: 4356.212890625, Val_Loss: 4556.47509765625\n",
      "Epoch 6489, Train_Loss: 4356.0732421875, Val_Loss: 4556.31494140625\n",
      "Epoch 6490, Train_Loss: 4355.9140625, Val_Loss: 4556.13671875\n",
      "Epoch 6491, Train_Loss: 4355.78759765625, Val_Loss: 4555.97900390625\n",
      "Epoch 6492, Train_Loss: 4355.6279296875, Val_Loss: 4555.83203125\n",
      "Epoch 6493, Train_Loss: 4355.4990234375, Val_Loss: 4555.66259765625\n",
      "Epoch 6494, Train_Loss: 4355.3544921875, Val_Loss: 4555.5146484375\n",
      "Epoch 6495, Train_Loss: 4355.24755859375, Val_Loss: 4555.34619140625\n",
      "Epoch 6496, Train_Loss: 4355.1220703125, Val_Loss: 4555.19140625\n",
      "Epoch 6497, Train_Loss: 4355.0, Val_Loss: 4555.03515625\n",
      "Epoch 6498, Train_Loss: 4354.875, Val_Loss: 4554.8759765625\n",
      "Epoch 6499, Train_Loss: 4354.73388671875, Val_Loss: 4554.728515625\n",
      "Epoch 6500, Train_Loss: 4354.6162109375, Val_Loss: 4554.56640625\n",
      "Epoch 6501, Train_Loss: 4354.4814453125, Val_Loss: 4554.41650390625\n",
      "Epoch 6502, Train_Loss: 4354.349609375, Val_Loss: 4554.23876953125\n",
      "Epoch 6503, Train_Loss: 4354.2197265625, Val_Loss: 4554.08740234375\n",
      "Epoch 6504, Train_Loss: 4354.07958984375, Val_Loss: 4553.93359375\n",
      "Epoch 6505, Train_Loss: 4353.90185546875, Val_Loss: 4553.77294921875\n",
      "Epoch 6506, Train_Loss: 4353.77392578125, Val_Loss: 4553.62451171875\n",
      "Epoch 6507, Train_Loss: 4353.66015625, Val_Loss: 4553.46435546875\n",
      "Epoch 6508, Train_Loss: 4353.53173828125, Val_Loss: 4553.30224609375\n",
      "Epoch 6509, Train_Loss: 4353.4111328125, Val_Loss: 4553.1337890625\n",
      "Epoch 6510, Train_Loss: 4353.27490234375, Val_Loss: 4552.99560546875\n",
      "Epoch 6511, Train_Loss: 4353.15087890625, Val_Loss: 4552.82275390625\n",
      "Epoch 6512, Train_Loss: 4353.025390625, Val_Loss: 4552.6806640625\n",
      "Epoch 6513, Train_Loss: 4352.90869140625, Val_Loss: 4552.5244140625\n",
      "Epoch 6514, Train_Loss: 4352.7763671875, Val_Loss: 4552.3623046875\n",
      "Epoch 6515, Train_Loss: 4352.63818359375, Val_Loss: 4552.19677734375\n",
      "Epoch 6516, Train_Loss: 4352.5185546875, Val_Loss: 4552.0380859375\n",
      "Epoch 6517, Train_Loss: 4352.38916015625, Val_Loss: 4551.8876953125\n",
      "Epoch 6518, Train_Loss: 4352.255859375, Val_Loss: 4551.70849609375\n",
      "Epoch 6519, Train_Loss: 4352.0947265625, Val_Loss: 4551.56396484375\n",
      "Epoch 6520, Train_Loss: 4351.9677734375, Val_Loss: 4551.40966796875\n",
      "Epoch 6521, Train_Loss: 4351.83251953125, Val_Loss: 4551.24658203125\n",
      "Epoch 6522, Train_Loss: 4351.7548828125, Val_Loss: 4551.08056640625\n",
      "Epoch 6523, Train_Loss: 4351.61865234375, Val_Loss: 4550.9326171875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6524, Train_Loss: 4351.49609375, Val_Loss: 4550.775390625\n",
      "Epoch 6525, Train_Loss: 4351.34619140625, Val_Loss: 4550.63330078125\n",
      "Epoch 6526, Train_Loss: 4351.2275390625, Val_Loss: 4550.47705078125\n",
      "Epoch 6527, Train_Loss: 4351.099609375, Val_Loss: 4550.302734375\n",
      "Epoch 6528, Train_Loss: 4350.962890625, Val_Loss: 4550.1162109375\n",
      "Epoch 6529, Train_Loss: 4350.84521484375, Val_Loss: 4549.94775390625\n",
      "Epoch 6530, Train_Loss: 4350.7060546875, Val_Loss: 4549.8017578125\n",
      "Epoch 6531, Train_Loss: 4350.5771484375, Val_Loss: 4549.658203125\n",
      "Epoch 6532, Train_Loss: 4350.39794921875, Val_Loss: 4549.50048828125\n",
      "Epoch 6533, Train_Loss: 4350.27587890625, Val_Loss: 4549.3447265625\n",
      "Epoch 6534, Train_Loss: 4350.16552734375, Val_Loss: 4549.18798828125\n",
      "Epoch 6535, Train_Loss: 4350.02490234375, Val_Loss: 4549.021484375\n",
      "Epoch 6536, Train_Loss: 4349.87939453125, Val_Loss: 4548.85400390625\n",
      "Epoch 6537, Train_Loss: 4349.70849609375, Val_Loss: 4548.70654296875\n",
      "Epoch 6538, Train_Loss: 4349.59765625, Val_Loss: 4548.5556640625\n",
      "Epoch 6539, Train_Loss: 4349.46435546875, Val_Loss: 4548.41015625\n",
      "Epoch 6540, Train_Loss: 4349.34326171875, Val_Loss: 4548.26025390625\n",
      "Epoch 6541, Train_Loss: 4349.2080078125, Val_Loss: 4548.0966796875\n",
      "Epoch 6542, Train_Loss: 4349.07421875, Val_Loss: 4547.92724609375\n",
      "Epoch 6543, Train_Loss: 4348.9560546875, Val_Loss: 4547.76513671875\n",
      "Epoch 6544, Train_Loss: 4348.83544921875, Val_Loss: 4547.61083984375\n",
      "Epoch 6545, Train_Loss: 4348.70068359375, Val_Loss: 4547.45947265625\n",
      "Epoch 6546, Train_Loss: 4348.53564453125, Val_Loss: 4547.30712890625\n",
      "Epoch 6547, Train_Loss: 4348.33642578125, Val_Loss: 4547.1533203125\n",
      "Epoch 6548, Train_Loss: 4348.20556640625, Val_Loss: 4546.99609375\n",
      "Epoch 6549, Train_Loss: 4348.068359375, Val_Loss: 4546.83056640625\n",
      "Epoch 6550, Train_Loss: 4347.9296875, Val_Loss: 4546.67626953125\n",
      "Epoch 6551, Train_Loss: 4347.8056640625, Val_Loss: 4546.5166015625\n",
      "Epoch 6552, Train_Loss: 4347.6748046875, Val_Loss: 4546.361328125\n",
      "Epoch 6553, Train_Loss: 4347.54736328125, Val_Loss: 4546.2177734375\n",
      "Epoch 6554, Train_Loss: 4347.41650390625, Val_Loss: 4546.06103515625\n",
      "Epoch 6555, Train_Loss: 4347.28662109375, Val_Loss: 4545.8994140625\n",
      "Epoch 6556, Train_Loss: 4347.14697265625, Val_Loss: 4545.7265625\n",
      "Epoch 6557, Train_Loss: 4347.02001953125, Val_Loss: 4545.578125\n",
      "Epoch 6558, Train_Loss: 4346.88623046875, Val_Loss: 4545.41552734375\n",
      "Epoch 6559, Train_Loss: 4346.73486328125, Val_Loss: 4545.27685546875\n",
      "Epoch 6560, Train_Loss: 4346.60498046875, Val_Loss: 4545.1083984375\n",
      "Epoch 6561, Train_Loss: 4346.48388671875, Val_Loss: 4544.9462890625\n",
      "Epoch 6562, Train_Loss: 4346.36572265625, Val_Loss: 4544.78173828125\n",
      "Epoch 6563, Train_Loss: 4346.25732421875, Val_Loss: 4544.6376953125\n",
      "Epoch 6564, Train_Loss: 4346.13037109375, Val_Loss: 4544.47265625\n",
      "Epoch 6565, Train_Loss: 4346.00048828125, Val_Loss: 4544.314453125\n",
      "Epoch 6566, Train_Loss: 4345.83642578125, Val_Loss: 4544.13671875\n",
      "Epoch 6567, Train_Loss: 4345.71728515625, Val_Loss: 4543.98876953125\n",
      "Epoch 6568, Train_Loss: 4345.59375, Val_Loss: 4543.83251953125\n",
      "Epoch 6569, Train_Loss: 4345.470703125, Val_Loss: 4543.66552734375\n",
      "Epoch 6570, Train_Loss: 4345.3349609375, Val_Loss: 4543.5078125\n",
      "Epoch 6571, Train_Loss: 4345.20263671875, Val_Loss: 4543.35888671875\n",
      "Epoch 6572, Train_Loss: 4345.08544921875, Val_Loss: 4543.203125\n",
      "Epoch 6573, Train_Loss: 4344.935546875, Val_Loss: 4543.056640625\n",
      "Epoch 6574, Train_Loss: 4344.79833984375, Val_Loss: 4542.89892578125\n",
      "Epoch 6575, Train_Loss: 4344.66552734375, Val_Loss: 4542.74462890625\n",
      "Epoch 6576, Train_Loss: 4344.5732421875, Val_Loss: 4542.5810546875\n",
      "Epoch 6577, Train_Loss: 4344.45654296875, Val_Loss: 4542.42578125\n",
      "Epoch 6578, Train_Loss: 4344.32373046875, Val_Loss: 4542.275390625\n",
      "Epoch 6579, Train_Loss: 4344.20556640625, Val_Loss: 4542.11865234375\n",
      "Epoch 6580, Train_Loss: 4344.07080078125, Val_Loss: 4541.96142578125\n",
      "Epoch 6581, Train_Loss: 4343.94921875, Val_Loss: 4541.8154296875\n",
      "Epoch 6582, Train_Loss: 4343.8173828125, Val_Loss: 4541.66259765625\n",
      "Epoch 6583, Train_Loss: 4343.68798828125, Val_Loss: 4541.49560546875\n",
      "Epoch 6584, Train_Loss: 4343.5576171875, Val_Loss: 4541.3310546875\n",
      "Epoch 6585, Train_Loss: 4343.42431640625, Val_Loss: 4541.173828125\n",
      "Epoch 6586, Train_Loss: 4343.2041015625, Val_Loss: 4541.0341796875\n",
      "Epoch 6587, Train_Loss: 4343.0810546875, Val_Loss: 4540.8818359375\n",
      "Epoch 6588, Train_Loss: 4342.9658203125, Val_Loss: 4540.7265625\n",
      "Epoch 6589, Train_Loss: 4342.8369140625, Val_Loss: 4540.5498046875\n",
      "Epoch 6590, Train_Loss: 4342.71728515625, Val_Loss: 4540.41064453125\n",
      "Epoch 6591, Train_Loss: 4342.580078125, Val_Loss: 4540.25341796875\n",
      "Epoch 6592, Train_Loss: 4342.44677734375, Val_Loss: 4540.0986328125\n",
      "Epoch 6593, Train_Loss: 4342.32080078125, Val_Loss: 4539.95654296875\n",
      "Epoch 6594, Train_Loss: 4342.1884765625, Val_Loss: 4539.77685546875\n",
      "Epoch 6595, Train_Loss: 4342.05908203125, Val_Loss: 4539.6259765625\n",
      "Epoch 6596, Train_Loss: 4341.939453125, Val_Loss: 4539.46142578125\n",
      "Epoch 6597, Train_Loss: 4341.82080078125, Val_Loss: 4539.30859375\n",
      "Epoch 6598, Train_Loss: 4341.68798828125, Val_Loss: 4539.1513671875\n",
      "Epoch 6599, Train_Loss: 4341.576171875, Val_Loss: 4538.99462890625\n",
      "Epoch 6600, Train_Loss: 4341.40234375, Val_Loss: 4538.8525390625\n",
      "Epoch 6601, Train_Loss: 4341.2783203125, Val_Loss: 4538.705078125\n",
      "Epoch 6602, Train_Loss: 4341.16259765625, Val_Loss: 4538.54736328125\n",
      "Epoch 6603, Train_Loss: 4341.0576171875, Val_Loss: 4538.37451171875\n",
      "Epoch 6604, Train_Loss: 4340.91748046875, Val_Loss: 4538.1865234375\n",
      "Epoch 6605, Train_Loss: 4340.79296875, Val_Loss: 4538.037109375\n",
      "Epoch 6606, Train_Loss: 4340.67822265625, Val_Loss: 4537.88818359375\n",
      "Epoch 6607, Train_Loss: 4340.54638671875, Val_Loss: 4537.73583984375\n",
      "Epoch 6608, Train_Loss: 4340.4150390625, Val_Loss: 4537.58447265625\n",
      "Epoch 6609, Train_Loss: 4340.2822265625, Val_Loss: 4537.42724609375\n",
      "Epoch 6610, Train_Loss: 4340.15234375, Val_Loss: 4537.26416015625\n",
      "Epoch 6611, Train_Loss: 4340.015625, Val_Loss: 4537.0986328125\n",
      "Epoch 6612, Train_Loss: 4339.87451171875, Val_Loss: 4536.95263671875\n",
      "Epoch 6613, Train_Loss: 4339.73193359375, Val_Loss: 4536.80615234375\n",
      "Epoch 6614, Train_Loss: 4339.6005859375, Val_Loss: 4536.65625\n",
      "Epoch 6615, Train_Loss: 4339.4853515625, Val_Loss: 4536.49658203125\n",
      "Epoch 6616, Train_Loss: 4339.37109375, Val_Loss: 4536.34228515625\n",
      "Epoch 6617, Train_Loss: 4339.22265625, Val_Loss: 4536.1884765625\n",
      "Epoch 6618, Train_Loss: 4339.0849609375, Val_Loss: 4536.0302734375\n",
      "Epoch 6619, Train_Loss: 4338.978515625, Val_Loss: 4535.87060546875\n",
      "Epoch 6620, Train_Loss: 4338.85302734375, Val_Loss: 4535.73583984375\n",
      "Epoch 6621, Train_Loss: 4338.734375, Val_Loss: 4535.58203125\n",
      "Epoch 6622, Train_Loss: 4338.60205078125, Val_Loss: 4535.41943359375\n",
      "Epoch 6623, Train_Loss: 4338.4228515625, Val_Loss: 4535.2431640625\n",
      "Epoch 6624, Train_Loss: 4338.2939453125, Val_Loss: 4535.10302734375\n",
      "Epoch 6625, Train_Loss: 4338.1611328125, Val_Loss: 4534.96142578125\n",
      "Epoch 6626, Train_Loss: 4338.0439453125, Val_Loss: 4534.79833984375\n",
      "Epoch 6627, Train_Loss: 4337.869140625, Val_Loss: 4534.65625\n",
      "Epoch 6628, Train_Loss: 4337.74853515625, Val_Loss: 4534.50048828125\n",
      "Epoch 6629, Train_Loss: 4337.62158203125, Val_Loss: 4534.34716796875\n",
      "Epoch 6630, Train_Loss: 4337.54345703125, Val_Loss: 4534.1787109375\n",
      "Epoch 6631, Train_Loss: 4337.41259765625, Val_Loss: 4534.02783203125\n",
      "Epoch 6632, Train_Loss: 4337.28662109375, Val_Loss: 4533.87548828125\n",
      "Epoch 6633, Train_Loss: 4337.17578125, Val_Loss: 4533.724609375\n",
      "Epoch 6634, Train_Loss: 4337.05517578125, Val_Loss: 4533.578125\n",
      "Epoch 6635, Train_Loss: 4336.931640625, Val_Loss: 4533.41064453125\n",
      "Epoch 6636, Train_Loss: 4336.787109375, Val_Loss: 4533.25048828125\n",
      "Epoch 6637, Train_Loss: 4336.65625, Val_Loss: 4533.09619140625\n",
      "Epoch 6638, Train_Loss: 4336.54150390625, Val_Loss: 4532.95703125\n",
      "Epoch 6639, Train_Loss: 4336.4072265625, Val_Loss: 4532.79296875\n",
      "Epoch 6640, Train_Loss: 4336.2294921875, Val_Loss: 4532.646484375\n",
      "Epoch 6641, Train_Loss: 4336.0966796875, Val_Loss: 4532.5068359375\n",
      "Epoch 6642, Train_Loss: 4335.94287109375, Val_Loss: 4532.31591796875\n",
      "Epoch 6643, Train_Loss: 4335.830078125, Val_Loss: 4532.14501953125\n",
      "Epoch 6644, Train_Loss: 4335.7158203125, Val_Loss: 4532.001953125\n",
      "Epoch 6645, Train_Loss: 4335.59521484375, Val_Loss: 4531.83984375\n",
      "Epoch 6646, Train_Loss: 4335.4658203125, Val_Loss: 4531.6904296875\n",
      "Epoch 6647, Train_Loss: 4335.3056640625, Val_Loss: 4531.5439453125\n",
      "Epoch 6648, Train_Loss: 4335.18701171875, Val_Loss: 4531.388671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6649, Train_Loss: 4335.0634765625, Val_Loss: 4531.2421875\n",
      "Epoch 6650, Train_Loss: 4334.9404296875, Val_Loss: 4531.07275390625\n",
      "Epoch 6651, Train_Loss: 4334.810546875, Val_Loss: 4530.912109375\n",
      "Epoch 6652, Train_Loss: 4334.67919921875, Val_Loss: 4530.76220703125\n",
      "Epoch 6653, Train_Loss: 4334.54052734375, Val_Loss: 4530.61328125\n",
      "Epoch 6654, Train_Loss: 4334.3740234375, Val_Loss: 4530.47900390625\n",
      "Epoch 6655, Train_Loss: 4334.24072265625, Val_Loss: 4530.32861328125\n",
      "Epoch 6656, Train_Loss: 4334.13525390625, Val_Loss: 4530.171875\n",
      "Epoch 6657, Train_Loss: 4334.00732421875, Val_Loss: 4530.00439453125\n",
      "Epoch 6658, Train_Loss: 4333.8681640625, Val_Loss: 4529.8583984375\n",
      "Epoch 6659, Train_Loss: 4333.74365234375, Val_Loss: 4529.70263671875\n",
      "Epoch 6660, Train_Loss: 4333.62353515625, Val_Loss: 4529.5625\n",
      "Epoch 6661, Train_Loss: 4333.48779296875, Val_Loss: 4529.40576171875\n",
      "Epoch 6662, Train_Loss: 4333.37255859375, Val_Loss: 4529.25048828125\n",
      "Epoch 6663, Train_Loss: 4333.2353515625, Val_Loss: 4529.0947265625\n",
      "Epoch 6664, Train_Loss: 4333.115234375, Val_Loss: 4528.93017578125\n",
      "Epoch 6665, Train_Loss: 4332.98828125, Val_Loss: 4528.7900390625\n",
      "Epoch 6666, Train_Loss: 4332.859375, Val_Loss: 4528.6220703125\n",
      "Epoch 6667, Train_Loss: 4332.7041015625, Val_Loss: 4528.48583984375\n",
      "Epoch 6668, Train_Loss: 4332.56396484375, Val_Loss: 4528.33935546875\n",
      "Epoch 6669, Train_Loss: 4332.4443359375, Val_Loss: 4528.19091796875\n",
      "Epoch 6670, Train_Loss: 4332.32568359375, Val_Loss: 4528.02587890625\n",
      "Epoch 6671, Train_Loss: 4332.2158203125, Val_Loss: 4527.8505859375\n",
      "Epoch 6672, Train_Loss: 4332.109375, Val_Loss: 4527.70068359375\n",
      "Epoch 6673, Train_Loss: 4331.97509765625, Val_Loss: 4527.54736328125\n",
      "Epoch 6674, Train_Loss: 4331.85205078125, Val_Loss: 4527.40576171875\n",
      "Epoch 6675, Train_Loss: 4331.72412109375, Val_Loss: 4527.26416015625\n",
      "Epoch 6676, Train_Loss: 4331.59765625, Val_Loss: 4527.10205078125\n",
      "Epoch 6677, Train_Loss: 4331.4833984375, Val_Loss: 4526.9306640625\n",
      "Epoch 6678, Train_Loss: 4331.3603515625, Val_Loss: 4526.78173828125\n",
      "Epoch 6679, Train_Loss: 4331.240234375, Val_Loss: 4526.6357421875\n",
      "Epoch 6680, Train_Loss: 4331.10498046875, Val_Loss: 4526.4501953125\n",
      "Epoch 6681, Train_Loss: 4330.97119140625, Val_Loss: 4526.3046875\n",
      "Epoch 6682, Train_Loss: 4330.85009765625, Val_Loss: 4526.15771484375\n",
      "Epoch 6683, Train_Loss: 4330.71826171875, Val_Loss: 4526.00341796875\n",
      "Epoch 6684, Train_Loss: 4330.63330078125, Val_Loss: 4525.84814453125\n",
      "Epoch 6685, Train_Loss: 4330.5029296875, Val_Loss: 4525.6923828125\n",
      "Epoch 6686, Train_Loss: 4330.36474609375, Val_Loss: 4525.54736328125\n",
      "Epoch 6687, Train_Loss: 4330.25390625, Val_Loss: 4525.40380859375\n",
      "Epoch 6688, Train_Loss: 4330.1025390625, Val_Loss: 4525.25537109375\n",
      "Epoch 6689, Train_Loss: 4329.9775390625, Val_Loss: 4525.10498046875\n",
      "Epoch 6690, Train_Loss: 4329.76904296875, Val_Loss: 4524.93896484375\n",
      "Epoch 6691, Train_Loss: 4329.6435546875, Val_Loss: 4524.7919921875\n",
      "Epoch 6692, Train_Loss: 4329.51904296875, Val_Loss: 4524.6240234375\n",
      "Epoch 6693, Train_Loss: 4329.3876953125, Val_Loss: 4524.4833984375\n",
      "Epoch 6694, Train_Loss: 4329.2080078125, Val_Loss: 4524.33447265625\n",
      "Epoch 6695, Train_Loss: 4329.09326171875, Val_Loss: 4524.17822265625\n",
      "Epoch 6696, Train_Loss: 4328.96435546875, Val_Loss: 4524.03662109375\n",
      "Epoch 6697, Train_Loss: 4328.857421875, Val_Loss: 4523.8818359375\n",
      "Epoch 6698, Train_Loss: 4328.73486328125, Val_Loss: 4523.72607421875\n",
      "Epoch 6699, Train_Loss: 4328.541015625, Val_Loss: 4523.5625\n",
      "Epoch 6700, Train_Loss: 4328.4189453125, Val_Loss: 4523.412109375\n",
      "Epoch 6701, Train_Loss: 4328.29736328125, Val_Loss: 4523.26953125\n",
      "Epoch 6702, Train_Loss: 4328.16796875, Val_Loss: 4523.11572265625\n",
      "Epoch 6703, Train_Loss: 4328.0390625, Val_Loss: 4522.95458984375\n",
      "Epoch 6704, Train_Loss: 4327.9091796875, Val_Loss: 4522.8115234375\n",
      "Epoch 6705, Train_Loss: 4327.775390625, Val_Loss: 4522.669921875\n",
      "Epoch 6706, Train_Loss: 4327.63916015625, Val_Loss: 4522.50634765625\n",
      "Epoch 6707, Train_Loss: 4327.5234375, Val_Loss: 4522.3662109375\n",
      "Epoch 6708, Train_Loss: 4327.37646484375, Val_Loss: 4522.2177734375\n",
      "Epoch 6709, Train_Loss: 4327.2353515625, Val_Loss: 4522.04345703125\n",
      "Epoch 6710, Train_Loss: 4327.1162109375, Val_Loss: 4521.89404296875\n",
      "Epoch 6711, Train_Loss: 4327.01708984375, Val_Loss: 4521.73095703125\n",
      "Epoch 6712, Train_Loss: 4326.88818359375, Val_Loss: 4521.5703125\n",
      "Epoch 6713, Train_Loss: 4326.7626953125, Val_Loss: 4521.4248046875\n",
      "Epoch 6714, Train_Loss: 4326.62939453125, Val_Loss: 4521.28857421875\n",
      "Epoch 6715, Train_Loss: 4326.5166015625, Val_Loss: 4521.1298828125\n",
      "Epoch 6716, Train_Loss: 4326.3759765625, Val_Loss: 4520.9814453125\n",
      "Epoch 6717, Train_Loss: 4326.251953125, Val_Loss: 4520.83935546875\n",
      "Epoch 6718, Train_Loss: 4326.11083984375, Val_Loss: 4520.64599609375\n",
      "Epoch 6719, Train_Loss: 4325.9794921875, Val_Loss: 4520.490234375\n",
      "Epoch 6720, Train_Loss: 4325.8671875, Val_Loss: 4520.33740234375\n",
      "Epoch 6721, Train_Loss: 4325.74462890625, Val_Loss: 4520.205078125\n",
      "Epoch 6722, Train_Loss: 4325.63330078125, Val_Loss: 4520.06201171875\n",
      "Epoch 6723, Train_Loss: 4325.5146484375, Val_Loss: 4519.89111328125\n",
      "Epoch 6724, Train_Loss: 4325.3857421875, Val_Loss: 4519.7421875\n",
      "Epoch 6725, Train_Loss: 4325.2705078125, Val_Loss: 4519.58349609375\n",
      "Epoch 6726, Train_Loss: 4325.1484375, Val_Loss: 4519.4384765625\n",
      "Epoch 6727, Train_Loss: 4325.0107421875, Val_Loss: 4519.28271484375\n",
      "Epoch 6728, Train_Loss: 4324.8935546875, Val_Loss: 4519.1455078125\n",
      "Epoch 6729, Train_Loss: 4324.73974609375, Val_Loss: 4518.9970703125\n",
      "Epoch 6730, Train_Loss: 4324.6142578125, Val_Loss: 4518.84326171875\n",
      "Epoch 6731, Train_Loss: 4324.49755859375, Val_Loss: 4518.68115234375\n",
      "Epoch 6732, Train_Loss: 4324.36865234375, Val_Loss: 4518.525390625\n",
      "Epoch 6733, Train_Loss: 4324.23681640625, Val_Loss: 4518.37060546875\n",
      "Epoch 6734, Train_Loss: 4324.10302734375, Val_Loss: 4518.22900390625\n",
      "Epoch 6735, Train_Loss: 4323.93408203125, Val_Loss: 4518.1015625\n",
      "Epoch 6736, Train_Loss: 4323.8115234375, Val_Loss: 4517.9384765625\n",
      "Epoch 6737, Train_Loss: 4323.6865234375, Val_Loss: 4517.7900390625\n",
      "Epoch 6738, Train_Loss: 4323.61474609375, Val_Loss: 4517.62353515625\n",
      "Epoch 6739, Train_Loss: 4323.50244140625, Val_Loss: 4517.474609375\n",
      "Epoch 6740, Train_Loss: 4323.365234375, Val_Loss: 4517.3203125\n",
      "Epoch 6741, Train_Loss: 4323.25244140625, Val_Loss: 4517.16796875\n",
      "Epoch 6742, Train_Loss: 4323.1376953125, Val_Loss: 4517.0322265625\n",
      "Epoch 6743, Train_Loss: 4323.02587890625, Val_Loss: 4516.8740234375\n",
      "Epoch 6744, Train_Loss: 4322.8974609375, Val_Loss: 4516.7265625\n",
      "Epoch 6745, Train_Loss: 4322.77734375, Val_Loss: 4516.5673828125\n",
      "Epoch 6746, Train_Loss: 4322.6435546875, Val_Loss: 4516.4130859375\n",
      "Epoch 6747, Train_Loss: 4322.521484375, Val_Loss: 4516.26123046875\n",
      "Epoch 6748, Train_Loss: 4322.3408203125, Val_Loss: 4516.1162109375\n",
      "Epoch 6749, Train_Loss: 4322.216796875, Val_Loss: 4515.96875\n",
      "Epoch 6750, Train_Loss: 4322.07421875, Val_Loss: 4515.81298828125\n",
      "Epoch 6751, Train_Loss: 4321.947265625, Val_Loss: 4515.6650390625\n",
      "Epoch 6752, Train_Loss: 4321.8291015625, Val_Loss: 4515.5048828125\n",
      "Epoch 6753, Train_Loss: 4321.71484375, Val_Loss: 4515.369140625\n",
      "Epoch 6754, Train_Loss: 4321.59814453125, Val_Loss: 4515.208984375\n",
      "Epoch 6755, Train_Loss: 4321.478515625, Val_Loss: 4515.068359375\n",
      "Epoch 6756, Train_Loss: 4321.35107421875, Val_Loss: 4514.88623046875\n",
      "Epoch 6757, Train_Loss: 4321.2294921875, Val_Loss: 4514.7255859375\n",
      "Epoch 6758, Train_Loss: 4321.11328125, Val_Loss: 4514.56103515625\n",
      "Epoch 6759, Train_Loss: 4320.994140625, Val_Loss: 4514.41552734375\n",
      "Epoch 6760, Train_Loss: 4320.8603515625, Val_Loss: 4514.2666015625\n",
      "Epoch 6761, Train_Loss: 4320.73486328125, Val_Loss: 4514.11962890625\n",
      "Epoch 6762, Train_Loss: 4320.58203125, Val_Loss: 4513.9833984375\n",
      "Epoch 6763, Train_Loss: 4320.47802734375, Val_Loss: 4513.83056640625\n",
      "Epoch 6764, Train_Loss: 4320.35498046875, Val_Loss: 4513.68310546875\n",
      "Epoch 6765, Train_Loss: 4320.23095703125, Val_Loss: 4513.52099609375\n",
      "Epoch 6766, Train_Loss: 4320.09130859375, Val_Loss: 4513.3720703125\n",
      "Epoch 6767, Train_Loss: 4319.95849609375, Val_Loss: 4513.212890625\n",
      "Epoch 6768, Train_Loss: 4319.83935546875, Val_Loss: 4513.076171875\n",
      "Epoch 6769, Train_Loss: 4319.71826171875, Val_Loss: 4512.93505859375\n",
      "Epoch 6770, Train_Loss: 4319.560546875, Val_Loss: 4512.78173828125\n",
      "Epoch 6771, Train_Loss: 4319.43017578125, Val_Loss: 4512.6298828125\n",
      "Epoch 6772, Train_Loss: 4319.30810546875, Val_Loss: 4512.47705078125\n",
      "Epoch 6773, Train_Loss: 4319.19775390625, Val_Loss: 4512.32958984375\n",
      "Epoch 6774, Train_Loss: 4319.06591796875, Val_Loss: 4512.17578125\n",
      "Epoch 6775, Train_Loss: 4318.900390625, Val_Loss: 4512.03857421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6776, Train_Loss: 4318.64892578125, Val_Loss: 4511.87841796875\n",
      "Epoch 6777, Train_Loss: 4318.55126953125, Val_Loss: 4511.72900390625\n",
      "Epoch 6778, Train_Loss: 4318.41650390625, Val_Loss: 4511.576171875\n",
      "Epoch 6779, Train_Loss: 4318.3134765625, Val_Loss: 4511.41943359375\n",
      "Epoch 6780, Train_Loss: 4318.18408203125, Val_Loss: 4511.26708984375\n",
      "Epoch 6781, Train_Loss: 4318.0634765625, Val_Loss: 4511.125\n",
      "Epoch 6782, Train_Loss: 4317.9345703125, Val_Loss: 4510.98046875\n",
      "Epoch 6783, Train_Loss: 4317.81201171875, Val_Loss: 4510.83154296875\n",
      "Epoch 6784, Train_Loss: 4317.69287109375, Val_Loss: 4510.68017578125\n",
      "Epoch 6785, Train_Loss: 4317.5703125, Val_Loss: 4510.5048828125\n",
      "Epoch 6786, Train_Loss: 4317.43701171875, Val_Loss: 4510.36181640625\n",
      "Epoch 6787, Train_Loss: 4317.30712890625, Val_Loss: 4510.2158203125\n",
      "Epoch 6788, Train_Loss: 4317.197265625, Val_Loss: 4510.072265625\n",
      "Epoch 6789, Train_Loss: 4317.0439453125, Val_Loss: 4509.93603515625\n",
      "Epoch 6790, Train_Loss: 4316.92822265625, Val_Loss: 4509.78759765625\n",
      "Epoch 6791, Train_Loss: 4316.78515625, Val_Loss: 4509.62744140625\n",
      "Epoch 6792, Train_Loss: 4316.71337890625, Val_Loss: 4509.46923828125\n",
      "Epoch 6793, Train_Loss: 4316.59912109375, Val_Loss: 4509.32177734375\n",
      "Epoch 6794, Train_Loss: 4316.48388671875, Val_Loss: 4509.1826171875\n",
      "Epoch 6795, Train_Loss: 4316.33544921875, Val_Loss: 4508.99169921875\n",
      "Epoch 6796, Train_Loss: 4316.21728515625, Val_Loss: 4508.8515625\n",
      "Epoch 6797, Train_Loss: 4316.09033203125, Val_Loss: 4508.7119140625\n",
      "Epoch 6798, Train_Loss: 4315.98193359375, Val_Loss: 4508.55908203125\n",
      "Epoch 6799, Train_Loss: 4315.85595703125, Val_Loss: 4508.39208984375\n",
      "Epoch 6800, Train_Loss: 4315.73828125, Val_Loss: 4508.2470703125\n",
      "Epoch 6801, Train_Loss: 4315.5986328125, Val_Loss: 4508.1044921875\n",
      "Epoch 6802, Train_Loss: 4315.4150390625, Val_Loss: 4507.95068359375\n",
      "Epoch 6803, Train_Loss: 4315.2939453125, Val_Loss: 4507.81982421875\n",
      "Epoch 6804, Train_Loss: 4315.17626953125, Val_Loss: 4507.6640625\n",
      "Epoch 6805, Train_Loss: 4315.05419921875, Val_Loss: 4507.50341796875\n",
      "Epoch 6806, Train_Loss: 4314.93408203125, Val_Loss: 4507.34619140625\n",
      "Epoch 6807, Train_Loss: 4314.8095703125, Val_Loss: 4507.201171875\n",
      "Epoch 6808, Train_Loss: 4314.67822265625, Val_Loss: 4507.0458984375\n",
      "Epoch 6809, Train_Loss: 4314.560546875, Val_Loss: 4506.91796875\n",
      "Epoch 6810, Train_Loss: 4314.43603515625, Val_Loss: 4506.765625\n",
      "Epoch 6811, Train_Loss: 4314.2861328125, Val_Loss: 4506.61376953125\n",
      "Epoch 6812, Train_Loss: 4314.1650390625, Val_Loss: 4506.478515625\n",
      "Epoch 6813, Train_Loss: 4314.06103515625, Val_Loss: 4506.3134765625\n",
      "Epoch 6814, Train_Loss: 4313.919921875, Val_Loss: 4506.15625\n",
      "Epoch 6815, Train_Loss: 4313.80322265625, Val_Loss: 4506.00537109375\n",
      "Epoch 6816, Train_Loss: 4313.64501953125, Val_Loss: 4505.8701171875\n",
      "Epoch 6817, Train_Loss: 4313.52734375, Val_Loss: 4505.73095703125\n",
      "Epoch 6818, Train_Loss: 4313.40869140625, Val_Loss: 4505.5791015625\n",
      "Epoch 6819, Train_Loss: 4313.2890625, Val_Loss: 4505.4189453125\n",
      "Epoch 6820, Train_Loss: 4313.16455078125, Val_Loss: 4505.271484375\n",
      "Epoch 6821, Train_Loss: 4313.03076171875, Val_Loss: 4505.125\n",
      "Epoch 6822, Train_Loss: 4312.9072265625, Val_Loss: 4504.9736328125\n",
      "Epoch 6823, Train_Loss: 4312.79150390625, Val_Loss: 4504.81103515625\n",
      "Epoch 6824, Train_Loss: 4312.6904296875, Val_Loss: 4504.66455078125\n",
      "Epoch 6825, Train_Loss: 4312.57861328125, Val_Loss: 4504.51953125\n",
      "Epoch 6826, Train_Loss: 4312.4462890625, Val_Loss: 4504.3662109375\n",
      "Epoch 6827, Train_Loss: 4312.32861328125, Val_Loss: 4504.21826171875\n",
      "Epoch 6828, Train_Loss: 4312.205078125, Val_Loss: 4504.07568359375\n",
      "Epoch 6829, Train_Loss: 4312.06591796875, Val_Loss: 4503.93310546875\n",
      "Epoch 6830, Train_Loss: 4311.9443359375, Val_Loss: 4503.77685546875\n",
      "Epoch 6831, Train_Loss: 4311.814453125, Val_Loss: 4503.63232421875\n",
      "Epoch 6832, Train_Loss: 4311.6865234375, Val_Loss: 4503.48583984375\n",
      "Epoch 6833, Train_Loss: 4311.576171875, Val_Loss: 4503.2880859375\n",
      "Epoch 6834, Train_Loss: 4311.4560546875, Val_Loss: 4503.1337890625\n",
      "Epoch 6835, Train_Loss: 4311.353515625, Val_Loss: 4502.99169921875\n",
      "Epoch 6836, Train_Loss: 4311.2392578125, Val_Loss: 4502.85400390625\n",
      "Epoch 6837, Train_Loss: 4311.107421875, Val_Loss: 4502.71142578125\n",
      "Epoch 6838, Train_Loss: 4310.99169921875, Val_Loss: 4502.56982421875\n",
      "Epoch 6839, Train_Loss: 4310.8701171875, Val_Loss: 4502.41943359375\n",
      "Epoch 6840, Train_Loss: 4310.74267578125, Val_Loss: 4502.2578125\n",
      "Epoch 6841, Train_Loss: 4310.6181640625, Val_Loss: 4502.1103515625\n",
      "Epoch 6842, Train_Loss: 4310.484375, Val_Loss: 4501.9697265625\n",
      "Epoch 6843, Train_Loss: 4310.3271484375, Val_Loss: 4501.83203125\n",
      "Epoch 6844, Train_Loss: 4310.205078125, Val_Loss: 4501.68310546875\n",
      "Epoch 6845, Train_Loss: 4310.09814453125, Val_Loss: 4501.53466796875\n",
      "Epoch 6846, Train_Loss: 4310.02490234375, Val_Loss: 4501.3720703125\n",
      "Epoch 6847, Train_Loss: 4309.90576171875, Val_Loss: 4501.22900390625\n",
      "Epoch 6848, Train_Loss: 4309.783203125, Val_Loss: 4501.083984375\n",
      "Epoch 6849, Train_Loss: 4309.66162109375, Val_Loss: 4500.939453125\n",
      "Epoch 6850, Train_Loss: 4309.546875, Val_Loss: 4500.79736328125\n",
      "Epoch 6851, Train_Loss: 4309.4150390625, Val_Loss: 4500.64892578125\n",
      "Epoch 6852, Train_Loss: 4309.20068359375, Val_Loss: 4500.49462890625\n",
      "Epoch 6853, Train_Loss: 4309.09326171875, Val_Loss: 4500.3330078125\n",
      "Epoch 6854, Train_Loss: 4308.97802734375, Val_Loss: 4500.1953125\n",
      "Epoch 6855, Train_Loss: 4308.8603515625, Val_Loss: 4500.0439453125\n",
      "Epoch 6856, Train_Loss: 4308.7431640625, Val_Loss: 4499.90283203125\n",
      "Epoch 6857, Train_Loss: 4308.54931640625, Val_Loss: 4499.75732421875\n",
      "Epoch 6858, Train_Loss: 4308.43701171875, Val_Loss: 4499.6044921875\n",
      "Epoch 6859, Train_Loss: 4308.31640625, Val_Loss: 4499.4501953125\n",
      "Epoch 6860, Train_Loss: 4308.19970703125, Val_Loss: 4499.30615234375\n",
      "Epoch 6861, Train_Loss: 4308.08984375, Val_Loss: 4499.14453125\n",
      "Epoch 6862, Train_Loss: 4307.95556640625, Val_Loss: 4498.99658203125\n",
      "Epoch 6863, Train_Loss: 4307.8212890625, Val_Loss: 4498.8583984375\n",
      "Epoch 6864, Train_Loss: 4307.69921875, Val_Loss: 4498.70703125\n",
      "Epoch 6865, Train_Loss: 4307.5810546875, Val_Loss: 4498.56298828125\n",
      "Epoch 6866, Train_Loss: 4307.45947265625, Val_Loss: 4498.42138671875\n",
      "Epoch 6867, Train_Loss: 4307.33544921875, Val_Loss: 4498.26220703125\n",
      "Epoch 6868, Train_Loss: 4307.220703125, Val_Loss: 4498.1181640625\n",
      "Epoch 6869, Train_Loss: 4307.1005859375, Val_Loss: 4497.96240234375\n",
      "Epoch 6870, Train_Loss: 4306.94970703125, Val_Loss: 4497.83056640625\n",
      "Epoch 6871, Train_Loss: 4306.81591796875, Val_Loss: 4497.64697265625\n",
      "Epoch 6872, Train_Loss: 4306.6923828125, Val_Loss: 4497.49951171875\n",
      "Epoch 6873, Train_Loss: 4306.544921875, Val_Loss: 4497.35205078125\n",
      "Epoch 6874, Train_Loss: 4306.43115234375, Val_Loss: 4497.19677734375\n",
      "Epoch 6875, Train_Loss: 4306.3125, Val_Loss: 4497.056640625\n",
      "Epoch 6876, Train_Loss: 4306.193359375, Val_Loss: 4496.9111328125\n",
      "Epoch 6877, Train_Loss: 4306.09033203125, Val_Loss: 4496.76416015625\n",
      "Epoch 6878, Train_Loss: 4305.96435546875, Val_Loss: 4496.6259765625\n",
      "Epoch 6879, Train_Loss: 4305.84375, Val_Loss: 4496.4794921875\n",
      "Epoch 6880, Train_Loss: 4305.72705078125, Val_Loss: 4496.31982421875\n",
      "Epoch 6881, Train_Loss: 4305.60791015625, Val_Loss: 4496.17236328125\n",
      "Epoch 6882, Train_Loss: 4305.48486328125, Val_Loss: 4496.01904296875\n",
      "Epoch 6883, Train_Loss: 4305.35302734375, Val_Loss: 4495.86962890625\n",
      "Epoch 6884, Train_Loss: 4305.208984375, Val_Loss: 4495.740234375\n",
      "Epoch 6885, Train_Loss: 4305.10888671875, Val_Loss: 4495.59912109375\n",
      "Epoch 6886, Train_Loss: 4304.984375, Val_Loss: 4495.4443359375\n",
      "Epoch 6887, Train_Loss: 4304.85986328125, Val_Loss: 4495.29248046875\n",
      "Epoch 6888, Train_Loss: 4304.7421875, Val_Loss: 4495.14501953125\n",
      "Epoch 6889, Train_Loss: 4304.62939453125, Val_Loss: 4494.9990234375\n",
      "Epoch 6890, Train_Loss: 4304.50048828125, Val_Loss: 4494.85888671875\n",
      "Epoch 6891, Train_Loss: 4304.38330078125, Val_Loss: 4494.7158203125\n",
      "Epoch 6892, Train_Loss: 4304.26904296875, Val_Loss: 4494.5576171875\n",
      "Epoch 6893, Train_Loss: 4304.12451171875, Val_Loss: 4494.41943359375\n",
      "Epoch 6894, Train_Loss: 4304.0029296875, Val_Loss: 4494.24853515625\n",
      "Epoch 6895, Train_Loss: 4303.88134765625, Val_Loss: 4494.11181640625\n",
      "Epoch 6896, Train_Loss: 4303.76806640625, Val_Loss: 4493.9716796875\n",
      "Epoch 6897, Train_Loss: 4303.62353515625, Val_Loss: 4493.8291015625\n",
      "Epoch 6898, Train_Loss: 4303.5, Val_Loss: 4493.67431640625\n",
      "Epoch 6899, Train_Loss: 4303.38818359375, Val_Loss: 4493.517578125\n",
      "Epoch 6900, Train_Loss: 4303.27392578125, Val_Loss: 4493.37255859375\n",
      "Epoch 6901, Train_Loss: 4303.18603515625, Val_Loss: 4493.21630859375\n",
      "Epoch 6902, Train_Loss: 4303.0712890625, Val_Loss: 4493.0712890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6903, Train_Loss: 4302.935546875, Val_Loss: 4492.9345703125\n",
      "Epoch 6904, Train_Loss: 4302.81884765625, Val_Loss: 4492.79541015625\n",
      "Epoch 6905, Train_Loss: 4302.70703125, Val_Loss: 4492.64794921875\n",
      "Epoch 6906, Train_Loss: 4302.60302734375, Val_Loss: 4492.49951171875\n",
      "Epoch 6907, Train_Loss: 4302.4794921875, Val_Loss: 4492.34375\n",
      "Epoch 6908, Train_Loss: 4302.35791015625, Val_Loss: 4492.19091796875\n",
      "Epoch 6909, Train_Loss: 4302.2294921875, Val_Loss: 4492.01416015625\n",
      "Epoch 6910, Train_Loss: 4302.10888671875, Val_Loss: 4491.86376953125\n",
      "Epoch 6911, Train_Loss: 4301.93359375, Val_Loss: 4491.7353515625\n",
      "Epoch 6912, Train_Loss: 4301.8203125, Val_Loss: 4491.5791015625\n",
      "Epoch 6913, Train_Loss: 4301.685546875, Val_Loss: 4491.43603515625\n",
      "Epoch 6914, Train_Loss: 4301.56298828125, Val_Loss: 4491.2822265625\n",
      "Epoch 6915, Train_Loss: 4301.43896484375, Val_Loss: 4491.12841796875\n",
      "Epoch 6916, Train_Loss: 4301.32958984375, Val_Loss: 4490.99462890625\n",
      "Epoch 6917, Train_Loss: 4301.2158203125, Val_Loss: 4490.8583984375\n",
      "Epoch 6918, Train_Loss: 4301.095703125, Val_Loss: 4490.7216796875\n",
      "Epoch 6919, Train_Loss: 4300.94873046875, Val_Loss: 4490.54931640625\n",
      "Epoch 6920, Train_Loss: 4300.82958984375, Val_Loss: 4490.41552734375\n",
      "Epoch 6921, Train_Loss: 4300.70703125, Val_Loss: 4490.24853515625\n",
      "Epoch 6922, Train_Loss: 4300.59375, Val_Loss: 4490.11572265625\n",
      "Epoch 6923, Train_Loss: 4300.47607421875, Val_Loss: 4489.970703125\n",
      "Epoch 6924, Train_Loss: 4300.31201171875, Val_Loss: 4489.83544921875\n",
      "Epoch 6925, Train_Loss: 4300.19970703125, Val_Loss: 4489.69189453125\n",
      "Epoch 6926, Train_Loss: 4300.0751953125, Val_Loss: 4489.53857421875\n",
      "Epoch 6927, Train_Loss: 4299.95556640625, Val_Loss: 4489.400390625\n",
      "Epoch 6928, Train_Loss: 4299.74658203125, Val_Loss: 4489.244140625\n",
      "Epoch 6929, Train_Loss: 4299.62353515625, Val_Loss: 4489.10498046875\n",
      "Epoch 6930, Train_Loss: 4299.5234375, Val_Loss: 4488.9521484375\n",
      "Epoch 6931, Train_Loss: 4299.4033203125, Val_Loss: 4488.8173828125\n",
      "Epoch 6932, Train_Loss: 4299.27734375, Val_Loss: 4488.66162109375\n",
      "Epoch 6933, Train_Loss: 4299.1650390625, Val_Loss: 4488.51513671875\n",
      "Epoch 6934, Train_Loss: 4299.025390625, Val_Loss: 4488.3681640625\n",
      "Epoch 6935, Train_Loss: 4298.9052734375, Val_Loss: 4488.2236328125\n",
      "Epoch 6936, Train_Loss: 4298.7744140625, Val_Loss: 4488.07373046875\n",
      "Epoch 6937, Train_Loss: 4298.65576171875, Val_Loss: 4487.923828125\n",
      "Epoch 6938, Train_Loss: 4298.513671875, Val_Loss: 4487.7841796875\n",
      "Epoch 6939, Train_Loss: 4298.39990234375, Val_Loss: 4487.63525390625\n",
      "Epoch 6940, Train_Loss: 4298.29052734375, Val_Loss: 4487.48974609375\n",
      "Epoch 6941, Train_Loss: 4298.17236328125, Val_Loss: 4487.33447265625\n",
      "Epoch 6942, Train_Loss: 4298.06201171875, Val_Loss: 4487.18603515625\n",
      "Epoch 6943, Train_Loss: 4297.95361328125, Val_Loss: 4487.04296875\n",
      "Epoch 6944, Train_Loss: 4297.8232421875, Val_Loss: 4486.9072265625\n",
      "Epoch 6945, Train_Loss: 4297.708984375, Val_Loss: 4486.7646484375\n",
      "Epoch 6946, Train_Loss: 4297.5888671875, Val_Loss: 4486.6201171875\n",
      "Epoch 6947, Train_Loss: 4297.44091796875, Val_Loss: 4486.43603515625\n",
      "Epoch 6948, Train_Loss: 4297.33349609375, Val_Loss: 4486.28369140625\n",
      "Epoch 6949, Train_Loss: 4297.203125, Val_Loss: 4486.14404296875\n",
      "Epoch 6950, Train_Loss: 4297.07275390625, Val_Loss: 4485.99755859375\n",
      "Epoch 6951, Train_Loss: 4296.9296875, Val_Loss: 4485.86328125\n",
      "Epoch 6952, Train_Loss: 4296.82275390625, Val_Loss: 4485.7099609375\n",
      "Epoch 6953, Train_Loss: 4296.7080078125, Val_Loss: 4485.5625\n",
      "Epoch 6954, Train_Loss: 4296.576171875, Val_Loss: 4485.412109375\n",
      "Epoch 6955, Train_Loss: 4296.5078125, Val_Loss: 4485.265625\n",
      "Epoch 6956, Train_Loss: 4296.376953125, Val_Loss: 4485.12548828125\n",
      "Epoch 6957, Train_Loss: 4296.25439453125, Val_Loss: 4484.97900390625\n",
      "Epoch 6958, Train_Loss: 4296.134765625, Val_Loss: 4484.83349609375\n",
      "Epoch 6959, Train_Loss: 4296.0166015625, Val_Loss: 4484.6923828125\n",
      "Epoch 6960, Train_Loss: 4295.88916015625, Val_Loss: 4484.556640625\n",
      "Epoch 6961, Train_Loss: 4295.759765625, Val_Loss: 4484.38818359375\n",
      "Epoch 6962, Train_Loss: 4295.63818359375, Val_Loss: 4484.240234375\n",
      "Epoch 6963, Train_Loss: 4295.52734375, Val_Loss: 4484.0927734375\n",
      "Epoch 6964, Train_Loss: 4295.41259765625, Val_Loss: 4483.94482421875\n",
      "Epoch 6965, Train_Loss: 4295.2392578125, Val_Loss: 4483.82275390625\n",
      "Epoch 6966, Train_Loss: 4295.12646484375, Val_Loss: 4483.6611328125\n",
      "Epoch 6967, Train_Loss: 4295.02197265625, Val_Loss: 4483.52197265625\n",
      "Epoch 6968, Train_Loss: 4294.90869140625, Val_Loss: 4483.37158203125\n",
      "Epoch 6969, Train_Loss: 4294.7978515625, Val_Loss: 4483.22705078125\n",
      "Epoch 6970, Train_Loss: 4294.67822265625, Val_Loss: 4483.076171875\n",
      "Epoch 6971, Train_Loss: 4294.56298828125, Val_Loss: 4482.93017578125\n",
      "Epoch 6972, Train_Loss: 4294.4375, Val_Loss: 4482.80322265625\n",
      "Epoch 6973, Train_Loss: 4294.3046875, Val_Loss: 4482.64892578125\n",
      "Epoch 6974, Train_Loss: 4294.19287109375, Val_Loss: 4482.49755859375\n",
      "Epoch 6975, Train_Loss: 4294.05859375, Val_Loss: 4482.34130859375\n",
      "Epoch 6976, Train_Loss: 4293.9423828125, Val_Loss: 4482.1865234375\n",
      "Epoch 6977, Train_Loss: 4293.8134765625, Val_Loss: 4482.04296875\n",
      "Epoch 6978, Train_Loss: 4293.66455078125, Val_Loss: 4481.9130859375\n",
      "Epoch 6979, Train_Loss: 4293.5380859375, Val_Loss: 4481.7744140625\n",
      "Epoch 6980, Train_Loss: 4293.41943359375, Val_Loss: 4481.6220703125\n",
      "Epoch 6981, Train_Loss: 4293.3017578125, Val_Loss: 4481.47509765625\n",
      "Epoch 6982, Train_Loss: 4293.17578125, Val_Loss: 4481.32080078125\n",
      "Epoch 6983, Train_Loss: 4293.056640625, Val_Loss: 4481.1845703125\n",
      "Epoch 6984, Train_Loss: 4292.94091796875, Val_Loss: 4481.033203125\n",
      "Epoch 6985, Train_Loss: 4292.81884765625, Val_Loss: 4480.87109375\n",
      "Epoch 6986, Train_Loss: 4292.7021484375, Val_Loss: 4480.708984375\n",
      "Epoch 6987, Train_Loss: 4292.591796875, Val_Loss: 4480.57958984375\n",
      "Epoch 6988, Train_Loss: 4292.474609375, Val_Loss: 4480.4326171875\n",
      "Epoch 6989, Train_Loss: 4292.369140625, Val_Loss: 4480.27734375\n",
      "Epoch 6990, Train_Loss: 4292.23779296875, Val_Loss: 4480.1318359375\n",
      "Epoch 6991, Train_Loss: 4292.12890625, Val_Loss: 4479.9833984375\n",
      "Epoch 6992, Train_Loss: 4291.9873046875, Val_Loss: 4479.8505859375\n",
      "Epoch 6993, Train_Loss: 4291.87109375, Val_Loss: 4479.712890625\n",
      "Epoch 6994, Train_Loss: 4291.759765625, Val_Loss: 4479.55859375\n",
      "Epoch 6995, Train_Loss: 4291.525390625, Val_Loss: 4479.4013671875\n",
      "Epoch 6996, Train_Loss: 4291.40625, Val_Loss: 4479.263671875\n",
      "Epoch 6997, Train_Loss: 4291.29248046875, Val_Loss: 4479.11083984375\n",
      "Epoch 6998, Train_Loss: 4291.17431640625, Val_Loss: 4478.96923828125\n",
      "Epoch 6999, Train_Loss: 4291.064453125, Val_Loss: 4478.83984375\n",
      "Epoch 7000, Train_Loss: 4290.94873046875, Val_Loss: 4478.6826171875\n",
      "Epoch 7001, Train_Loss: 4290.82421875, Val_Loss: 4478.5439453125\n",
      "Epoch 7002, Train_Loss: 4290.71923828125, Val_Loss: 4478.39013671875\n",
      "Epoch 7003, Train_Loss: 4290.60888671875, Val_Loss: 4478.2578125\n",
      "Epoch 7004, Train_Loss: 4290.49169921875, Val_Loss: 4478.10546875\n",
      "Epoch 7005, Train_Loss: 4290.3369140625, Val_Loss: 4477.95458984375\n",
      "Epoch 7006, Train_Loss: 4290.193359375, Val_Loss: 4477.830078125\n",
      "Epoch 7007, Train_Loss: 4290.0859375, Val_Loss: 4477.6845703125\n",
      "Epoch 7008, Train_Loss: 4289.97509765625, Val_Loss: 4477.5302734375\n",
      "Epoch 7009, Train_Loss: 4289.90283203125, Val_Loss: 4477.375\n",
      "Epoch 7010, Train_Loss: 4289.78076171875, Val_Loss: 4477.22802734375\n",
      "Epoch 7011, Train_Loss: 4289.6630859375, Val_Loss: 4477.09326171875\n",
      "Epoch 7012, Train_Loss: 4289.53955078125, Val_Loss: 4476.958984375\n",
      "Epoch 7013, Train_Loss: 4289.427734375, Val_Loss: 4476.80810546875\n",
      "Epoch 7014, Train_Loss: 4289.3134765625, Val_Loss: 4476.654296875\n",
      "Epoch 7015, Train_Loss: 4289.18896484375, Val_Loss: 4476.50927734375\n",
      "Epoch 7016, Train_Loss: 4289.0361328125, Val_Loss: 4476.35498046875\n",
      "Epoch 7017, Train_Loss: 4288.91796875, Val_Loss: 4476.220703125\n",
      "Epoch 7018, Train_Loss: 4288.798828125, Val_Loss: 4476.07373046875\n",
      "Epoch 7019, Train_Loss: 4288.62548828125, Val_Loss: 4475.93505859375\n",
      "Epoch 7020, Train_Loss: 4288.50048828125, Val_Loss: 4475.78759765625\n",
      "Epoch 7021, Train_Loss: 4288.39697265625, Val_Loss: 4475.63623046875\n",
      "Epoch 7022, Train_Loss: 4288.29833984375, Val_Loss: 4475.48779296875\n",
      "Epoch 7023, Train_Loss: 4288.16943359375, Val_Loss: 4475.33056640625\n",
      "Epoch 7024, Train_Loss: 4288.048828125, Val_Loss: 4475.166015625\n",
      "Epoch 7025, Train_Loss: 4287.92578125, Val_Loss: 4475.02685546875\n",
      "Epoch 7026, Train_Loss: 4287.791015625, Val_Loss: 4474.88232421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7027, Train_Loss: 4287.66796875, Val_Loss: 4474.7333984375\n",
      "Epoch 7028, Train_Loss: 4287.5595703125, Val_Loss: 4474.5947265625\n",
      "Epoch 7029, Train_Loss: 4287.4541015625, Val_Loss: 4474.4296875\n",
      "Epoch 7030, Train_Loss: 4287.32568359375, Val_Loss: 4474.30224609375\n",
      "Epoch 7031, Train_Loss: 4287.208984375, Val_Loss: 4474.15625\n",
      "Epoch 7032, Train_Loss: 4287.08837890625, Val_Loss: 4474.01416015625\n",
      "Epoch 7033, Train_Loss: 4286.93212890625, Val_Loss: 4473.87890625\n",
      "Epoch 7034, Train_Loss: 4286.8212890625, Val_Loss: 4473.7294921875\n",
      "Epoch 7035, Train_Loss: 4286.712890625, Val_Loss: 4473.591796875\n",
      "Epoch 7036, Train_Loss: 4286.5908203125, Val_Loss: 4473.44287109375\n",
      "Epoch 7037, Train_Loss: 4286.47216796875, Val_Loss: 4473.302734375\n",
      "Epoch 7038, Train_Loss: 4286.35107421875, Val_Loss: 4473.15625\n",
      "Epoch 7039, Train_Loss: 4286.2392578125, Val_Loss: 4473.025390625\n",
      "Epoch 7040, Train_Loss: 4286.125, Val_Loss: 4472.875\n",
      "Epoch 7041, Train_Loss: 4286.0029296875, Val_Loss: 4472.73779296875\n",
      "Epoch 7042, Train_Loss: 4285.87451171875, Val_Loss: 4472.58984375\n",
      "Epoch 7043, Train_Loss: 4285.7578125, Val_Loss: 4472.42578125\n",
      "Epoch 7044, Train_Loss: 4285.64404296875, Val_Loss: 4472.287109375\n",
      "Epoch 7045, Train_Loss: 4285.53564453125, Val_Loss: 4472.1455078125\n",
      "Epoch 7046, Train_Loss: 4285.42138671875, Val_Loss: 4472.001953125\n",
      "Epoch 7047, Train_Loss: 4285.30126953125, Val_Loss: 4471.8701171875\n",
      "Epoch 7048, Train_Loss: 4285.20361328125, Val_Loss: 4471.73291015625\n",
      "Epoch 7049, Train_Loss: 4285.09814453125, Val_Loss: 4471.58740234375\n",
      "Epoch 7050, Train_Loss: 4284.99658203125, Val_Loss: 4471.42822265625\n",
      "Epoch 7051, Train_Loss: 4284.892578125, Val_Loss: 4471.291015625\n",
      "Epoch 7052, Train_Loss: 4284.76904296875, Val_Loss: 4471.14111328125\n",
      "Epoch 7053, Train_Loss: 4284.650390625, Val_Loss: 4471.0009765625\n",
      "Epoch 7054, Train_Loss: 4284.52490234375, Val_Loss: 4470.8662109375\n",
      "Epoch 7055, Train_Loss: 4284.408203125, Val_Loss: 4470.7080078125\n",
      "Epoch 7056, Train_Loss: 4284.30224609375, Val_Loss: 4470.56103515625\n",
      "Epoch 7057, Train_Loss: 4284.1611328125, Val_Loss: 4470.42431640625\n",
      "Epoch 7058, Train_Loss: 4284.03955078125, Val_Loss: 4470.28173828125\n",
      "Epoch 7059, Train_Loss: 4283.91259765625, Val_Loss: 4470.1513671875\n",
      "Epoch 7060, Train_Loss: 4283.7685546875, Val_Loss: 4470.017578125\n",
      "Epoch 7061, Train_Loss: 4283.6591796875, Val_Loss: 4469.87060546875\n",
      "Epoch 7062, Train_Loss: 4283.52099609375, Val_Loss: 4469.69140625\n",
      "Epoch 7063, Train_Loss: 4283.4658203125, Val_Loss: 4469.53076171875\n",
      "Epoch 7064, Train_Loss: 4283.3564453125, Val_Loss: 4469.39794921875\n",
      "Epoch 7065, Train_Loss: 4283.23095703125, Val_Loss: 4469.26220703125\n",
      "Epoch 7066, Train_Loss: 4283.10791015625, Val_Loss: 4469.11767578125\n",
      "Epoch 7067, Train_Loss: 4282.98828125, Val_Loss: 4468.98779296875\n",
      "Epoch 7068, Train_Loss: 4282.8759765625, Val_Loss: 4468.8369140625\n",
      "Epoch 7069, Train_Loss: 4282.75634765625, Val_Loss: 4468.7041015625\n",
      "Epoch 7070, Train_Loss: 4282.65576171875, Val_Loss: 4468.5498046875\n",
      "Epoch 7071, Train_Loss: 4282.53173828125, Val_Loss: 4468.40869140625\n",
      "Epoch 7072, Train_Loss: 4282.40478515625, Val_Loss: 4468.26416015625\n",
      "Epoch 7073, Train_Loss: 4282.22705078125, Val_Loss: 4468.1376953125\n",
      "Epoch 7074, Train_Loss: 4282.10400390625, Val_Loss: 4467.9921875\n",
      "Epoch 7075, Train_Loss: 4281.99365234375, Val_Loss: 4467.8369140625\n",
      "Epoch 7076, Train_Loss: 4281.88134765625, Val_Loss: 4467.70703125\n",
      "Epoch 7077, Train_Loss: 4281.7451171875, Val_Loss: 4467.5595703125\n",
      "Epoch 7078, Train_Loss: 4281.630859375, Val_Loss: 4467.4150390625\n",
      "Epoch 7079, Train_Loss: 4281.52392578125, Val_Loss: 4467.27294921875\n",
      "Epoch 7080, Train_Loss: 4281.41015625, Val_Loss: 4467.14697265625\n",
      "Epoch 7081, Train_Loss: 4281.13134765625, Val_Loss: 4466.99072265625\n",
      "Epoch 7082, Train_Loss: 4281.0263671875, Val_Loss: 4466.861328125\n",
      "Epoch 7083, Train_Loss: 4280.904296875, Val_Loss: 4466.6943359375\n",
      "Epoch 7084, Train_Loss: 4280.7890625, Val_Loss: 4466.5517578125\n",
      "Epoch 7085, Train_Loss: 4280.669921875, Val_Loss: 4466.4189453125\n",
      "Epoch 7086, Train_Loss: 4280.55615234375, Val_Loss: 4466.2783203125\n",
      "Epoch 7087, Train_Loss: 4280.404296875, Val_Loss: 4466.142578125\n",
      "Epoch 7088, Train_Loss: 4280.28662109375, Val_Loss: 4466.00927734375\n",
      "Epoch 7089, Train_Loss: 4280.17529296875, Val_Loss: 4465.86767578125\n",
      "Epoch 7090, Train_Loss: 4280.060546875, Val_Loss: 4465.69580078125\n",
      "Epoch 7091, Train_Loss: 4279.9482421875, Val_Loss: 4465.5595703125\n",
      "Epoch 7092, Train_Loss: 4279.841796875, Val_Loss: 4465.4208984375\n",
      "Epoch 7093, Train_Loss: 4279.72705078125, Val_Loss: 4465.279296875\n",
      "Epoch 7094, Train_Loss: 4279.61376953125, Val_Loss: 4465.15234375\n",
      "Epoch 7095, Train_Loss: 4279.50390625, Val_Loss: 4465.0107421875\n",
      "Epoch 7096, Train_Loss: 4279.3837890625, Val_Loss: 4464.87109375\n",
      "Epoch 7097, Train_Loss: 4279.26318359375, Val_Loss: 4464.708984375\n",
      "Epoch 7098, Train_Loss: 4279.11669921875, Val_Loss: 4464.5673828125\n",
      "Epoch 7099, Train_Loss: 4278.99169921875, Val_Loss: 4464.427734375\n",
      "Epoch 7100, Train_Loss: 4278.8291015625, Val_Loss: 4464.26904296875\n",
      "Epoch 7101, Train_Loss: 4278.71826171875, Val_Loss: 4464.13330078125\n",
      "Epoch 7102, Train_Loss: 4278.60546875, Val_Loss: 4463.98779296875\n",
      "Epoch 7103, Train_Loss: 4278.48095703125, Val_Loss: 4463.8466796875\n",
      "Epoch 7104, Train_Loss: 4278.37744140625, Val_Loss: 4463.69384765625\n",
      "Epoch 7105, Train_Loss: 4278.27392578125, Val_Loss: 4463.5595703125\n",
      "Epoch 7106, Train_Loss: 4278.162109375, Val_Loss: 4463.41943359375\n",
      "Epoch 7107, Train_Loss: 4278.0537109375, Val_Loss: 4463.291015625\n",
      "Epoch 7108, Train_Loss: 4277.9345703125, Val_Loss: 4463.14453125\n",
      "Epoch 7109, Train_Loss: 4277.81787109375, Val_Loss: 4463.0078125\n",
      "Epoch 7110, Train_Loss: 4277.70458984375, Val_Loss: 4462.85693359375\n",
      "Epoch 7111, Train_Loss: 4277.5947265625, Val_Loss: 4462.71337890625\n",
      "Epoch 7112, Train_Loss: 4277.48046875, Val_Loss: 4462.576171875\n",
      "Epoch 7113, Train_Loss: 4277.35693359375, Val_Loss: 4462.4453125\n",
      "Epoch 7114, Train_Loss: 4277.2080078125, Val_Loss: 4462.30322265625\n",
      "Epoch 7115, Train_Loss: 4277.08544921875, Val_Loss: 4462.16455078125\n",
      "Epoch 7116, Train_Loss: 4276.97900390625, Val_Loss: 4462.02294921875\n",
      "Epoch 7117, Train_Loss: 4276.90673828125, Val_Loss: 4461.88525390625\n",
      "Epoch 7118, Train_Loss: 4276.77783203125, Val_Loss: 4461.74169921875\n",
      "Epoch 7119, Train_Loss: 4276.6533203125, Val_Loss: 4461.59326171875\n",
      "Epoch 7120, Train_Loss: 4276.5380859375, Val_Loss: 4461.44482421875\n",
      "Epoch 7121, Train_Loss: 4276.43603515625, Val_Loss: 4461.3193359375\n",
      "Epoch 7122, Train_Loss: 4276.3291015625, Val_Loss: 4461.18994140625\n",
      "Epoch 7123, Train_Loss: 4276.21533203125, Val_Loss: 4461.0419921875\n",
      "Epoch 7124, Train_Loss: 4276.0849609375, Val_Loss: 4460.892578125\n",
      "Epoch 7125, Train_Loss: 4275.978515625, Val_Loss: 4460.7421875\n",
      "Epoch 7126, Train_Loss: 4275.8642578125, Val_Loss: 4460.607421875\n",
      "Epoch 7127, Train_Loss: 4275.7431640625, Val_Loss: 4460.4609375\n",
      "Epoch 7128, Train_Loss: 4275.5673828125, Val_Loss: 4460.33544921875\n",
      "Epoch 7129, Train_Loss: 4275.46142578125, Val_Loss: 4460.17822265625\n",
      "Epoch 7130, Train_Loss: 4275.3447265625, Val_Loss: 4460.044921875\n",
      "Epoch 7131, Train_Loss: 4275.24169921875, Val_Loss: 4459.88916015625\n",
      "Epoch 7132, Train_Loss: 4275.12109375, Val_Loss: 4459.7490234375\n",
      "Epoch 7133, Train_Loss: 4275.0185546875, Val_Loss: 4459.6123046875\n",
      "Epoch 7134, Train_Loss: 4274.91259765625, Val_Loss: 4459.49462890625\n",
      "Epoch 7135, Train_Loss: 4274.7939453125, Val_Loss: 4459.34423828125\n",
      "Epoch 7136, Train_Loss: 4274.68310546875, Val_Loss: 4459.2080078125\n",
      "Epoch 7137, Train_Loss: 4274.56689453125, Val_Loss: 4459.07080078125\n",
      "Epoch 7138, Train_Loss: 4274.4482421875, Val_Loss: 4458.884765625\n",
      "Epoch 7139, Train_Loss: 4274.3046875, Val_Loss: 4458.75537109375\n",
      "Epoch 7140, Train_Loss: 4274.2041015625, Val_Loss: 4458.607421875\n",
      "Epoch 7141, Train_Loss: 4274.06103515625, Val_Loss: 4458.474609375\n",
      "Epoch 7142, Train_Loss: 4273.931640625, Val_Loss: 4458.34423828125\n",
      "Epoch 7143, Train_Loss: 4273.81298828125, Val_Loss: 4458.19775390625\n",
      "Epoch 7144, Train_Loss: 4273.71435546875, Val_Loss: 4458.05029296875\n",
      "Epoch 7145, Train_Loss: 4273.59765625, Val_Loss: 4457.908203125\n",
      "Epoch 7146, Train_Loss: 4273.48681640625, Val_Loss: 4457.775390625\n",
      "Epoch 7147, Train_Loss: 4273.380859375, Val_Loss: 4457.6318359375\n",
      "Epoch 7148, Train_Loss: 4273.23828125, Val_Loss: 4457.5009765625\n",
      "Epoch 7149, Train_Loss: 4273.10498046875, Val_Loss: 4457.35888671875\n",
      "Epoch 7150, Train_Loss: 4272.98779296875, Val_Loss: 4457.22607421875\n",
      "Epoch 7151, Train_Loss: 4272.87109375, Val_Loss: 4457.078125\n",
      "Epoch 7152, Train_Loss: 4272.77490234375, Val_Loss: 4456.93359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7153, Train_Loss: 4272.65869140625, Val_Loss: 4456.79443359375\n",
      "Epoch 7154, Train_Loss: 4272.5498046875, Val_Loss: 4456.66943359375\n",
      "Epoch 7155, Train_Loss: 4272.423828125, Val_Loss: 4456.52978515625\n",
      "Epoch 7156, Train_Loss: 4272.31640625, Val_Loss: 4456.38623046875\n",
      "Epoch 7157, Train_Loss: 4272.12353515625, Val_Loss: 4456.2470703125\n",
      "Epoch 7158, Train_Loss: 4272.0185546875, Val_Loss: 4456.09619140625\n",
      "Epoch 7159, Train_Loss: 4271.8935546875, Val_Loss: 4455.95556640625\n",
      "Epoch 7160, Train_Loss: 4271.77783203125, Val_Loss: 4455.82275390625\n",
      "Epoch 7161, Train_Loss: 4271.66845703125, Val_Loss: 4455.68310546875\n",
      "Epoch 7162, Train_Loss: 4271.54296875, Val_Loss: 4455.54541015625\n",
      "Epoch 7163, Train_Loss: 4271.4267578125, Val_Loss: 4455.4130859375\n",
      "Epoch 7164, Train_Loss: 4271.30322265625, Val_Loss: 4455.27587890625\n",
      "Epoch 7165, Train_Loss: 4271.197265625, Val_Loss: 4455.125\n",
      "Epoch 7166, Train_Loss: 4271.09228515625, Val_Loss: 4454.99072265625\n",
      "Epoch 7167, Train_Loss: 4270.96337890625, Val_Loss: 4454.83740234375\n",
      "Epoch 7168, Train_Loss: 4270.8134765625, Val_Loss: 4454.7177734375\n",
      "Epoch 7169, Train_Loss: 4270.69970703125, Val_Loss: 4454.5703125\n",
      "Epoch 7170, Train_Loss: 4270.57763671875, Val_Loss: 4454.43310546875\n",
      "Epoch 7171, Train_Loss: 4270.51025390625, Val_Loss: 4454.2841796875\n",
      "Epoch 7172, Train_Loss: 4270.404296875, Val_Loss: 4454.1494140625\n",
      "Epoch 7173, Train_Loss: 4270.29638671875, Val_Loss: 4454.009765625\n",
      "Epoch 7174, Train_Loss: 4270.18359375, Val_Loss: 4453.8681640625\n",
      "Epoch 7175, Train_Loss: 4270.06689453125, Val_Loss: 4453.7412109375\n",
      "Epoch 7176, Train_Loss: 4269.939453125, Val_Loss: 4453.56884765625\n",
      "Epoch 7177, Train_Loss: 4269.83544921875, Val_Loss: 4453.43408203125\n",
      "Epoch 7178, Train_Loss: 4269.7158203125, Val_Loss: 4453.27587890625\n",
      "Epoch 7179, Train_Loss: 4269.6005859375, Val_Loss: 4453.130859375\n",
      "Epoch 7180, Train_Loss: 4269.47021484375, Val_Loss: 4452.998046875\n",
      "Epoch 7181, Train_Loss: 4269.357421875, Val_Loss: 4452.861328125\n",
      "Epoch 7182, Train_Loss: 4269.18310546875, Val_Loss: 4452.73583984375\n",
      "Epoch 7183, Train_Loss: 4269.08203125, Val_Loss: 4452.59521484375\n",
      "Epoch 7184, Train_Loss: 4268.97119140625, Val_Loss: 4452.46533203125\n",
      "Epoch 7185, Train_Loss: 4268.86376953125, Val_Loss: 4452.318359375\n",
      "Epoch 7186, Train_Loss: 4268.74169921875, Val_Loss: 4452.16748046875\n",
      "Epoch 7187, Train_Loss: 4268.64453125, Val_Loss: 4452.0302734375\n",
      "Epoch 7188, Train_Loss: 4268.53662109375, Val_Loss: 4451.900390625\n",
      "Epoch 7189, Train_Loss: 4268.4287109375, Val_Loss: 4451.7568359375\n",
      "Epoch 7190, Train_Loss: 4268.29443359375, Val_Loss: 4451.62060546875\n",
      "Epoch 7191, Train_Loss: 4268.19189453125, Val_Loss: 4451.48974609375\n",
      "Epoch 7192, Train_Loss: 4268.0830078125, Val_Loss: 4451.3408203125\n",
      "Epoch 7193, Train_Loss: 4267.9609375, Val_Loss: 4451.2001953125\n",
      "Epoch 7194, Train_Loss: 4267.83837890625, Val_Loss: 4451.0634765625\n",
      "Epoch 7195, Train_Loss: 4267.67626953125, Val_Loss: 4450.92724609375\n",
      "Epoch 7196, Train_Loss: 4267.56591796875, Val_Loss: 4450.79443359375\n",
      "Epoch 7197, Train_Loss: 4267.4375, Val_Loss: 4450.6533203125\n",
      "Epoch 7198, Train_Loss: 4267.32275390625, Val_Loss: 4450.5234375\n",
      "Epoch 7199, Train_Loss: 4267.21337890625, Val_Loss: 4450.37255859375\n",
      "Epoch 7200, Train_Loss: 4267.07666015625, Val_Loss: 4450.2392578125\n",
      "Epoch 7201, Train_Loss: 4266.955078125, Val_Loss: 4450.0986328125\n",
      "Epoch 7202, Train_Loss: 4266.86279296875, Val_Loss: 4449.97216796875\n",
      "Epoch 7203, Train_Loss: 4266.7568359375, Val_Loss: 4449.83056640625\n",
      "Epoch 7204, Train_Loss: 4266.6474609375, Val_Loss: 4449.69580078125\n",
      "Epoch 7205, Train_Loss: 4266.52685546875, Val_Loss: 4449.51904296875\n",
      "Epoch 7206, Train_Loss: 4266.41552734375, Val_Loss: 4449.39501953125\n",
      "Epoch 7207, Train_Loss: 4266.29931640625, Val_Loss: 4449.25830078125\n",
      "Epoch 7208, Train_Loss: 4266.18994140625, Val_Loss: 4449.12255859375\n",
      "Epoch 7209, Train_Loss: 4266.056640625, Val_Loss: 4448.9873046875\n",
      "Epoch 7210, Train_Loss: 4265.96142578125, Val_Loss: 4448.853515625\n",
      "Epoch 7211, Train_Loss: 4265.8427734375, Val_Loss: 4448.724609375\n",
      "Epoch 7212, Train_Loss: 4265.7353515625, Val_Loss: 4448.58056640625\n",
      "Epoch 7213, Train_Loss: 4265.63134765625, Val_Loss: 4448.42919921875\n",
      "Epoch 7214, Train_Loss: 4265.525390625, Val_Loss: 4448.29541015625\n",
      "Epoch 7215, Train_Loss: 4265.4091796875, Val_Loss: 4448.115234375\n",
      "Epoch 7216, Train_Loss: 4265.3037109375, Val_Loss: 4447.9931640625\n",
      "Epoch 7217, Train_Loss: 4265.17724609375, Val_Loss: 4447.85595703125\n",
      "Epoch 7218, Train_Loss: 4265.06884765625, Val_Loss: 4447.72265625\n",
      "Epoch 7219, Train_Loss: 4264.9716796875, Val_Loss: 4447.56884765625\n",
      "Epoch 7220, Train_Loss: 4264.86328125, Val_Loss: 4447.42626953125\n",
      "Epoch 7221, Train_Loss: 4264.72509765625, Val_Loss: 4447.298828125\n",
      "Epoch 7222, Train_Loss: 4264.59033203125, Val_Loss: 4447.17236328125\n",
      "Epoch 7223, Train_Loss: 4264.490234375, Val_Loss: 4447.0341796875\n",
      "Epoch 7224, Train_Loss: 4264.3564453125, Val_Loss: 4446.89697265625\n",
      "Epoch 7225, Train_Loss: 4264.23876953125, Val_Loss: 4446.759765625\n",
      "Epoch 7226, Train_Loss: 4264.15869140625, Val_Loss: 4446.609375\n",
      "Epoch 7227, Train_Loss: 4264.0380859375, Val_Loss: 4446.474609375\n",
      "Epoch 7228, Train_Loss: 4263.93505859375, Val_Loss: 4446.33349609375\n",
      "Epoch 7229, Train_Loss: 4263.826171875, Val_Loss: 4446.2060546875\n",
      "Epoch 7230, Train_Loss: 4263.716796875, Val_Loss: 4446.0634765625\n",
      "Epoch 7231, Train_Loss: 4263.59716796875, Val_Loss: 4445.927734375\n",
      "Epoch 7232, Train_Loss: 4263.478515625, Val_Loss: 4445.7783203125\n",
      "Epoch 7233, Train_Loss: 4263.37255859375, Val_Loss: 4445.64794921875\n",
      "Epoch 7234, Train_Loss: 4263.197265625, Val_Loss: 4445.513671875\n",
      "Epoch 7235, Train_Loss: 4263.08203125, Val_Loss: 4445.37255859375\n",
      "Epoch 7236, Train_Loss: 4262.91650390625, Val_Loss: 4445.25146484375\n",
      "Epoch 7237, Train_Loss: 4262.78955078125, Val_Loss: 4445.10205078125\n",
      "Epoch 7238, Train_Loss: 4262.6953125, Val_Loss: 4444.9736328125\n",
      "Epoch 7239, Train_Loss: 4262.587890625, Val_Loss: 4444.8232421875\n",
      "Epoch 7240, Train_Loss: 4262.47705078125, Val_Loss: 4444.68994140625\n",
      "Epoch 7241, Train_Loss: 4262.35205078125, Val_Loss: 4444.55517578125\n",
      "Epoch 7242, Train_Loss: 4262.24072265625, Val_Loss: 4444.4189453125\n",
      "Epoch 7243, Train_Loss: 4262.11669921875, Val_Loss: 4444.279296875\n",
      "Epoch 7244, Train_Loss: 4262.009765625, Val_Loss: 4444.13330078125\n",
      "Epoch 7245, Train_Loss: 4261.89453125, Val_Loss: 4444.0\n",
      "Epoch 7246, Train_Loss: 4261.7783203125, Val_Loss: 4443.8447265625\n",
      "Epoch 7247, Train_Loss: 4261.666015625, Val_Loss: 4443.72216796875\n",
      "Epoch 7248, Train_Loss: 4261.5625, Val_Loss: 4443.5859375\n",
      "Epoch 7249, Train_Loss: 4261.41943359375, Val_Loss: 4443.45458984375\n",
      "Epoch 7250, Train_Loss: 4261.30712890625, Val_Loss: 4443.3134765625\n",
      "Epoch 7251, Train_Loss: 4261.18359375, Val_Loss: 4443.17822265625\n",
      "Epoch 7252, Train_Loss: 4261.0791015625, Val_Loss: 4443.0400390625\n",
      "Epoch 7253, Train_Loss: 4260.962890625, Val_Loss: 4442.859375\n",
      "Epoch 7254, Train_Loss: 4260.8486328125, Val_Loss: 4442.724609375\n",
      "Epoch 7255, Train_Loss: 4260.7470703125, Val_Loss: 4442.58984375\n",
      "Epoch 7256, Train_Loss: 4260.64404296875, Val_Loss: 4442.46337890625\n",
      "Epoch 7257, Train_Loss: 4260.51708984375, Val_Loss: 4442.32421875\n",
      "Epoch 7258, Train_Loss: 4260.39404296875, Val_Loss: 4442.1845703125\n",
      "Epoch 7259, Train_Loss: 4260.275390625, Val_Loss: 4442.0478515625\n",
      "Epoch 7260, Train_Loss: 4260.16357421875, Val_Loss: 4441.90380859375\n",
      "Epoch 7261, Train_Loss: 4260.05029296875, Val_Loss: 4441.76904296875\n",
      "Epoch 7262, Train_Loss: 4259.9169921875, Val_Loss: 4441.625\n",
      "Epoch 7263, Train_Loss: 4259.79248046875, Val_Loss: 4441.50439453125\n",
      "Epoch 7264, Train_Loss: 4259.68359375, Val_Loss: 4441.36767578125\n",
      "Epoch 7265, Train_Loss: 4259.59912109375, Val_Loss: 4441.23046875\n",
      "Epoch 7266, Train_Loss: 4259.501953125, Val_Loss: 4441.0849609375\n",
      "Epoch 7267, Train_Loss: 4259.39599609375, Val_Loss: 4440.9521484375\n",
      "Epoch 7268, Train_Loss: 4259.2880859375, Val_Loss: 4440.806640625\n",
      "Epoch 7269, Train_Loss: 4259.17138671875, Val_Loss: 4440.67919921875\n",
      "Epoch 7270, Train_Loss: 4259.0595703125, Val_Loss: 4440.54833984375\n",
      "Epoch 7271, Train_Loss: 4258.9306640625, Val_Loss: 4440.41162109375\n",
      "Epoch 7272, Train_Loss: 4258.80029296875, Val_Loss: 4440.27197265625\n",
      "Epoch 7273, Train_Loss: 4258.68212890625, Val_Loss: 4440.12060546875\n",
      "Epoch 7274, Train_Loss: 4258.56884765625, Val_Loss: 4439.9765625\n",
      "Epoch 7275, Train_Loss: 4258.4609375, Val_Loss: 4439.85302734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7276, Train_Loss: 4258.361328125, Val_Loss: 4439.716796875\n",
      "Epoch 7277, Train_Loss: 4258.208984375, Val_Loss: 4439.5966796875\n",
      "Epoch 7278, Train_Loss: 4258.1005859375, Val_Loss: 4439.45703125\n",
      "Epoch 7279, Train_Loss: 4257.9853515625, Val_Loss: 4439.3154296875\n",
      "Epoch 7280, Train_Loss: 4257.9248046875, Val_Loss: 4439.1767578125\n",
      "Epoch 7281, Train_Loss: 4257.82275390625, Val_Loss: 4439.0234375\n",
      "Epoch 7282, Train_Loss: 4257.69580078125, Val_Loss: 4438.88671875\n",
      "Epoch 7283, Train_Loss: 4257.591796875, Val_Loss: 4438.76123046875\n",
      "Epoch 7284, Train_Loss: 4257.47265625, Val_Loss: 4438.62548828125\n",
      "Epoch 7285, Train_Loss: 4257.36328125, Val_Loss: 4438.48828125\n",
      "Epoch 7286, Train_Loss: 4257.2548828125, Val_Loss: 4438.36669921875\n",
      "Epoch 7287, Train_Loss: 4257.12939453125, Val_Loss: 4438.21044921875\n",
      "Epoch 7288, Train_Loss: 4257.01953125, Val_Loss: 4438.064453125\n",
      "Epoch 7289, Train_Loss: 4256.9140625, Val_Loss: 4437.93359375\n",
      "Epoch 7290, Train_Loss: 4256.74072265625, Val_Loss: 4437.80322265625\n",
      "Epoch 7291, Train_Loss: 4256.63525390625, Val_Loss: 4437.63134765625\n",
      "Epoch 7292, Train_Loss: 4256.5322265625, Val_Loss: 4437.50048828125\n",
      "Epoch 7293, Train_Loss: 4256.41552734375, Val_Loss: 4437.3525390625\n",
      "Epoch 7294, Train_Loss: 4256.29736328125, Val_Loss: 4437.2265625\n",
      "Epoch 7295, Train_Loss: 4256.193359375, Val_Loss: 4437.091796875\n",
      "Epoch 7296, Train_Loss: 4256.078125, Val_Loss: 4436.94140625\n",
      "Epoch 7297, Train_Loss: 4255.966796875, Val_Loss: 4436.8330078125\n",
      "Epoch 7298, Train_Loss: 4255.853515625, Val_Loss: 4436.685546875\n",
      "Epoch 7299, Train_Loss: 4255.736328125, Val_Loss: 4436.54931640625\n",
      "Epoch 7300, Train_Loss: 4255.62548828125, Val_Loss: 4436.41259765625\n",
      "Epoch 7301, Train_Loss: 4255.4189453125, Val_Loss: 4436.27880859375\n",
      "Epoch 7302, Train_Loss: 4255.29443359375, Val_Loss: 4436.142578125\n",
      "Epoch 7303, Train_Loss: 4255.177734375, Val_Loss: 4436.0078125\n",
      "Epoch 7304, Train_Loss: 4255.03564453125, Val_Loss: 4435.87841796875\n",
      "Epoch 7305, Train_Loss: 4254.92138671875, Val_Loss: 4435.7392578125\n",
      "Epoch 7306, Train_Loss: 4254.82275390625, Val_Loss: 4435.603515625\n",
      "Epoch 7307, Train_Loss: 4254.71826171875, Val_Loss: 4435.46875\n",
      "Epoch 7308, Train_Loss: 4254.591796875, Val_Loss: 4435.3203125\n",
      "Epoch 7309, Train_Loss: 4254.49072265625, Val_Loss: 4435.19189453125\n",
      "Epoch 7310, Train_Loss: 4254.32861328125, Val_Loss: 4435.0732421875\n",
      "Epoch 7311, Train_Loss: 4254.228515625, Val_Loss: 4434.93017578125\n",
      "Epoch 7312, Train_Loss: 4254.12060546875, Val_Loss: 4434.7861328125\n",
      "Epoch 7313, Train_Loss: 4253.99462890625, Val_Loss: 4434.658203125\n",
      "Epoch 7314, Train_Loss: 4253.884765625, Val_Loss: 4434.50634765625\n",
      "Epoch 7315, Train_Loss: 4253.7705078125, Val_Loss: 4434.37646484375\n",
      "Epoch 7316, Train_Loss: 4253.66162109375, Val_Loss: 4434.2412109375\n",
      "Epoch 7317, Train_Loss: 4253.52490234375, Val_Loss: 4434.123046875\n",
      "Epoch 7318, Train_Loss: 4253.4189453125, Val_Loss: 4433.99169921875\n",
      "Epoch 7319, Train_Loss: 4253.31689453125, Val_Loss: 4433.85107421875\n",
      "Epoch 7320, Train_Loss: 4253.19580078125, Val_Loss: 4433.70458984375\n",
      "Epoch 7321, Train_Loss: 4253.10693359375, Val_Loss: 4433.55419921875\n",
      "Epoch 7322, Train_Loss: 4252.99609375, Val_Loss: 4433.40771484375\n",
      "Epoch 7323, Train_Loss: 4252.86962890625, Val_Loss: 4433.27978515625\n",
      "Epoch 7324, Train_Loss: 4252.75341796875, Val_Loss: 4433.1630859375\n",
      "Epoch 7325, Train_Loss: 4252.64453125, Val_Loss: 4433.0341796875\n",
      "Epoch 7326, Train_Loss: 4252.53759765625, Val_Loss: 4432.90478515625\n",
      "Epoch 7327, Train_Loss: 4252.41796875, Val_Loss: 4432.75\n",
      "Epoch 7328, Train_Loss: 4252.2919921875, Val_Loss: 4432.61962890625\n",
      "Epoch 7329, Train_Loss: 4252.16845703125, Val_Loss: 4432.45166015625\n",
      "Epoch 7330, Train_Loss: 4252.06591796875, Val_Loss: 4432.32177734375\n",
      "Epoch 7331, Train_Loss: 4251.9423828125, Val_Loss: 4432.19287109375\n",
      "Epoch 7332, Train_Loss: 4251.84716796875, Val_Loss: 4432.052734375\n",
      "Epoch 7333, Train_Loss: 4251.7392578125, Val_Loss: 4431.9140625\n",
      "Epoch 7334, Train_Loss: 4251.66650390625, Val_Loss: 4431.7705078125\n",
      "Epoch 7335, Train_Loss: 4251.5673828125, Val_Loss: 4431.642578125\n",
      "Epoch 7336, Train_Loss: 4251.45556640625, Val_Loss: 4431.501953125\n",
      "Epoch 7337, Train_Loss: 4251.3466796875, Val_Loss: 4431.3603515625\n",
      "Epoch 7338, Train_Loss: 4251.23876953125, Val_Loss: 4431.240234375\n",
      "Epoch 7339, Train_Loss: 4251.11328125, Val_Loss: 4431.1044921875\n",
      "Epoch 7340, Train_Loss: 4250.99462890625, Val_Loss: 4430.9716796875\n",
      "Epoch 7341, Train_Loss: 4250.89306640625, Val_Loss: 4430.8310546875\n",
      "Epoch 7342, Train_Loss: 4250.798828125, Val_Loss: 4430.6923828125\n",
      "Epoch 7343, Train_Loss: 4250.69140625, Val_Loss: 4430.5673828125\n",
      "Epoch 7344, Train_Loss: 4250.4912109375, Val_Loss: 4430.431640625\n",
      "Epoch 7345, Train_Loss: 4250.384765625, Val_Loss: 4430.30810546875\n",
      "Epoch 7346, Train_Loss: 4250.27099609375, Val_Loss: 4430.1787109375\n",
      "Epoch 7347, Train_Loss: 4250.16015625, Val_Loss: 4430.0341796875\n",
      "Epoch 7348, Train_Loss: 4250.05419921875, Val_Loss: 4429.880859375\n",
      "Epoch 7349, Train_Loss: 4249.94140625, Val_Loss: 4429.75537109375\n",
      "Epoch 7350, Train_Loss: 4249.83740234375, Val_Loss: 4429.609375\n",
      "Epoch 7351, Train_Loss: 4249.73046875, Val_Loss: 4429.4853515625\n",
      "Epoch 7352, Train_Loss: 4249.61865234375, Val_Loss: 4429.35302734375\n",
      "Epoch 7353, Train_Loss: 4249.5087890625, Val_Loss: 4429.21630859375\n",
      "Epoch 7354, Train_Loss: 4249.390625, Val_Loss: 4429.09619140625\n",
      "Epoch 7355, Train_Loss: 4249.28515625, Val_Loss: 4428.94384765625\n",
      "Epoch 7356, Train_Loss: 4249.16455078125, Val_Loss: 4428.810546875\n",
      "Epoch 7357, Train_Loss: 4249.0654296875, Val_Loss: 4428.6787109375\n",
      "Epoch 7358, Train_Loss: 4248.916015625, Val_Loss: 4428.53759765625\n",
      "Epoch 7359, Train_Loss: 4248.78857421875, Val_Loss: 4428.40869140625\n",
      "Epoch 7360, Train_Loss: 4248.6982421875, Val_Loss: 4428.2763671875\n",
      "Epoch 7361, Train_Loss: 4248.60693359375, Val_Loss: 4428.134765625\n",
      "Epoch 7362, Train_Loss: 4248.4990234375, Val_Loss: 4427.998046875\n",
      "Epoch 7363, Train_Loss: 4248.392578125, Val_Loss: 4427.8662109375\n",
      "Epoch 7364, Train_Loss: 4248.267578125, Val_Loss: 4427.73388671875\n",
      "Epoch 7365, Train_Loss: 4248.1572265625, Val_Loss: 4427.60693359375\n",
      "Epoch 7366, Train_Loss: 4248.0546875, Val_Loss: 4427.47265625\n",
      "Epoch 7367, Train_Loss: 4247.947265625, Val_Loss: 4427.3349609375\n",
      "Epoch 7368, Train_Loss: 4247.8369140625, Val_Loss: 4427.1630859375\n",
      "Epoch 7369, Train_Loss: 4247.7080078125, Val_Loss: 4427.02490234375\n",
      "Epoch 7370, Train_Loss: 4247.59326171875, Val_Loss: 4426.89697265625\n",
      "Epoch 7371, Train_Loss: 4247.47998046875, Val_Loss: 4426.7578125\n",
      "Epoch 7372, Train_Loss: 4247.3681640625, Val_Loss: 4426.63671875\n",
      "Epoch 7373, Train_Loss: 4247.263671875, Val_Loss: 4426.509765625\n",
      "Epoch 7374, Train_Loss: 4247.1455078125, Val_Loss: 4426.373046875\n",
      "Epoch 7375, Train_Loss: 4247.0419921875, Val_Loss: 4426.2265625\n",
      "Epoch 7376, Train_Loss: 4246.93310546875, Val_Loss: 4426.10498046875\n",
      "Epoch 7377, Train_Loss: 4246.81591796875, Val_Loss: 4425.970703125\n",
      "Epoch 7378, Train_Loss: 4246.7197265625, Val_Loss: 4425.841796875\n",
      "Epoch 7379, Train_Loss: 4246.60888671875, Val_Loss: 4425.7109375\n",
      "Epoch 7380, Train_Loss: 4246.49658203125, Val_Loss: 4425.56982421875\n",
      "Epoch 7381, Train_Loss: 4246.39208984375, Val_Loss: 4425.43896484375\n",
      "Epoch 7382, Train_Loss: 4246.28564453125, Val_Loss: 4425.291015625\n",
      "Epoch 7383, Train_Loss: 4246.18408203125, Val_Loss: 4425.1669921875\n",
      "Epoch 7384, Train_Loss: 4246.08154296875, Val_Loss: 4425.029296875\n",
      "Epoch 7385, Train_Loss: 4245.91064453125, Val_Loss: 4424.90966796875\n",
      "Epoch 7386, Train_Loss: 4245.802734375, Val_Loss: 4424.7705078125\n",
      "Epoch 7387, Train_Loss: 4245.5498046875, Val_Loss: 4424.6337890625\n",
      "Epoch 7388, Train_Loss: 4245.443359375, Val_Loss: 4424.50634765625\n",
      "Epoch 7389, Train_Loss: 4245.3828125, Val_Loss: 4424.36279296875\n",
      "Epoch 7390, Train_Loss: 4245.2626953125, Val_Loss: 4424.22509765625\n",
      "Epoch 7391, Train_Loss: 4245.16064453125, Val_Loss: 4424.09423828125\n",
      "Epoch 7392, Train_Loss: 4245.05029296875, Val_Loss: 4423.966796875\n",
      "Epoch 7393, Train_Loss: 4244.951171875, Val_Loss: 4423.83837890625\n",
      "Epoch 7394, Train_Loss: 4244.84033203125, Val_Loss: 4423.69287109375\n",
      "Epoch 7395, Train_Loss: 4244.71435546875, Val_Loss: 4423.560546875\n",
      "Epoch 7396, Train_Loss: 4244.60546875, Val_Loss: 4423.42138671875\n",
      "Epoch 7397, Train_Loss: 4244.50537109375, Val_Loss: 4423.287109375\n",
      "Epoch 7398, Train_Loss: 4244.40185546875, Val_Loss: 4423.14697265625\n",
      "Epoch 7399, Train_Loss: 4244.24169921875, Val_Loss: 4423.0380859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7400, Train_Loss: 4244.13330078125, Val_Loss: 4422.88720703125\n",
      "Epoch 7401, Train_Loss: 4244.02587890625, Val_Loss: 4422.765625\n",
      "Epoch 7402, Train_Loss: 4243.92236328125, Val_Loss: 4422.61474609375\n",
      "Epoch 7403, Train_Loss: 4243.822265625, Val_Loss: 4422.49267578125\n",
      "Epoch 7404, Train_Loss: 4243.71826171875, Val_Loss: 4422.359375\n",
      "Epoch 7405, Train_Loss: 4243.57568359375, Val_Loss: 4422.1884765625\n",
      "Epoch 7406, Train_Loss: 4243.4541015625, Val_Loss: 4422.06640625\n",
      "Epoch 7407, Train_Loss: 4243.3447265625, Val_Loss: 4421.92578125\n",
      "Epoch 7408, Train_Loss: 4243.23193359375, Val_Loss: 4421.806640625\n",
      "Epoch 7409, Train_Loss: 4243.130859375, Val_Loss: 4421.64697265625\n",
      "Epoch 7410, Train_Loss: 4243.03564453125, Val_Loss: 4421.5166015625\n",
      "Epoch 7411, Train_Loss: 4242.91064453125, Val_Loss: 4421.37548828125\n",
      "Epoch 7412, Train_Loss: 4242.75146484375, Val_Loss: 4421.2470703125\n",
      "Epoch 7413, Train_Loss: 4242.63427734375, Val_Loss: 4421.10693359375\n",
      "Epoch 7414, Train_Loss: 4242.541015625, Val_Loss: 4420.95703125\n",
      "Epoch 7415, Train_Loss: 4242.41064453125, Val_Loss: 4420.8037109375\n",
      "Epoch 7416, Train_Loss: 4242.29052734375, Val_Loss: 4420.658203125\n",
      "Epoch 7417, Train_Loss: 4242.20068359375, Val_Loss: 4420.51416015625\n",
      "Epoch 7418, Train_Loss: 4242.09521484375, Val_Loss: 4420.38671875\n",
      "Epoch 7419, Train_Loss: 4241.9892578125, Val_Loss: 4420.24609375\n",
      "Epoch 7420, Train_Loss: 4241.8603515625, Val_Loss: 4420.095703125\n",
      "Epoch 7421, Train_Loss: 4241.744140625, Val_Loss: 4419.955078125\n",
      "Epoch 7422, Train_Loss: 4241.62841796875, Val_Loss: 4419.79736328125\n",
      "Epoch 7423, Train_Loss: 4241.48095703125, Val_Loss: 4419.642578125\n",
      "Epoch 7424, Train_Loss: 4241.36572265625, Val_Loss: 4419.5078125\n",
      "Epoch 7425, Train_Loss: 4241.201171875, Val_Loss: 4419.375\n",
      "Epoch 7426, Train_Loss: 4241.1015625, Val_Loss: 4419.21728515625\n",
      "Epoch 7427, Train_Loss: 4240.9921875, Val_Loss: 4419.08349609375\n",
      "Epoch 7428, Train_Loss: 4240.8916015625, Val_Loss: 4418.93310546875\n",
      "Epoch 7429, Train_Loss: 4240.78271484375, Val_Loss: 4418.791015625\n",
      "Epoch 7430, Train_Loss: 4240.6787109375, Val_Loss: 4418.65087890625\n",
      "Epoch 7431, Train_Loss: 4240.56884765625, Val_Loss: 4418.50830078125\n",
      "Epoch 7432, Train_Loss: 4240.46630859375, Val_Loss: 4418.36474609375\n",
      "Epoch 7433, Train_Loss: 4240.34521484375, Val_Loss: 4418.21533203125\n",
      "Epoch 7434, Train_Loss: 4240.216796875, Val_Loss: 4418.068359375\n",
      "Epoch 7435, Train_Loss: 4240.111328125, Val_Loss: 4417.92578125\n",
      "Epoch 7436, Train_Loss: 4239.9775390625, Val_Loss: 4417.7744140625\n",
      "Epoch 7437, Train_Loss: 4239.83935546875, Val_Loss: 4417.64892578125\n",
      "Epoch 7438, Train_Loss: 4239.7099609375, Val_Loss: 4417.50341796875\n",
      "Epoch 7439, Train_Loss: 4239.58984375, Val_Loss: 4417.36669921875\n",
      "Epoch 7440, Train_Loss: 4239.51806640625, Val_Loss: 4417.22216796875\n",
      "Epoch 7441, Train_Loss: 4239.41064453125, Val_Loss: 4417.03759765625\n",
      "Epoch 7442, Train_Loss: 4239.30810546875, Val_Loss: 4416.89453125\n",
      "Epoch 7443, Train_Loss: 4239.18603515625, Val_Loss: 4416.759765625\n",
      "Epoch 7444, Train_Loss: 4239.04443359375, Val_Loss: 4416.638671875\n",
      "Epoch 7445, Train_Loss: 4238.93798828125, Val_Loss: 4416.49658203125\n",
      "Epoch 7446, Train_Loss: 4238.82568359375, Val_Loss: 4416.35302734375\n",
      "Epoch 7447, Train_Loss: 4238.716796875, Val_Loss: 4416.20263671875\n",
      "Epoch 7448, Train_Loss: 4238.60986328125, Val_Loss: 4416.06103515625\n",
      "Epoch 7449, Train_Loss: 4238.4873046875, Val_Loss: 4415.92822265625\n",
      "Epoch 7450, Train_Loss: 4238.31982421875, Val_Loss: 4415.79541015625\n",
      "Epoch 7451, Train_Loss: 4238.20166015625, Val_Loss: 4415.66162109375\n",
      "Epoch 7452, Train_Loss: 4238.08984375, Val_Loss: 4415.51806640625\n",
      "Epoch 7453, Train_Loss: 4237.96875, Val_Loss: 4415.36181640625\n",
      "Epoch 7454, Train_Loss: 4237.83251953125, Val_Loss: 4415.21826171875\n",
      "Epoch 7455, Train_Loss: 4237.71826171875, Val_Loss: 4415.0810546875\n",
      "Epoch 7456, Train_Loss: 4237.60791015625, Val_Loss: 4414.951171875\n",
      "Epoch 7457, Train_Loss: 4237.51025390625, Val_Loss: 4414.8173828125\n",
      "Epoch 7458, Train_Loss: 4237.4111328125, Val_Loss: 4414.68359375\n",
      "Epoch 7459, Train_Loss: 4237.24072265625, Val_Loss: 4414.54833984375\n",
      "Epoch 7460, Train_Loss: 4237.13232421875, Val_Loss: 4414.39404296875\n",
      "Epoch 7461, Train_Loss: 4237.01416015625, Val_Loss: 4414.25390625\n",
      "Epoch 7462, Train_Loss: 4236.90283203125, Val_Loss: 4414.10693359375\n",
      "Epoch 7463, Train_Loss: 4236.748046875, Val_Loss: 4413.98046875\n",
      "Epoch 7464, Train_Loss: 4236.62841796875, Val_Loss: 4413.8486328125\n",
      "Epoch 7465, Train_Loss: 4236.5263671875, Val_Loss: 4413.71044921875\n",
      "Epoch 7466, Train_Loss: 4236.435546875, Val_Loss: 4413.57080078125\n",
      "Epoch 7467, Train_Loss: 4236.32666015625, Val_Loss: 4413.4365234375\n",
      "Epoch 7468, Train_Loss: 4236.2041015625, Val_Loss: 4413.28125\n",
      "Epoch 7469, Train_Loss: 4236.1015625, Val_Loss: 4413.15478515625\n",
      "Epoch 7470, Train_Loss: 4235.9951171875, Val_Loss: 4413.01953125\n",
      "Epoch 7471, Train_Loss: 4235.87939453125, Val_Loss: 4412.87451171875\n",
      "Epoch 7472, Train_Loss: 4235.7578125, Val_Loss: 4412.73291015625\n",
      "Epoch 7473, Train_Loss: 4235.630859375, Val_Loss: 4412.59423828125\n",
      "Epoch 7474, Train_Loss: 4235.5078125, Val_Loss: 4412.45458984375\n",
      "Epoch 7475, Train_Loss: 4235.388671875, Val_Loss: 4412.3212890625\n",
      "Epoch 7476, Train_Loss: 4235.279296875, Val_Loss: 4412.1923828125\n",
      "Epoch 7477, Train_Loss: 4235.17138671875, Val_Loss: 4412.02197265625\n",
      "Epoch 7478, Train_Loss: 4235.05712890625, Val_Loss: 4411.888671875\n",
      "Epoch 7479, Train_Loss: 4234.9609375, Val_Loss: 4411.74658203125\n",
      "Epoch 7480, Train_Loss: 4234.8525390625, Val_Loss: 4411.60546875\n",
      "Epoch 7481, Train_Loss: 4234.74560546875, Val_Loss: 4411.47607421875\n",
      "Epoch 7482, Train_Loss: 4234.630859375, Val_Loss: 4411.3466796875\n",
      "Epoch 7483, Train_Loss: 4234.49609375, Val_Loss: 4411.21533203125\n",
      "Epoch 7484, Train_Loss: 4234.3818359375, Val_Loss: 4411.07275390625\n",
      "Epoch 7485, Train_Loss: 4234.275390625, Val_Loss: 4410.9326171875\n",
      "Epoch 7486, Train_Loss: 4234.16455078125, Val_Loss: 4410.7822265625\n",
      "Epoch 7487, Train_Loss: 4234.04931640625, Val_Loss: 4410.65576171875\n",
      "Epoch 7488, Train_Loss: 4233.94091796875, Val_Loss: 4410.52587890625\n",
      "Epoch 7489, Train_Loss: 4233.802734375, Val_Loss: 4410.3876953125\n",
      "Epoch 7490, Train_Loss: 4233.68505859375, Val_Loss: 4410.25244140625\n",
      "Epoch 7491, Train_Loss: 4233.5888671875, Val_Loss: 4410.12158203125\n",
      "Epoch 7492, Train_Loss: 4233.5322265625, Val_Loss: 4409.97607421875\n",
      "Epoch 7493, Train_Loss: 4233.40771484375, Val_Loss: 4409.84375\n",
      "Epoch 7494, Train_Loss: 4233.29443359375, Val_Loss: 4409.7080078125\n",
      "Epoch 7495, Train_Loss: 4233.17236328125, Val_Loss: 4409.57373046875\n",
      "Epoch 7496, Train_Loss: 4233.07861328125, Val_Loss: 4409.44091796875\n",
      "Epoch 7497, Train_Loss: 4232.97216796875, Val_Loss: 4409.30419921875\n",
      "Epoch 7498, Train_Loss: 4232.85009765625, Val_Loss: 4409.1689453125\n",
      "Epoch 7499, Train_Loss: 4232.74755859375, Val_Loss: 4409.0234375\n",
      "Epoch 7500, Train_Loss: 4232.646484375, Val_Loss: 4408.88671875\n",
      "Epoch 7501, Train_Loss: 4232.53515625, Val_Loss: 4408.74755859375\n",
      "Epoch 7502, Train_Loss: 4232.3525390625, Val_Loss: 4408.6240234375\n",
      "Epoch 7503, Train_Loss: 4232.2333984375, Val_Loss: 4408.48974609375\n",
      "Epoch 7504, Train_Loss: 4232.1162109375, Val_Loss: 4408.3427734375\n",
      "Epoch 7505, Train_Loss: 4232.0068359375, Val_Loss: 4408.18994140625\n",
      "Epoch 7506, Train_Loss: 4231.90283203125, Val_Loss: 4408.0673828125\n",
      "Epoch 7507, Train_Loss: 4231.80322265625, Val_Loss: 4407.92578125\n",
      "Epoch 7508, Train_Loss: 4231.6796875, Val_Loss: 4407.81103515625\n",
      "Epoch 7509, Train_Loss: 4231.58984375, Val_Loss: 4407.66943359375\n",
      "Epoch 7510, Train_Loss: 4231.48388671875, Val_Loss: 4407.53857421875\n",
      "Epoch 7511, Train_Loss: 4231.36962890625, Val_Loss: 4407.4033203125\n",
      "Epoch 7512, Train_Loss: 4231.2626953125, Val_Loss: 4407.25390625\n",
      "Epoch 7513, Train_Loss: 4231.1318359375, Val_Loss: 4407.1201171875\n",
      "Epoch 7514, Train_Loss: 4231.0126953125, Val_Loss: 4406.94775390625\n",
      "Epoch 7515, Train_Loss: 4230.87646484375, Val_Loss: 4406.822265625\n",
      "Epoch 7516, Train_Loss: 4230.7705078125, Val_Loss: 4406.701171875\n",
      "Epoch 7517, Train_Loss: 4230.6552734375, Val_Loss: 4406.556640625\n",
      "Epoch 7518, Train_Loss: 4230.53662109375, Val_Loss: 4406.40625\n",
      "Epoch 7519, Train_Loss: 4230.4208984375, Val_Loss: 4406.28271484375\n",
      "Epoch 7520, Train_Loss: 4230.3125, Val_Loss: 4406.14453125\n",
      "Epoch 7521, Train_Loss: 4230.22314453125, Val_Loss: 4406.01611328125\n",
      "Epoch 7522, Train_Loss: 4230.1142578125, Val_Loss: 4405.88818359375\n",
      "Epoch 7523, Train_Loss: 4229.96435546875, Val_Loss: 4405.7509765625\n",
      "Epoch 7524, Train_Loss: 4229.85595703125, Val_Loss: 4405.62548828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7525, Train_Loss: 4229.75244140625, Val_Loss: 4405.4736328125\n",
      "Epoch 7526, Train_Loss: 4229.64111328125, Val_Loss: 4405.33544921875\n",
      "Epoch 7527, Train_Loss: 4229.51953125, Val_Loss: 4405.208984375\n",
      "Epoch 7528, Train_Loss: 4229.3896484375, Val_Loss: 4405.08154296875\n",
      "Epoch 7529, Train_Loss: 4229.29150390625, Val_Loss: 4404.9443359375\n",
      "Epoch 7530, Train_Loss: 4229.185546875, Val_Loss: 4404.81884765625\n",
      "Epoch 7531, Train_Loss: 4229.08935546875, Val_Loss: 4404.6728515625\n",
      "Epoch 7532, Train_Loss: 4228.85693359375, Val_Loss: 4404.52880859375\n",
      "Epoch 7533, Train_Loss: 4228.7509765625, Val_Loss: 4404.39697265625\n",
      "Epoch 7534, Train_Loss: 4228.65234375, Val_Loss: 4404.267578125\n",
      "Epoch 7535, Train_Loss: 4228.56689453125, Val_Loss: 4404.1484375\n",
      "Epoch 7536, Train_Loss: 4228.45458984375, Val_Loss: 4404.0029296875\n",
      "Epoch 7537, Train_Loss: 4228.34912109375, Val_Loss: 4403.8759765625\n",
      "Epoch 7538, Train_Loss: 4228.248046875, Val_Loss: 4403.72412109375\n",
      "Epoch 7539, Train_Loss: 4228.12548828125, Val_Loss: 4403.59375\n",
      "Epoch 7540, Train_Loss: 4228.0166015625, Val_Loss: 4403.47216796875\n",
      "Epoch 7541, Train_Loss: 4227.88623046875, Val_Loss: 4403.33056640625\n",
      "Epoch 7542, Train_Loss: 4227.76171875, Val_Loss: 4403.20263671875\n",
      "Epoch 7543, Train_Loss: 4227.6591796875, Val_Loss: 4403.06494140625\n",
      "Epoch 7544, Train_Loss: 4227.59716796875, Val_Loss: 4402.908203125\n",
      "Epoch 7545, Train_Loss: 4227.48828125, Val_Loss: 4402.78173828125\n",
      "Epoch 7546, Train_Loss: 4227.39453125, Val_Loss: 4402.66552734375\n",
      "Epoch 7547, Train_Loss: 4227.2734375, Val_Loss: 4402.5166015625\n",
      "Epoch 7548, Train_Loss: 4227.16748046875, Val_Loss: 4402.39794921875\n",
      "Epoch 7549, Train_Loss: 4227.0576171875, Val_Loss: 4402.26220703125\n",
      "Epoch 7550, Train_Loss: 4226.908203125, Val_Loss: 4402.10400390625\n",
      "Epoch 7551, Train_Loss: 4226.7880859375, Val_Loss: 4401.95458984375\n",
      "Epoch 7552, Train_Loss: 4226.67236328125, Val_Loss: 4401.81640625\n",
      "Epoch 7553, Train_Loss: 4226.5673828125, Val_Loss: 4401.6845703125\n",
      "Epoch 7554, Train_Loss: 4226.39306640625, Val_Loss: 4401.5615234375\n",
      "Epoch 7555, Train_Loss: 4226.291015625, Val_Loss: 4401.43798828125\n",
      "Epoch 7556, Train_Loss: 4226.19677734375, Val_Loss: 4401.30419921875\n",
      "Epoch 7557, Train_Loss: 4226.08251953125, Val_Loss: 4401.16357421875\n",
      "Epoch 7558, Train_Loss: 4225.97802734375, Val_Loss: 4401.0185546875\n",
      "Epoch 7559, Train_Loss: 4225.87353515625, Val_Loss: 4400.87939453125\n",
      "Epoch 7560, Train_Loss: 4225.76220703125, Val_Loss: 4400.74853515625\n",
      "Epoch 7561, Train_Loss: 4225.6513671875, Val_Loss: 4400.6318359375\n",
      "Epoch 7562, Train_Loss: 4225.52001953125, Val_Loss: 4400.5078125\n",
      "Epoch 7563, Train_Loss: 4225.41455078125, Val_Loss: 4400.3720703125\n",
      "Epoch 7564, Train_Loss: 4225.3173828125, Val_Loss: 4400.22802734375\n",
      "Epoch 7565, Train_Loss: 4225.216796875, Val_Loss: 4400.0927734375\n",
      "Epoch 7566, Train_Loss: 4225.115234375, Val_Loss: 4399.96337890625\n",
      "Epoch 7567, Train_Loss: 4224.99658203125, Val_Loss: 4399.83447265625\n",
      "Epoch 7568, Train_Loss: 4224.85546875, Val_Loss: 4399.70458984375\n",
      "Epoch 7569, Train_Loss: 4224.7451171875, Val_Loss: 4399.56298828125\n",
      "Epoch 7570, Train_Loss: 4224.63525390625, Val_Loss: 4399.4345703125\n",
      "Epoch 7571, Train_Loss: 4224.53759765625, Val_Loss: 4399.2880859375\n",
      "Epoch 7572, Train_Loss: 4224.42041015625, Val_Loss: 4399.1630859375\n",
      "Epoch 7573, Train_Loss: 4224.33349609375, Val_Loss: 4399.02978515625\n",
      "Epoch 7574, Train_Loss: 4224.2392578125, Val_Loss: 4398.90625\n",
      "Epoch 7575, Train_Loss: 4224.1376953125, Val_Loss: 4398.7822265625\n",
      "Epoch 7576, Train_Loss: 4224.021484375, Val_Loss: 4398.63671875\n",
      "Epoch 7577, Train_Loss: 4223.90234375, Val_Loss: 4398.5068359375\n",
      "Epoch 7578, Train_Loss: 4223.79638671875, Val_Loss: 4398.3544921875\n",
      "Epoch 7579, Train_Loss: 4223.685546875, Val_Loss: 4398.2216796875\n",
      "Epoch 7580, Train_Loss: 4223.57470703125, Val_Loss: 4398.0986328125\n",
      "Epoch 7581, Train_Loss: 4223.45361328125, Val_Loss: 4397.974609375\n",
      "Epoch 7582, Train_Loss: 4223.33837890625, Val_Loss: 4397.8427734375\n",
      "Epoch 7583, Train_Loss: 4223.23486328125, Val_Loss: 4397.7177734375\n",
      "Epoch 7584, Train_Loss: 4223.14208984375, Val_Loss: 4397.5634765625\n",
      "Epoch 7585, Train_Loss: 4223.0400390625, Val_Loss: 4397.431640625\n",
      "Epoch 7586, Train_Loss: 4222.9462890625, Val_Loss: 4397.3037109375\n",
      "Epoch 7587, Train_Loss: 4222.81884765625, Val_Loss: 4397.17578125\n",
      "Epoch 7588, Train_Loss: 4222.7197265625, Val_Loss: 4397.01953125\n",
      "Epoch 7589, Train_Loss: 4222.625, Val_Loss: 4396.88623046875\n",
      "Epoch 7590, Train_Loss: 4222.51904296875, Val_Loss: 4396.75341796875\n",
      "Epoch 7591, Train_Loss: 4222.4189453125, Val_Loss: 4396.61181640625\n",
      "Epoch 7592, Train_Loss: 4222.291015625, Val_Loss: 4396.47900390625\n",
      "Epoch 7593, Train_Loss: 4222.1796875, Val_Loss: 4396.3603515625\n",
      "Epoch 7594, Train_Loss: 4222.03515625, Val_Loss: 4396.23095703125\n",
      "Epoch 7595, Train_Loss: 4221.9375, Val_Loss: 4396.0927734375\n",
      "Epoch 7596, Train_Loss: 4221.830078125, Val_Loss: 4395.96240234375\n",
      "Epoch 7597, Train_Loss: 4221.673828125, Val_Loss: 4395.81884765625\n",
      "Epoch 7598, Train_Loss: 4221.564453125, Val_Loss: 4395.685546875\n",
      "Epoch 7599, Train_Loss: 4221.46142578125, Val_Loss: 4395.56005859375\n",
      "Epoch 7600, Train_Loss: 4221.353515625, Val_Loss: 4395.43115234375\n",
      "Epoch 7601, Train_Loss: 4221.2490234375, Val_Loss: 4395.30712890625\n",
      "Epoch 7602, Train_Loss: 4221.12353515625, Val_Loss: 4395.16650390625\n",
      "Epoch 7603, Train_Loss: 4221.0361328125, Val_Loss: 4395.04736328125\n",
      "Epoch 7604, Train_Loss: 4220.9296875, Val_Loss: 4394.9130859375\n",
      "Epoch 7605, Train_Loss: 4220.818359375, Val_Loss: 4394.77587890625\n",
      "Epoch 7606, Train_Loss: 4220.66455078125, Val_Loss: 4394.64013671875\n",
      "Epoch 7607, Train_Loss: 4220.49560546875, Val_Loss: 4394.52099609375\n",
      "Epoch 7608, Train_Loss: 4220.388671875, Val_Loss: 4394.40576171875\n",
      "Epoch 7609, Train_Loss: 4220.30419921875, Val_Loss: 4394.2529296875\n",
      "Epoch 7610, Train_Loss: 4220.19970703125, Val_Loss: 4394.12060546875\n",
      "Epoch 7611, Train_Loss: 4220.09375, Val_Loss: 4393.98681640625\n",
      "Epoch 7612, Train_Loss: 4219.99609375, Val_Loss: 4393.857421875\n",
      "Epoch 7613, Train_Loss: 4219.87939453125, Val_Loss: 4393.7353515625\n",
      "Epoch 7614, Train_Loss: 4219.787109375, Val_Loss: 4393.6083984375\n",
      "Epoch 7615, Train_Loss: 4219.6904296875, Val_Loss: 4393.48291015625\n",
      "Epoch 7616, Train_Loss: 4219.57470703125, Val_Loss: 4393.333984375\n",
      "Epoch 7617, Train_Loss: 4219.4716796875, Val_Loss: 4393.2001953125\n",
      "Epoch 7618, Train_Loss: 4219.3564453125, Val_Loss: 4393.06591796875\n",
      "Epoch 7619, Train_Loss: 4219.25830078125, Val_Loss: 4392.9384765625\n",
      "Epoch 7620, Train_Loss: 4219.1142578125, Val_Loss: 4392.8232421875\n",
      "Epoch 7621, Train_Loss: 4218.998046875, Val_Loss: 4392.68798828125\n",
      "Epoch 7622, Train_Loss: 4218.86474609375, Val_Loss: 4392.55859375\n",
      "Epoch 7623, Train_Loss: 4218.7548828125, Val_Loss: 4392.4287109375\n",
      "Epoch 7624, Train_Loss: 4218.654296875, Val_Loss: 4392.2705078125\n",
      "Epoch 7625, Train_Loss: 4218.5390625, Val_Loss: 4392.12109375\n",
      "Epoch 7626, Train_Loss: 4218.4404296875, Val_Loss: 4391.9873046875\n",
      "Epoch 7627, Train_Loss: 4218.34375, Val_Loss: 4391.86181640625\n",
      "Epoch 7628, Train_Loss: 4218.2431640625, Val_Loss: 4391.74462890625\n",
      "Epoch 7629, Train_Loss: 4218.15283203125, Val_Loss: 4391.61181640625\n",
      "Epoch 7630, Train_Loss: 4218.05126953125, Val_Loss: 4391.48095703125\n",
      "Epoch 7631, Train_Loss: 4217.94873046875, Val_Loss: 4391.34765625\n",
      "Epoch 7632, Train_Loss: 4217.81201171875, Val_Loss: 4391.21923828125\n",
      "Epoch 7633, Train_Loss: 4217.7119140625, Val_Loss: 4391.09423828125\n",
      "Epoch 7634, Train_Loss: 4217.58935546875, Val_Loss: 4390.9697265625\n",
      "Epoch 7635, Train_Loss: 4217.4892578125, Val_Loss: 4390.8310546875\n",
      "Epoch 7636, Train_Loss: 4217.3701171875, Val_Loss: 4390.6953125\n",
      "Epoch 7637, Train_Loss: 4217.26708984375, Val_Loss: 4390.56005859375\n",
      "Epoch 7638, Train_Loss: 4217.17236328125, Val_Loss: 4390.4287109375\n",
      "Epoch 7639, Train_Loss: 4217.07177734375, Val_Loss: 4390.306640625\n",
      "Epoch 7640, Train_Loss: 4216.9619140625, Val_Loss: 4390.1708984375\n",
      "Epoch 7641, Train_Loss: 4216.85791015625, Val_Loss: 4390.04931640625\n",
      "Epoch 7642, Train_Loss: 4216.73681640625, Val_Loss: 4389.9248046875\n",
      "Epoch 7643, Train_Loss: 4216.64501953125, Val_Loss: 4389.787109375\n",
      "Epoch 7644, Train_Loss: 4216.533203125, Val_Loss: 4389.6396484375\n",
      "Epoch 7645, Train_Loss: 4216.4375, Val_Loss: 4389.513671875\n",
      "Epoch 7646, Train_Loss: 4216.32177734375, Val_Loss: 4389.39306640625\n",
      "Epoch 7647, Train_Loss: 4216.18896484375, Val_Loss: 4389.275390625\n",
      "Epoch 7648, Train_Loss: 4216.0849609375, Val_Loss: 4389.14599609375\n",
      "Epoch 7649, Train_Loss: 4215.99365234375, Val_Loss: 4389.015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7650, Train_Loss: 4215.93115234375, Val_Loss: 4388.875\n",
      "Epoch 7651, Train_Loss: 4215.828125, Val_Loss: 4388.7421875\n",
      "Epoch 7652, Train_Loss: 4215.705078125, Val_Loss: 4388.6201171875\n",
      "Epoch 7653, Train_Loss: 4215.58984375, Val_Loss: 4388.4755859375\n",
      "Epoch 7654, Train_Loss: 4215.48388671875, Val_Loss: 4388.349609375\n",
      "Epoch 7655, Train_Loss: 4215.3916015625, Val_Loss: 4388.2177734375\n",
      "Epoch 7656, Train_Loss: 4215.283203125, Val_Loss: 4388.0888671875\n",
      "Epoch 7657, Train_Loss: 4215.171875, Val_Loss: 4387.953125\n",
      "Epoch 7658, Train_Loss: 4215.0732421875, Val_Loss: 4387.8291015625\n",
      "Epoch 7659, Train_Loss: 4214.97119140625, Val_Loss: 4387.7001953125\n",
      "Epoch 7660, Train_Loss: 4214.810546875, Val_Loss: 4387.56982421875\n",
      "Epoch 7661, Train_Loss: 4214.7080078125, Val_Loss: 4387.4462890625\n",
      "Epoch 7662, Train_Loss: 4214.5810546875, Val_Loss: 4387.29052734375\n",
      "Epoch 7663, Train_Loss: 4214.4736328125, Val_Loss: 4387.16162109375\n",
      "Epoch 7664, Train_Loss: 4214.3759765625, Val_Loss: 4387.02099609375\n",
      "Epoch 7665, Train_Loss: 4214.28564453125, Val_Loss: 4386.892578125\n",
      "Epoch 7666, Train_Loss: 4214.17041015625, Val_Loss: 4386.765625\n",
      "Epoch 7667, Train_Loss: 4214.06201171875, Val_Loss: 4386.64404296875\n",
      "Epoch 7668, Train_Loss: 4213.9716796875, Val_Loss: 4386.51513671875\n",
      "Epoch 7669, Train_Loss: 4213.88330078125, Val_Loss: 4386.3916015625\n",
      "Epoch 7670, Train_Loss: 4213.787109375, Val_Loss: 4386.25634765625\n",
      "Epoch 7671, Train_Loss: 4213.68994140625, Val_Loss: 4386.11669921875\n",
      "Epoch 7672, Train_Loss: 4213.55712890625, Val_Loss: 4385.9951171875\n",
      "Epoch 7673, Train_Loss: 4213.44287109375, Val_Loss: 4385.85400390625\n",
      "Epoch 7674, Train_Loss: 4213.2939453125, Val_Loss: 4385.74853515625\n",
      "Epoch 7675, Train_Loss: 4213.193359375, Val_Loss: 4385.62109375\n",
      "Epoch 7676, Train_Loss: 4213.08837890625, Val_Loss: 4385.4921875\n",
      "Epoch 7677, Train_Loss: 4212.9990234375, Val_Loss: 4385.345703125\n",
      "Epoch 7678, Train_Loss: 4212.88525390625, Val_Loss: 4385.2041015625\n",
      "Epoch 7679, Train_Loss: 4212.79052734375, Val_Loss: 4385.09912109375\n",
      "Epoch 7680, Train_Loss: 4212.7060546875, Val_Loss: 4384.98095703125\n",
      "Epoch 7681, Train_Loss: 4212.44189453125, Val_Loss: 4384.83740234375\n",
      "Epoch 7682, Train_Loss: 4212.34033203125, Val_Loss: 4384.71044921875\n",
      "Epoch 7683, Train_Loss: 4212.2177734375, Val_Loss: 4384.578125\n",
      "Epoch 7684, Train_Loss: 4212.11474609375, Val_Loss: 4384.44677734375\n",
      "Epoch 7685, Train_Loss: 4212.01806640625, Val_Loss: 4384.31982421875\n",
      "Epoch 7686, Train_Loss: 4211.9091796875, Val_Loss: 4384.19970703125\n",
      "Epoch 7687, Train_Loss: 4211.798828125, Val_Loss: 4384.08056640625\n",
      "Epoch 7688, Train_Loss: 4211.6865234375, Val_Loss: 4383.93896484375\n",
      "Epoch 7689, Train_Loss: 4211.58154296875, Val_Loss: 4383.8154296875\n",
      "Epoch 7690, Train_Loss: 4211.49169921875, Val_Loss: 4383.67529296875\n",
      "Epoch 7691, Train_Loss: 4211.37841796875, Val_Loss: 4383.53076171875\n",
      "Epoch 7692, Train_Loss: 4211.2783203125, Val_Loss: 4383.41259765625\n",
      "Epoch 7693, Train_Loss: 4211.16650390625, Val_Loss: 4383.29443359375\n",
      "Epoch 7694, Train_Loss: 4211.0537109375, Val_Loss: 4383.16796875\n",
      "Epoch 7695, Train_Loss: 4210.97265625, Val_Loss: 4383.03759765625\n",
      "Epoch 7696, Train_Loss: 4210.87255859375, Val_Loss: 4382.908203125\n",
      "Epoch 7697, Train_Loss: 4210.77490234375, Val_Loss: 4382.775390625\n",
      "Epoch 7698, Train_Loss: 4210.658203125, Val_Loss: 4382.640625\n",
      "Epoch 7699, Train_Loss: 4210.556640625, Val_Loss: 4382.50732421875\n",
      "Epoch 7700, Train_Loss: 4210.3935546875, Val_Loss: 4382.361328125\n",
      "Epoch 7701, Train_Loss: 4210.29052734375, Val_Loss: 4382.24365234375\n",
      "Epoch 7702, Train_Loss: 4210.2041015625, Val_Loss: 4382.10693359375\n",
      "Epoch 7703, Train_Loss: 4210.12646484375, Val_Loss: 4381.96337890625\n",
      "Epoch 7704, Train_Loss: 4210.01708984375, Val_Loss: 4381.8427734375\n",
      "Epoch 7705, Train_Loss: 4209.90380859375, Val_Loss: 4381.708984375\n",
      "Epoch 7706, Train_Loss: 4209.8095703125, Val_Loss: 4381.576171875\n",
      "Epoch 7707, Train_Loss: 4209.70361328125, Val_Loss: 4381.46728515625\n",
      "Epoch 7708, Train_Loss: 4209.59375, Val_Loss: 4381.33984375\n",
      "Epoch 7709, Train_Loss: 4209.49462890625, Val_Loss: 4381.2158203125\n",
      "Epoch 7710, Train_Loss: 4209.39599609375, Val_Loss: 4381.0888671875\n",
      "Epoch 7711, Train_Loss: 4209.29052734375, Val_Loss: 4380.9453125\n",
      "Epoch 7712, Train_Loss: 4209.20263671875, Val_Loss: 4380.81689453125\n",
      "Epoch 7713, Train_Loss: 4209.03466796875, Val_Loss: 4380.697265625\n",
      "Epoch 7714, Train_Loss: 4208.9287109375, Val_Loss: 4380.568359375\n",
      "Epoch 7715, Train_Loss: 4208.837890625, Val_Loss: 4380.4501953125\n",
      "Epoch 7716, Train_Loss: 4208.7314453125, Val_Loss: 4380.32373046875\n",
      "Epoch 7717, Train_Loss: 4208.63037109375, Val_Loss: 4380.17529296875\n",
      "Epoch 7718, Train_Loss: 4208.51611328125, Val_Loss: 4380.05322265625\n",
      "Epoch 7719, Train_Loss: 4208.40771484375, Val_Loss: 4379.9228515625\n",
      "Epoch 7720, Train_Loss: 4208.2919921875, Val_Loss: 4379.79541015625\n",
      "Epoch 7721, Train_Loss: 4208.20751953125, Val_Loss: 4379.673828125\n",
      "Epoch 7722, Train_Loss: 4208.11328125, Val_Loss: 4379.54736328125\n",
      "Epoch 7723, Train_Loss: 4207.98828125, Val_Loss: 4379.4150390625\n",
      "Epoch 7724, Train_Loss: 4207.8935546875, Val_Loss: 4379.275390625\n",
      "Epoch 7725, Train_Loss: 4207.8017578125, Val_Loss: 4379.15966796875\n",
      "Epoch 7726, Train_Loss: 4207.70166015625, Val_Loss: 4379.02587890625\n",
      "Epoch 7727, Train_Loss: 4207.5703125, Val_Loss: 4378.91162109375\n",
      "Epoch 7728, Train_Loss: 4207.46240234375, Val_Loss: 4378.77099609375\n",
      "Epoch 7729, Train_Loss: 4207.3642578125, Val_Loss: 4378.6474609375\n",
      "Epoch 7730, Train_Loss: 4207.2568359375, Val_Loss: 4378.505859375\n",
      "Epoch 7731, Train_Loss: 4207.158203125, Val_Loss: 4378.37451171875\n",
      "Epoch 7732, Train_Loss: 4207.05517578125, Val_Loss: 4378.244140625\n",
      "Epoch 7733, Train_Loss: 4206.93896484375, Val_Loss: 4378.13427734375\n",
      "Epoch 7734, Train_Loss: 4206.84521484375, Val_Loss: 4378.00830078125\n",
      "Epoch 7735, Train_Loss: 4206.74658203125, Val_Loss: 4377.88134765625\n",
      "Epoch 7736, Train_Loss: 4206.65869140625, Val_Loss: 4377.75\n",
      "Epoch 7737, Train_Loss: 4206.54638671875, Val_Loss: 4377.57177734375\n",
      "Epoch 7738, Train_Loss: 4206.4267578125, Val_Loss: 4377.45263671875\n",
      "Epoch 7739, Train_Loss: 4206.3173828125, Val_Loss: 4377.31396484375\n",
      "Epoch 7740, Train_Loss: 4206.18701171875, Val_Loss: 4377.2099609375\n",
      "Epoch 7741, Train_Loss: 4206.0966796875, Val_Loss: 4377.07080078125\n",
      "Epoch 7742, Train_Loss: 4206.00927734375, Val_Loss: 4376.9453125\n",
      "Epoch 7743, Train_Loss: 4205.90625, Val_Loss: 4376.8154296875\n",
      "Epoch 7744, Train_Loss: 4205.79296875, Val_Loss: 4376.6845703125\n",
      "Epoch 7745, Train_Loss: 4205.68994140625, Val_Loss: 4376.5556640625\n",
      "Epoch 7746, Train_Loss: 4205.58984375, Val_Loss: 4376.43798828125\n",
      "Epoch 7747, Train_Loss: 4205.45166015625, Val_Loss: 4376.31396484375\n",
      "Epoch 7748, Train_Loss: 4205.3466796875, Val_Loss: 4376.19580078125\n",
      "Epoch 7749, Train_Loss: 4205.2470703125, Val_Loss: 4376.064453125\n",
      "Epoch 7750, Train_Loss: 4205.150390625, Val_Loss: 4375.93505859375\n",
      "Epoch 7751, Train_Loss: 4205.05712890625, Val_Loss: 4375.8017578125\n",
      "Epoch 7752, Train_Loss: 4204.95703125, Val_Loss: 4375.6748046875\n",
      "Epoch 7753, Train_Loss: 4204.82275390625, Val_Loss: 4375.5478515625\n",
      "Epoch 7754, Train_Loss: 4204.72412109375, Val_Loss: 4375.42822265625\n",
      "Epoch 7755, Train_Loss: 4204.62548828125, Val_Loss: 4375.29541015625\n",
      "Epoch 7756, Train_Loss: 4204.4658203125, Val_Loss: 4375.15576171875\n",
      "Epoch 7757, Train_Loss: 4204.41015625, Val_Loss: 4375.02294921875\n",
      "Epoch 7758, Train_Loss: 4204.302734375, Val_Loss: 4374.89990234375\n",
      "Epoch 7759, Train_Loss: 4204.1943359375, Val_Loss: 4374.7734375\n",
      "Epoch 7760, Train_Loss: 4204.0908203125, Val_Loss: 4374.66162109375\n",
      "Epoch 7761, Train_Loss: 4203.99365234375, Val_Loss: 4374.53515625\n",
      "Epoch 7762, Train_Loss: 4203.8896484375, Val_Loss: 4374.39794921875\n",
      "Epoch 7763, Train_Loss: 4203.7841796875, Val_Loss: 4374.2705078125\n",
      "Epoch 7764, Train_Loss: 4203.6923828125, Val_Loss: 4374.13818359375\n",
      "Epoch 7765, Train_Loss: 4203.5830078125, Val_Loss: 4374.009765625\n",
      "Epoch 7766, Train_Loss: 4203.47607421875, Val_Loss: 4373.87060546875\n",
      "Epoch 7767, Train_Loss: 4203.31201171875, Val_Loss: 4373.75390625\n",
      "Epoch 7768, Train_Loss: 4203.20654296875, Val_Loss: 4373.6337890625\n",
      "Epoch 7769, Train_Loss: 4203.1083984375, Val_Loss: 4373.50244140625\n",
      "Epoch 7770, Train_Loss: 4203.01416015625, Val_Loss: 4373.369140625\n",
      "Epoch 7771, Train_Loss: 4202.9208984375, Val_Loss: 4373.244140625\n",
      "Epoch 7772, Train_Loss: 4202.81591796875, Val_Loss: 4373.107421875\n",
      "Epoch 7773, Train_Loss: 4202.71337890625, Val_Loss: 4372.97998046875\n",
      "Epoch 7774, Train_Loss: 4202.61962890625, Val_Loss: 4372.8720703125\n",
      "Epoch 7775, Train_Loss: 4202.509765625, Val_Loss: 4372.71142578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7776, Train_Loss: 4202.41943359375, Val_Loss: 4372.58984375\n",
      "Epoch 7777, Train_Loss: 4202.31591796875, Val_Loss: 4372.44580078125\n",
      "Epoch 7778, Train_Loss: 4202.201171875, Val_Loss: 4372.32666015625\n",
      "Epoch 7779, Train_Loss: 4202.0947265625, Val_Loss: 4372.19091796875\n",
      "Epoch 7780, Train_Loss: 4201.966796875, Val_Loss: 4372.07373046875\n",
      "Epoch 7781, Train_Loss: 4201.87890625, Val_Loss: 4371.953125\n",
      "Epoch 7782, Train_Loss: 4201.77001953125, Val_Loss: 4371.82373046875\n",
      "Epoch 7783, Train_Loss: 4201.6494140625, Val_Loss: 4371.69580078125\n",
      "Epoch 7784, Train_Loss: 4201.55908203125, Val_Loss: 4371.55224609375\n",
      "Epoch 7785, Train_Loss: 4201.46337890625, Val_Loss: 4371.42724609375\n",
      "Epoch 7786, Train_Loss: 4201.3603515625, Val_Loss: 4371.31298828125\n",
      "Epoch 7787, Train_Loss: 4201.263671875, Val_Loss: 4371.1953125\n",
      "Epoch 7788, Train_Loss: 4201.16796875, Val_Loss: 4371.072265625\n",
      "Epoch 7789, Train_Loss: 4201.06201171875, Val_Loss: 4370.947265625\n",
      "Epoch 7790, Train_Loss: 4200.97119140625, Val_Loss: 4370.80712890625\n",
      "Epoch 7791, Train_Loss: 4200.86376953125, Val_Loss: 4370.67822265625\n",
      "Epoch 7792, Train_Loss: 4200.7607421875, Val_Loss: 4370.5625\n",
      "Epoch 7793, Train_Loss: 4200.66259765625, Val_Loss: 4370.427734375\n",
      "Epoch 7794, Train_Loss: 4200.52587890625, Val_Loss: 4370.30419921875\n",
      "Epoch 7795, Train_Loss: 4200.43701171875, Val_Loss: 4370.1865234375\n",
      "Epoch 7796, Train_Loss: 4200.33544921875, Val_Loss: 4370.04345703125\n",
      "Epoch 7797, Train_Loss: 4200.24755859375, Val_Loss: 4369.91455078125\n",
      "Epoch 7798, Train_Loss: 4200.1455078125, Val_Loss: 4369.79296875\n",
      "Epoch 7799, Train_Loss: 4200.04541015625, Val_Loss: 4369.6669921875\n",
      "Epoch 7800, Train_Loss: 4199.9443359375, Val_Loss: 4369.546875\n",
      "Epoch 7801, Train_Loss: 4199.84814453125, Val_Loss: 4369.43359375\n",
      "Epoch 7802, Train_Loss: 4199.7451171875, Val_Loss: 4369.3046875\n",
      "Epoch 7803, Train_Loss: 4199.634765625, Val_Loss: 4369.15576171875\n",
      "Epoch 7804, Train_Loss: 4199.50390625, Val_Loss: 4369.025390625\n",
      "Epoch 7805, Train_Loss: 4199.40478515625, Val_Loss: 4368.90478515625\n",
      "Epoch 7806, Train_Loss: 4199.3056640625, Val_Loss: 4368.7734375\n",
      "Epoch 7807, Train_Loss: 4199.19091796875, Val_Loss: 4368.65625\n",
      "Epoch 7808, Train_Loss: 4199.091796875, Val_Loss: 4368.5302734375\n",
      "Epoch 7809, Train_Loss: 4198.99462890625, Val_Loss: 4368.40380859375\n",
      "Epoch 7810, Train_Loss: 4198.94189453125, Val_Loss: 4368.26416015625\n",
      "Epoch 7811, Train_Loss: 4198.8408203125, Val_Loss: 4368.1474609375\n",
      "Epoch 7812, Train_Loss: 4198.751953125, Val_Loss: 4368.0234375\n",
      "Epoch 7813, Train_Loss: 4198.63525390625, Val_Loss: 4367.86767578125\n",
      "Epoch 7814, Train_Loss: 4198.51904296875, Val_Loss: 4367.7392578125\n",
      "Epoch 7815, Train_Loss: 4198.4287109375, Val_Loss: 4367.609375\n",
      "Epoch 7816, Train_Loss: 4198.33056640625, Val_Loss: 4367.48876953125\n",
      "Epoch 7817, Train_Loss: 4198.23291015625, Val_Loss: 4367.35693359375\n",
      "Epoch 7818, Train_Loss: 4198.130859375, Val_Loss: 4367.22802734375\n",
      "Epoch 7819, Train_Loss: 4198.0322265625, Val_Loss: 4367.111328125\n",
      "Epoch 7820, Train_Loss: 4197.8798828125, Val_Loss: 4366.9853515625\n",
      "Epoch 7821, Train_Loss: 4197.7802734375, Val_Loss: 4366.86328125\n",
      "Epoch 7822, Train_Loss: 4197.68017578125, Val_Loss: 4366.74169921875\n",
      "Epoch 7823, Train_Loss: 4197.583984375, Val_Loss: 4366.61328125\n",
      "Epoch 7824, Train_Loss: 4197.46044921875, Val_Loss: 4366.474609375\n",
      "Epoch 7825, Train_Loss: 4197.35693359375, Val_Loss: 4366.3525390625\n",
      "Epoch 7826, Train_Loss: 4197.25830078125, Val_Loss: 4366.248046875\n",
      "Epoch 7827, Train_Loss: 4197.1630859375, Val_Loss: 4366.1318359375\n",
      "Epoch 7828, Train_Loss: 4197.06396484375, Val_Loss: 4365.990234375\n",
      "Epoch 7829, Train_Loss: 4196.96435546875, Val_Loss: 4365.85498046875\n",
      "Epoch 7830, Train_Loss: 4196.86669921875, Val_Loss: 4365.7275390625\n",
      "Epoch 7831, Train_Loss: 4196.7705078125, Val_Loss: 4365.59912109375\n",
      "Epoch 7832, Train_Loss: 4196.6181640625, Val_Loss: 4365.4736328125\n",
      "Epoch 7833, Train_Loss: 4196.52685546875, Val_Loss: 4365.35205078125\n",
      "Epoch 7834, Train_Loss: 4196.38134765625, Val_Loss: 4365.24072265625\n",
      "Epoch 7835, Train_Loss: 4196.26953125, Val_Loss: 4365.107421875\n",
      "Epoch 7836, Train_Loss: 4196.1826171875, Val_Loss: 4364.974609375\n",
      "Epoch 7837, Train_Loss: 4196.076171875, Val_Loss: 4364.85205078125\n",
      "Epoch 7838, Train_Loss: 4195.970703125, Val_Loss: 4364.72119140625\n",
      "Epoch 7839, Train_Loss: 4195.86865234375, Val_Loss: 4364.6005859375\n",
      "Epoch 7840, Train_Loss: 4195.7666015625, Val_Loss: 4364.48095703125\n",
      "Epoch 7841, Train_Loss: 4195.64453125, Val_Loss: 4364.3427734375\n",
      "Epoch 7842, Train_Loss: 4195.5478515625, Val_Loss: 4364.22021484375\n",
      "Epoch 7843, Train_Loss: 4195.46142578125, Val_Loss: 4364.099609375\n",
      "Epoch 7844, Train_Loss: 4195.34228515625, Val_Loss: 4363.95654296875\n",
      "Epoch 7845, Train_Loss: 4195.248046875, Val_Loss: 4363.83740234375\n",
      "Epoch 7846, Train_Loss: 4195.1572265625, Val_Loss: 4363.70751953125\n",
      "Epoch 7847, Train_Loss: 4195.0390625, Val_Loss: 4363.59765625\n",
      "Epoch 7848, Train_Loss: 4194.94482421875, Val_Loss: 4363.474609375\n",
      "Epoch 7849, Train_Loss: 4194.8466796875, Val_Loss: 4363.3466796875\n",
      "Epoch 7850, Train_Loss: 4194.751953125, Val_Loss: 4363.2275390625\n",
      "Epoch 7851, Train_Loss: 4194.6435546875, Val_Loss: 4363.05908203125\n",
      "Epoch 7852, Train_Loss: 4194.53515625, Val_Loss: 4362.93505859375\n",
      "Epoch 7853, Train_Loss: 4194.43603515625, Val_Loss: 4362.81103515625\n",
      "Epoch 7854, Train_Loss: 4194.3271484375, Val_Loss: 4362.6904296875\n",
      "Epoch 7855, Train_Loss: 4194.232421875, Val_Loss: 4362.5634765625\n",
      "Epoch 7856, Train_Loss: 4194.1328125, Val_Loss: 4362.443359375\n",
      "Epoch 7857, Train_Loss: 4194.03369140625, Val_Loss: 4362.30810546875\n",
      "Epoch 7858, Train_Loss: 4193.931640625, Val_Loss: 4362.1796875\n",
      "Epoch 7859, Train_Loss: 4193.83935546875, Val_Loss: 4362.06298828125\n",
      "Epoch 7860, Train_Loss: 4193.732421875, Val_Loss: 4361.93798828125\n",
      "Epoch 7861, Train_Loss: 4193.61962890625, Val_Loss: 4361.81689453125\n",
      "Epoch 7862, Train_Loss: 4193.53369140625, Val_Loss: 4361.69970703125\n",
      "Epoch 7863, Train_Loss: 4193.43359375, Val_Loss: 4361.56298828125\n",
      "Epoch 7864, Train_Loss: 4193.35400390625, Val_Loss: 4361.43310546875\n",
      "Epoch 7865, Train_Loss: 4193.259765625, Val_Loss: 4361.3095703125\n",
      "Epoch 7866, Train_Loss: 4193.16015625, Val_Loss: 4361.1875\n",
      "Epoch 7867, Train_Loss: 4193.0576171875, Val_Loss: 4361.08251953125\n",
      "Epoch 7868, Train_Loss: 4192.958984375, Val_Loss: 4360.94873046875\n",
      "Epoch 7869, Train_Loss: 4192.86181640625, Val_Loss: 4360.8173828125\n",
      "Epoch 7870, Train_Loss: 4192.7490234375, Val_Loss: 4360.69287109375\n",
      "Epoch 7871, Train_Loss: 4192.666015625, Val_Loss: 4360.5478515625\n",
      "Epoch 7872, Train_Loss: 4192.57421875, Val_Loss: 4360.44677734375\n",
      "Epoch 7873, Train_Loss: 4192.46630859375, Val_Loss: 4360.31982421875\n",
      "Epoch 7874, Train_Loss: 4192.3203125, Val_Loss: 4360.201171875\n",
      "Epoch 7875, Train_Loss: 4192.2119140625, Val_Loss: 4360.07470703125\n",
      "Epoch 7876, Train_Loss: 4192.10400390625, Val_Loss: 4359.94091796875\n",
      "Epoch 7877, Train_Loss: 4192.015625, Val_Loss: 4359.81298828125\n",
      "Epoch 7878, Train_Loss: 4191.9150390625, Val_Loss: 4359.69287109375\n",
      "Epoch 7879, Train_Loss: 4191.8173828125, Val_Loss: 4359.55908203125\n",
      "Epoch 7880, Train_Loss: 4191.72119140625, Val_Loss: 4359.43310546875\n",
      "Epoch 7881, Train_Loss: 4191.619140625, Val_Loss: 4359.3193359375\n",
      "Epoch 7882, Train_Loss: 4191.521484375, Val_Loss: 4359.19677734375\n",
      "Epoch 7883, Train_Loss: 4191.4365234375, Val_Loss: 4359.06884765625\n",
      "Epoch 7884, Train_Loss: 4191.33935546875, Val_Loss: 4358.94091796875\n",
      "Epoch 7885, Train_Loss: 4191.22265625, Val_Loss: 4358.80859375\n",
      "Epoch 7886, Train_Loss: 4191.1171875, Val_Loss: 4358.68896484375\n",
      "Epoch 7887, Train_Loss: 4190.9814453125, Val_Loss: 4358.576171875\n",
      "Epoch 7888, Train_Loss: 4190.880859375, Val_Loss: 4358.4208984375\n",
      "Epoch 7889, Train_Loss: 4190.78369140625, Val_Loss: 4358.29736328125\n",
      "Epoch 7890, Train_Loss: 4190.685546875, Val_Loss: 4358.1708984375\n",
      "Epoch 7891, Train_Loss: 4190.60546875, Val_Loss: 4358.03857421875\n",
      "Epoch 7892, Train_Loss: 4190.5029296875, Val_Loss: 4357.91552734375\n",
      "Epoch 7893, Train_Loss: 4190.40576171875, Val_Loss: 4357.78759765625\n",
      "Epoch 7894, Train_Loss: 4190.3076171875, Val_Loss: 4357.68359375\n",
      "Epoch 7895, Train_Loss: 4190.19580078125, Val_Loss: 4357.56396484375\n",
      "Epoch 7896, Train_Loss: 4190.09326171875, Val_Loss: 4357.42822265625\n",
      "Epoch 7897, Train_Loss: 4190.01171875, Val_Loss: 4357.29833984375\n",
      "Epoch 7898, Train_Loss: 4189.8271484375, Val_Loss: 4357.18017578125\n",
      "Epoch 7899, Train_Loss: 4189.73193359375, Val_Loss: 4357.05517578125\n",
      "Epoch 7900, Train_Loss: 4189.6396484375, Val_Loss: 4356.9345703125\n",
      "Epoch 7901, Train_Loss: 4189.533203125, Val_Loss: 4356.82275390625\n",
      "Epoch 7902, Train_Loss: 4189.43798828125, Val_Loss: 4356.7021484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7903, Train_Loss: 4189.35888671875, Val_Loss: 4356.5751953125\n",
      "Epoch 7904, Train_Loss: 4189.25927734375, Val_Loss: 4356.4404296875\n",
      "Epoch 7905, Train_Loss: 4189.14599609375, Val_Loss: 4356.322265625\n",
      "Epoch 7906, Train_Loss: 4189.0380859375, Val_Loss: 4356.1943359375\n",
      "Epoch 7907, Train_Loss: 4188.88671875, Val_Loss: 4356.0732421875\n",
      "Epoch 7908, Train_Loss: 4188.791015625, Val_Loss: 4355.9541015625\n",
      "Epoch 7909, Train_Loss: 4188.6962890625, Val_Loss: 4355.82861328125\n",
      "Epoch 7910, Train_Loss: 4188.587890625, Val_Loss: 4355.7177734375\n",
      "Epoch 7911, Train_Loss: 4188.494140625, Val_Loss: 4355.58349609375\n",
      "Epoch 7912, Train_Loss: 4188.38720703125, Val_Loss: 4355.455078125\n",
      "Epoch 7913, Train_Loss: 4188.29296875, Val_Loss: 4355.333984375\n",
      "Epoch 7914, Train_Loss: 4188.1640625, Val_Loss: 4355.22314453125\n",
      "Epoch 7915, Train_Loss: 4188.0615234375, Val_Loss: 4355.1025390625\n",
      "Epoch 7916, Train_Loss: 4187.97021484375, Val_Loss: 4354.98046875\n",
      "Epoch 7917, Train_Loss: 4187.873046875, Val_Loss: 4354.8427734375\n",
      "Epoch 7918, Train_Loss: 4187.8271484375, Val_Loss: 4354.7109375\n",
      "Epoch 7919, Train_Loss: 4187.720703125, Val_Loss: 4354.5849609375\n",
      "Epoch 7920, Train_Loss: 4187.61767578125, Val_Loss: 4354.45947265625\n",
      "Epoch 7921, Train_Loss: 4187.5244140625, Val_Loss: 4354.35009765625\n",
      "Epoch 7922, Train_Loss: 4187.42236328125, Val_Loss: 4354.23876953125\n",
      "Epoch 7923, Train_Loss: 4187.32275390625, Val_Loss: 4354.10498046875\n",
      "Epoch 7924, Train_Loss: 4187.22509765625, Val_Loss: 4353.9814453125\n",
      "Epoch 7925, Train_Loss: 4187.10205078125, Val_Loss: 4353.84814453125\n",
      "Epoch 7926, Train_Loss: 4187.0, Val_Loss: 4353.705078125\n",
      "Epoch 7927, Train_Loss: 4186.91162109375, Val_Loss: 4353.5751953125\n",
      "Epoch 7928, Train_Loss: 4186.76416015625, Val_Loss: 4353.4638671875\n",
      "Epoch 7929, Train_Loss: 4186.67529296875, Val_Loss: 4353.34228515625\n",
      "Epoch 7930, Train_Loss: 4186.5673828125, Val_Loss: 4353.21337890625\n",
      "Epoch 7931, Train_Loss: 4186.45458984375, Val_Loss: 4353.08203125\n",
      "Epoch 7932, Train_Loss: 4186.35595703125, Val_Loss: 4352.96533203125\n",
      "Epoch 7933, Train_Loss: 4186.26318359375, Val_Loss: 4352.84375\n",
      "Epoch 7934, Train_Loss: 4186.1630859375, Val_Loss: 4352.73193359375\n",
      "Epoch 7935, Train_Loss: 4186.0537109375, Val_Loss: 4352.61572265625\n",
      "Epoch 7936, Train_Loss: 4185.95654296875, Val_Loss: 4352.49169921875\n",
      "Epoch 7937, Train_Loss: 4185.86328125, Val_Loss: 4352.36767578125\n",
      "Epoch 7938, Train_Loss: 4185.77587890625, Val_Loss: 4352.23583984375\n",
      "Epoch 7939, Train_Loss: 4185.68359375, Val_Loss: 4352.11279296875\n",
      "Epoch 7940, Train_Loss: 4185.57861328125, Val_Loss: 4351.998046875\n",
      "Epoch 7941, Train_Loss: 4185.4443359375, Val_Loss: 4351.87646484375\n",
      "Epoch 7942, Train_Loss: 4185.36279296875, Val_Loss: 4351.75537109375\n",
      "Epoch 7943, Train_Loss: 4185.26318359375, Val_Loss: 4351.6318359375\n",
      "Epoch 7944, Train_Loss: 4185.166015625, Val_Loss: 4351.49169921875\n",
      "Epoch 7945, Train_Loss: 4185.07080078125, Val_Loss: 4351.375\n",
      "Epoch 7946, Train_Loss: 4184.96337890625, Val_Loss: 4351.25732421875\n",
      "Epoch 7947, Train_Loss: 4184.8671875, Val_Loss: 4351.13232421875\n",
      "Epoch 7948, Train_Loss: 4184.76171875, Val_Loss: 4351.01708984375\n",
      "Epoch 7949, Train_Loss: 4184.67724609375, Val_Loss: 4350.88525390625\n",
      "Epoch 7950, Train_Loss: 4184.583984375, Val_Loss: 4350.77197265625\n",
      "Epoch 7951, Train_Loss: 4184.48974609375, Val_Loss: 4350.638671875\n",
      "Epoch 7952, Train_Loss: 4184.39697265625, Val_Loss: 4350.51171875\n",
      "Epoch 7953, Train_Loss: 4184.28564453125, Val_Loss: 4350.3935546875\n",
      "Epoch 7954, Train_Loss: 4184.1943359375, Val_Loss: 4350.2734375\n",
      "Epoch 7955, Train_Loss: 4184.06982421875, Val_Loss: 4350.1416015625\n",
      "Epoch 7956, Train_Loss: 4183.9609375, Val_Loss: 4350.025390625\n",
      "Epoch 7957, Train_Loss: 4183.8623046875, Val_Loss: 4349.91064453125\n",
      "Epoch 7958, Train_Loss: 4183.78369140625, Val_Loss: 4349.77490234375\n",
      "Epoch 7959, Train_Loss: 4183.67626953125, Val_Loss: 4349.65380859375\n",
      "Epoch 7960, Train_Loss: 4183.57958984375, Val_Loss: 4349.52490234375\n",
      "Epoch 7961, Train_Loss: 4183.474609375, Val_Loss: 4349.4130859375\n",
      "Epoch 7962, Train_Loss: 4183.37939453125, Val_Loss: 4349.28759765625\n",
      "Epoch 7963, Train_Loss: 4183.27490234375, Val_Loss: 4349.166015625\n",
      "Epoch 7964, Train_Loss: 4183.181640625, Val_Loss: 4349.0126953125\n",
      "Epoch 7965, Train_Loss: 4183.08447265625, Val_Loss: 4348.8857421875\n",
      "Epoch 7966, Train_Loss: 4182.9697265625, Val_Loss: 4348.76513671875\n",
      "Epoch 7967, Train_Loss: 4182.873046875, Val_Loss: 4348.6435546875\n",
      "Epoch 7968, Train_Loss: 4182.7568359375, Val_Loss: 4348.5361328125\n",
      "Epoch 7969, Train_Loss: 4182.66015625, Val_Loss: 4348.419921875\n",
      "Epoch 7970, Train_Loss: 4182.57275390625, Val_Loss: 4348.28369140625\n",
      "Epoch 7971, Train_Loss: 4182.521484375, Val_Loss: 4348.1572265625\n",
      "Epoch 7972, Train_Loss: 4182.4375, Val_Loss: 4348.02880859375\n",
      "Epoch 7973, Train_Loss: 4182.35400390625, Val_Loss: 4347.9150390625\n",
      "Epoch 7974, Train_Loss: 4182.25439453125, Val_Loss: 4347.79345703125\n",
      "Epoch 7975, Train_Loss: 4182.15185546875, Val_Loss: 4347.6865234375\n",
      "Epoch 7976, Train_Loss: 4182.04345703125, Val_Loss: 4347.564453125\n",
      "Epoch 7977, Train_Loss: 4181.95947265625, Val_Loss: 4347.44384765625\n",
      "Epoch 7978, Train_Loss: 4181.86376953125, Val_Loss: 4347.31396484375\n",
      "Epoch 7979, Train_Loss: 4181.76220703125, Val_Loss: 4347.1953125\n",
      "Epoch 7980, Train_Loss: 4181.671875, Val_Loss: 4347.068359375\n",
      "Epoch 7981, Train_Loss: 4181.501953125, Val_Loss: 4346.95263671875\n",
      "Epoch 7982, Train_Loss: 4181.40380859375, Val_Loss: 4346.83544921875\n",
      "Epoch 7983, Train_Loss: 4181.16064453125, Val_Loss: 4346.70703125\n",
      "Epoch 7984, Train_Loss: 4181.0732421875, Val_Loss: 4346.59765625\n",
      "Epoch 7985, Train_Loss: 4180.98486328125, Val_Loss: 4346.4609375\n",
      "Epoch 7986, Train_Loss: 4180.865234375, Val_Loss: 4346.3359375\n",
      "Epoch 7987, Train_Loss: 4180.77197265625, Val_Loss: 4346.234375\n",
      "Epoch 7988, Train_Loss: 4180.67724609375, Val_Loss: 4346.10986328125\n",
      "Epoch 7989, Train_Loss: 4180.57861328125, Val_Loss: 4345.99755859375\n",
      "Epoch 7990, Train_Loss: 4180.4814453125, Val_Loss: 4345.87060546875\n",
      "Epoch 7991, Train_Loss: 4180.3681640625, Val_Loss: 4345.75341796875\n",
      "Epoch 7992, Train_Loss: 4180.271484375, Val_Loss: 4345.6220703125\n",
      "Epoch 7993, Train_Loss: 4180.18212890625, Val_Loss: 4345.4931640625\n",
      "Epoch 7994, Train_Loss: 4180.08984375, Val_Loss: 4345.3818359375\n",
      "Epoch 7995, Train_Loss: 4179.97216796875, Val_Loss: 4345.259765625\n",
      "Epoch 7996, Train_Loss: 4179.8544921875, Val_Loss: 4345.14013671875\n",
      "Epoch 7997, Train_Loss: 4179.75634765625, Val_Loss: 4345.01904296875\n",
      "Epoch 7998, Train_Loss: 4179.68505859375, Val_Loss: 4344.87939453125\n",
      "Epoch 7999, Train_Loss: 4179.59716796875, Val_Loss: 4344.7607421875\n",
      "Epoch 8000, Train_Loss: 4179.49462890625, Val_Loss: 4344.65234375\n",
      "Epoch 8001, Train_Loss: 4179.40771484375, Val_Loss: 4344.5322265625\n",
      "Epoch 8002, Train_Loss: 4179.2880859375, Val_Loss: 4344.38623046875\n",
      "Epoch 8003, Train_Loss: 4179.19189453125, Val_Loss: 4344.265625\n",
      "Epoch 8004, Train_Loss: 4179.1015625, Val_Loss: 4344.13916015625\n",
      "Epoch 8005, Train_Loss: 4179.00732421875, Val_Loss: 4344.005859375\n",
      "Epoch 8006, Train_Loss: 4178.90673828125, Val_Loss: 4343.89501953125\n",
      "Epoch 8007, Train_Loss: 4178.78125, Val_Loss: 4343.77880859375\n",
      "Epoch 8008, Train_Loss: 4178.66796875, Val_Loss: 4343.6728515625\n",
      "Epoch 8009, Train_Loss: 4178.5791015625, Val_Loss: 4343.54833984375\n",
      "Epoch 8010, Train_Loss: 4178.50244140625, Val_Loss: 4343.42724609375\n",
      "Epoch 8011, Train_Loss: 4178.41015625, Val_Loss: 4343.3076171875\n",
      "Epoch 8012, Train_Loss: 4178.31884765625, Val_Loss: 4343.17822265625\n",
      "Epoch 8013, Train_Loss: 4178.2216796875, Val_Loss: 4343.05712890625\n",
      "Epoch 8014, Train_Loss: 4178.126953125, Val_Loss: 4342.9423828125\n",
      "Epoch 8015, Train_Loss: 4178.02587890625, Val_Loss: 4342.833984375\n",
      "Epoch 8016, Train_Loss: 4177.92724609375, Val_Loss: 4342.708984375\n",
      "Epoch 8017, Train_Loss: 4177.830078125, Val_Loss: 4342.5869140625\n",
      "Epoch 8018, Train_Loss: 4177.73388671875, Val_Loss: 4342.4619140625\n",
      "Epoch 8019, Train_Loss: 4177.6572265625, Val_Loss: 4342.35302734375\n",
      "Epoch 8020, Train_Loss: 4177.5546875, Val_Loss: 4342.23974609375\n",
      "Epoch 8021, Train_Loss: 4177.453125, Val_Loss: 4342.10205078125\n",
      "Epoch 8022, Train_Loss: 4177.33251953125, Val_Loss: 4341.98779296875\n",
      "Epoch 8023, Train_Loss: 4177.2451171875, Val_Loss: 4341.869140625\n",
      "Epoch 8024, Train_Loss: 4177.14990234375, Val_Loss: 4341.7490234375\n",
      "Epoch 8025, Train_Loss: 4177.09228515625, Val_Loss: 4341.61328125\n",
      "Epoch 8026, Train_Loss: 4176.9833984375, Val_Loss: 4341.50537109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8027, Train_Loss: 4176.875, Val_Loss: 4341.38916015625\n",
      "Epoch 8028, Train_Loss: 4176.76904296875, Val_Loss: 4341.2783203125\n",
      "Epoch 8029, Train_Loss: 4176.68212890625, Val_Loss: 4341.158203125\n",
      "Epoch 8030, Train_Loss: 4176.5830078125, Val_Loss: 4341.01904296875\n",
      "Epoch 8031, Train_Loss: 4176.4951171875, Val_Loss: 4340.8974609375\n",
      "Epoch 8032, Train_Loss: 4176.3935546875, Val_Loss: 4340.7734375\n",
      "Epoch 8033, Train_Loss: 4176.31591796875, Val_Loss: 4340.65283203125\n",
      "Epoch 8034, Train_Loss: 4176.2236328125, Val_Loss: 4340.537109375\n",
      "Epoch 8035, Train_Loss: 4176.080078125, Val_Loss: 4340.43505859375\n",
      "Epoch 8036, Train_Loss: 4175.9873046875, Val_Loss: 4340.31591796875\n",
      "Epoch 8037, Train_Loss: 4175.88525390625, Val_Loss: 4340.19921875\n",
      "Epoch 8038, Train_Loss: 4175.80029296875, Val_Loss: 4340.076171875\n",
      "Epoch 8039, Train_Loss: 4175.71240234375, Val_Loss: 4339.9482421875\n",
      "Epoch 8040, Train_Loss: 4175.5986328125, Val_Loss: 4339.787109375\n",
      "Epoch 8041, Train_Loss: 4175.50537109375, Val_Loss: 4339.673828125\n",
      "Epoch 8042, Train_Loss: 4175.4072265625, Val_Loss: 4339.578125\n",
      "Epoch 8043, Train_Loss: 4175.3154296875, Val_Loss: 4339.4609375\n",
      "Epoch 8044, Train_Loss: 4175.22314453125, Val_Loss: 4339.33544921875\n",
      "Epoch 8045, Train_Loss: 4175.13134765625, Val_Loss: 4339.19384765625\n",
      "Epoch 8046, Train_Loss: 4175.03515625, Val_Loss: 4339.076171875\n",
      "Epoch 8047, Train_Loss: 4174.91357421875, Val_Loss: 4338.9638671875\n",
      "Epoch 8048, Train_Loss: 4174.8193359375, Val_Loss: 4338.84765625\n",
      "Epoch 8049, Train_Loss: 4174.68505859375, Val_Loss: 4338.7490234375\n",
      "Epoch 8050, Train_Loss: 4174.5986328125, Val_Loss: 4338.6279296875\n",
      "Epoch 8051, Train_Loss: 4174.5048828125, Val_Loss: 4338.50146484375\n",
      "Epoch 8052, Train_Loss: 4174.40771484375, Val_Loss: 4338.37646484375\n",
      "Epoch 8053, Train_Loss: 4174.30712890625, Val_Loss: 4338.263671875\n",
      "Epoch 8054, Train_Loss: 4174.2275390625, Val_Loss: 4338.14697265625\n",
      "Epoch 8055, Train_Loss: 4174.1513671875, Val_Loss: 4338.041015625\n",
      "Epoch 8056, Train_Loss: 4174.0673828125, Val_Loss: 4337.91943359375\n",
      "Epoch 8057, Train_Loss: 4173.97265625, Val_Loss: 4337.80224609375\n",
      "Epoch 8058, Train_Loss: 4173.87255859375, Val_Loss: 4337.6806640625\n",
      "Epoch 8059, Train_Loss: 4173.7119140625, Val_Loss: 4337.53857421875\n",
      "Epoch 8060, Train_Loss: 4173.61669921875, Val_Loss: 4337.42431640625\n",
      "Epoch 8061, Train_Loss: 4173.52001953125, Val_Loss: 4337.30859375\n",
      "Epoch 8062, Train_Loss: 4173.39306640625, Val_Loss: 4337.2021484375\n",
      "Epoch 8063, Train_Loss: 4173.314453125, Val_Loss: 4337.080078125\n",
      "Epoch 8064, Train_Loss: 4173.236328125, Val_Loss: 4336.9609375\n",
      "Epoch 8065, Train_Loss: 4173.13330078125, Val_Loss: 4336.84130859375\n",
      "Epoch 8066, Train_Loss: 4173.05224609375, Val_Loss: 4336.7177734375\n",
      "Epoch 8067, Train_Loss: 4172.947265625, Val_Loss: 4336.603515625\n",
      "Epoch 8068, Train_Loss: 4172.8330078125, Val_Loss: 4336.4697265625\n",
      "Epoch 8069, Train_Loss: 4172.73974609375, Val_Loss: 4336.36181640625\n",
      "Epoch 8070, Train_Loss: 4172.650390625, Val_Loss: 4336.2392578125\n",
      "Epoch 8071, Train_Loss: 4172.5625, Val_Loss: 4336.13037109375\n",
      "Epoch 8072, Train_Loss: 4172.462890625, Val_Loss: 4335.99462890625\n",
      "Epoch 8073, Train_Loss: 4172.3603515625, Val_Loss: 4335.88232421875\n",
      "Epoch 8074, Train_Loss: 4172.26904296875, Val_Loss: 4335.75537109375\n",
      "Epoch 8075, Train_Loss: 4172.17138671875, Val_Loss: 4335.63818359375\n",
      "Epoch 8076, Train_Loss: 4172.06201171875, Val_Loss: 4335.53125\n",
      "Epoch 8077, Train_Loss: 4171.9794921875, Val_Loss: 4335.4208984375\n",
      "Epoch 8078, Train_Loss: 4171.8525390625, Val_Loss: 4335.26318359375\n",
      "Epoch 8079, Train_Loss: 4171.80810546875, Val_Loss: 4335.13818359375\n",
      "Epoch 8080, Train_Loss: 4171.7265625, Val_Loss: 4335.0107421875\n",
      "Epoch 8081, Train_Loss: 4171.634765625, Val_Loss: 4334.90234375\n",
      "Epoch 8082, Train_Loss: 4171.53515625, Val_Loss: 4334.78564453125\n",
      "Epoch 8083, Train_Loss: 4171.42724609375, Val_Loss: 4334.6669921875\n",
      "Epoch 8084, Train_Loss: 4171.326171875, Val_Loss: 4334.55126953125\n",
      "Epoch 8085, Train_Loss: 4171.236328125, Val_Loss: 4334.44287109375\n",
      "Epoch 8086, Train_Loss: 4171.13916015625, Val_Loss: 4334.31591796875\n",
      "Epoch 8087, Train_Loss: 4171.044921875, Val_Loss: 4334.19140625\n",
      "Epoch 8088, Train_Loss: 4170.9287109375, Val_Loss: 4334.0703125\n",
      "Epoch 8089, Train_Loss: 4170.78662109375, Val_Loss: 4333.9677734375\n",
      "Epoch 8090, Train_Loss: 4170.69189453125, Val_Loss: 4333.85205078125\n",
      "Epoch 8091, Train_Loss: 4170.60595703125, Val_Loss: 4333.73583984375\n",
      "Epoch 8092, Train_Loss: 4170.5224609375, Val_Loss: 4333.609375\n",
      "Epoch 8093, Train_Loss: 4170.416015625, Val_Loss: 4333.49365234375\n",
      "Epoch 8094, Train_Loss: 4170.31396484375, Val_Loss: 4333.37646484375\n",
      "Epoch 8095, Train_Loss: 4170.2177734375, Val_Loss: 4333.26171875\n",
      "Epoch 8096, Train_Loss: 4170.125, Val_Loss: 4333.1630859375\n",
      "Epoch 8097, Train_Loss: 4170.0322265625, Val_Loss: 4333.01611328125\n",
      "Epoch 8098, Train_Loss: 4169.94140625, Val_Loss: 4332.908203125\n",
      "Epoch 8099, Train_Loss: 4169.84375, Val_Loss: 4332.77978515625\n",
      "Epoch 8100, Train_Loss: 4169.7685546875, Val_Loss: 4332.66015625\n",
      "Epoch 8101, Train_Loss: 4169.6728515625, Val_Loss: 4332.548828125\n",
      "Epoch 8102, Train_Loss: 4169.57421875, Val_Loss: 4332.42822265625\n",
      "Epoch 8103, Train_Loss: 4169.45166015625, Val_Loss: 4332.3203125\n",
      "Epoch 8104, Train_Loss: 4169.3623046875, Val_Loss: 4332.20703125\n",
      "Epoch 8105, Train_Loss: 4169.28466796875, Val_Loss: 4332.0849609375\n",
      "Epoch 8106, Train_Loss: 4169.2109375, Val_Loss: 4331.9521484375\n",
      "Epoch 8107, Train_Loss: 4169.1171875, Val_Loss: 4331.83349609375\n",
      "Epoch 8108, Train_Loss: 4169.0302734375, Val_Loss: 4331.712890625\n",
      "Epoch 8109, Train_Loss: 4168.9208984375, Val_Loss: 4331.6181640625\n",
      "Epoch 8110, Train_Loss: 4168.82861328125, Val_Loss: 4331.49658203125\n",
      "Epoch 8111, Train_Loss: 4168.7412109375, Val_Loss: 4331.37548828125\n",
      "Epoch 8112, Train_Loss: 4168.63623046875, Val_Loss: 4331.26220703125\n",
      "Epoch 8113, Train_Loss: 4168.546875, Val_Loss: 4331.1318359375\n",
      "Epoch 8114, Train_Loss: 4168.43798828125, Val_Loss: 4331.01171875\n",
      "Epoch 8115, Train_Loss: 4168.35595703125, Val_Loss: 4330.9013671875\n",
      "Epoch 8116, Train_Loss: 4168.25927734375, Val_Loss: 4330.75537109375\n",
      "Epoch 8117, Train_Loss: 4168.16650390625, Val_Loss: 4330.6376953125\n",
      "Epoch 8118, Train_Loss: 4168.08544921875, Val_Loss: 4330.521484375\n",
      "Epoch 8119, Train_Loss: 4167.9697265625, Val_Loss: 4330.4140625\n",
      "Epoch 8120, Train_Loss: 4167.8857421875, Val_Loss: 4330.28369140625\n",
      "Epoch 8121, Train_Loss: 4167.80810546875, Val_Loss: 4330.17138671875\n",
      "Epoch 8122, Train_Loss: 4167.71826171875, Val_Loss: 4330.05712890625\n",
      "Epoch 8123, Train_Loss: 4167.61669921875, Val_Loss: 4329.939453125\n",
      "Epoch 8124, Train_Loss: 4167.5263671875, Val_Loss: 4329.830078125\n",
      "Epoch 8125, Train_Loss: 4167.41259765625, Val_Loss: 4329.7080078125\n",
      "Epoch 8126, Train_Loss: 4167.32373046875, Val_Loss: 4329.58447265625\n",
      "Epoch 8127, Train_Loss: 4167.236328125, Val_Loss: 4329.455078125\n",
      "Epoch 8128, Train_Loss: 4167.1298828125, Val_Loss: 4329.345703125\n",
      "Epoch 8129, Train_Loss: 4167.0205078125, Val_Loss: 4329.228515625\n",
      "Epoch 8130, Train_Loss: 4166.8984375, Val_Loss: 4329.12939453125\n",
      "Epoch 8131, Train_Loss: 4166.81005859375, Val_Loss: 4329.00830078125\n",
      "Epoch 8132, Train_Loss: 4166.73291015625, Val_Loss: 4328.900390625\n",
      "Epoch 8133, Train_Loss: 4166.6875, Val_Loss: 4328.775390625\n",
      "Epoch 8134, Train_Loss: 4166.595703125, Val_Loss: 4328.65673828125\n",
      "Epoch 8135, Train_Loss: 4166.38671875, Val_Loss: 4328.5322265625\n",
      "Epoch 8136, Train_Loss: 4166.291015625, Val_Loss: 4328.4326171875\n",
      "Epoch 8137, Train_Loss: 4166.1982421875, Val_Loss: 4328.3134765625\n",
      "Epoch 8138, Train_Loss: 4166.11767578125, Val_Loss: 4328.20068359375\n",
      "Epoch 8139, Train_Loss: 4166.0166015625, Val_Loss: 4328.0859375\n",
      "Epoch 8140, Train_Loss: 4165.92626953125, Val_Loss: 4327.96337890625\n",
      "Epoch 8141, Train_Loss: 4165.84765625, Val_Loss: 4327.84814453125\n",
      "Epoch 8142, Train_Loss: 4165.7529296875, Val_Loss: 4327.72705078125\n",
      "Epoch 8143, Train_Loss: 4165.59228515625, Val_Loss: 4327.62158203125\n",
      "Epoch 8144, Train_Loss: 4165.4912109375, Val_Loss: 4327.51171875\n",
      "Epoch 8145, Train_Loss: 4165.41259765625, Val_Loss: 4327.3828125\n",
      "Epoch 8146, Train_Loss: 4165.31689453125, Val_Loss: 4327.26318359375\n",
      "Epoch 8147, Train_Loss: 4165.23583984375, Val_Loss: 4327.1455078125\n",
      "Epoch 8148, Train_Loss: 4165.154296875, Val_Loss: 4327.02783203125\n",
      "Epoch 8149, Train_Loss: 4165.03515625, Val_Loss: 4326.91064453125\n",
      "Epoch 8150, Train_Loss: 4164.95263671875, Val_Loss: 4326.80859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8151, Train_Loss: 4164.86474609375, Val_Loss: 4326.69482421875\n",
      "Epoch 8152, Train_Loss: 4164.765625, Val_Loss: 4326.57421875\n",
      "Epoch 8153, Train_Loss: 4164.67578125, Val_Loss: 4326.4462890625\n",
      "Epoch 8154, Train_Loss: 4164.5498046875, Val_Loss: 4326.29150390625\n",
      "Epoch 8155, Train_Loss: 4164.4521484375, Val_Loss: 4326.18994140625\n",
      "Epoch 8156, Train_Loss: 4164.3759765625, Val_Loss: 4326.07470703125\n",
      "Epoch 8157, Train_Loss: 4164.244140625, Val_Loss: 4325.96337890625\n",
      "Epoch 8158, Train_Loss: 4164.15966796875, Val_Loss: 4325.84912109375\n",
      "Epoch 8159, Train_Loss: 4164.0673828125, Val_Loss: 4325.7392578125\n",
      "Epoch 8160, Train_Loss: 4163.96728515625, Val_Loss: 4325.607421875\n",
      "Epoch 8161, Train_Loss: 4163.86572265625, Val_Loss: 4325.4921875\n",
      "Epoch 8162, Train_Loss: 4163.7861328125, Val_Loss: 4325.3779296875\n",
      "Epoch 8163, Train_Loss: 4163.69921875, Val_Loss: 4325.2763671875\n",
      "Epoch 8164, Train_Loss: 4163.59326171875, Val_Loss: 4325.1708984375\n",
      "Epoch 8165, Train_Loss: 4163.50830078125, Val_Loss: 4325.0546875\n",
      "Epoch 8166, Train_Loss: 4163.4228515625, Val_Loss: 4324.92431640625\n",
      "Epoch 8167, Train_Loss: 4163.34375, Val_Loss: 4324.798828125\n",
      "Epoch 8168, Train_Loss: 4163.26025390625, Val_Loss: 4324.69677734375\n",
      "Epoch 8169, Train_Loss: 4163.15771484375, Val_Loss: 4324.576171875\n",
      "Epoch 8170, Train_Loss: 4163.03369140625, Val_Loss: 4324.47314453125\n",
      "Epoch 8171, Train_Loss: 4162.9560546875, Val_Loss: 4324.3564453125\n",
      "Epoch 8172, Train_Loss: 4162.8740234375, Val_Loss: 4324.24462890625\n",
      "Epoch 8173, Train_Loss: 4162.76953125, Val_Loss: 4324.12548828125\n",
      "Epoch 8174, Train_Loss: 4162.6865234375, Val_Loss: 4324.00244140625\n",
      "Epoch 8175, Train_Loss: 4162.58740234375, Val_Loss: 4323.8798828125\n",
      "Epoch 8176, Train_Loss: 4162.5029296875, Val_Loss: 4323.77978515625\n",
      "Epoch 8177, Train_Loss: 4162.40966796875, Val_Loss: 4323.67138671875\n",
      "Epoch 8178, Train_Loss: 4162.31982421875, Val_Loss: 4323.55859375\n",
      "Epoch 8179, Train_Loss: 4162.23095703125, Val_Loss: 4323.4267578125\n",
      "Epoch 8180, Train_Loss: 4162.12939453125, Val_Loss: 4323.3251953125\n",
      "Epoch 8181, Train_Loss: 4162.0341796875, Val_Loss: 4323.2041015625\n",
      "Epoch 8182, Train_Loss: 4161.9482421875, Val_Loss: 4323.09228515625\n",
      "Epoch 8183, Train_Loss: 4161.86865234375, Val_Loss: 4322.95263671875\n",
      "Epoch 8184, Train_Loss: 4161.75390625, Val_Loss: 4322.85302734375\n",
      "Epoch 8185, Train_Loss: 4161.658203125, Val_Loss: 4322.740234375\n",
      "Epoch 8186, Train_Loss: 4161.57861328125, Val_Loss: 4322.625\n",
      "Epoch 8187, Train_Loss: 4161.5283203125, Val_Loss: 4322.50537109375\n",
      "Epoch 8188, Train_Loss: 4161.44384765625, Val_Loss: 4322.3837890625\n",
      "Epoch 8189, Train_Loss: 4161.34326171875, Val_Loss: 4322.27685546875\n",
      "Epoch 8190, Train_Loss: 4161.2275390625, Val_Loss: 4322.16796875\n",
      "Epoch 8191, Train_Loss: 4161.13525390625, Val_Loss: 4322.05615234375\n",
      "Epoch 8192, Train_Loss: 4161.037109375, Val_Loss: 4321.90869140625\n",
      "Epoch 8193, Train_Loss: 4160.962890625, Val_Loss: 4321.80126953125\n",
      "Epoch 8194, Train_Loss: 4160.8759765625, Val_Loss: 4321.66748046875\n",
      "Epoch 8195, Train_Loss: 4160.80029296875, Val_Loss: 4321.548828125\n",
      "Epoch 8196, Train_Loss: 4160.7060546875, Val_Loss: 4321.447265625\n",
      "Epoch 8197, Train_Loss: 4160.6171875, Val_Loss: 4321.3330078125\n",
      "Epoch 8198, Train_Loss: 4160.47216796875, Val_Loss: 4321.216796875\n",
      "Epoch 8199, Train_Loss: 4160.38623046875, Val_Loss: 4321.10498046875\n",
      "Epoch 8200, Train_Loss: 4160.28759765625, Val_Loss: 4321.0\n",
      "Epoch 8201, Train_Loss: 4160.193359375, Val_Loss: 4320.8720703125\n",
      "Epoch 8202, Train_Loss: 4159.99853515625, Val_Loss: 4320.7529296875\n",
      "Epoch 8203, Train_Loss: 4159.9111328125, Val_Loss: 4320.64013671875\n",
      "Epoch 8204, Train_Loss: 4159.822265625, Val_Loss: 4320.541015625\n",
      "Epoch 8205, Train_Loss: 4159.7275390625, Val_Loss: 4320.42822265625\n",
      "Epoch 8206, Train_Loss: 4159.63818359375, Val_Loss: 4320.306640625\n",
      "Epoch 8207, Train_Loss: 4159.5439453125, Val_Loss: 4320.1845703125\n",
      "Epoch 8208, Train_Loss: 4159.47216796875, Val_Loss: 4320.06787109375\n",
      "Epoch 8209, Train_Loss: 4159.38916015625, Val_Loss: 4319.95947265625\n",
      "Epoch 8210, Train_Loss: 4159.30908203125, Val_Loss: 4319.83984375\n",
      "Epoch 8211, Train_Loss: 4159.12353515625, Val_Loss: 4319.73388671875\n",
      "Epoch 8212, Train_Loss: 4159.041015625, Val_Loss: 4319.60986328125\n",
      "Epoch 8213, Train_Loss: 4158.9580078125, Val_Loss: 4319.4990234375\n",
      "Epoch 8214, Train_Loss: 4158.8779296875, Val_Loss: 4319.3837890625\n",
      "Epoch 8215, Train_Loss: 4158.7802734375, Val_Loss: 4319.26416015625\n",
      "Epoch 8216, Train_Loss: 4158.68310546875, Val_Loss: 4319.14697265625\n",
      "Epoch 8217, Train_Loss: 4158.59326171875, Val_Loss: 4319.0341796875\n",
      "Epoch 8218, Train_Loss: 4158.52197265625, Val_Loss: 4318.93896484375\n",
      "Epoch 8219, Train_Loss: 4158.4345703125, Val_Loss: 4318.82861328125\n",
      "Epoch 8220, Train_Loss: 4158.337890625, Val_Loss: 4318.70654296875\n",
      "Epoch 8221, Train_Loss: 4158.2255859375, Val_Loss: 4318.56982421875\n",
      "Epoch 8222, Train_Loss: 4158.1396484375, Val_Loss: 4318.46630859375\n",
      "Epoch 8223, Train_Loss: 4158.03955078125, Val_Loss: 4318.34619140625\n",
      "Epoch 8224, Train_Loss: 4157.96240234375, Val_Loss: 4318.2333984375\n",
      "Epoch 8225, Train_Loss: 4157.86376953125, Val_Loss: 4318.1279296875\n",
      "Epoch 8226, Train_Loss: 4157.783203125, Val_Loss: 4318.001953125\n",
      "Epoch 8227, Train_Loss: 4157.69970703125, Val_Loss: 4317.8974609375\n",
      "Epoch 8228, Train_Loss: 4157.6181640625, Val_Loss: 4317.77294921875\n",
      "Epoch 8229, Train_Loss: 4157.53759765625, Val_Loss: 4317.66015625\n",
      "Epoch 8230, Train_Loss: 4157.4541015625, Val_Loss: 4317.5439453125\n",
      "Epoch 8231, Train_Loss: 4157.3466796875, Val_Loss: 4317.4033203125\n",
      "Epoch 8232, Train_Loss: 4157.224609375, Val_Loss: 4317.291015625\n",
      "Epoch 8233, Train_Loss: 4157.1328125, Val_Loss: 4317.18505859375\n",
      "Epoch 8234, Train_Loss: 4157.04345703125, Val_Loss: 4317.072265625\n",
      "Epoch 8235, Train_Loss: 4156.95703125, Val_Loss: 4316.94091796875\n",
      "Epoch 8236, Train_Loss: 4156.859375, Val_Loss: 4316.81689453125\n",
      "Epoch 8237, Train_Loss: 4156.76513671875, Val_Loss: 4316.71337890625\n",
      "Epoch 8238, Train_Loss: 4156.642578125, Val_Loss: 4316.61328125\n",
      "Epoch 8239, Train_Loss: 4156.54931640625, Val_Loss: 4316.5\n",
      "Epoch 8240, Train_Loss: 4156.45849609375, Val_Loss: 4316.38623046875\n",
      "Epoch 8241, Train_Loss: 4156.37451171875, Val_Loss: 4316.27099609375\n",
      "Epoch 8242, Train_Loss: 4156.31201171875, Val_Loss: 4316.14208984375\n",
      "Epoch 8243, Train_Loss: 4156.2158203125, Val_Loss: 4316.0283203125\n",
      "Epoch 8244, Train_Loss: 4156.14404296875, Val_Loss: 4315.9150390625\n",
      "Epoch 8245, Train_Loss: 4156.0537109375, Val_Loss: 4315.81884765625\n",
      "Epoch 8246, Train_Loss: 4155.95703125, Val_Loss: 4315.71337890625\n",
      "Epoch 8247, Train_Loss: 4155.869140625, Val_Loss: 4315.6025390625\n",
      "Epoch 8248, Train_Loss: 4155.794921875, Val_Loss: 4315.47216796875\n",
      "Epoch 8249, Train_Loss: 4155.69482421875, Val_Loss: 4315.3642578125\n",
      "Epoch 8250, Train_Loss: 4155.59130859375, Val_Loss: 4315.2470703125\n",
      "Epoch 8251, Train_Loss: 4155.505859375, Val_Loss: 4315.12451171875\n",
      "Epoch 8252, Train_Loss: 4155.33447265625, Val_Loss: 4315.0166015625\n",
      "Epoch 8253, Train_Loss: 4155.2509765625, Val_Loss: 4314.90185546875\n",
      "Epoch 8254, Train_Loss: 4155.16357421875, Val_Loss: 4314.79248046875\n",
      "Epoch 8255, Train_Loss: 4155.08251953125, Val_Loss: 4314.66845703125\n",
      "Epoch 8256, Train_Loss: 4155.00537109375, Val_Loss: 4314.5556640625\n",
      "Epoch 8257, Train_Loss: 4154.92041015625, Val_Loss: 4314.4462890625\n",
      "Epoch 8258, Train_Loss: 4154.837890625, Val_Loss: 4314.33349609375\n",
      "Epoch 8259, Train_Loss: 4154.7451171875, Val_Loss: 4314.21435546875\n",
      "Epoch 8260, Train_Loss: 4154.654296875, Val_Loss: 4314.08935546875\n",
      "Epoch 8261, Train_Loss: 4154.5654296875, Val_Loss: 4313.98388671875\n",
      "Epoch 8262, Train_Loss: 4154.4658203125, Val_Loss: 4313.86376953125\n",
      "Epoch 8263, Train_Loss: 4154.38037109375, Val_Loss: 4313.763671875\n",
      "Epoch 8264, Train_Loss: 4154.2880859375, Val_Loss: 4313.64453125\n",
      "Epoch 8265, Train_Loss: 4154.16162109375, Val_Loss: 4313.5419921875\n",
      "Epoch 8266, Train_Loss: 4154.0791015625, Val_Loss: 4313.42138671875\n",
      "Epoch 8267, Train_Loss: 4153.98095703125, Val_Loss: 4313.30859375\n",
      "Epoch 8268, Train_Loss: 4153.89990234375, Val_Loss: 4313.20458984375\n",
      "Epoch 8269, Train_Loss: 4153.802734375, Val_Loss: 4313.03662109375\n",
      "Epoch 8270, Train_Loss: 4153.712890625, Val_Loss: 4312.92431640625\n",
      "Epoch 8271, Train_Loss: 4153.61376953125, Val_Loss: 4312.81494140625\n",
      "Epoch 8272, Train_Loss: 4153.52978515625, Val_Loss: 4312.71630859375\n",
      "Epoch 8273, Train_Loss: 4153.4306640625, Val_Loss: 4312.6083984375\n",
      "Epoch 8274, Train_Loss: 4153.345703125, Val_Loss: 4312.490234375\n",
      "Epoch 8275, Train_Loss: 4153.26953125, Val_Loss: 4312.37890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8276, Train_Loss: 4153.18310546875, Val_Loss: 4312.2587890625\n",
      "Epoch 8277, Train_Loss: 4153.08935546875, Val_Loss: 4312.14501953125\n",
      "Epoch 8278, Train_Loss: 4152.98193359375, Val_Loss: 4312.03662109375\n",
      "Epoch 8279, Train_Loss: 4152.875, Val_Loss: 4311.93603515625\n",
      "Epoch 8280, Train_Loss: 4152.80419921875, Val_Loss: 4311.82470703125\n",
      "Epoch 8281, Train_Loss: 4152.73193359375, Val_Loss: 4311.7158203125\n",
      "Epoch 8282, Train_Loss: 4152.64990234375, Val_Loss: 4311.59130859375\n",
      "Epoch 8283, Train_Loss: 4152.54248046875, Val_Loss: 4311.4765625\n",
      "Epoch 8284, Train_Loss: 4152.43994140625, Val_Loss: 4311.3662109375\n",
      "Epoch 8285, Train_Loss: 4152.3466796875, Val_Loss: 4311.25927734375\n",
      "Epoch 8286, Train_Loss: 4152.2626953125, Val_Loss: 4311.16015625\n",
      "Epoch 8287, Train_Loss: 4152.1875, Val_Loss: 4311.0439453125\n",
      "Epoch 8288, Train_Loss: 4151.92138671875, Val_Loss: 4310.923828125\n",
      "Epoch 8289, Train_Loss: 4151.8388671875, Val_Loss: 4310.79248046875\n",
      "Epoch 8290, Train_Loss: 4151.75634765625, Val_Loss: 4310.68115234375\n",
      "Epoch 8291, Train_Loss: 4151.67138671875, Val_Loss: 4310.5751953125\n",
      "Epoch 8292, Train_Loss: 4151.55859375, Val_Loss: 4310.4765625\n",
      "Epoch 8293, Train_Loss: 4151.45751953125, Val_Loss: 4310.3623046875\n",
      "Epoch 8294, Train_Loss: 4151.376953125, Val_Loss: 4310.25048828125\n",
      "Epoch 8295, Train_Loss: 4151.2900390625, Val_Loss: 4310.1376953125\n",
      "Epoch 8296, Train_Loss: 4151.25830078125, Val_Loss: 4310.02197265625\n",
      "Epoch 8297, Train_Loss: 4151.1650390625, Val_Loss: 4309.9013671875\n",
      "Epoch 8298, Train_Loss: 4151.06640625, Val_Loss: 4309.78759765625\n",
      "Epoch 8299, Train_Loss: 4150.9873046875, Val_Loss: 4309.68017578125\n",
      "Epoch 8300, Train_Loss: 4150.90380859375, Val_Loss: 4309.5615234375\n",
      "Epoch 8301, Train_Loss: 4150.8154296875, Val_Loss: 4309.4580078125\n",
      "Epoch 8302, Train_Loss: 4150.7255859375, Val_Loss: 4309.3515625\n",
      "Epoch 8303, Train_Loss: 4150.62744140625, Val_Loss: 4309.23291015625\n",
      "Epoch 8304, Train_Loss: 4150.54833984375, Val_Loss: 4309.1240234375\n",
      "Epoch 8305, Train_Loss: 4150.45556640625, Val_Loss: 4309.01416015625\n",
      "Epoch 8306, Train_Loss: 4150.3173828125, Val_Loss: 4308.90771484375\n",
      "Epoch 8307, Train_Loss: 4150.20361328125, Val_Loss: 4308.75732421875\n",
      "Epoch 8308, Train_Loss: 4150.1162109375, Val_Loss: 4308.654296875\n",
      "Epoch 8309, Train_Loss: 4150.02587890625, Val_Loss: 4308.53857421875\n",
      "Epoch 8310, Train_Loss: 4149.93896484375, Val_Loss: 4308.41162109375\n",
      "Epoch 8311, Train_Loss: 4149.85302734375, Val_Loss: 4308.306640625\n",
      "Epoch 8312, Train_Loss: 4149.771484375, Val_Loss: 4308.19580078125\n",
      "Epoch 8313, Train_Loss: 4149.68896484375, Val_Loss: 4308.091796875\n",
      "Epoch 8314, Train_Loss: 4149.5908203125, Val_Loss: 4307.97607421875\n",
      "Epoch 8315, Train_Loss: 4149.5126953125, Val_Loss: 4307.87109375\n",
      "Epoch 8316, Train_Loss: 4149.4345703125, Val_Loss: 4307.748046875\n",
      "Epoch 8317, Train_Loss: 4149.33544921875, Val_Loss: 4307.625\n",
      "Epoch 8318, Train_Loss: 4149.2578125, Val_Loss: 4307.51904296875\n",
      "Epoch 8319, Train_Loss: 4149.17578125, Val_Loss: 4307.41748046875\n",
      "Epoch 8320, Train_Loss: 4149.04150390625, Val_Loss: 4307.31103515625\n",
      "Epoch 8321, Train_Loss: 4148.95458984375, Val_Loss: 4307.21337890625\n",
      "Epoch 8322, Train_Loss: 4148.87744140625, Val_Loss: 4307.08935546875\n",
      "Epoch 8323, Train_Loss: 4148.81201171875, Val_Loss: 4306.96142578125\n",
      "Epoch 8324, Train_Loss: 4148.7080078125, Val_Loss: 4306.859375\n",
      "Epoch 8325, Train_Loss: 4148.61767578125, Val_Loss: 4306.755859375\n",
      "Epoch 8326, Train_Loss: 4148.52685546875, Val_Loss: 4306.64208984375\n",
      "Epoch 8327, Train_Loss: 4148.453125, Val_Loss: 4306.53369140625\n",
      "Epoch 8328, Train_Loss: 4148.36572265625, Val_Loss: 4306.421875\n",
      "Epoch 8329, Train_Loss: 4148.2783203125, Val_Loss: 4306.31884765625\n",
      "Epoch 8330, Train_Loss: 4148.189453125, Val_Loss: 4306.197265625\n",
      "Epoch 8331, Train_Loss: 4148.11181640625, Val_Loss: 4306.078125\n",
      "Epoch 8332, Train_Loss: 4148.03125, Val_Loss: 4305.9775390625\n",
      "Epoch 8333, Train_Loss: 4147.94189453125, Val_Loss: 4305.87890625\n",
      "Epoch 8334, Train_Loss: 4147.83642578125, Val_Loss: 4305.77197265625\n",
      "Epoch 8335, Train_Loss: 4147.76123046875, Val_Loss: 4305.66162109375\n",
      "Epoch 8336, Train_Loss: 4147.673828125, Val_Loss: 4305.52783203125\n",
      "Epoch 8337, Train_Loss: 4147.6015625, Val_Loss: 4305.41015625\n",
      "Epoch 8338, Train_Loss: 4147.51220703125, Val_Loss: 4305.29833984375\n",
      "Epoch 8339, Train_Loss: 4147.4228515625, Val_Loss: 4305.20458984375\n",
      "Epoch 8340, Train_Loss: 4147.33642578125, Val_Loss: 4305.1005859375\n",
      "Epoch 8341, Train_Loss: 4147.2509765625, Val_Loss: 4304.98779296875\n",
      "Epoch 8342, Train_Loss: 4147.16748046875, Val_Loss: 4304.87939453125\n",
      "Epoch 8343, Train_Loss: 4147.08447265625, Val_Loss: 4304.77099609375\n",
      "Epoch 8344, Train_Loss: 4147.00634765625, Val_Loss: 4304.65234375\n",
      "Epoch 8345, Train_Loss: 4146.921875, Val_Loss: 4304.54443359375\n",
      "Epoch 8346, Train_Loss: 4146.8203125, Val_Loss: 4304.40234375\n",
      "Epoch 8347, Train_Loss: 4146.69921875, Val_Loss: 4304.3017578125\n",
      "Epoch 8348, Train_Loss: 4146.6123046875, Val_Loss: 4304.1943359375\n",
      "Epoch 8349, Train_Loss: 4146.541015625, Val_Loss: 4304.08251953125\n",
      "Epoch 8350, Train_Loss: 4146.46044921875, Val_Loss: 4303.97412109375\n",
      "Epoch 8351, Train_Loss: 4146.41357421875, Val_Loss: 4303.85009765625\n",
      "Epoch 8352, Train_Loss: 4146.3291015625, Val_Loss: 4303.751953125\n",
      "Epoch 8353, Train_Loss: 4146.24658203125, Val_Loss: 4303.6396484375\n",
      "Epoch 8354, Train_Loss: 4146.16162109375, Val_Loss: 4303.5517578125\n",
      "Epoch 8355, Train_Loss: 4146.0205078125, Val_Loss: 4303.4287109375\n",
      "Epoch 8356, Train_Loss: 4145.9560546875, Val_Loss: 4303.32421875\n",
      "Epoch 8357, Train_Loss: 4145.87841796875, Val_Loss: 4303.21484375\n",
      "Epoch 8358, Train_Loss: 4145.7939453125, Val_Loss: 4303.09716796875\n",
      "Epoch 8359, Train_Loss: 4145.71044921875, Val_Loss: 4302.98583984375\n",
      "Epoch 8360, Train_Loss: 4145.62353515625, Val_Loss: 4302.875\n",
      "Epoch 8361, Train_Loss: 4145.482421875, Val_Loss: 4302.78271484375\n",
      "Epoch 8362, Train_Loss: 4145.39794921875, Val_Loss: 4302.671875\n",
      "Epoch 8363, Train_Loss: 4145.30615234375, Val_Loss: 4302.556640625\n",
      "Epoch 8364, Train_Loss: 4145.2216796875, Val_Loss: 4302.4443359375\n",
      "Epoch 8365, Train_Loss: 4145.0712890625, Val_Loss: 4302.3291015625\n",
      "Epoch 8366, Train_Loss: 4144.97021484375, Val_Loss: 4302.22021484375\n",
      "Epoch 8367, Train_Loss: 4144.880859375, Val_Loss: 4302.107421875\n",
      "Epoch 8368, Train_Loss: 4144.79248046875, Val_Loss: 4302.0166015625\n",
      "Epoch 8369, Train_Loss: 4144.71337890625, Val_Loss: 4301.90380859375\n",
      "Epoch 8370, Train_Loss: 4144.63330078125, Val_Loss: 4301.7998046875\n",
      "Epoch 8371, Train_Loss: 4144.55029296875, Val_Loss: 4301.68115234375\n",
      "Epoch 8372, Train_Loss: 4144.474609375, Val_Loss: 4301.5693359375\n",
      "Epoch 8373, Train_Loss: 4144.38916015625, Val_Loss: 4301.455078125\n",
      "Epoch 8374, Train_Loss: 4144.30810546875, Val_Loss: 4301.34765625\n",
      "Epoch 8375, Train_Loss: 4144.17822265625, Val_Loss: 4301.2412109375\n",
      "Epoch 8376, Train_Loss: 4144.06982421875, Val_Loss: 4301.13720703125\n",
      "Epoch 8377, Train_Loss: 4143.98388671875, Val_Loss: 4301.02685546875\n",
      "Epoch 8378, Train_Loss: 4143.9033203125, Val_Loss: 4300.90283203125\n",
      "Epoch 8379, Train_Loss: 4143.81787109375, Val_Loss: 4300.79736328125\n",
      "Epoch 8380, Train_Loss: 4143.72607421875, Val_Loss: 4300.69384765625\n",
      "Epoch 8381, Train_Loss: 4143.6376953125, Val_Loss: 4300.58935546875\n",
      "Epoch 8382, Train_Loss: 4143.5517578125, Val_Loss: 4300.48828125\n",
      "Epoch 8383, Train_Loss: 4143.46728515625, Val_Loss: 4300.38330078125\n",
      "Epoch 8384, Train_Loss: 4143.384765625, Val_Loss: 4300.24267578125\n",
      "Epoch 8385, Train_Loss: 4143.3037109375, Val_Loss: 4300.12060546875\n",
      "Epoch 8386, Train_Loss: 4143.21484375, Val_Loss: 4300.0234375\n",
      "Epoch 8387, Train_Loss: 4143.126953125, Val_Loss: 4299.900390625\n",
      "Epoch 8388, Train_Loss: 4143.03076171875, Val_Loss: 4299.8037109375\n",
      "Epoch 8389, Train_Loss: 4142.9501953125, Val_Loss: 4299.701171875\n",
      "Epoch 8390, Train_Loss: 4142.86767578125, Val_Loss: 4299.5830078125\n",
      "Epoch 8391, Train_Loss: 4142.771484375, Val_Loss: 4299.48779296875\n",
      "Epoch 8392, Train_Loss: 4142.7021484375, Val_Loss: 4299.36767578125\n",
      "Epoch 8393, Train_Loss: 4142.61767578125, Val_Loss: 4299.2470703125\n",
      "Epoch 8394, Train_Loss: 4142.52587890625, Val_Loss: 4299.14501953125\n",
      "Epoch 8395, Train_Loss: 4142.44287109375, Val_Loss: 4299.0458984375\n",
      "Epoch 8396, Train_Loss: 4142.36376953125, Val_Loss: 4298.93701171875\n",
      "Epoch 8397, Train_Loss: 4142.26904296875, Val_Loss: 4298.8369140625\n",
      "Epoch 8398, Train_Loss: 4142.18310546875, Val_Loss: 4298.73388671875\n",
      "Epoch 8399, Train_Loss: 4142.109375, Val_Loss: 4298.61376953125\n",
      "Epoch 8400, Train_Loss: 4142.0283203125, Val_Loss: 4298.49853515625\n",
      "Epoch 8401, Train_Loss: 4141.95556640625, Val_Loss: 4298.392578125\n",
      "Epoch 8402, Train_Loss: 4141.845703125, Val_Loss: 4298.30224609375\n",
      "Epoch 8403, Train_Loss: 4141.76513671875, Val_Loss: 4298.19482421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8404, Train_Loss: 4141.67578125, Val_Loss: 4298.08251953125\n",
      "Epoch 8405, Train_Loss: 4141.59716796875, Val_Loss: 4297.970703125\n",
      "Epoch 8406, Train_Loss: 4141.55322265625, Val_Loss: 4297.859375\n",
      "Epoch 8407, Train_Loss: 4141.44580078125, Val_Loss: 4297.744140625\n",
      "Epoch 8408, Train_Loss: 4141.35400390625, Val_Loss: 4297.6328125\n",
      "Epoch 8409, Train_Loss: 4141.2705078125, Val_Loss: 4297.53955078125\n",
      "Epoch 8410, Train_Loss: 4141.19970703125, Val_Loss: 4297.4326171875\n",
      "Epoch 8411, Train_Loss: 4141.1220703125, Val_Loss: 4297.31494140625\n",
      "Epoch 8412, Train_Loss: 4141.04833984375, Val_Loss: 4297.203125\n",
      "Epoch 8413, Train_Loss: 4140.9599609375, Val_Loss: 4297.09716796875\n",
      "Epoch 8414, Train_Loss: 4140.8662109375, Val_Loss: 4296.96826171875\n",
      "Epoch 8415, Train_Loss: 4140.78564453125, Val_Loss: 4296.87060546875\n",
      "Epoch 8416, Train_Loss: 4140.654296875, Val_Loss: 4296.767578125\n",
      "Epoch 8417, Train_Loss: 4140.57080078125, Val_Loss: 4296.66064453125\n",
      "Epoch 8418, Train_Loss: 4140.470703125, Val_Loss: 4296.556640625\n",
      "Epoch 8419, Train_Loss: 4140.38720703125, Val_Loss: 4296.435546875\n",
      "Epoch 8420, Train_Loss: 4140.30078125, Val_Loss: 4296.33056640625\n",
      "Epoch 8421, Train_Loss: 4140.2138671875, Val_Loss: 4296.212890625\n",
      "Epoch 8422, Train_Loss: 4140.1357421875, Val_Loss: 4296.11474609375\n",
      "Epoch 8423, Train_Loss: 4140.03271484375, Val_Loss: 4295.98583984375\n",
      "Epoch 8424, Train_Loss: 4139.95458984375, Val_Loss: 4295.875\n",
      "Epoch 8425, Train_Loss: 4139.85400390625, Val_Loss: 4295.76220703125\n",
      "Epoch 8426, Train_Loss: 4139.75537109375, Val_Loss: 4295.6474609375\n",
      "Epoch 8427, Train_Loss: 4139.67529296875, Val_Loss: 4295.54541015625\n",
      "Epoch 8428, Train_Loss: 4139.5712890625, Val_Loss: 4295.4423828125\n",
      "Epoch 8429, Train_Loss: 4139.490234375, Val_Loss: 4295.32861328125\n",
      "Epoch 8430, Train_Loss: 4139.3798828125, Val_Loss: 4295.23779296875\n",
      "Epoch 8431, Train_Loss: 4139.30908203125, Val_Loss: 4295.12060546875\n",
      "Epoch 8432, Train_Loss: 4139.22412109375, Val_Loss: 4295.009765625\n",
      "Epoch 8433, Train_Loss: 4139.13623046875, Val_Loss: 4294.89208984375\n",
      "Epoch 8434, Train_Loss: 4139.05419921875, Val_Loss: 4294.78759765625\n",
      "Epoch 8435, Train_Loss: 4138.978515625, Val_Loss: 4294.6884765625\n",
      "Epoch 8436, Train_Loss: 4138.90087890625, Val_Loss: 4294.57958984375\n",
      "Epoch 8437, Train_Loss: 4138.826171875, Val_Loss: 4294.47607421875\n",
      "Epoch 8438, Train_Loss: 4138.7177734375, Val_Loss: 4294.3671875\n",
      "Epoch 8439, Train_Loss: 4138.64794921875, Val_Loss: 4294.25537109375\n",
      "Epoch 8440, Train_Loss: 4138.56884765625, Val_Loss: 4294.13623046875\n",
      "Epoch 8441, Train_Loss: 4138.48388671875, Val_Loss: 4294.0283203125\n",
      "Epoch 8442, Train_Loss: 4138.390625, Val_Loss: 4293.92822265625\n",
      "Epoch 8443, Train_Loss: 4138.24462890625, Val_Loss: 4293.83447265625\n",
      "Epoch 8444, Train_Loss: 4138.16357421875, Val_Loss: 4293.72265625\n",
      "Epoch 8445, Train_Loss: 4138.09521484375, Val_Loss: 4293.609375\n",
      "Epoch 8446, Train_Loss: 4138.00341796875, Val_Loss: 4293.5146484375\n",
      "Epoch 8447, Train_Loss: 4137.921875, Val_Loss: 4293.37744140625\n",
      "Epoch 8448, Train_Loss: 4137.8486328125, Val_Loss: 4293.27978515625\n",
      "Epoch 8449, Train_Loss: 4137.7529296875, Val_Loss: 4293.18798828125\n",
      "Epoch 8450, Train_Loss: 4137.662109375, Val_Loss: 4293.07763671875\n",
      "Epoch 8451, Train_Loss: 4137.587890625, Val_Loss: 4292.97705078125\n",
      "Epoch 8452, Train_Loss: 4137.50341796875, Val_Loss: 4292.8564453125\n",
      "Epoch 8453, Train_Loss: 4137.416015625, Val_Loss: 4292.75439453125\n",
      "Epoch 8454, Train_Loss: 4137.33837890625, Val_Loss: 4292.6376953125\n",
      "Epoch 8455, Train_Loss: 4137.25390625, Val_Loss: 4292.52294921875\n",
      "Epoch 8456, Train_Loss: 4137.1875, Val_Loss: 4292.42041015625\n",
      "Epoch 8457, Train_Loss: 4137.07177734375, Val_Loss: 4292.3203125\n",
      "Epoch 8458, Train_Loss: 4136.98876953125, Val_Loss: 4292.21435546875\n",
      "Epoch 8459, Train_Loss: 4136.90478515625, Val_Loss: 4292.109375\n",
      "Epoch 8460, Train_Loss: 4136.81640625, Val_Loss: 4291.99462890625\n",
      "Epoch 8461, Train_Loss: 4136.77685546875, Val_Loss: 4291.87939453125\n",
      "Epoch 8462, Train_Loss: 4136.67529296875, Val_Loss: 4291.74462890625\n",
      "Epoch 8463, Train_Loss: 4136.60595703125, Val_Loss: 4291.640625\n",
      "Epoch 8464, Train_Loss: 4136.51904296875, Val_Loss: 4291.544921875\n",
      "Epoch 8465, Train_Loss: 4136.4443359375, Val_Loss: 4291.43408203125\n",
      "Epoch 8466, Train_Loss: 4136.3603515625, Val_Loss: 4291.31103515625\n",
      "Epoch 8467, Train_Loss: 4136.2734375, Val_Loss: 4291.2001953125\n",
      "Epoch 8468, Train_Loss: 4136.1845703125, Val_Loss: 4291.09228515625\n",
      "Epoch 8469, Train_Loss: 4136.10498046875, Val_Loss: 4290.9931640625\n",
      "Epoch 8470, Train_Loss: 4136.0107421875, Val_Loss: 4290.88134765625\n",
      "Epoch 8471, Train_Loss: 4135.88232421875, Val_Loss: 4290.783203125\n",
      "Epoch 8472, Train_Loss: 4135.80029296875, Val_Loss: 4290.68505859375\n",
      "Epoch 8473, Train_Loss: 4135.7265625, Val_Loss: 4290.576171875\n",
      "Epoch 8474, Train_Loss: 4135.6494140625, Val_Loss: 4290.455078125\n",
      "Epoch 8475, Train_Loss: 4135.5712890625, Val_Loss: 4290.34619140625\n",
      "Epoch 8476, Train_Loss: 4135.48486328125, Val_Loss: 4290.244140625\n",
      "Epoch 8477, Train_Loss: 4135.416015625, Val_Loss: 4290.14013671875\n",
      "Epoch 8478, Train_Loss: 4135.32958984375, Val_Loss: 4290.0478515625\n",
      "Epoch 8479, Train_Loss: 4135.2333984375, Val_Loss: 4289.9326171875\n",
      "Epoch 8480, Train_Loss: 4135.119140625, Val_Loss: 4289.82861328125\n",
      "Epoch 8481, Train_Loss: 4135.017578125, Val_Loss: 4289.70263671875\n",
      "Epoch 8482, Train_Loss: 4134.94189453125, Val_Loss: 4289.603515625\n",
      "Epoch 8483, Train_Loss: 4134.85791015625, Val_Loss: 4289.494140625\n",
      "Epoch 8484, Train_Loss: 4134.77685546875, Val_Loss: 4289.38525390625\n",
      "Epoch 8485, Train_Loss: 4134.64599609375, Val_Loss: 4289.2939453125\n",
      "Epoch 8486, Train_Loss: 4134.56689453125, Val_Loss: 4289.17822265625\n",
      "Epoch 8487, Train_Loss: 4134.47802734375, Val_Loss: 4289.06982421875\n",
      "Epoch 8488, Train_Loss: 4134.39453125, Val_Loss: 4288.96630859375\n",
      "Epoch 8489, Train_Loss: 4134.3125, Val_Loss: 4288.85302734375\n",
      "Epoch 8490, Train_Loss: 4134.22119140625, Val_Loss: 4288.74462890625\n",
      "Epoch 8491, Train_Loss: 4134.146484375, Val_Loss: 4288.63232421875\n",
      "Epoch 8492, Train_Loss: 4134.06201171875, Val_Loss: 4288.52880859375\n",
      "Epoch 8493, Train_Loss: 4133.98779296875, Val_Loss: 4288.42333984375\n",
      "Epoch 8494, Train_Loss: 4133.91357421875, Val_Loss: 4288.32177734375\n",
      "Epoch 8495, Train_Loss: 4133.8291015625, Val_Loss: 4288.1982421875\n",
      "Epoch 8496, Train_Loss: 4133.7421875, Val_Loss: 4288.09765625\n",
      "Epoch 8497, Train_Loss: 4133.6494140625, Val_Loss: 4287.99755859375\n",
      "Epoch 8498, Train_Loss: 4133.54443359375, Val_Loss: 4287.90625\n",
      "Epoch 8499, Train_Loss: 4133.47314453125, Val_Loss: 4287.78466796875\n",
      "Epoch 8500, Train_Loss: 4133.39599609375, Val_Loss: 4287.6845703125\n",
      "Epoch 8501, Train_Loss: 4133.3017578125, Val_Loss: 4287.5498046875\n",
      "Epoch 8502, Train_Loss: 4133.22802734375, Val_Loss: 4287.41748046875\n",
      "Epoch 8503, Train_Loss: 4133.166015625, Val_Loss: 4287.330078125\n",
      "Epoch 8504, Train_Loss: 4133.099609375, Val_Loss: 4287.22265625\n",
      "Epoch 8505, Train_Loss: 4133.0, Val_Loss: 4287.115234375\n",
      "Epoch 8506, Train_Loss: 4132.90234375, Val_Loss: 4287.025390625\n",
      "Epoch 8507, Train_Loss: 4132.82666015625, Val_Loss: 4286.9072265625\n",
      "Epoch 8508, Train_Loss: 4132.7412109375, Val_Loss: 4286.80517578125\n",
      "Epoch 8509, Train_Loss: 4132.66259765625, Val_Loss: 4286.6865234375\n",
      "Epoch 8510, Train_Loss: 4132.4921875, Val_Loss: 4286.58984375\n",
      "Epoch 8511, Train_Loss: 4132.41748046875, Val_Loss: 4286.48486328125\n",
      "Epoch 8512, Train_Loss: 4132.3193359375, Val_Loss: 4286.38818359375\n",
      "Epoch 8513, Train_Loss: 4132.2421875, Val_Loss: 4286.27783203125\n",
      "Epoch 8514, Train_Loss: 4132.154296875, Val_Loss: 4286.17578125\n",
      "Epoch 8515, Train_Loss: 4132.07080078125, Val_Loss: 4286.0712890625\n",
      "Epoch 8516, Train_Loss: 4132.033203125, Val_Loss: 4285.95849609375\n",
      "Epoch 8517, Train_Loss: 4131.95166015625, Val_Loss: 4285.8486328125\n",
      "Epoch 8518, Train_Loss: 4131.86376953125, Val_Loss: 4285.7421875\n",
      "Epoch 8519, Train_Loss: 4131.79150390625, Val_Loss: 4285.6533203125\n",
      "Epoch 8520, Train_Loss: 4131.6640625, Val_Loss: 4285.5361328125\n",
      "Epoch 8521, Train_Loss: 4131.5888671875, Val_Loss: 4285.43701171875\n",
      "Epoch 8522, Train_Loss: 4131.49462890625, Val_Loss: 4285.33056640625\n",
      "Epoch 8523, Train_Loss: 4131.42138671875, Val_Loss: 4285.20556640625\n",
      "Epoch 8524, Train_Loss: 4131.3330078125, Val_Loss: 4285.09765625\n",
      "Epoch 8525, Train_Loss: 4131.2490234375, Val_Loss: 4284.9970703125\n",
      "Epoch 8526, Train_Loss: 4131.1103515625, Val_Loss: 4284.89990234375\n",
      "Epoch 8527, Train_Loss: 4131.03125, Val_Loss: 4284.796875\n",
      "Epoch 8528, Train_Loss: 4130.9521484375, Val_Loss: 4284.6904296875\n",
      "Epoch 8529, Train_Loss: 4130.873046875, Val_Loss: 4284.57958984375\n",
      "Epoch 8530, Train_Loss: 4130.78271484375, Val_Loss: 4284.45947265625\n",
      "Epoch 8531, Train_Loss: 4130.6962890625, Val_Loss: 4284.36181640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8532, Train_Loss: 4130.60595703125, Val_Loss: 4284.26123046875\n",
      "Epoch 8533, Train_Loss: 4130.498046875, Val_Loss: 4284.16162109375\n",
      "Epoch 8534, Train_Loss: 4130.41943359375, Val_Loss: 4284.05859375\n",
      "Epoch 8535, Train_Loss: 4130.33984375, Val_Loss: 4283.9609375\n",
      "Epoch 8536, Train_Loss: 4130.2646484375, Val_Loss: 4283.8427734375\n",
      "Epoch 8537, Train_Loss: 4130.1748046875, Val_Loss: 4283.73681640625\n",
      "Epoch 8538, Train_Loss: 4130.09375, Val_Loss: 4283.62744140625\n",
      "Epoch 8539, Train_Loss: 4130.021484375, Val_Loss: 4283.525390625\n",
      "Epoch 8540, Train_Loss: 4129.91357421875, Val_Loss: 4283.3994140625\n",
      "Epoch 8541, Train_Loss: 4129.837890625, Val_Loss: 4283.296875\n",
      "Epoch 8542, Train_Loss: 4129.74169921875, Val_Loss: 4283.19921875\n",
      "Epoch 8543, Train_Loss: 4129.66845703125, Val_Loss: 4283.07568359375\n",
      "Epoch 8544, Train_Loss: 4129.59765625, Val_Loss: 4282.9775390625\n",
      "Epoch 8545, Train_Loss: 4129.51513671875, Val_Loss: 4282.87158203125\n",
      "Epoch 8546, Train_Loss: 4129.4384765625, Val_Loss: 4282.76220703125\n",
      "Epoch 8547, Train_Loss: 4129.36376953125, Val_Loss: 4282.67724609375\n",
      "Epoch 8548, Train_Loss: 4129.27783203125, Val_Loss: 4282.5771484375\n",
      "Epoch 8549, Train_Loss: 4129.197265625, Val_Loss: 4282.46923828125\n",
      "Epoch 8550, Train_Loss: 4129.11474609375, Val_Loss: 4282.3515625\n",
      "Epoch 8551, Train_Loss: 4129.05029296875, Val_Loss: 4282.2490234375\n",
      "Epoch 8552, Train_Loss: 4128.96435546875, Val_Loss: 4282.14697265625\n",
      "Epoch 8553, Train_Loss: 4128.87890625, Val_Loss: 4282.03955078125\n",
      "Epoch 8554, Train_Loss: 4128.79833984375, Val_Loss: 4281.9560546875\n",
      "Epoch 8555, Train_Loss: 4128.73486328125, Val_Loss: 4281.84228515625\n",
      "Epoch 8556, Train_Loss: 4128.66796875, Val_Loss: 4281.73486328125\n",
      "Epoch 8557, Train_Loss: 4128.58837890625, Val_Loss: 4281.6240234375\n",
      "Epoch 8558, Train_Loss: 4128.50048828125, Val_Loss: 4281.51953125\n",
      "Epoch 8559, Train_Loss: 4128.40478515625, Val_Loss: 4281.41259765625\n",
      "Epoch 8560, Train_Loss: 4128.3232421875, Val_Loss: 4281.32275390625\n",
      "Epoch 8561, Train_Loss: 4128.24169921875, Val_Loss: 4281.212890625\n",
      "Epoch 8562, Train_Loss: 4128.16064453125, Val_Loss: 4281.1162109375\n",
      "Epoch 8563, Train_Loss: 4128.06982421875, Val_Loss: 4281.01513671875\n",
      "Epoch 8564, Train_Loss: 4127.9873046875, Val_Loss: 4280.89697265625\n",
      "Epoch 8565, Train_Loss: 4127.9033203125, Val_Loss: 4280.7958984375\n",
      "Epoch 8566, Train_Loss: 4127.82275390625, Val_Loss: 4280.68896484375\n",
      "Epoch 8567, Train_Loss: 4127.71826171875, Val_Loss: 4280.58544921875\n",
      "Epoch 8568, Train_Loss: 4127.63623046875, Val_Loss: 4280.5009765625\n",
      "Epoch 8569, Train_Loss: 4127.548828125, Val_Loss: 4280.375\n",
      "Epoch 8570, Train_Loss: 4127.46875, Val_Loss: 4280.27734375\n",
      "Epoch 8571, Train_Loss: 4127.43359375, Val_Loss: 4280.1650390625\n",
      "Epoch 8572, Train_Loss: 4127.35693359375, Val_Loss: 4280.06298828125\n",
      "Epoch 8573, Train_Loss: 4127.265625, Val_Loss: 4279.96044921875\n",
      "Epoch 8574, Train_Loss: 4127.17919921875, Val_Loss: 4279.86376953125\n",
      "Epoch 8575, Train_Loss: 4127.09521484375, Val_Loss: 4279.77099609375\n",
      "Epoch 8576, Train_Loss: 4127.0283203125, Val_Loss: 4279.6708984375\n",
      "Epoch 8577, Train_Loss: 4126.95263671875, Val_Loss: 4279.56640625\n",
      "Epoch 8578, Train_Loss: 4126.87548828125, Val_Loss: 4279.451171875\n",
      "Epoch 8579, Train_Loss: 4126.7880859375, Val_Loss: 4279.3115234375\n",
      "Epoch 8580, Train_Loss: 4126.71875, Val_Loss: 4279.2119140625\n",
      "Epoch 8581, Train_Loss: 4126.58447265625, Val_Loss: 4279.119140625\n",
      "Epoch 8582, Train_Loss: 4126.50732421875, Val_Loss: 4279.0166015625\n",
      "Epoch 8583, Train_Loss: 4126.43505859375, Val_Loss: 4278.9130859375\n",
      "Epoch 8584, Train_Loss: 4126.31982421875, Val_Loss: 4278.810546875\n",
      "Epoch 8585, Train_Loss: 4126.22998046875, Val_Loss: 4278.69384765625\n",
      "Epoch 8586, Train_Loss: 4126.14697265625, Val_Loss: 4278.59228515625\n",
      "Epoch 8587, Train_Loss: 4126.05908203125, Val_Loss: 4278.50244140625\n",
      "Epoch 8588, Train_Loss: 4125.9609375, Val_Loss: 4278.40673828125\n",
      "Epoch 8589, Train_Loss: 4125.87451171875, Val_Loss: 4278.3017578125\n",
      "Epoch 8590, Train_Loss: 4125.8056640625, Val_Loss: 4278.20263671875\n",
      "Epoch 8591, Train_Loss: 4125.74560546875, Val_Loss: 4278.091796875\n",
      "Epoch 8592, Train_Loss: 4125.67431640625, Val_Loss: 4277.98583984375\n",
      "Epoch 8593, Train_Loss: 4125.60498046875, Val_Loss: 4277.880859375\n",
      "Epoch 8594, Train_Loss: 4125.5302734375, Val_Loss: 4277.77978515625\n",
      "Epoch 8595, Train_Loss: 4125.4091796875, Val_Loss: 4277.6884765625\n",
      "Epoch 8596, Train_Loss: 4125.3427734375, Val_Loss: 4277.58154296875\n",
      "Epoch 8597, Train_Loss: 4125.2626953125, Val_Loss: 4277.4755859375\n",
      "Epoch 8598, Train_Loss: 4125.037109375, Val_Loss: 4277.3671875\n",
      "Epoch 8599, Train_Loss: 4124.962890625, Val_Loss: 4277.263671875\n",
      "Epoch 8600, Train_Loss: 4124.87255859375, Val_Loss: 4277.16845703125\n",
      "Epoch 8601, Train_Loss: 4124.7919921875, Val_Loss: 4277.06982421875\n",
      "Epoch 8602, Train_Loss: 4124.72607421875, Val_Loss: 4276.9697265625\n",
      "Epoch 8603, Train_Loss: 4124.6484375, Val_Loss: 4276.869140625\n",
      "Epoch 8604, Train_Loss: 4124.57666015625, Val_Loss: 4276.76513671875\n",
      "Epoch 8605, Train_Loss: 4124.47119140625, Val_Loss: 4276.65576171875\n",
      "Epoch 8606, Train_Loss: 4124.39697265625, Val_Loss: 4276.55224609375\n",
      "Epoch 8607, Train_Loss: 4124.3134765625, Val_Loss: 4276.44580078125\n",
      "Epoch 8608, Train_Loss: 4124.23486328125, Val_Loss: 4276.3466796875\n",
      "Epoch 8609, Train_Loss: 4124.13232421875, Val_Loss: 4276.25537109375\n",
      "Epoch 8610, Train_Loss: 4124.048828125, Val_Loss: 4276.1337890625\n",
      "Epoch 8611, Train_Loss: 4123.9755859375, Val_Loss: 4276.04052734375\n",
      "Epoch 8612, Train_Loss: 4123.908203125, Val_Loss: 4275.92919921875\n",
      "Epoch 8613, Train_Loss: 4123.8330078125, Val_Loss: 4275.8251953125\n",
      "Epoch 8614, Train_Loss: 4123.7646484375, Val_Loss: 4275.7353515625\n",
      "Epoch 8615, Train_Loss: 4123.68701171875, Val_Loss: 4275.626953125\n",
      "Epoch 8616, Train_Loss: 4123.58984375, Val_Loss: 4275.52880859375\n",
      "Epoch 8617, Train_Loss: 4123.4990234375, Val_Loss: 4275.43701171875\n",
      "Epoch 8618, Train_Loss: 4123.40576171875, Val_Loss: 4275.3017578125\n",
      "Epoch 8619, Train_Loss: 4123.32958984375, Val_Loss: 4275.18359375\n",
      "Epoch 8620, Train_Loss: 4123.25244140625, Val_Loss: 4275.08056640625\n",
      "Epoch 8621, Train_Loss: 4123.1650390625, Val_Loss: 4274.98876953125\n",
      "Epoch 8622, Train_Loss: 4123.0859375, Val_Loss: 4274.8798828125\n",
      "Epoch 8623, Train_Loss: 4122.98779296875, Val_Loss: 4274.78662109375\n",
      "Epoch 8624, Train_Loss: 4122.908203125, Val_Loss: 4274.69580078125\n",
      "Epoch 8625, Train_Loss: 4122.8349609375, Val_Loss: 4274.58984375\n",
      "Epoch 8626, Train_Loss: 4122.7890625, Val_Loss: 4274.4765625\n",
      "Epoch 8627, Train_Loss: 4122.69873046875, Val_Loss: 4274.36962890625\n",
      "Epoch 8628, Train_Loss: 4122.62451171875, Val_Loss: 4274.2646484375\n",
      "Epoch 8629, Train_Loss: 4122.5517578125, Val_Loss: 4274.17236328125\n",
      "Epoch 8630, Train_Loss: 4122.470703125, Val_Loss: 4274.08203125\n",
      "Epoch 8631, Train_Loss: 4122.37744140625, Val_Loss: 4273.978515625\n",
      "Epoch 8632, Train_Loss: 4122.30126953125, Val_Loss: 4273.876953125\n",
      "Epoch 8633, Train_Loss: 4122.232421875, Val_Loss: 4273.759765625\n",
      "Epoch 8634, Train_Loss: 4122.1611328125, Val_Loss: 4273.66357421875\n",
      "Epoch 8635, Train_Loss: 4122.0751953125, Val_Loss: 4273.56103515625\n",
      "Epoch 8636, Train_Loss: 4122.0126953125, Val_Loss: 4273.45654296875\n",
      "Epoch 8637, Train_Loss: 4121.8583984375, Val_Loss: 4273.37060546875\n",
      "Epoch 8638, Train_Loss: 4121.78271484375, Val_Loss: 4273.25537109375\n",
      "Epoch 8639, Train_Loss: 4121.7255859375, Val_Loss: 4273.1572265625\n",
      "Epoch 8640, Train_Loss: 4121.6572265625, Val_Loss: 4273.05224609375\n",
      "Epoch 8641, Train_Loss: 4121.5693359375, Val_Loss: 4272.93603515625\n",
      "Epoch 8642, Train_Loss: 4121.50048828125, Val_Loss: 4272.84716796875\n",
      "Epoch 8643, Train_Loss: 4121.4208984375, Val_Loss: 4272.73779296875\n",
      "Epoch 8644, Train_Loss: 4121.34423828125, Val_Loss: 4272.64501953125\n",
      "Epoch 8645, Train_Loss: 4121.263671875, Val_Loss: 4272.54931640625\n",
      "Epoch 8646, Train_Loss: 4121.17919921875, Val_Loss: 4272.44580078125\n",
      "Epoch 8647, Train_Loss: 4121.08251953125, Val_Loss: 4272.3251953125\n",
      "Epoch 8648, Train_Loss: 4121.0048828125, Val_Loss: 4272.224609375\n",
      "Epoch 8649, Train_Loss: 4120.923828125, Val_Loss: 4272.12353515625\n",
      "Epoch 8650, Train_Loss: 4120.84130859375, Val_Loss: 4272.021484375\n",
      "Epoch 8651, Train_Loss: 4120.728515625, Val_Loss: 4271.9228515625\n",
      "Epoch 8652, Train_Loss: 4120.6630859375, Val_Loss: 4271.83056640625\n",
      "Epoch 8653, Train_Loss: 4120.58349609375, Val_Loss: 4271.73583984375\n",
      "Epoch 8654, Train_Loss: 4120.521484375, Val_Loss: 4271.6103515625\n",
      "Epoch 8655, Train_Loss: 4120.44921875, Val_Loss: 4271.50830078125\n",
      "Epoch 8656, Train_Loss: 4120.3779296875, Val_Loss: 4271.41162109375\n",
      "Epoch 8657, Train_Loss: 4120.27685546875, Val_Loss: 4271.27490234375\n",
      "Epoch 8658, Train_Loss: 4120.1884765625, Val_Loss: 4271.1826171875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8659, Train_Loss: 4120.111328125, Val_Loss: 4271.0849609375\n",
      "Epoch 8660, Train_Loss: 4120.03466796875, Val_Loss: 4270.984375\n",
      "Epoch 8661, Train_Loss: 4119.9775390625, Val_Loss: 4270.87353515625\n",
      "Epoch 8662, Train_Loss: 4119.89453125, Val_Loss: 4270.77587890625\n",
      "Epoch 8663, Train_Loss: 4119.80224609375, Val_Loss: 4270.67822265625\n",
      "Epoch 8664, Train_Loss: 4119.69970703125, Val_Loss: 4270.5791015625\n",
      "Epoch 8665, Train_Loss: 4119.6337890625, Val_Loss: 4270.47607421875\n",
      "Epoch 8666, Train_Loss: 4119.564453125, Val_Loss: 4270.3720703125\n",
      "Epoch 8667, Train_Loss: 4119.498046875, Val_Loss: 4270.2705078125\n",
      "Epoch 8668, Train_Loss: 4119.3994140625, Val_Loss: 4270.1650390625\n",
      "Epoch 8669, Train_Loss: 4119.32373046875, Val_Loss: 4270.0703125\n",
      "Epoch 8670, Train_Loss: 4119.24365234375, Val_Loss: 4269.9658203125\n",
      "Epoch 8671, Train_Loss: 4119.15869140625, Val_Loss: 4269.875\n",
      "Epoch 8672, Train_Loss: 4119.1005859375, Val_Loss: 4269.77734375\n",
      "Epoch 8673, Train_Loss: 4119.0078125, Val_Loss: 4269.6669921875\n",
      "Epoch 8674, Train_Loss: 4118.93359375, Val_Loss: 4269.56982421875\n",
      "Epoch 8675, Train_Loss: 4118.86328125, Val_Loss: 4269.4619140625\n",
      "Epoch 8676, Train_Loss: 4118.7314453125, Val_Loss: 4269.36376953125\n",
      "Epoch 8677, Train_Loss: 4118.6572265625, Val_Loss: 4269.25537109375\n",
      "Epoch 8678, Train_Loss: 4118.548828125, Val_Loss: 4269.171875\n",
      "Epoch 8679, Train_Loss: 4118.46337890625, Val_Loss: 4269.07373046875\n",
      "Epoch 8680, Train_Loss: 4118.3876953125, Val_Loss: 4268.96728515625\n",
      "Epoch 8681, Train_Loss: 4118.3193359375, Val_Loss: 4268.87109375\n",
      "Epoch 8682, Train_Loss: 4118.28662109375, Val_Loss: 4268.7490234375\n",
      "Epoch 8683, Train_Loss: 4118.21142578125, Val_Loss: 4268.6552734375\n",
      "Epoch 8684, Train_Loss: 4118.12744140625, Val_Loss: 4268.54931640625\n",
      "Epoch 8685, Train_Loss: 4118.0458984375, Val_Loss: 4268.4599609375\n",
      "Epoch 8686, Train_Loss: 4117.97607421875, Val_Loss: 4268.3623046875\n",
      "Epoch 8687, Train_Loss: 4117.89306640625, Val_Loss: 4268.2626953125\n",
      "Epoch 8688, Train_Loss: 4117.82568359375, Val_Loss: 4268.1611328125\n",
      "Epoch 8689, Train_Loss: 4117.740234375, Val_Loss: 4268.060546875\n",
      "Epoch 8690, Train_Loss: 4117.68017578125, Val_Loss: 4267.9599609375\n",
      "Epoch 8691, Train_Loss: 4117.603515625, Val_Loss: 4267.85302734375\n",
      "Epoch 8692, Train_Loss: 4117.4580078125, Val_Loss: 4267.7734375\n",
      "Epoch 8693, Train_Loss: 4117.38232421875, Val_Loss: 4267.66845703125\n",
      "Epoch 8694, Train_Loss: 4117.310546875, Val_Loss: 4267.5732421875\n",
      "Epoch 8695, Train_Loss: 4117.22216796875, Val_Loss: 4267.47314453125\n",
      "Epoch 8696, Train_Loss: 4117.14306640625, Val_Loss: 4267.33203125\n",
      "Epoch 8697, Train_Loss: 4117.07421875, Val_Loss: 4267.2353515625\n",
      "Epoch 8698, Train_Loss: 4116.99169921875, Val_Loss: 4267.130859375\n",
      "Epoch 8699, Train_Loss: 4116.9130859375, Val_Loss: 4267.04296875\n",
      "Epoch 8700, Train_Loss: 4116.810546875, Val_Loss: 4266.94140625\n",
      "Epoch 8701, Train_Loss: 4116.7197265625, Val_Loss: 4266.8408203125\n",
      "Epoch 8702, Train_Loss: 4116.65185546875, Val_Loss: 4266.7353515625\n",
      "Epoch 8703, Train_Loss: 4116.58056640625, Val_Loss: 4266.640625\n",
      "Epoch 8704, Train_Loss: 4116.51220703125, Val_Loss: 4266.5341796875\n",
      "Epoch 8705, Train_Loss: 4116.43310546875, Val_Loss: 4266.43994140625\n",
      "Epoch 8706, Train_Loss: 4116.31982421875, Val_Loss: 4266.35546875\n",
      "Epoch 8707, Train_Loss: 4116.2568359375, Val_Loss: 4266.2607421875\n",
      "Epoch 8708, Train_Loss: 4116.1865234375, Val_Loss: 4266.16259765625\n",
      "Epoch 8709, Train_Loss: 4116.1162109375, Val_Loss: 4266.06982421875\n",
      "Epoch 8710, Train_Loss: 4116.02392578125, Val_Loss: 4265.9658203125\n",
      "Epoch 8711, Train_Loss: 4115.9375, Val_Loss: 4265.86181640625\n",
      "Epoch 8712, Train_Loss: 4115.8486328125, Val_Loss: 4265.75439453125\n",
      "Epoch 8713, Train_Loss: 4115.79638671875, Val_Loss: 4265.67724609375\n",
      "Epoch 8714, Train_Loss: 4115.73095703125, Val_Loss: 4265.57763671875\n",
      "Epoch 8715, Train_Loss: 4115.64794921875, Val_Loss: 4265.48291015625\n",
      "Epoch 8716, Train_Loss: 4115.55322265625, Val_Loss: 4265.37158203125\n",
      "Epoch 8717, Train_Loss: 4115.47900390625, Val_Loss: 4265.25634765625\n",
      "Epoch 8718, Train_Loss: 4115.39892578125, Val_Loss: 4265.158203125\n",
      "Epoch 8719, Train_Loss: 4115.32275390625, Val_Loss: 4265.06396484375\n",
      "Epoch 8720, Train_Loss: 4115.22021484375, Val_Loss: 4264.9697265625\n",
      "Epoch 8721, Train_Loss: 4115.154296875, Val_Loss: 4264.88037109375\n",
      "Epoch 8722, Train_Loss: 4115.08935546875, Val_Loss: 4264.7802734375\n",
      "Epoch 8723, Train_Loss: 4115.01416015625, Val_Loss: 4264.68701171875\n",
      "Epoch 8724, Train_Loss: 4114.9365234375, Val_Loss: 4264.57958984375\n",
      "Epoch 8725, Train_Loss: 4114.85400390625, Val_Loss: 4264.46875\n",
      "Epoch 8726, Train_Loss: 4114.78759765625, Val_Loss: 4264.36865234375\n",
      "Epoch 8727, Train_Loss: 4114.71484375, Val_Loss: 4264.27490234375\n",
      "Epoch 8728, Train_Loss: 4114.6416015625, Val_Loss: 4264.18798828125\n",
      "Epoch 8729, Train_Loss: 4114.556640625, Val_Loss: 4264.0859375\n",
      "Epoch 8730, Train_Loss: 4114.4873046875, Val_Loss: 4263.998046875\n",
      "Epoch 8731, Train_Loss: 4114.39013671875, Val_Loss: 4263.8857421875\n",
      "Epoch 8732, Train_Loss: 4114.318359375, Val_Loss: 4263.78759765625\n",
      "Epoch 8733, Train_Loss: 4114.236328125, Val_Loss: 4263.68896484375\n",
      "Epoch 8734, Train_Loss: 4114.15625, Val_Loss: 4263.6025390625\n",
      "Epoch 8735, Train_Loss: 4114.08154296875, Val_Loss: 4263.4716796875\n",
      "Epoch 8736, Train_Loss: 4114.01416015625, Val_Loss: 4263.37158203125\n",
      "Epoch 8737, Train_Loss: 4113.93212890625, Val_Loss: 4263.28955078125\n",
      "Epoch 8738, Train_Loss: 4113.9013671875, Val_Loss: 4263.1708984375\n",
      "Epoch 8739, Train_Loss: 4113.8349609375, Val_Loss: 4263.07568359375\n",
      "Epoch 8740, Train_Loss: 4113.75830078125, Val_Loss: 4262.97900390625\n",
      "Epoch 8741, Train_Loss: 4113.68115234375, Val_Loss: 4262.888671875\n",
      "Epoch 8742, Train_Loss: 4113.59619140625, Val_Loss: 4262.79248046875\n",
      "Epoch 8743, Train_Loss: 4113.52099609375, Val_Loss: 4262.6875\n",
      "Epoch 8744, Train_Loss: 4113.4453125, Val_Loss: 4262.5927734375\n",
      "Epoch 8745, Train_Loss: 4113.365234375, Val_Loss: 4262.4921875\n",
      "Epoch 8746, Train_Loss: 4113.28173828125, Val_Loss: 4262.390625\n",
      "Epoch 8747, Train_Loss: 4113.19873046875, Val_Loss: 4262.306640625\n",
      "Epoch 8748, Train_Loss: 4113.078125, Val_Loss: 4262.22021484375\n",
      "Epoch 8749, Train_Loss: 4112.990234375, Val_Loss: 4262.123046875\n",
      "Epoch 8750, Train_Loss: 4112.9130859375, Val_Loss: 4262.02880859375\n",
      "Epoch 8751, Train_Loss: 4112.8408203125, Val_Loss: 4261.93115234375\n",
      "Epoch 8752, Train_Loss: 4112.7822265625, Val_Loss: 4261.8251953125\n",
      "Epoch 8753, Train_Loss: 4112.681640625, Val_Loss: 4261.72607421875\n",
      "Epoch 8754, Train_Loss: 4112.61083984375, Val_Loss: 4261.630859375\n",
      "Epoch 8755, Train_Loss: 4112.4228515625, Val_Loss: 4261.5322265625\n",
      "Epoch 8756, Train_Loss: 4112.35595703125, Val_Loss: 4261.4345703125\n",
      "Epoch 8757, Train_Loss: 4112.27392578125, Val_Loss: 4261.3466796875\n",
      "Epoch 8758, Train_Loss: 4112.18994140625, Val_Loss: 4261.244140625\n",
      "Epoch 8759, Train_Loss: 4112.12158203125, Val_Loss: 4261.1337890625\n",
      "Epoch 8760, Train_Loss: 4112.056640625, Val_Loss: 4261.037109375\n",
      "Epoch 8761, Train_Loss: 4111.978515625, Val_Loss: 4260.93701171875\n",
      "Epoch 8762, Train_Loss: 4111.8798828125, Val_Loss: 4260.85498046875\n",
      "Epoch 8763, Train_Loss: 4111.802734375, Val_Loss: 4260.75927734375\n",
      "Epoch 8764, Train_Loss: 4111.732421875, Val_Loss: 4260.6630859375\n",
      "Epoch 8765, Train_Loss: 4111.66357421875, Val_Loss: 4260.5625\n",
      "Epoch 8766, Train_Loss: 4111.61376953125, Val_Loss: 4260.45947265625\n",
      "Epoch 8767, Train_Loss: 4111.544921875, Val_Loss: 4260.3681640625\n",
      "Epoch 8768, Train_Loss: 4111.447265625, Val_Loss: 4260.265625\n",
      "Epoch 8769, Train_Loss: 4111.380859375, Val_Loss: 4260.19091796875\n",
      "Epoch 8770, Train_Loss: 4111.302734375, Val_Loss: 4260.091796875\n",
      "Epoch 8771, Train_Loss: 4111.2275390625, Val_Loss: 4259.99462890625\n",
      "Epoch 8772, Train_Loss: 4111.14794921875, Val_Loss: 4259.8974609375\n",
      "Epoch 8773, Train_Loss: 4111.06640625, Val_Loss: 4259.791015625\n",
      "Epoch 8774, Train_Loss: 4110.9775390625, Val_Loss: 4259.69775390625\n",
      "Epoch 8775, Train_Loss: 4110.8681640625, Val_Loss: 4259.5576171875\n",
      "Epoch 8776, Train_Loss: 4110.79150390625, Val_Loss: 4259.4755859375\n",
      "Epoch 8777, Train_Loss: 4110.728515625, Val_Loss: 4259.3828125\n",
      "Epoch 8778, Train_Loss: 4110.67626953125, Val_Loss: 4259.27783203125\n",
      "Epoch 8779, Train_Loss: 4110.59423828125, Val_Loss: 4259.1884765625\n",
      "Epoch 8780, Train_Loss: 4110.53515625, Val_Loss: 4259.08056640625\n",
      "Epoch 8781, Train_Loss: 4110.46044921875, Val_Loss: 4258.982421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8782, Train_Loss: 4110.3759765625, Val_Loss: 4258.89599609375\n",
      "Epoch 8783, Train_Loss: 4110.3037109375, Val_Loss: 4258.80126953125\n",
      "Epoch 8784, Train_Loss: 4110.2138671875, Val_Loss: 4258.71240234375\n",
      "Epoch 8785, Train_Loss: 4110.14404296875, Val_Loss: 4258.62646484375\n",
      "Epoch 8786, Train_Loss: 4110.06689453125, Val_Loss: 4258.52490234375\n",
      "Epoch 8787, Train_Loss: 4109.98876953125, Val_Loss: 4258.4306640625\n",
      "Epoch 8788, Train_Loss: 4109.91162109375, Val_Loss: 4258.33154296875\n",
      "Epoch 8789, Train_Loss: 4109.830078125, Val_Loss: 4258.2421875\n",
      "Epoch 8790, Train_Loss: 4109.724609375, Val_Loss: 4258.14794921875\n",
      "Epoch 8791, Train_Loss: 4109.64501953125, Val_Loss: 4258.0576171875\n",
      "Epoch 8792, Train_Loss: 4109.58056640625, Val_Loss: 4257.9580078125\n",
      "Epoch 8793, Train_Loss: 4109.52099609375, Val_Loss: 4257.87060546875\n",
      "Epoch 8794, Train_Loss: 4109.49951171875, Val_Loss: 4257.75341796875\n",
      "Epoch 8795, Train_Loss: 4109.38330078125, Val_Loss: 4257.65576171875\n",
      "Epoch 8796, Train_Loss: 4109.31005859375, Val_Loss: 4257.55810546875\n",
      "Epoch 8797, Train_Loss: 4109.24072265625, Val_Loss: 4257.47705078125\n",
      "Epoch 8798, Train_Loss: 4109.1826171875, Val_Loss: 4257.37939453125\n",
      "Epoch 8799, Train_Loss: 4109.10595703125, Val_Loss: 4257.27783203125\n",
      "Epoch 8800, Train_Loss: 4109.0126953125, Val_Loss: 4257.18408203125\n",
      "Epoch 8801, Train_Loss: 4108.9423828125, Val_Loss: 4257.08984375\n",
      "Epoch 8802, Train_Loss: 4108.8759765625, Val_Loss: 4256.98974609375\n",
      "Epoch 8803, Train_Loss: 4108.802734375, Val_Loss: 4256.90283203125\n",
      "Epoch 8804, Train_Loss: 4108.66650390625, Val_Loss: 4256.8154296875\n",
      "Epoch 8805, Train_Loss: 4108.5966796875, Val_Loss: 4256.716796875\n",
      "Epoch 8806, Train_Loss: 4108.5283203125, Val_Loss: 4256.6162109375\n",
      "Epoch 8807, Train_Loss: 4108.46826171875, Val_Loss: 4256.5263671875\n",
      "Epoch 8808, Train_Loss: 4108.419921875, Val_Loss: 4256.42236328125\n",
      "Epoch 8809, Train_Loss: 4108.3525390625, Val_Loss: 4256.3251953125\n",
      "Epoch 8810, Train_Loss: 4108.28955078125, Val_Loss: 4256.22265625\n",
      "Epoch 8811, Train_Loss: 4108.18896484375, Val_Loss: 4256.14697265625\n",
      "Epoch 8812, Train_Loss: 4108.1142578125, Val_Loss: 4256.06103515625\n",
      "Epoch 8813, Train_Loss: 4108.03466796875, Val_Loss: 4255.9521484375\n",
      "Epoch 8814, Train_Loss: 4107.9443359375, Val_Loss: 4255.82421875\n",
      "Epoch 8815, Train_Loss: 4107.87109375, Val_Loss: 4255.72509765625\n",
      "Epoch 8816, Train_Loss: 4107.77880859375, Val_Loss: 4255.6337890625\n",
      "Epoch 8817, Train_Loss: 4107.70263671875, Val_Loss: 4255.54443359375\n",
      "Epoch 8818, Train_Loss: 4107.60693359375, Val_Loss: 4255.45703125\n",
      "Epoch 8819, Train_Loss: 4107.52197265625, Val_Loss: 4255.3681640625\n",
      "Epoch 8820, Train_Loss: 4107.4453125, Val_Loss: 4255.27392578125\n",
      "Epoch 8821, Train_Loss: 4107.369140625, Val_Loss: 4255.1826171875\n",
      "Epoch 8822, Train_Loss: 4107.27490234375, Val_Loss: 4255.0693359375\n",
      "Epoch 8823, Train_Loss: 4107.21337890625, Val_Loss: 4254.974609375\n",
      "Epoch 8824, Train_Loss: 4107.13232421875, Val_Loss: 4254.8935546875\n",
      "Epoch 8825, Train_Loss: 4107.0703125, Val_Loss: 4254.81640625\n",
      "Epoch 8826, Train_Loss: 4106.9912109375, Val_Loss: 4254.7275390625\n",
      "Epoch 8827, Train_Loss: 4106.90625, Val_Loss: 4254.62890625\n",
      "Epoch 8828, Train_Loss: 4106.8330078125, Val_Loss: 4254.537109375\n",
      "Epoch 8829, Train_Loss: 4106.7666015625, Val_Loss: 4254.42578125\n",
      "Epoch 8830, Train_Loss: 4106.7099609375, Val_Loss: 4254.341796875\n",
      "Epoch 8831, Train_Loss: 4106.63916015625, Val_Loss: 4254.24658203125\n",
      "Epoch 8832, Train_Loss: 4106.541015625, Val_Loss: 4254.16796875\n",
      "Epoch 8833, Train_Loss: 4106.46826171875, Val_Loss: 4254.0673828125\n",
      "Epoch 8834, Train_Loss: 4106.36328125, Val_Loss: 4253.97021484375\n",
      "Epoch 8835, Train_Loss: 4106.28564453125, Val_Loss: 4253.8759765625\n",
      "Epoch 8836, Train_Loss: 4106.22900390625, Val_Loss: 4253.78076171875\n",
      "Epoch 8837, Train_Loss: 4106.15234375, Val_Loss: 4253.67529296875\n",
      "Epoch 8838, Train_Loss: 4106.06298828125, Val_Loss: 4253.58056640625\n",
      "Epoch 8839, Train_Loss: 4105.98291015625, Val_Loss: 4253.48193359375\n",
      "Epoch 8840, Train_Loss: 4105.90771484375, Val_Loss: 4253.40869140625\n",
      "Epoch 8841, Train_Loss: 4105.83642578125, Val_Loss: 4253.3115234375\n",
      "Epoch 8842, Train_Loss: 4105.76513671875, Val_Loss: 4253.22900390625\n",
      "Epoch 8843, Train_Loss: 4105.68701171875, Val_Loss: 4253.1240234375\n",
      "Epoch 8844, Train_Loss: 4105.67724609375, Val_Loss: 4253.02685546875\n",
      "Epoch 8845, Train_Loss: 4105.60400390625, Val_Loss: 4252.9306640625\n",
      "Epoch 8846, Train_Loss: 4105.52880859375, Val_Loss: 4252.84130859375\n",
      "Epoch 8847, Train_Loss: 4105.447265625, Val_Loss: 4252.76220703125\n",
      "Epoch 8848, Train_Loss: 4105.37158203125, Val_Loss: 4252.66650390625\n",
      "Epoch 8849, Train_Loss: 4105.30712890625, Val_Loss: 4252.57275390625\n",
      "Epoch 8850, Train_Loss: 4105.28955078125, Val_Loss: 4252.4716796875\n",
      "Epoch 8851, Train_Loss: 4105.22314453125, Val_Loss: 4252.37646484375\n",
      "Epoch 8852, Train_Loss: 4105.15771484375, Val_Loss: 4252.2783203125\n",
      "Epoch 8853, Train_Loss: 4105.08203125, Val_Loss: 4252.19482421875\n",
      "Epoch 8854, Train_Loss: 4105.00439453125, Val_Loss: 4252.0810546875\n",
      "Epoch 8855, Train_Loss: 4104.92578125, Val_Loss: 4251.9814453125\n",
      "Epoch 8856, Train_Loss: 4104.857421875, Val_Loss: 4251.8916015625\n",
      "Epoch 8857, Train_Loss: 4104.78662109375, Val_Loss: 4251.79541015625\n",
      "Epoch 8858, Train_Loss: 4104.7197265625, Val_Loss: 4251.69580078125\n",
      "Epoch 8859, Train_Loss: 4104.6201171875, Val_Loss: 4251.60498046875\n",
      "Epoch 8860, Train_Loss: 4104.53857421875, Val_Loss: 4251.521484375\n",
      "Epoch 8861, Train_Loss: 4104.40869140625, Val_Loss: 4251.4345703125\n",
      "Epoch 8862, Train_Loss: 4104.337890625, Val_Loss: 4251.341796875\n",
      "Epoch 8863, Train_Loss: 4104.26611328125, Val_Loss: 4251.24755859375\n",
      "Epoch 8864, Train_Loss: 4104.1943359375, Val_Loss: 4251.15625\n",
      "Epoch 8865, Train_Loss: 4104.13232421875, Val_Loss: 4251.06591796875\n",
      "Epoch 8866, Train_Loss: 4104.05126953125, Val_Loss: 4250.97216796875\n",
      "Epoch 8867, Train_Loss: 4103.97509765625, Val_Loss: 4250.8798828125\n",
      "Epoch 8868, Train_Loss: 4103.90869140625, Val_Loss: 4250.79736328125\n",
      "Epoch 8869, Train_Loss: 4103.8515625, Val_Loss: 4250.6923828125\n",
      "Epoch 8870, Train_Loss: 4103.77783203125, Val_Loss: 4250.611328125\n",
      "Epoch 8871, Train_Loss: 4103.68798828125, Val_Loss: 4250.5166015625\n",
      "Epoch 8872, Train_Loss: 4103.619140625, Val_Loss: 4250.41796875\n",
      "Epoch 8873, Train_Loss: 4103.54248046875, Val_Loss: 4250.3349609375\n",
      "Epoch 8874, Train_Loss: 4103.47021484375, Val_Loss: 4250.23681640625\n",
      "Epoch 8875, Train_Loss: 4103.36865234375, Val_Loss: 4250.15380859375\n",
      "Epoch 8876, Train_Loss: 4103.28515625, Val_Loss: 4250.06103515625\n",
      "Epoch 8877, Train_Loss: 4103.2041015625, Val_Loss: 4249.96435546875\n",
      "Epoch 8878, Train_Loss: 4103.15087890625, Val_Loss: 4249.86376953125\n",
      "Epoch 8879, Train_Loss: 4103.09130859375, Val_Loss: 4249.775390625\n",
      "Epoch 8880, Train_Loss: 4102.98828125, Val_Loss: 4249.6826171875\n",
      "Epoch 8881, Train_Loss: 4102.931640625, Val_Loss: 4249.5849609375\n",
      "Epoch 8882, Train_Loss: 4102.8583984375, Val_Loss: 4249.50341796875\n",
      "Epoch 8883, Train_Loss: 4102.77734375, Val_Loss: 4249.41943359375\n",
      "Epoch 8884, Train_Loss: 4102.708984375, Val_Loss: 4249.314453125\n",
      "Epoch 8885, Train_Loss: 4102.650390625, Val_Loss: 4249.2265625\n",
      "Epoch 8886, Train_Loss: 4102.572265625, Val_Loss: 4249.13427734375\n",
      "Epoch 8887, Train_Loss: 4102.5, Val_Loss: 4249.04150390625\n",
      "Epoch 8888, Train_Loss: 4102.43359375, Val_Loss: 4248.95166015625\n",
      "Epoch 8889, Train_Loss: 4102.35400390625, Val_Loss: 4248.86962890625\n",
      "Epoch 8890, Train_Loss: 4102.2783203125, Val_Loss: 4248.77978515625\n",
      "Epoch 8891, Train_Loss: 4102.19189453125, Val_Loss: 4248.69580078125\n",
      "Epoch 8892, Train_Loss: 4102.138671875, Val_Loss: 4248.583984375\n",
      "Epoch 8893, Train_Loss: 4102.07958984375, Val_Loss: 4248.486328125\n",
      "Epoch 8894, Train_Loss: 4102.009765625, Val_Loss: 4248.37841796875\n",
      "Epoch 8895, Train_Loss: 4101.93212890625, Val_Loss: 4248.2822265625\n",
      "Epoch 8896, Train_Loss: 4101.8525390625, Val_Loss: 4248.1943359375\n",
      "Epoch 8897, Train_Loss: 4101.77685546875, Val_Loss: 4248.11083984375\n",
      "Epoch 8898, Train_Loss: 4101.71630859375, Val_Loss: 4248.02490234375\n",
      "Epoch 8899, Train_Loss: 4101.63525390625, Val_Loss: 4247.91455078125\n",
      "Epoch 8900, Train_Loss: 4101.5712890625, Val_Loss: 4247.82421875\n",
      "Epoch 8901, Train_Loss: 4101.50048828125, Val_Loss: 4247.732421875\n",
      "Epoch 8902, Train_Loss: 4101.41064453125, Val_Loss: 4247.64697265625\n",
      "Epoch 8903, Train_Loss: 4101.302734375, Val_Loss: 4247.560546875\n",
      "Epoch 8904, Train_Loss: 4101.2294921875, Val_Loss: 4247.47900390625\n",
      "Epoch 8905, Train_Loss: 4101.15234375, Val_Loss: 4247.39208984375\n",
      "Epoch 8906, Train_Loss: 4101.1328125, Val_Loss: 4247.2841796875\n",
      "Epoch 8907, Train_Loss: 4101.05029296875, Val_Loss: 4247.19873046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8908, Train_Loss: 4100.98681640625, Val_Loss: 4247.1162109375\n",
      "Epoch 8909, Train_Loss: 4100.92822265625, Val_Loss: 4247.0302734375\n",
      "Epoch 8910, Train_Loss: 4100.85205078125, Val_Loss: 4246.93896484375\n",
      "Epoch 8911, Train_Loss: 4100.7763671875, Val_Loss: 4246.84765625\n",
      "Epoch 8912, Train_Loss: 4100.689453125, Val_Loss: 4246.75927734375\n",
      "Epoch 8913, Train_Loss: 4100.62353515625, Val_Loss: 4246.669921875\n",
      "Epoch 8914, Train_Loss: 4100.40625, Val_Loss: 4246.57861328125\n",
      "Epoch 8915, Train_Loss: 4100.34423828125, Val_Loss: 4246.48486328125\n",
      "Epoch 8916, Train_Loss: 4100.27685546875, Val_Loss: 4246.3857421875\n",
      "Epoch 8917, Train_Loss: 4100.15185546875, Val_Loss: 4246.306640625\n",
      "Epoch 8918, Train_Loss: 4100.072265625, Val_Loss: 4246.22119140625\n",
      "Epoch 8919, Train_Loss: 4099.99658203125, Val_Loss: 4246.119140625\n",
      "Epoch 8920, Train_Loss: 4099.927734375, Val_Loss: 4246.02978515625\n",
      "Epoch 8921, Train_Loss: 4099.849609375, Val_Loss: 4245.93505859375\n",
      "Epoch 8922, Train_Loss: 4099.79345703125, Val_Loss: 4245.83984375\n",
      "Epoch 8923, Train_Loss: 4099.7109375, Val_Loss: 4245.75927734375\n",
      "Epoch 8924, Train_Loss: 4099.6533203125, Val_Loss: 4245.6611328125\n",
      "Epoch 8925, Train_Loss: 4099.58251953125, Val_Loss: 4245.5869140625\n",
      "Epoch 8926, Train_Loss: 4099.51611328125, Val_Loss: 4245.48974609375\n",
      "Epoch 8927, Train_Loss: 4099.44482421875, Val_Loss: 4245.39892578125\n",
      "Epoch 8928, Train_Loss: 4099.36328125, Val_Loss: 4245.29931640625\n",
      "Epoch 8929, Train_Loss: 4099.2861328125, Val_Loss: 4245.208984375\n",
      "Epoch 8930, Train_Loss: 4099.21142578125, Val_Loss: 4245.12255859375\n",
      "Epoch 8931, Train_Loss: 4099.1064453125, Val_Loss: 4245.0498046875\n",
      "Epoch 8932, Train_Loss: 4099.0458984375, Val_Loss: 4244.951171875\n",
      "Epoch 8933, Train_Loss: 4098.9775390625, Val_Loss: 4244.85498046875\n",
      "Epoch 8934, Train_Loss: 4098.884765625, Val_Loss: 4244.73291015625\n",
      "Epoch 8935, Train_Loss: 4098.8134765625, Val_Loss: 4244.634765625\n",
      "Epoch 8936, Train_Loss: 4098.75244140625, Val_Loss: 4244.5498046875\n",
      "Epoch 8937, Train_Loss: 4098.68701171875, Val_Loss: 4244.45947265625\n",
      "Epoch 8938, Train_Loss: 4098.6201171875, Val_Loss: 4244.38671875\n",
      "Epoch 8939, Train_Loss: 4098.55712890625, Val_Loss: 4244.29150390625\n",
      "Epoch 8940, Train_Loss: 4098.4833984375, Val_Loss: 4244.20166015625\n",
      "Epoch 8941, Train_Loss: 4098.40869140625, Val_Loss: 4244.12060546875\n",
      "Epoch 8942, Train_Loss: 4098.3427734375, Val_Loss: 4244.02001953125\n",
      "Epoch 8943, Train_Loss: 4098.26611328125, Val_Loss: 4243.92578125\n",
      "Epoch 8944, Train_Loss: 4098.16943359375, Val_Loss: 4243.85205078125\n",
      "Epoch 8945, Train_Loss: 4098.07568359375, Val_Loss: 4243.77880859375\n",
      "Epoch 8946, Train_Loss: 4098.0107421875, Val_Loss: 4243.6826171875\n",
      "Epoch 8947, Train_Loss: 4097.953125, Val_Loss: 4243.59619140625\n",
      "Epoch 8948, Train_Loss: 4097.89453125, Val_Loss: 4243.50537109375\n",
      "Epoch 8949, Train_Loss: 4097.83447265625, Val_Loss: 4243.4013671875\n",
      "Epoch 8950, Train_Loss: 4097.7568359375, Val_Loss: 4243.314453125\n",
      "Epoch 8951, Train_Loss: 4097.69580078125, Val_Loss: 4243.23486328125\n",
      "Epoch 8952, Train_Loss: 4097.61181640625, Val_Loss: 4243.140625\n",
      "Epoch 8953, Train_Loss: 4097.54638671875, Val_Loss: 4243.0546875\n",
      "Epoch 8954, Train_Loss: 4097.46337890625, Val_Loss: 4242.9619140625\n",
      "Epoch 8955, Train_Loss: 4097.3828125, Val_Loss: 4242.8671875\n",
      "Epoch 8956, Train_Loss: 4097.31494140625, Val_Loss: 4242.77392578125\n",
      "Epoch 8957, Train_Loss: 4097.24658203125, Val_Loss: 4242.6923828125\n",
      "Epoch 8958, Train_Loss: 4097.18212890625, Val_Loss: 4242.6083984375\n",
      "Epoch 8959, Train_Loss: 4097.1083984375, Val_Loss: 4242.5205078125\n",
      "Epoch 8960, Train_Loss: 4097.02392578125, Val_Loss: 4242.44140625\n",
      "Epoch 8961, Train_Loss: 4096.95556640625, Val_Loss: 4242.35205078125\n",
      "Epoch 8962, Train_Loss: 4096.88818359375, Val_Loss: 4242.263671875\n",
      "Epoch 8963, Train_Loss: 4096.8671875, Val_Loss: 4242.16259765625\n",
      "Epoch 8964, Train_Loss: 4096.806640625, Val_Loss: 4242.068359375\n",
      "Epoch 8965, Train_Loss: 4096.74267578125, Val_Loss: 4241.9833984375\n",
      "Epoch 8966, Train_Loss: 4096.654296875, Val_Loss: 4241.890625\n",
      "Epoch 8967, Train_Loss: 4096.5791015625, Val_Loss: 4241.822265625\n",
      "Epoch 8968, Train_Loss: 4096.513671875, Val_Loss: 4241.7373046875\n",
      "Epoch 8969, Train_Loss: 4096.43603515625, Val_Loss: 4241.6513671875\n",
      "Epoch 8970, Train_Loss: 4096.3701171875, Val_Loss: 4241.5615234375\n",
      "Epoch 8971, Train_Loss: 4096.28955078125, Val_Loss: 4241.470703125\n",
      "Epoch 8972, Train_Loss: 4096.2197265625, Val_Loss: 4241.37060546875\n",
      "Epoch 8973, Train_Loss: 4096.16015625, Val_Loss: 4241.28955078125\n",
      "Epoch 8974, Train_Loss: 4096.021484375, Val_Loss: 4241.18017578125\n",
      "Epoch 8975, Train_Loss: 4095.9560546875, Val_Loss: 4241.09130859375\n",
      "Epoch 8976, Train_Loss: 4095.894775390625, Val_Loss: 4241.00439453125\n",
      "Epoch 8977, Train_Loss: 4095.818115234375, Val_Loss: 4240.89501953125\n",
      "Epoch 8978, Train_Loss: 4095.7578125, Val_Loss: 4240.81298828125\n",
      "Epoch 8979, Train_Loss: 4095.6982421875, Val_Loss: 4240.7275390625\n",
      "Epoch 8980, Train_Loss: 4095.62060546875, Val_Loss: 4240.63427734375\n",
      "Epoch 8981, Train_Loss: 4095.5556640625, Val_Loss: 4240.55810546875\n",
      "Epoch 8982, Train_Loss: 4095.48291015625, Val_Loss: 4240.4755859375\n",
      "Epoch 8983, Train_Loss: 4095.424072265625, Val_Loss: 4240.3818359375\n",
      "Epoch 8984, Train_Loss: 4095.352294921875, Val_Loss: 4240.29833984375\n",
      "Epoch 8985, Train_Loss: 4095.279541015625, Val_Loss: 4240.21240234375\n",
      "Epoch 8986, Train_Loss: 4095.209716796875, Val_Loss: 4240.1171875\n",
      "Epoch 8987, Train_Loss: 4095.114990234375, Val_Loss: 4240.0439453125\n",
      "Epoch 8988, Train_Loss: 4095.023193359375, Val_Loss: 4239.95751953125\n",
      "Epoch 8989, Train_Loss: 4094.949462890625, Val_Loss: 4239.86962890625\n",
      "Epoch 8990, Train_Loss: 4094.87451171875, Val_Loss: 4239.78125\n",
      "Epoch 8991, Train_Loss: 4094.821044921875, Val_Loss: 4239.70166015625\n",
      "Epoch 8992, Train_Loss: 4094.768798828125, Val_Loss: 4239.60693359375\n",
      "Epoch 8993, Train_Loss: 4094.6875, Val_Loss: 4239.51416015625\n",
      "Epoch 8994, Train_Loss: 4094.572265625, Val_Loss: 4239.41748046875\n",
      "Epoch 8995, Train_Loss: 4094.496826171875, Val_Loss: 4239.34619140625\n",
      "Epoch 8996, Train_Loss: 4094.439697265625, Val_Loss: 4239.27001953125\n",
      "Epoch 8997, Train_Loss: 4094.36474609375, Val_Loss: 4239.16943359375\n",
      "Epoch 8998, Train_Loss: 4094.281982421875, Val_Loss: 4239.09619140625\n",
      "Epoch 8999, Train_Loss: 4094.218994140625, Val_Loss: 4238.990234375\n",
      "Epoch 9000, Train_Loss: 4094.161376953125, Val_Loss: 4238.91015625\n",
      "Epoch 9001, Train_Loss: 4094.0869140625, Val_Loss: 4238.8193359375\n",
      "Epoch 9002, Train_Loss: 4094.015625, Val_Loss: 4238.7314453125\n",
      "Epoch 9003, Train_Loss: 4093.9345703125, Val_Loss: 4238.6494140625\n",
      "Epoch 9004, Train_Loss: 4093.89208984375, Val_Loss: 4238.56298828125\n",
      "Epoch 9005, Train_Loss: 4093.833251953125, Val_Loss: 4238.47607421875\n",
      "Epoch 9006, Train_Loss: 4093.77197265625, Val_Loss: 4238.37744140625\n",
      "Epoch 9007, Train_Loss: 4093.703857421875, Val_Loss: 4238.29248046875\n",
      "Epoch 9008, Train_Loss: 4093.645751953125, Val_Loss: 4238.20166015625\n",
      "Epoch 9009, Train_Loss: 4093.553466796875, Val_Loss: 4238.12744140625\n",
      "Epoch 9010, Train_Loss: 4093.478515625, Val_Loss: 4238.037109375\n",
      "Epoch 9011, Train_Loss: 4093.417236328125, Val_Loss: 4237.94482421875\n",
      "Epoch 9012, Train_Loss: 4093.350830078125, Val_Loss: 4237.86376953125\n",
      "Epoch 9013, Train_Loss: 4093.279541015625, Val_Loss: 4237.77685546875\n",
      "Epoch 9014, Train_Loss: 4093.19580078125, Val_Loss: 4237.64892578125\n",
      "Epoch 9015, Train_Loss: 4093.12548828125, Val_Loss: 4237.572265625\n",
      "Epoch 9016, Train_Loss: 4093.068359375, Val_Loss: 4237.478515625\n",
      "Epoch 9017, Train_Loss: 4092.970458984375, Val_Loss: 4237.400390625\n",
      "Epoch 9018, Train_Loss: 4092.90478515625, Val_Loss: 4237.31201171875\n",
      "Epoch 9019, Train_Loss: 4092.843017578125, Val_Loss: 4237.22216796875\n",
      "Epoch 9020, Train_Loss: 4092.82421875, Val_Loss: 4237.1318359375\n",
      "Epoch 9021, Train_Loss: 4092.758056640625, Val_Loss: 4237.044921875\n",
      "Epoch 9022, Train_Loss: 4092.697509765625, Val_Loss: 4236.95458984375\n",
      "Epoch 9023, Train_Loss: 4092.63818359375, Val_Loss: 4236.87939453125\n",
      "Epoch 9024, Train_Loss: 4092.564453125, Val_Loss: 4236.8134765625\n",
      "Epoch 9025, Train_Loss: 4092.490478515625, Val_Loss: 4236.73046875\n",
      "Epoch 9026, Train_Loss: 4092.43017578125, Val_Loss: 4236.63134765625\n",
      "Epoch 9027, Train_Loss: 4092.367919921875, Val_Loss: 4236.5361328125\n",
      "Epoch 9028, Train_Loss: 4092.302978515625, Val_Loss: 4236.44873046875\n",
      "Epoch 9029, Train_Loss: 4092.23486328125, Val_Loss: 4236.37255859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9030, Train_Loss: 4092.132080078125, Val_Loss: 4236.279296875\n",
      "Epoch 9031, Train_Loss: 4092.019775390625, Val_Loss: 4236.2001953125\n",
      "Epoch 9032, Train_Loss: 4091.943359375, Val_Loss: 4236.10986328125\n",
      "Epoch 9033, Train_Loss: 4091.881103515625, Val_Loss: 4236.0224609375\n",
      "Epoch 9034, Train_Loss: 4091.802978515625, Val_Loss: 4235.92529296875\n",
      "Epoch 9035, Train_Loss: 4091.73388671875, Val_Loss: 4235.84619140625\n",
      "Epoch 9036, Train_Loss: 4091.652587890625, Val_Loss: 4235.7578125\n",
      "Epoch 9037, Train_Loss: 4091.589111328125, Val_Loss: 4235.67138671875\n",
      "Epoch 9038, Train_Loss: 4091.51171875, Val_Loss: 4235.58740234375\n",
      "Epoch 9039, Train_Loss: 4091.445556640625, Val_Loss: 4235.50537109375\n",
      "Epoch 9040, Train_Loss: 4091.38232421875, Val_Loss: 4235.4189453125\n",
      "Epoch 9041, Train_Loss: 4091.29638671875, Val_Loss: 4235.32958984375\n",
      "Epoch 9042, Train_Loss: 4091.2392578125, Val_Loss: 4235.23486328125\n",
      "Epoch 9043, Train_Loss: 4091.1689453125, Val_Loss: 4235.1572265625\n",
      "Epoch 9044, Train_Loss: 4091.106201171875, Val_Loss: 4235.0537109375\n",
      "Epoch 9045, Train_Loss: 4091.000732421875, Val_Loss: 4234.97998046875\n",
      "Epoch 9046, Train_Loss: 4090.93798828125, Val_Loss: 4234.90234375\n",
      "Epoch 9047, Train_Loss: 4090.85888671875, Val_Loss: 4234.81982421875\n",
      "Epoch 9048, Train_Loss: 4090.8037109375, Val_Loss: 4234.73095703125\n",
      "Epoch 9049, Train_Loss: 4090.74609375, Val_Loss: 4234.63037109375\n",
      "Epoch 9050, Train_Loss: 4090.6884765625, Val_Loss: 4234.54931640625\n",
      "Epoch 9051, Train_Loss: 4090.625732421875, Val_Loss: 4234.462890625\n",
      "Epoch 9052, Train_Loss: 4090.53662109375, Val_Loss: 4234.3896484375\n",
      "Epoch 9053, Train_Loss: 4090.484130859375, Val_Loss: 4234.30224609375\n",
      "Epoch 9054, Train_Loss: 4090.411865234375, Val_Loss: 4234.17919921875\n",
      "Epoch 9055, Train_Loss: 4090.346923828125, Val_Loss: 4234.1015625\n",
      "Epoch 9056, Train_Loss: 4090.28515625, Val_Loss: 4234.0078125\n",
      "Epoch 9057, Train_Loss: 4090.22412109375, Val_Loss: 4233.91796875\n",
      "Epoch 9058, Train_Loss: 4090.12841796875, Val_Loss: 4233.83349609375\n",
      "Epoch 9059, Train_Loss: 4090.06298828125, Val_Loss: 4233.75048828125\n",
      "Epoch 9060, Train_Loss: 4089.973388671875, Val_Loss: 4233.66162109375\n",
      "Epoch 9061, Train_Loss: 4089.91552734375, Val_Loss: 4233.57666015625\n",
      "Epoch 9062, Train_Loss: 4089.84130859375, Val_Loss: 4233.4990234375\n",
      "Epoch 9063, Train_Loss: 4089.782470703125, Val_Loss: 4233.4013671875\n",
      "Epoch 9064, Train_Loss: 4089.71923828125, Val_Loss: 4233.32763671875\n",
      "Epoch 9065, Train_Loss: 4089.652587890625, Val_Loss: 4233.25634765625\n",
      "Epoch 9066, Train_Loss: 4089.593505859375, Val_Loss: 4233.16259765625\n",
      "Epoch 9067, Train_Loss: 4089.522705078125, Val_Loss: 4233.0888671875\n",
      "Epoch 9068, Train_Loss: 4089.45703125, Val_Loss: 4233.0078125\n",
      "Epoch 9069, Train_Loss: 4089.39111328125, Val_Loss: 4232.91259765625\n",
      "Epoch 9070, Train_Loss: 4089.317138671875, Val_Loss: 4232.822265625\n",
      "Epoch 9071, Train_Loss: 4089.25439453125, Val_Loss: 4232.734375\n",
      "Epoch 9072, Train_Loss: 4089.1982421875, Val_Loss: 4232.6533203125\n",
      "Epoch 9073, Train_Loss: 4089.12353515625, Val_Loss: 4232.5751953125\n",
      "Epoch 9074, Train_Loss: 4088.96923828125, Val_Loss: 4232.48486328125\n",
      "Epoch 9075, Train_Loss: 4088.91845703125, Val_Loss: 4232.40234375\n",
      "Epoch 9076, Train_Loss: 4088.855712890625, Val_Loss: 4232.318359375\n",
      "Epoch 9077, Train_Loss: 4088.834228515625, Val_Loss: 4232.2265625\n",
      "Epoch 9078, Train_Loss: 4088.771240234375, Val_Loss: 4232.1337890625\n",
      "Epoch 9079, Train_Loss: 4088.698486328125, Val_Loss: 4232.05126953125\n",
      "Epoch 9080, Train_Loss: 4088.635986328125, Val_Loss: 4231.9677734375\n",
      "Epoch 9081, Train_Loss: 4088.575927734375, Val_Loss: 4231.89697265625\n",
      "Epoch 9082, Train_Loss: 4088.501953125, Val_Loss: 4231.81982421875\n",
      "Epoch 9083, Train_Loss: 4088.442138671875, Val_Loss: 4231.7236328125\n",
      "Epoch 9084, Train_Loss: 4088.37890625, Val_Loss: 4231.64404296875\n",
      "Epoch 9085, Train_Loss: 4088.296630859375, Val_Loss: 4231.54638671875\n",
      "Epoch 9086, Train_Loss: 4088.231689453125, Val_Loss: 4231.4560546875\n",
      "Epoch 9087, Train_Loss: 4088.180908203125, Val_Loss: 4231.37548828125\n",
      "Epoch 9088, Train_Loss: 4088.05078125, Val_Loss: 4231.3037109375\n",
      "Epoch 9089, Train_Loss: 4087.980712890625, Val_Loss: 4231.21337890625\n",
      "Epoch 9090, Train_Loss: 4087.908935546875, Val_Loss: 4231.12451171875\n",
      "Epoch 9091, Train_Loss: 4087.852294921875, Val_Loss: 4231.04541015625\n",
      "Epoch 9092, Train_Loss: 4087.782958984375, Val_Loss: 4230.9560546875\n",
      "Epoch 9093, Train_Loss: 4087.71923828125, Val_Loss: 4230.8681640625\n",
      "Epoch 9094, Train_Loss: 4087.6552734375, Val_Loss: 4230.77978515625\n",
      "Epoch 9095, Train_Loss: 4087.5478515625, Val_Loss: 4230.67919921875\n",
      "Epoch 9096, Train_Loss: 4087.48388671875, Val_Loss: 4230.6005859375\n",
      "Epoch 9097, Train_Loss: 4087.42529296875, Val_Loss: 4230.50537109375\n",
      "Epoch 9098, Train_Loss: 4087.351318359375, Val_Loss: 4230.423828125\n",
      "Epoch 9099, Train_Loss: 4087.29931640625, Val_Loss: 4230.337890625\n",
      "Epoch 9100, Train_Loss: 4087.24658203125, Val_Loss: 4230.24609375\n",
      "Epoch 9101, Train_Loss: 4087.164794921875, Val_Loss: 4230.1650390625\n",
      "Epoch 9102, Train_Loss: 4087.09130859375, Val_Loss: 4230.0830078125\n",
      "Epoch 9103, Train_Loss: 4086.994384765625, Val_Loss: 4230.01220703125\n",
      "Epoch 9104, Train_Loss: 4086.9326171875, Val_Loss: 4229.9208984375\n",
      "Epoch 9105, Train_Loss: 4086.862548828125, Val_Loss: 4229.8447265625\n",
      "Epoch 9106, Train_Loss: 4086.8017578125, Val_Loss: 4229.7490234375\n",
      "Epoch 9107, Train_Loss: 4086.729248046875, Val_Loss: 4229.662109375\n",
      "Epoch 9108, Train_Loss: 4086.648681640625, Val_Loss: 4229.556640625\n",
      "Epoch 9109, Train_Loss: 4086.5810546875, Val_Loss: 4229.47216796875\n",
      "Epoch 9110, Train_Loss: 4086.5166015625, Val_Loss: 4229.40380859375\n",
      "Epoch 9111, Train_Loss: 4086.443359375, Val_Loss: 4229.31298828125\n",
      "Epoch 9112, Train_Loss: 4086.388671875, Val_Loss: 4229.23095703125\n",
      "Epoch 9113, Train_Loss: 4086.319091796875, Val_Loss: 4229.1357421875\n",
      "Epoch 9114, Train_Loss: 4086.260986328125, Val_Loss: 4229.0517578125\n",
      "Epoch 9115, Train_Loss: 4086.176513671875, Val_Loss: 4228.96044921875\n",
      "Epoch 9116, Train_Loss: 4086.1162109375, Val_Loss: 4228.87548828125\n",
      "Epoch 9117, Train_Loss: 4086.017333984375, Val_Loss: 4228.80419921875\n",
      "Epoch 9118, Train_Loss: 4085.959228515625, Val_Loss: 4228.7158203125\n",
      "Epoch 9119, Train_Loss: 4085.90576171875, Val_Loss: 4228.63818359375\n",
      "Epoch 9120, Train_Loss: 4085.866943359375, Val_Loss: 4228.54345703125\n",
      "Epoch 9121, Train_Loss: 4085.8076171875, Val_Loss: 4228.46142578125\n",
      "Epoch 9122, Train_Loss: 4085.738525390625, Val_Loss: 4228.380859375\n",
      "Epoch 9123, Train_Loss: 4085.65673828125, Val_Loss: 4228.29248046875\n",
      "Epoch 9124, Train_Loss: 4085.595458984375, Val_Loss: 4228.22021484375\n",
      "Epoch 9125, Train_Loss: 4085.53271484375, Val_Loss: 4228.13037109375\n",
      "Epoch 9126, Train_Loss: 4085.467529296875, Val_Loss: 4228.0419921875\n",
      "Epoch 9127, Train_Loss: 4085.41162109375, Val_Loss: 4227.96142578125\n",
      "Epoch 9128, Train_Loss: 4085.34814453125, Val_Loss: 4227.8662109375\n",
      "Epoch 9129, Train_Loss: 4085.263427734375, Val_Loss: 4227.78515625\n",
      "Epoch 9130, Train_Loss: 4085.192626953125, Val_Loss: 4227.69921875\n",
      "Epoch 9131, Train_Loss: 4085.100341796875, Val_Loss: 4227.6337890625\n",
      "Epoch 9132, Train_Loss: 4085.033203125, Val_Loss: 4227.552734375\n",
      "Epoch 9133, Train_Loss: 4084.963623046875, Val_Loss: 4227.46435546875\n",
      "Epoch 9134, Train_Loss: 4084.8974609375, Val_Loss: 4227.3818359375\n",
      "Epoch 9135, Train_Loss: 4084.859375, Val_Loss: 4227.24365234375\n",
      "Epoch 9136, Train_Loss: 4084.797607421875, Val_Loss: 4227.16259765625\n",
      "Epoch 9137, Train_Loss: 4084.733642578125, Val_Loss: 4227.07861328125\n",
      "Epoch 9138, Train_Loss: 4084.67236328125, Val_Loss: 4227.0078125\n",
      "Epoch 9139, Train_Loss: 4084.582763671875, Val_Loss: 4226.9306640625\n",
      "Epoch 9140, Train_Loss: 4084.53173828125, Val_Loss: 4226.8544921875\n",
      "Epoch 9141, Train_Loss: 4084.466796875, Val_Loss: 4226.763671875\n",
      "Epoch 9142, Train_Loss: 4084.413818359375, Val_Loss: 4226.67578125\n",
      "Epoch 9143, Train_Loss: 4084.355224609375, Val_Loss: 4226.58740234375\n",
      "Epoch 9144, Train_Loss: 4084.281982421875, Val_Loss: 4226.50927734375\n",
      "Epoch 9145, Train_Loss: 4084.205810546875, Val_Loss: 4226.4453125\n",
      "Epoch 9146, Train_Loss: 4084.09423828125, Val_Loss: 4226.36669921875\n",
      "Epoch 9147, Train_Loss: 4084.031005859375, Val_Loss: 4226.27978515625\n",
      "Epoch 9148, Train_Loss: 4083.978515625, Val_Loss: 4226.1962890625\n",
      "Epoch 9149, Train_Loss: 4083.9140625, Val_Loss: 4226.11083984375\n",
      "Epoch 9150, Train_Loss: 4083.838134765625, Val_Loss: 4226.03125\n",
      "Epoch 9151, Train_Loss: 4083.781494140625, Val_Loss: 4225.947265625\n",
      "Epoch 9152, Train_Loss: 4083.70703125, Val_Loss: 4225.8701171875\n",
      "Epoch 9153, Train_Loss: 4083.638427734375, Val_Loss: 4225.798828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9154, Train_Loss: 4083.573486328125, Val_Loss: 4225.7080078125\n",
      "Epoch 9155, Train_Loss: 4083.44873046875, Val_Loss: 4225.61376953125\n",
      "Epoch 9156, Train_Loss: 4083.394775390625, Val_Loss: 4225.52490234375\n",
      "Epoch 9157, Train_Loss: 4083.331298828125, Val_Loss: 4225.4482421875\n",
      "Epoch 9158, Train_Loss: 4083.27001953125, Val_Loss: 4225.3681640625\n",
      "Epoch 9159, Train_Loss: 4083.2041015625, Val_Loss: 4225.28857421875\n",
      "Epoch 9160, Train_Loss: 4083.1123046875, Val_Loss: 4225.21630859375\n",
      "Epoch 9161, Train_Loss: 4083.0390625, Val_Loss: 4225.134765625\n",
      "Epoch 9162, Train_Loss: 4082.98388671875, Val_Loss: 4225.0419921875\n",
      "Epoch 9163, Train_Loss: 4082.91845703125, Val_Loss: 4224.97216796875\n",
      "Epoch 9164, Train_Loss: 4082.845703125, Val_Loss: 4224.880859375\n",
      "Epoch 9165, Train_Loss: 4082.8515625, Val_Loss: 4224.79150390625\n",
      "Epoch 9166, Train_Loss: 4082.77490234375, Val_Loss: 4224.70751953125\n",
      "Epoch 9167, Train_Loss: 4082.720458984375, Val_Loss: 4224.634765625\n",
      "Epoch 9168, Train_Loss: 4082.650634765625, Val_Loss: 4224.55419921875\n",
      "Epoch 9169, Train_Loss: 4082.584716796875, Val_Loss: 4224.46826171875\n",
      "Epoch 9170, Train_Loss: 4082.513427734375, Val_Loss: 4224.392578125\n",
      "Epoch 9171, Train_Loss: 4082.431640625, Val_Loss: 4224.3046875\n",
      "Epoch 9172, Train_Loss: 4082.3818359375, Val_Loss: 4224.22314453125\n",
      "Epoch 9173, Train_Loss: 4082.318603515625, Val_Loss: 4224.14013671875\n",
      "Epoch 9174, Train_Loss: 4082.24365234375, Val_Loss: 4224.06494140625\n",
      "Epoch 9175, Train_Loss: 4082.17138671875, Val_Loss: 4223.951171875\n",
      "Epoch 9176, Train_Loss: 4082.123046875, Val_Loss: 4223.86181640625\n",
      "Epoch 9177, Train_Loss: 4082.0576171875, Val_Loss: 4223.787109375\n",
      "Epoch 9178, Train_Loss: 4082.00537109375, Val_Loss: 4223.693359375\n",
      "Epoch 9179, Train_Loss: 4081.9287109375, Val_Loss: 4223.6162109375\n",
      "Epoch 9180, Train_Loss: 4081.866943359375, Val_Loss: 4223.54541015625\n",
      "Epoch 9181, Train_Loss: 4081.798828125, Val_Loss: 4223.46826171875\n",
      "Epoch 9182, Train_Loss: 4081.720703125, Val_Loss: 4223.392578125\n",
      "Epoch 9183, Train_Loss: 4081.64892578125, Val_Loss: 4223.2978515625\n",
      "Epoch 9184, Train_Loss: 4081.583984375, Val_Loss: 4223.21923828125\n",
      "Epoch 9185, Train_Loss: 4081.5283203125, Val_Loss: 4223.13330078125\n",
      "Epoch 9186, Train_Loss: 4081.4619140625, Val_Loss: 4223.0517578125\n",
      "Epoch 9187, Train_Loss: 4081.388427734375, Val_Loss: 4222.9794921875\n",
      "Epoch 9188, Train_Loss: 4081.31884765625, Val_Loss: 4222.89453125\n",
      "Epoch 9189, Train_Loss: 4081.23974609375, Val_Loss: 4222.82568359375\n",
      "Epoch 9190, Train_Loss: 4081.1845703125, Val_Loss: 4222.740234375\n",
      "Epoch 9191, Train_Loss: 4081.12548828125, Val_Loss: 4222.6650390625\n",
      "Epoch 9192, Train_Loss: 4081.1103515625, Val_Loss: 4222.5751953125\n",
      "Epoch 9193, Train_Loss: 4081.03369140625, Val_Loss: 4222.48876953125\n",
      "Epoch 9194, Train_Loss: 4080.969482421875, Val_Loss: 4222.40673828125\n",
      "Epoch 9195, Train_Loss: 4080.905029296875, Val_Loss: 4222.3271484375\n",
      "Epoch 9196, Train_Loss: 4080.8251953125, Val_Loss: 4222.25439453125\n",
      "Epoch 9197, Train_Loss: 4080.76171875, Val_Loss: 4222.17626953125\n",
      "Epoch 9198, Train_Loss: 4080.707763671875, Val_Loss: 4222.0947265625\n",
      "Epoch 9199, Train_Loss: 4080.649658203125, Val_Loss: 4222.01220703125\n",
      "Epoch 9200, Train_Loss: 4080.5947265625, Val_Loss: 4221.921875\n",
      "Epoch 9201, Train_Loss: 4080.533935546875, Val_Loss: 4221.84375\n",
      "Epoch 9202, Train_Loss: 4080.474853515625, Val_Loss: 4221.7666015625\n",
      "Epoch 9203, Train_Loss: 4080.356201171875, Val_Loss: 4221.68701171875\n",
      "Epoch 9204, Train_Loss: 4080.269287109375, Val_Loss: 4221.6025390625\n",
      "Epoch 9205, Train_Loss: 4080.191650390625, Val_Loss: 4221.52783203125\n",
      "Epoch 9206, Train_Loss: 4080.132080078125, Val_Loss: 4221.4482421875\n",
      "Epoch 9207, Train_Loss: 4080.068603515625, Val_Loss: 4221.35498046875\n",
      "Epoch 9208, Train_Loss: 4080.00732421875, Val_Loss: 4221.279296875\n",
      "Epoch 9209, Train_Loss: 4079.96337890625, Val_Loss: 4221.20068359375\n",
      "Epoch 9210, Train_Loss: 4079.888427734375, Val_Loss: 4221.12451171875\n",
      "Epoch 9211, Train_Loss: 4079.82861328125, Val_Loss: 4221.04345703125\n",
      "Epoch 9212, Train_Loss: 4079.760498046875, Val_Loss: 4220.9716796875\n",
      "Epoch 9213, Train_Loss: 4079.689208984375, Val_Loss: 4220.890625\n",
      "Epoch 9214, Train_Loss: 4079.629638671875, Val_Loss: 4220.8017578125\n",
      "Epoch 9215, Train_Loss: 4079.566162109375, Val_Loss: 4220.71484375\n",
      "Epoch 9216, Train_Loss: 4079.4853515625, Val_Loss: 4220.60498046875\n",
      "Epoch 9217, Train_Loss: 4079.43408203125, Val_Loss: 4220.5234375\n",
      "Epoch 9218, Train_Loss: 4079.33251953125, Val_Loss: 4220.458984375\n",
      "Epoch 9219, Train_Loss: 4079.270751953125, Val_Loss: 4220.3701171875\n",
      "Epoch 9220, Train_Loss: 4079.194580078125, Val_Loss: 4220.2880859375\n",
      "Epoch 9221, Train_Loss: 4079.135009765625, Val_Loss: 4220.19482421875\n",
      "Epoch 9222, Train_Loss: 4079.080322265625, Val_Loss: 4220.123046875\n",
      "Epoch 9223, Train_Loss: 4079.02685546875, Val_Loss: 4220.0498046875\n",
      "Epoch 9224, Train_Loss: 4078.9736328125, Val_Loss: 4219.96728515625\n",
      "Epoch 9225, Train_Loss: 4078.9169921875, Val_Loss: 4219.89208984375\n",
      "Epoch 9226, Train_Loss: 4078.835205078125, Val_Loss: 4219.82421875\n",
      "Epoch 9227, Train_Loss: 4078.768798828125, Val_Loss: 4219.74267578125\n",
      "Epoch 9228, Train_Loss: 4078.708984375, Val_Loss: 4219.65283203125\n",
      "Epoch 9229, Train_Loss: 4078.649169921875, Val_Loss: 4219.576171875\n",
      "Epoch 9230, Train_Loss: 4078.568115234375, Val_Loss: 4219.49267578125\n",
      "Epoch 9231, Train_Loss: 4078.504150390625, Val_Loss: 4219.4130859375\n",
      "Epoch 9232, Train_Loss: 4078.4365234375, Val_Loss: 4219.35107421875\n",
      "Epoch 9233, Train_Loss: 4078.3662109375, Val_Loss: 4219.2607421875\n",
      "Epoch 9234, Train_Loss: 4078.312744140625, Val_Loss: 4219.18017578125\n",
      "Epoch 9235, Train_Loss: 4078.2568359375, Val_Loss: 4219.0966796875\n",
      "Epoch 9236, Train_Loss: 4078.074951171875, Val_Loss: 4218.998046875\n",
      "Epoch 9237, Train_Loss: 4078.023681640625, Val_Loss: 4218.9287109375\n",
      "Epoch 9238, Train_Loss: 4077.981201171875, Val_Loss: 4218.8486328125\n",
      "Epoch 9239, Train_Loss: 4077.92041015625, Val_Loss: 4218.7724609375\n",
      "Epoch 9240, Train_Loss: 4077.84912109375, Val_Loss: 4218.693359375\n",
      "Epoch 9241, Train_Loss: 4077.787353515625, Val_Loss: 4218.62255859375\n",
      "Epoch 9242, Train_Loss: 4077.716552734375, Val_Loss: 4218.53759765625\n",
      "Epoch 9243, Train_Loss: 4077.659912109375, Val_Loss: 4218.44970703125\n",
      "Epoch 9244, Train_Loss: 4077.5966796875, Val_Loss: 4218.37744140625\n",
      "Epoch 9245, Train_Loss: 4077.523681640625, Val_Loss: 4218.29296875\n",
      "Epoch 9246, Train_Loss: 4077.43310546875, Val_Loss: 4218.22607421875\n",
      "Epoch 9247, Train_Loss: 4077.380126953125, Val_Loss: 4218.13623046875\n",
      "Epoch 9248, Train_Loss: 4077.292236328125, Val_Loss: 4218.05517578125\n",
      "Epoch 9249, Train_Loss: 4077.237060546875, Val_Loss: 4217.974609375\n",
      "Epoch 9250, Train_Loss: 4077.229736328125, Val_Loss: 4217.890625\n",
      "Epoch 9251, Train_Loss: 4077.170654296875, Val_Loss: 4217.8095703125\n",
      "Epoch 9252, Train_Loss: 4077.11181640625, Val_Loss: 4217.7333984375\n",
      "Epoch 9253, Train_Loss: 4077.043701171875, Val_Loss: 4217.6474609375\n",
      "Epoch 9254, Train_Loss: 4076.971923828125, Val_Loss: 4217.5791015625\n",
      "Epoch 9255, Train_Loss: 4076.916015625, Val_Loss: 4217.49658203125\n",
      "Epoch 9256, Train_Loss: 4076.8525390625, Val_Loss: 4217.42041015625\n",
      "Epoch 9257, Train_Loss: 4076.777099609375, Val_Loss: 4217.296875\n",
      "Epoch 9258, Train_Loss: 4076.713134765625, Val_Loss: 4217.22265625\n",
      "Epoch 9259, Train_Loss: 4076.632568359375, Val_Loss: 4217.1396484375\n",
      "Epoch 9260, Train_Loss: 4076.555419921875, Val_Loss: 4217.05859375\n",
      "Epoch 9261, Train_Loss: 4076.43603515625, Val_Loss: 4216.98193359375\n",
      "Epoch 9262, Train_Loss: 4076.3857421875, Val_Loss: 4216.9140625\n",
      "Epoch 9263, Train_Loss: 4076.341064453125, Val_Loss: 4216.82177734375\n",
      "Epoch 9264, Train_Loss: 4076.27197265625, Val_Loss: 4216.74072265625\n",
      "Epoch 9265, Train_Loss: 4076.208984375, Val_Loss: 4216.66748046875\n",
      "Epoch 9266, Train_Loss: 4076.15087890625, Val_Loss: 4216.58837890625\n",
      "Epoch 9267, Train_Loss: 4076.0849609375, Val_Loss: 4216.5234375\n",
      "Epoch 9268, Train_Loss: 4076.026123046875, Val_Loss: 4216.4521484375\n",
      "Epoch 9269, Train_Loss: 4075.94140625, Val_Loss: 4216.3701171875\n",
      "Epoch 9270, Train_Loss: 4075.872802734375, Val_Loss: 4216.27978515625\n",
      "Epoch 9271, Train_Loss: 4075.81201171875, Val_Loss: 4216.20458984375\n",
      "Epoch 9272, Train_Loss: 4075.75439453125, Val_Loss: 4216.1181640625\n",
      "Epoch 9273, Train_Loss: 4075.703369140625, Val_Loss: 4216.04736328125\n",
      "Epoch 9274, Train_Loss: 4075.640625, Val_Loss: 4215.9677734375\n",
      "Epoch 9275, Train_Loss: 4075.536376953125, Val_Loss: 4215.8916015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9276, Train_Loss: 4075.474365234375, Val_Loss: 4215.8193359375\n",
      "Epoch 9277, Train_Loss: 4075.394775390625, Val_Loss: 4215.7314453125\n",
      "Epoch 9278, Train_Loss: 4075.3125, Val_Loss: 4215.6474609375\n",
      "Epoch 9279, Train_Loss: 4075.258056640625, Val_Loss: 4215.5615234375\n",
      "Epoch 9280, Train_Loss: 4075.19677734375, Val_Loss: 4215.48095703125\n",
      "Epoch 9281, Train_Loss: 4075.140625, Val_Loss: 4215.3916015625\n",
      "Epoch 9282, Train_Loss: 4075.0888671875, Val_Loss: 4215.32763671875\n",
      "Epoch 9283, Train_Loss: 4075.027587890625, Val_Loss: 4215.26123046875\n",
      "Epoch 9284, Train_Loss: 4074.970947265625, Val_Loss: 4215.16748046875\n",
      "Epoch 9285, Train_Loss: 4074.91064453125, Val_Loss: 4215.0859375\n",
      "Epoch 9286, Train_Loss: 4074.851318359375, Val_Loss: 4215.0107421875\n",
      "Epoch 9287, Train_Loss: 4074.7900390625, Val_Loss: 4214.923828125\n",
      "Epoch 9288, Train_Loss: 4074.72509765625, Val_Loss: 4214.8359375\n",
      "Epoch 9289, Train_Loss: 4074.661376953125, Val_Loss: 4214.76611328125\n",
      "Epoch 9290, Train_Loss: 4074.575927734375, Val_Loss: 4214.69677734375\n",
      "Epoch 9291, Train_Loss: 4074.501953125, Val_Loss: 4214.619140625\n",
      "Epoch 9292, Train_Loss: 4074.4365234375, Val_Loss: 4214.5400390625\n",
      "Epoch 9293, Train_Loss: 4074.38720703125, Val_Loss: 4214.4482421875\n",
      "Epoch 9294, Train_Loss: 4074.337890625, Val_Loss: 4214.36474609375\n",
      "Epoch 9295, Train_Loss: 4074.270263671875, Val_Loss: 4214.28857421875\n",
      "Epoch 9296, Train_Loss: 4074.218505859375, Val_Loss: 4214.2060546875\n",
      "Epoch 9297, Train_Loss: 4074.132080078125, Val_Loss: 4214.1064453125\n",
      "Epoch 9298, Train_Loss: 4074.07177734375, Val_Loss: 4214.0341796875\n",
      "Epoch 9299, Train_Loss: 4074.005859375, Val_Loss: 4213.94970703125\n",
      "Epoch 9300, Train_Loss: 4073.94384765625, Val_Loss: 4213.86962890625\n",
      "Epoch 9301, Train_Loss: 4073.887939453125, Val_Loss: 4213.78564453125\n",
      "Epoch 9302, Train_Loss: 4073.811767578125, Val_Loss: 4213.7119140625\n",
      "Epoch 9303, Train_Loss: 4073.74853515625, Val_Loss: 4213.63037109375\n",
      "Epoch 9304, Train_Loss: 4073.66455078125, Val_Loss: 4213.5595703125\n",
      "Epoch 9305, Train_Loss: 4073.608642578125, Val_Loss: 4213.48828125\n",
      "Epoch 9306, Train_Loss: 4073.551025390625, Val_Loss: 4213.40673828125\n",
      "Epoch 9307, Train_Loss: 4073.490234375, Val_Loss: 4213.34228515625\n",
      "Epoch 9308, Train_Loss: 4073.459228515625, Val_Loss: 4213.25341796875\n",
      "Epoch 9309, Train_Loss: 4073.390625, Val_Loss: 4213.173828125\n",
      "Epoch 9310, Train_Loss: 4073.3349609375, Val_Loss: 4213.0888671875\n",
      "Epoch 9311, Train_Loss: 4073.27490234375, Val_Loss: 4213.025390625\n",
      "Epoch 9312, Train_Loss: 4073.21923828125, Val_Loss: 4212.95751953125\n",
      "Epoch 9313, Train_Loss: 4073.143310546875, Val_Loss: 4212.86962890625\n",
      "Epoch 9314, Train_Loss: 4073.0830078125, Val_Loss: 4212.791015625\n",
      "Epoch 9315, Train_Loss: 4073.036865234375, Val_Loss: 4212.70556640625\n",
      "Epoch 9316, Train_Loss: 4072.977294921875, Val_Loss: 4212.626953125\n",
      "Epoch 9317, Train_Loss: 4072.913818359375, Val_Loss: 4212.54736328125\n",
      "Epoch 9318, Train_Loss: 4072.742919921875, Val_Loss: 4212.47802734375\n",
      "Epoch 9319, Train_Loss: 4072.6875, Val_Loss: 4212.39404296875\n",
      "Epoch 9320, Train_Loss: 4072.625732421875, Val_Loss: 4212.31982421875\n",
      "Epoch 9321, Train_Loss: 4072.56103515625, Val_Loss: 4212.24267578125\n",
      "Epoch 9322, Train_Loss: 4072.504638671875, Val_Loss: 4212.15283203125\n",
      "Epoch 9323, Train_Loss: 4072.441650390625, Val_Loss: 4212.0703125\n",
      "Epoch 9324, Train_Loss: 4072.362060546875, Val_Loss: 4211.99658203125\n",
      "Epoch 9325, Train_Loss: 4072.30078125, Val_Loss: 4211.92138671875\n",
      "Epoch 9326, Train_Loss: 4072.246337890625, Val_Loss: 4211.8544921875\n",
      "Epoch 9327, Train_Loss: 4072.1943359375, Val_Loss: 4211.78125\n",
      "Epoch 9328, Train_Loss: 4072.13671875, Val_Loss: 4211.705078125\n",
      "Epoch 9329, Train_Loss: 4072.074951171875, Val_Loss: 4211.61328125\n",
      "Epoch 9330, Train_Loss: 4072.0126953125, Val_Loss: 4211.53759765625\n",
      "Epoch 9331, Train_Loss: 4071.939697265625, Val_Loss: 4211.45361328125\n",
      "Epoch 9332, Train_Loss: 4071.881103515625, Val_Loss: 4211.37353515625\n",
      "Epoch 9333, Train_Loss: 4071.795654296875, Val_Loss: 4211.31005859375\n",
      "Epoch 9334, Train_Loss: 4071.73291015625, Val_Loss: 4211.22900390625\n",
      "Epoch 9335, Train_Loss: 4071.65673828125, Val_Loss: 4211.1611328125\n",
      "Epoch 9336, Train_Loss: 4071.59326171875, Val_Loss: 4211.08154296875\n",
      "Epoch 9337, Train_Loss: 4071.56884765625, Val_Loss: 4210.990234375\n",
      "Epoch 9338, Train_Loss: 4071.494384765625, Val_Loss: 4210.87939453125\n",
      "Epoch 9339, Train_Loss: 4071.435546875, Val_Loss: 4210.8037109375\n",
      "Epoch 9340, Train_Loss: 4071.3759765625, Val_Loss: 4210.73388671875\n",
      "Epoch 9341, Train_Loss: 4071.318359375, Val_Loss: 4210.65771484375\n",
      "Epoch 9342, Train_Loss: 4071.258056640625, Val_Loss: 4210.57373046875\n",
      "Epoch 9343, Train_Loss: 4071.19091796875, Val_Loss: 4210.48681640625\n",
      "Epoch 9344, Train_Loss: 4071.1328125, Val_Loss: 4210.40283203125\n",
      "Epoch 9345, Train_Loss: 4071.071044921875, Val_Loss: 4210.33203125\n",
      "Epoch 9346, Train_Loss: 4070.99609375, Val_Loss: 4210.259765625\n",
      "Epoch 9347, Train_Loss: 4070.922607421875, Val_Loss: 4210.1796875\n",
      "Epoch 9348, Train_Loss: 4070.8603515625, Val_Loss: 4210.1083984375\n",
      "Epoch 9349, Train_Loss: 4070.8095703125, Val_Loss: 4210.044921875\n",
      "Epoch 9350, Train_Loss: 4070.741943359375, Val_Loss: 4209.96240234375\n",
      "Epoch 9351, Train_Loss: 4070.691162109375, Val_Loss: 4209.87646484375\n",
      "Epoch 9352, Train_Loss: 4070.6376953125, Val_Loss: 4209.7939453125\n",
      "Epoch 9353, Train_Loss: 4070.5751953125, Val_Loss: 4209.7177734375\n",
      "Epoch 9354, Train_Loss: 4070.51025390625, Val_Loss: 4209.6513671875\n",
      "Epoch 9355, Train_Loss: 4070.449462890625, Val_Loss: 4209.58984375\n",
      "Epoch 9356, Train_Loss: 4070.388671875, Val_Loss: 4209.50341796875\n",
      "Epoch 9357, Train_Loss: 4070.318603515625, Val_Loss: 4209.43310546875\n",
      "Epoch 9358, Train_Loss: 4070.24462890625, Val_Loss: 4209.333984375\n",
      "Epoch 9359, Train_Loss: 4070.182373046875, Val_Loss: 4209.2607421875\n",
      "Epoch 9360, Train_Loss: 4070.137939453125, Val_Loss: 4209.18310546875\n",
      "Epoch 9361, Train_Loss: 4070.080810546875, Val_Loss: 4209.109375\n",
      "Epoch 9362, Train_Loss: 4069.9931640625, Val_Loss: 4209.0390625\n",
      "Epoch 9363, Train_Loss: 4069.93701171875, Val_Loss: 4208.95751953125\n",
      "Epoch 9364, Train_Loss: 4069.890625, Val_Loss: 4208.87548828125\n",
      "Epoch 9365, Train_Loss: 4069.832275390625, Val_Loss: 4208.80615234375\n",
      "Epoch 9366, Train_Loss: 4069.811767578125, Val_Loss: 4208.72705078125\n",
      "Epoch 9367, Train_Loss: 4069.74853515625, Val_Loss: 4208.64501953125\n",
      "Epoch 9368, Train_Loss: 4069.68408203125, Val_Loss: 4208.56689453125\n",
      "Epoch 9369, Train_Loss: 4069.636474609375, Val_Loss: 4208.498046875\n",
      "Epoch 9370, Train_Loss: 4069.574951171875, Val_Loss: 4208.4130859375\n",
      "Epoch 9371, Train_Loss: 4069.52001953125, Val_Loss: 4208.33935546875\n",
      "Epoch 9372, Train_Loss: 4069.446044921875, Val_Loss: 4208.265625\n",
      "Epoch 9373, Train_Loss: 4069.381103515625, Val_Loss: 4208.17919921875\n",
      "Epoch 9374, Train_Loss: 4069.328369140625, Val_Loss: 4208.10595703125\n",
      "Epoch 9375, Train_Loss: 4069.266845703125, Val_Loss: 4208.02294921875\n",
      "Epoch 9376, Train_Loss: 4069.157470703125, Val_Loss: 4207.9658203125\n",
      "Epoch 9377, Train_Loss: 4069.10498046875, Val_Loss: 4207.888671875\n",
      "Epoch 9378, Train_Loss: 4069.05517578125, Val_Loss: 4207.81201171875\n",
      "Epoch 9379, Train_Loss: 4068.962646484375, Val_Loss: 4207.70654296875\n",
      "Epoch 9380, Train_Loss: 4068.9013671875, Val_Loss: 4207.62353515625\n",
      "Epoch 9381, Train_Loss: 4068.8447265625, Val_Loss: 4207.54638671875\n",
      "Epoch 9382, Train_Loss: 4068.787109375, Val_Loss: 4207.46435546875\n",
      "Epoch 9383, Train_Loss: 4068.72412109375, Val_Loss: 4207.3896484375\n",
      "Epoch 9384, Train_Loss: 4068.66064453125, Val_Loss: 4207.328125\n",
      "Epoch 9385, Train_Loss: 4068.605712890625, Val_Loss: 4207.25537109375\n",
      "Epoch 9386, Train_Loss: 4068.55322265625, Val_Loss: 4207.171875\n",
      "Epoch 9387, Train_Loss: 4068.492431640625, Val_Loss: 4207.08642578125\n",
      "Epoch 9388, Train_Loss: 4068.44384765625, Val_Loss: 4207.02392578125\n",
      "Epoch 9389, Train_Loss: 4068.369873046875, Val_Loss: 4206.9482421875\n",
      "Epoch 9390, Train_Loss: 4068.311767578125, Val_Loss: 4206.87646484375\n",
      "Epoch 9391, Train_Loss: 4068.220703125, Val_Loss: 4206.8232421875\n",
      "Epoch 9392, Train_Loss: 4068.15087890625, Val_Loss: 4206.7392578125\n",
      "Epoch 9393, Train_Loss: 4068.09326171875, Val_Loss: 4206.658203125\n",
      "Epoch 9394, Train_Loss: 4068.02880859375, Val_Loss: 4206.576171875\n",
      "Epoch 9395, Train_Loss: 4067.9638671875, Val_Loss: 4206.4921875\n",
      "Epoch 9396, Train_Loss: 4067.91357421875, Val_Loss: 4206.42138671875\n",
      "Epoch 9397, Train_Loss: 4067.857421875, Val_Loss: 4206.34619140625\n",
      "Epoch 9398, Train_Loss: 4067.803955078125, Val_Loss: 4206.2822265625\n",
      "Epoch 9399, Train_Loss: 4067.63330078125, Val_Loss: 4206.2001953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9400, Train_Loss: 4067.581787109375, Val_Loss: 4206.125\n",
      "Epoch 9401, Train_Loss: 4067.493896484375, Val_Loss: 4206.0439453125\n",
      "Epoch 9402, Train_Loss: 4067.44140625, Val_Loss: 4205.958984375\n",
      "Epoch 9403, Train_Loss: 4067.381103515625, Val_Loss: 4205.880859375\n",
      "Epoch 9404, Train_Loss: 4067.318359375, Val_Loss: 4205.81640625\n",
      "Epoch 9405, Train_Loss: 4067.2626953125, Val_Loss: 4205.73046875\n",
      "Epoch 9406, Train_Loss: 4067.164306640625, Val_Loss: 4205.6640625\n",
      "Epoch 9407, Train_Loss: 4067.11279296875, Val_Loss: 4205.59375\n",
      "Epoch 9408, Train_Loss: 4067.051513671875, Val_Loss: 4205.51416015625\n",
      "Epoch 9409, Train_Loss: 4067.000732421875, Val_Loss: 4205.42919921875\n",
      "Epoch 9410, Train_Loss: 4066.95263671875, Val_Loss: 4205.35302734375\n",
      "Epoch 9411, Train_Loss: 4066.906982421875, Val_Loss: 4205.2763671875\n",
      "Epoch 9412, Train_Loss: 4066.841064453125, Val_Loss: 4205.1962890625\n",
      "Epoch 9413, Train_Loss: 4066.783447265625, Val_Loss: 4205.126953125\n",
      "Epoch 9414, Train_Loss: 4066.72412109375, Val_Loss: 4205.06396484375\n",
      "Epoch 9415, Train_Loss: 4066.663818359375, Val_Loss: 4204.98779296875\n",
      "Epoch 9416, Train_Loss: 4066.6083984375, Val_Loss: 4204.90966796875\n",
      "Epoch 9417, Train_Loss: 4066.55126953125, Val_Loss: 4204.82421875\n",
      "Epoch 9418, Train_Loss: 4066.5, Val_Loss: 4204.740234375\n",
      "Epoch 9419, Train_Loss: 4066.44287109375, Val_Loss: 4204.6796875\n",
      "Epoch 9420, Train_Loss: 4066.335205078125, Val_Loss: 4204.583984375\n",
      "Epoch 9421, Train_Loss: 4066.271484375, Val_Loss: 4204.49072265625\n",
      "Epoch 9422, Train_Loss: 4066.213134765625, Val_Loss: 4204.41015625\n",
      "Epoch 9423, Train_Loss: 4066.140380859375, Val_Loss: 4204.34375\n",
      "Epoch 9424, Train_Loss: 4066.12744140625, Val_Loss: 4204.26025390625\n",
      "Epoch 9425, Train_Loss: 4066.062744140625, Val_Loss: 4204.18212890625\n",
      "Epoch 9426, Train_Loss: 4066.0068359375, Val_Loss: 4204.1064453125\n",
      "Epoch 9427, Train_Loss: 4065.94970703125, Val_Loss: 4204.04736328125\n",
      "Epoch 9428, Train_Loss: 4065.88525390625, Val_Loss: 4203.98046875\n",
      "Epoch 9429, Train_Loss: 4065.83349609375, Val_Loss: 4203.90380859375\n",
      "Epoch 9430, Train_Loss: 4065.77734375, Val_Loss: 4203.82958984375\n",
      "Epoch 9431, Train_Loss: 4065.729736328125, Val_Loss: 4203.75537109375\n",
      "Epoch 9432, Train_Loss: 4065.67041015625, Val_Loss: 4203.67578125\n",
      "Epoch 9433, Train_Loss: 4065.625244140625, Val_Loss: 4203.59765625\n",
      "Epoch 9434, Train_Loss: 4065.552978515625, Val_Loss: 4203.5234375\n",
      "Epoch 9435, Train_Loss: 4065.45361328125, Val_Loss: 4203.4638671875\n",
      "Epoch 9436, Train_Loss: 4065.387451171875, Val_Loss: 4203.38916015625\n",
      "Epoch 9437, Train_Loss: 4065.343994140625, Val_Loss: 4203.310546875\n",
      "Epoch 9438, Train_Loss: 4065.288330078125, Val_Loss: 4203.22216796875\n",
      "Epoch 9439, Train_Loss: 4065.22705078125, Val_Loss: 4203.150390625\n",
      "Epoch 9440, Train_Loss: 4065.172607421875, Val_Loss: 4203.076171875\n",
      "Epoch 9441, Train_Loss: 4065.0888671875, Val_Loss: 4202.9951171875\n",
      "Epoch 9442, Train_Loss: 4065.020751953125, Val_Loss: 4202.94140625\n",
      "Epoch 9443, Train_Loss: 4064.965087890625, Val_Loss: 4202.8662109375\n",
      "Epoch 9444, Train_Loss: 4064.915771484375, Val_Loss: 4202.791015625\n",
      "Epoch 9445, Train_Loss: 4064.83984375, Val_Loss: 4202.70947265625\n",
      "Epoch 9446, Train_Loss: 4064.78857421875, Val_Loss: 4202.63427734375\n",
      "Epoch 9447, Train_Loss: 4064.7177734375, Val_Loss: 4202.55712890625\n",
      "Epoch 9448, Train_Loss: 4064.656005859375, Val_Loss: 4202.4892578125\n",
      "Epoch 9449, Train_Loss: 4064.5732421875, Val_Loss: 4202.416015625\n",
      "Epoch 9450, Train_Loss: 4064.501953125, Val_Loss: 4202.34228515625\n",
      "Epoch 9451, Train_Loss: 4064.45263671875, Val_Loss: 4202.26513671875\n",
      "Epoch 9452, Train_Loss: 4064.4072265625, Val_Loss: 4202.193359375\n",
      "Epoch 9453, Train_Loss: 4064.354736328125, Val_Loss: 4202.10693359375\n",
      "Epoch 9454, Train_Loss: 4064.29443359375, Val_Loss: 4202.03271484375\n",
      "Epoch 9455, Train_Loss: 4064.240966796875, Val_Loss: 4201.9560546875\n",
      "Epoch 9456, Train_Loss: 4064.175048828125, Val_Loss: 4201.88232421875\n",
      "Epoch 9457, Train_Loss: 4064.12255859375, Val_Loss: 4201.81982421875\n",
      "Epoch 9458, Train_Loss: 4064.066650390625, Val_Loss: 4201.74853515625\n",
      "Epoch 9459, Train_Loss: 4064.0146484375, Val_Loss: 4201.677734375\n",
      "Epoch 9460, Train_Loss: 4063.959228515625, Val_Loss: 4201.58984375\n",
      "Epoch 9461, Train_Loss: 4063.8984375, Val_Loss: 4201.4833984375\n",
      "Epoch 9462, Train_Loss: 4063.840576171875, Val_Loss: 4201.40478515625\n",
      "Epoch 9463, Train_Loss: 4063.77685546875, Val_Loss: 4201.330078125\n",
      "Epoch 9464, Train_Loss: 4063.715576171875, Val_Loss: 4201.2734375\n",
      "Epoch 9465, Train_Loss: 4063.665771484375, Val_Loss: 4201.19580078125\n",
      "Epoch 9466, Train_Loss: 4063.61865234375, Val_Loss: 4201.12060546875\n",
      "Epoch 9467, Train_Loss: 4063.538330078125, Val_Loss: 4201.0478515625\n",
      "Epoch 9468, Train_Loss: 4063.4990234375, Val_Loss: 4200.958984375\n",
      "Epoch 9469, Train_Loss: 4063.447265625, Val_Loss: 4200.89111328125\n",
      "Epoch 9470, Train_Loss: 4063.37451171875, Val_Loss: 4200.81787109375\n",
      "Epoch 9471, Train_Loss: 4063.302490234375, Val_Loss: 4200.7587890625\n",
      "Epoch 9472, Train_Loss: 4063.249755859375, Val_Loss: 4200.68701171875\n",
      "Epoch 9473, Train_Loss: 4063.1884765625, Val_Loss: 4200.615234375\n",
      "Epoch 9474, Train_Loss: 4063.122314453125, Val_Loss: 4200.53662109375\n",
      "Epoch 9475, Train_Loss: 4063.063720703125, Val_Loss: 4200.455078125\n",
      "Epoch 9476, Train_Loss: 4063.00537109375, Val_Loss: 4200.37890625\n",
      "Epoch 9477, Train_Loss: 4062.94091796875, Val_Loss: 4200.31298828125\n",
      "Epoch 9478, Train_Loss: 4062.8525390625, Val_Loss: 4200.240234375\n",
      "Epoch 9479, Train_Loss: 4062.793701171875, Val_Loss: 4200.17041015625\n",
      "Epoch 9480, Train_Loss: 4062.74658203125, Val_Loss: 4200.09423828125\n",
      "Epoch 9481, Train_Loss: 4062.69677734375, Val_Loss: 4200.03125\n",
      "Epoch 9482, Train_Loss: 4062.6513671875, Val_Loss: 4199.939453125\n",
      "Epoch 9483, Train_Loss: 4062.580078125, Val_Loss: 4199.86474609375\n",
      "Epoch 9484, Train_Loss: 4062.520263671875, Val_Loss: 4199.7822265625\n",
      "Epoch 9485, Train_Loss: 4062.45849609375, Val_Loss: 4199.72119140625\n",
      "Epoch 9486, Train_Loss: 4062.400390625, Val_Loss: 4199.65087890625\n",
      "Epoch 9487, Train_Loss: 4062.345703125, Val_Loss: 4199.58056640625\n",
      "Epoch 9488, Train_Loss: 4062.28857421875, Val_Loss: 4199.5\n",
      "Epoch 9489, Train_Loss: 4062.225341796875, Val_Loss: 4199.41943359375\n",
      "Epoch 9490, Train_Loss: 4062.161865234375, Val_Loss: 4199.3427734375\n",
      "Epoch 9491, Train_Loss: 4062.104248046875, Val_Loss: 4199.2724609375\n",
      "Epoch 9492, Train_Loss: 4062.1142578125, Val_Loss: 4199.20947265625\n",
      "Epoch 9493, Train_Loss: 4062.00048828125, Val_Loss: 4199.142578125\n",
      "Epoch 9494, Train_Loss: 4061.946533203125, Val_Loss: 4199.0673828125\n",
      "Epoch 9495, Train_Loss: 4061.89111328125, Val_Loss: 4199.00146484375\n",
      "Epoch 9496, Train_Loss: 4061.832275390625, Val_Loss: 4198.93017578125\n",
      "Epoch 9497, Train_Loss: 4061.773681640625, Val_Loss: 4198.8427734375\n",
      "Epoch 9498, Train_Loss: 4061.717041015625, Val_Loss: 4198.76416015625\n",
      "Epoch 9499, Train_Loss: 4061.669921875, Val_Loss: 4198.69775390625\n",
      "Epoch 9500, Train_Loss: 4061.600830078125, Val_Loss: 4198.6318359375\n",
      "Epoch 9501, Train_Loss: 4061.55029296875, Val_Loss: 4198.55712890625\n",
      "Epoch 9502, Train_Loss: 4061.488525390625, Val_Loss: 4198.46337890625\n",
      "Epoch 9503, Train_Loss: 4061.428466796875, Val_Loss: 4198.38818359375\n",
      "Epoch 9504, Train_Loss: 4061.374267578125, Val_Loss: 4198.30322265625\n",
      "Epoch 9505, Train_Loss: 4061.323974609375, Val_Loss: 4198.2353515625\n",
      "Epoch 9506, Train_Loss: 4061.263916015625, Val_Loss: 4198.1611328125\n",
      "Epoch 9507, Train_Loss: 4061.214111328125, Val_Loss: 4198.0791015625\n",
      "Epoch 9508, Train_Loss: 4061.128173828125, Val_Loss: 4198.0166015625\n",
      "Epoch 9509, Train_Loss: 4061.070068359375, Val_Loss: 4197.9521484375\n",
      "Epoch 9510, Train_Loss: 4061.009033203125, Val_Loss: 4197.8662109375\n",
      "Epoch 9511, Train_Loss: 4060.929931640625, Val_Loss: 4197.7900390625\n",
      "Epoch 9512, Train_Loss: 4060.888671875, Val_Loss: 4197.73583984375\n",
      "Epoch 9513, Train_Loss: 4060.83203125, Val_Loss: 4197.67431640625\n",
      "Epoch 9514, Train_Loss: 4060.76806640625, Val_Loss: 4197.58984375\n",
      "Epoch 9515, Train_Loss: 4060.71142578125, Val_Loss: 4197.5302734375\n",
      "Epoch 9516, Train_Loss: 4060.65283203125, Val_Loss: 4197.46044921875\n",
      "Epoch 9517, Train_Loss: 4060.598388671875, Val_Loss: 4197.39013671875\n",
      "Epoch 9518, Train_Loss: 4060.554931640625, Val_Loss: 4197.31884765625\n",
      "Epoch 9519, Train_Loss: 4060.49560546875, Val_Loss: 4197.240234375\n",
      "Epoch 9520, Train_Loss: 4060.431640625, Val_Loss: 4197.17138671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9521, Train_Loss: 4060.3720703125, Val_Loss: 4197.09228515625\n",
      "Epoch 9522, Train_Loss: 4060.27880859375, Val_Loss: 4197.02490234375\n",
      "Epoch 9523, Train_Loss: 4060.219970703125, Val_Loss: 4196.9462890625\n",
      "Epoch 9524, Train_Loss: 4060.153564453125, Val_Loss: 4196.88037109375\n",
      "Epoch 9525, Train_Loss: 4060.09912109375, Val_Loss: 4196.81640625\n",
      "Epoch 9526, Train_Loss: 4060.04296875, Val_Loss: 4196.72900390625\n",
      "Epoch 9527, Train_Loss: 4059.974365234375, Val_Loss: 4196.6494140625\n",
      "Epoch 9528, Train_Loss: 4059.9228515625, Val_Loss: 4196.58447265625\n",
      "Epoch 9529, Train_Loss: 4059.881103515625, Val_Loss: 4196.50927734375\n",
      "Epoch 9530, Train_Loss: 4059.817626953125, Val_Loss: 4196.4521484375\n",
      "Epoch 9531, Train_Loss: 4059.7578125, Val_Loss: 4196.38623046875\n",
      "Epoch 9532, Train_Loss: 4059.695068359375, Val_Loss: 4196.31494140625\n",
      "Epoch 9533, Train_Loss: 4059.624267578125, Val_Loss: 4196.22900390625\n",
      "Epoch 9534, Train_Loss: 4059.5693359375, Val_Loss: 4196.1591796875\n",
      "Epoch 9535, Train_Loss: 4059.508056640625, Val_Loss: 4196.07177734375\n",
      "Epoch 9536, Train_Loss: 4059.46826171875, Val_Loss: 4196.00537109375\n",
      "Epoch 9537, Train_Loss: 4059.402587890625, Val_Loss: 4195.94580078125\n",
      "Epoch 9538, Train_Loss: 4059.350341796875, Val_Loss: 4195.88037109375\n",
      "Epoch 9539, Train_Loss: 4059.29150390625, Val_Loss: 4195.81201171875\n",
      "Epoch 9540, Train_Loss: 4059.24755859375, Val_Loss: 4195.7470703125\n",
      "Epoch 9541, Train_Loss: 4059.2314453125, Val_Loss: 4195.65576171875\n",
      "Epoch 9542, Train_Loss: 4059.171630859375, Val_Loss: 4195.59619140625\n",
      "Epoch 9543, Train_Loss: 4059.11865234375, Val_Loss: 4195.4892578125\n",
      "Epoch 9544, Train_Loss: 4059.049560546875, Val_Loss: 4195.43505859375\n",
      "Epoch 9545, Train_Loss: 4058.9951171875, Val_Loss: 4195.3701171875\n",
      "Epoch 9546, Train_Loss: 4058.94775390625, Val_Loss: 4195.2919921875\n",
      "Epoch 9547, Train_Loss: 4058.901123046875, Val_Loss: 4195.216796875\n",
      "Epoch 9548, Train_Loss: 4058.84423828125, Val_Loss: 4195.134765625\n",
      "Epoch 9549, Train_Loss: 4058.791748046875, Val_Loss: 4195.0556640625\n",
      "Epoch 9550, Train_Loss: 4058.733154296875, Val_Loss: 4194.98828125\n",
      "Epoch 9551, Train_Loss: 4058.6806640625, Val_Loss: 4194.923828125\n",
      "Epoch 9552, Train_Loss: 4058.575927734375, Val_Loss: 4194.87353515625\n",
      "Epoch 9553, Train_Loss: 4058.498779296875, Val_Loss: 4194.79052734375\n",
      "Epoch 9554, Train_Loss: 4058.446533203125, Val_Loss: 4194.73681640625\n",
      "Epoch 9555, Train_Loss: 4058.37255859375, Val_Loss: 4194.6484375\n",
      "Epoch 9556, Train_Loss: 4058.327880859375, Val_Loss: 4194.5810546875\n",
      "Epoch 9557, Train_Loss: 4058.267333984375, Val_Loss: 4194.5048828125\n",
      "Epoch 9558, Train_Loss: 4058.205810546875, Val_Loss: 4194.43701171875\n",
      "Epoch 9559, Train_Loss: 4058.15234375, Val_Loss: 4194.37548828125\n",
      "Epoch 9560, Train_Loss: 4058.095947265625, Val_Loss: 4194.30615234375\n",
      "Epoch 9561, Train_Loss: 4058.04296875, Val_Loss: 4194.23681640625\n",
      "Epoch 9562, Train_Loss: 4057.995849609375, Val_Loss: 4194.16943359375\n",
      "Epoch 9563, Train_Loss: 4057.938720703125, Val_Loss: 4194.09765625\n",
      "Epoch 9564, Train_Loss: 4057.722412109375, Val_Loss: 4194.0205078125\n",
      "Epoch 9565, Train_Loss: 4057.675048828125, Val_Loss: 4193.94140625\n",
      "Epoch 9566, Train_Loss: 4057.5751953125, Val_Loss: 4193.89501953125\n",
      "Epoch 9567, Train_Loss: 4057.5185546875, Val_Loss: 4193.8203125\n",
      "Epoch 9568, Train_Loss: 4057.4619140625, Val_Loss: 4193.7421875\n",
      "Epoch 9569, Train_Loss: 4057.4140625, Val_Loss: 4193.67138671875\n",
      "Epoch 9570, Train_Loss: 4057.376953125, Val_Loss: 4193.60205078125\n",
      "Epoch 9571, Train_Loss: 4057.31884765625, Val_Loss: 4193.53466796875\n",
      "Epoch 9572, Train_Loss: 4057.25341796875, Val_Loss: 4193.458984375\n",
      "Epoch 9573, Train_Loss: 4057.199951171875, Val_Loss: 4193.392578125\n",
      "Epoch 9574, Train_Loss: 4057.1416015625, Val_Loss: 4193.33447265625\n",
      "Epoch 9575, Train_Loss: 4057.087890625, Val_Loss: 4193.25927734375\n",
      "Epoch 9576, Train_Loss: 4057.035400390625, Val_Loss: 4193.18896484375\n",
      "Epoch 9577, Train_Loss: 4056.98876953125, Val_Loss: 4193.11474609375\n",
      "Epoch 9578, Train_Loss: 4056.928466796875, Val_Loss: 4193.03955078125\n",
      "Epoch 9579, Train_Loss: 4056.8671875, Val_Loss: 4192.9736328125\n",
      "Epoch 9580, Train_Loss: 4056.8251953125, Val_Loss: 4192.9013671875\n",
      "Epoch 9581, Train_Loss: 4056.7587890625, Val_Loss: 4192.84619140625\n",
      "Epoch 9582, Train_Loss: 4056.708984375, Val_Loss: 4192.771484375\n",
      "Epoch 9583, Train_Loss: 4056.657470703125, Val_Loss: 4192.70703125\n",
      "Epoch 9584, Train_Loss: 4056.6015625, Val_Loss: 4192.6376953125\n",
      "Epoch 9585, Train_Loss: 4056.54345703125, Val_Loss: 4192.5244140625\n",
      "Epoch 9586, Train_Loss: 4056.489501953125, Val_Loss: 4192.4658203125\n",
      "Epoch 9587, Train_Loss: 4056.43115234375, Val_Loss: 4192.392578125\n",
      "Epoch 9588, Train_Loss: 4056.374755859375, Val_Loss: 4192.33251953125\n",
      "Epoch 9589, Train_Loss: 4056.317138671875, Val_Loss: 4192.26220703125\n",
      "Epoch 9590, Train_Loss: 4056.2509765625, Val_Loss: 4192.19970703125\n",
      "Epoch 9591, Train_Loss: 4056.1943359375, Val_Loss: 4192.12841796875\n",
      "Epoch 9592, Train_Loss: 4056.14453125, Val_Loss: 4192.05224609375\n",
      "Epoch 9593, Train_Loss: 4056.1142578125, Val_Loss: 4191.98291015625\n",
      "Epoch 9594, Train_Loss: 4056.046142578125, Val_Loss: 4191.91455078125\n",
      "Epoch 9595, Train_Loss: 4055.9873046875, Val_Loss: 4191.84814453125\n",
      "Epoch 9596, Train_Loss: 4055.895263671875, Val_Loss: 4191.7890625\n",
      "Epoch 9597, Train_Loss: 4055.85791015625, Val_Loss: 4191.7216796875\n",
      "Epoch 9598, Train_Loss: 4055.81298828125, Val_Loss: 4191.66064453125\n",
      "Epoch 9599, Train_Loss: 4055.810791015625, Val_Loss: 4191.58447265625\n",
      "Epoch 9600, Train_Loss: 4055.739013671875, Val_Loss: 4191.51171875\n",
      "Epoch 9601, Train_Loss: 4055.666748046875, Val_Loss: 4191.439453125\n",
      "Epoch 9602, Train_Loss: 4055.6123046875, Val_Loss: 4191.3759765625\n",
      "Epoch 9603, Train_Loss: 4055.566162109375, Val_Loss: 4191.31689453125\n",
      "Epoch 9604, Train_Loss: 4055.51953125, Val_Loss: 4191.2548828125\n",
      "Epoch 9605, Train_Loss: 4055.451904296875, Val_Loss: 4191.16943359375\n",
      "Epoch 9606, Train_Loss: 4055.39208984375, Val_Loss: 4191.10546875\n",
      "Epoch 9607, Train_Loss: 4055.33154296875, Val_Loss: 4191.02099609375\n",
      "Epoch 9608, Train_Loss: 4055.26806640625, Val_Loss: 4190.96630859375\n",
      "Epoch 9609, Train_Loss: 4055.213623046875, Val_Loss: 4190.8955078125\n",
      "Epoch 9610, Train_Loss: 4055.11279296875, Val_Loss: 4190.84130859375\n",
      "Epoch 9611, Train_Loss: 4055.05419921875, Val_Loss: 4190.76708984375\n",
      "Epoch 9612, Train_Loss: 4055.00537109375, Val_Loss: 4190.68701171875\n",
      "Epoch 9613, Train_Loss: 4054.948486328125, Val_Loss: 4190.625\n",
      "Epoch 9614, Train_Loss: 4054.896484375, Val_Loss: 4190.548828125\n",
      "Epoch 9615, Train_Loss: 4054.8369140625, Val_Loss: 4190.47900390625\n",
      "Epoch 9616, Train_Loss: 4054.78662109375, Val_Loss: 4190.41162109375\n",
      "Epoch 9617, Train_Loss: 4054.740234375, Val_Loss: 4190.34765625\n",
      "Epoch 9618, Train_Loss: 4054.685302734375, Val_Loss: 4190.28076171875\n",
      "Epoch 9619, Train_Loss: 4054.619873046875, Val_Loss: 4190.21533203125\n",
      "Epoch 9620, Train_Loss: 4054.56787109375, Val_Loss: 4190.14794921875\n",
      "Epoch 9621, Train_Loss: 4054.50927734375, Val_Loss: 4190.06201171875\n",
      "Epoch 9622, Train_Loss: 4054.433837890625, Val_Loss: 4189.99462890625\n",
      "Epoch 9623, Train_Loss: 4054.38134765625, Val_Loss: 4189.9296875\n",
      "Epoch 9624, Train_Loss: 4054.331787109375, Val_Loss: 4189.8623046875\n",
      "Epoch 9625, Train_Loss: 4054.252685546875, Val_Loss: 4189.80908203125\n",
      "Epoch 9626, Train_Loss: 4054.188720703125, Val_Loss: 4189.72216796875\n",
      "Epoch 9627, Train_Loss: 4054.1279296875, Val_Loss: 4189.640625\n",
      "Epoch 9628, Train_Loss: 4054.05908203125, Val_Loss: 4189.57421875\n",
      "Epoch 9629, Train_Loss: 4054.004638671875, Val_Loss: 4189.48779296875\n",
      "Epoch 9630, Train_Loss: 4053.954345703125, Val_Loss: 4189.43310546875\n",
      "Epoch 9631, Train_Loss: 4053.909423828125, Val_Loss: 4189.36376953125\n",
      "Epoch 9632, Train_Loss: 4053.85546875, Val_Loss: 4189.29833984375\n",
      "Epoch 9633, Train_Loss: 4053.80712890625, Val_Loss: 4189.23388671875\n",
      "Epoch 9634, Train_Loss: 4053.753173828125, Val_Loss: 4189.1728515625\n",
      "Epoch 9635, Train_Loss: 4053.70556640625, Val_Loss: 4189.107421875\n",
      "Epoch 9636, Train_Loss: 4053.64990234375, Val_Loss: 4189.03369140625\n",
      "Epoch 9637, Train_Loss: 4053.59228515625, Val_Loss: 4188.95703125\n",
      "Epoch 9638, Train_Loss: 4053.52783203125, Val_Loss: 4188.89404296875\n",
      "Epoch 9639, Train_Loss: 4053.4775390625, Val_Loss: 4188.8310546875\n",
      "Epoch 9640, Train_Loss: 4053.400146484375, Val_Loss: 4188.77783203125\n",
      "Epoch 9641, Train_Loss: 4053.35302734375, Val_Loss: 4188.71044921875\n",
      "Epoch 9642, Train_Loss: 4053.2900390625, Val_Loss: 4188.6328125\n",
      "Epoch 9643, Train_Loss: 4053.240234375, Val_Loss: 4188.55078125\n",
      "Epoch 9644, Train_Loss: 4053.173583984375, Val_Loss: 4188.4921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9645, Train_Loss: 4053.122802734375, Val_Loss: 4188.42919921875\n",
      "Epoch 9646, Train_Loss: 4053.071533203125, Val_Loss: 4188.3603515625\n",
      "Epoch 9647, Train_Loss: 4052.957275390625, Val_Loss: 4188.3046875\n",
      "Epoch 9648, Train_Loss: 4052.907958984375, Val_Loss: 4188.23046875\n",
      "Epoch 9649, Train_Loss: 4052.84716796875, Val_Loss: 4188.16748046875\n",
      "Epoch 9650, Train_Loss: 4052.792724609375, Val_Loss: 4188.0986328125\n",
      "Epoch 9651, Train_Loss: 4052.7470703125, Val_Loss: 4188.017578125\n",
      "Epoch 9652, Train_Loss: 4052.68017578125, Val_Loss: 4187.96142578125\n",
      "Epoch 9653, Train_Loss: 4052.626220703125, Val_Loss: 4187.8857421875\n",
      "Epoch 9654, Train_Loss: 4052.5517578125, Val_Loss: 4187.8232421875\n",
      "Epoch 9655, Train_Loss: 4052.48779296875, Val_Loss: 4187.76904296875\n",
      "Epoch 9656, Train_Loss: 4052.43701171875, Val_Loss: 4187.69775390625\n",
      "Epoch 9657, Train_Loss: 4052.40966796875, Val_Loss: 4187.6240234375\n",
      "Epoch 9658, Train_Loss: 4052.40869140625, Val_Loss: 4187.5478515625\n",
      "Epoch 9659, Train_Loss: 4052.355224609375, Val_Loss: 4187.48193359375\n",
      "Epoch 9660, Train_Loss: 4052.295654296875, Val_Loss: 4187.4091796875\n",
      "Epoch 9661, Train_Loss: 4052.239990234375, Val_Loss: 4187.35693359375\n",
      "Epoch 9662, Train_Loss: 4052.180908203125, Val_Loss: 4187.29541015625\n",
      "Epoch 9663, Train_Loss: 4052.132568359375, Val_Loss: 4187.236328125\n",
      "Epoch 9664, Train_Loss: 4052.087890625, Val_Loss: 4187.17578125\n",
      "Epoch 9665, Train_Loss: 4052.040771484375, Val_Loss: 4187.0947265625\n",
      "Epoch 9666, Train_Loss: 4051.982666015625, Val_Loss: 4187.0244140625\n",
      "Epoch 9667, Train_Loss: 4051.90576171875, Val_Loss: 4186.958984375\n",
      "Epoch 9668, Train_Loss: 4051.840576171875, Val_Loss: 4186.8603515625\n",
      "Epoch 9669, Train_Loss: 4051.739501953125, Val_Loss: 4186.80615234375\n",
      "Epoch 9670, Train_Loss: 4051.69091796875, Val_Loss: 4186.732421875\n",
      "Epoch 9671, Train_Loss: 4051.645751953125, Val_Loss: 4186.67138671875\n",
      "Epoch 9672, Train_Loss: 4051.59716796875, Val_Loss: 4186.609375\n",
      "Epoch 9673, Train_Loss: 4051.546630859375, Val_Loss: 4186.52685546875\n",
      "Epoch 9674, Train_Loss: 4051.490234375, Val_Loss: 4186.453125\n",
      "Epoch 9675, Train_Loss: 4051.436279296875, Val_Loss: 4186.3994140625\n",
      "Epoch 9676, Train_Loss: 4051.392822265625, Val_Loss: 4186.32861328125\n",
      "Epoch 9677, Train_Loss: 4051.338134765625, Val_Loss: 4186.271484375\n",
      "Epoch 9678, Train_Loss: 4051.284912109375, Val_Loss: 4186.21630859375\n",
      "Epoch 9679, Train_Loss: 4051.23486328125, Val_Loss: 4186.1474609375\n",
      "Epoch 9680, Train_Loss: 4051.194580078125, Val_Loss: 4186.08447265625\n",
      "Epoch 9681, Train_Loss: 4051.143310546875, Val_Loss: 4186.00830078125\n",
      "Epoch 9682, Train_Loss: 4051.0830078125, Val_Loss: 4185.94677734375\n",
      "Epoch 9683, Train_Loss: 4051.03076171875, Val_Loss: 4185.89013671875\n",
      "Epoch 9684, Train_Loss: 4050.9443359375, Val_Loss: 4185.826171875\n",
      "Epoch 9685, Train_Loss: 4050.8955078125, Val_Loss: 4185.7607421875\n",
      "Epoch 9686, Train_Loss: 4050.85205078125, Val_Loss: 4185.69677734375\n",
      "Epoch 9687, Train_Loss: 4050.790771484375, Val_Loss: 4185.62890625\n",
      "Epoch 9688, Train_Loss: 4050.750732421875, Val_Loss: 4185.54736328125\n",
      "Epoch 9689, Train_Loss: 4050.671630859375, Val_Loss: 4185.49267578125\n",
      "Epoch 9690, Train_Loss: 4050.61083984375, Val_Loss: 4185.4189453125\n",
      "Epoch 9691, Train_Loss: 4050.55859375, Val_Loss: 4185.36376953125\n",
      "Epoch 9692, Train_Loss: 4050.505126953125, Val_Loss: 4185.30029296875\n",
      "Epoch 9693, Train_Loss: 4050.45166015625, Val_Loss: 4185.234375\n",
      "Epoch 9694, Train_Loss: 4050.41455078125, Val_Loss: 4185.17138671875\n",
      "Epoch 9695, Train_Loss: 4050.356201171875, Val_Loss: 4185.0986328125\n",
      "Epoch 9696, Train_Loss: 4050.304931640625, Val_Loss: 4185.02099609375\n",
      "Epoch 9697, Train_Loss: 4050.25, Val_Loss: 4184.95849609375\n",
      "Epoch 9698, Train_Loss: 4050.181884765625, Val_Loss: 4184.8935546875\n",
      "Epoch 9699, Train_Loss: 4050.13720703125, Val_Loss: 4184.8359375\n",
      "Epoch 9700, Train_Loss: 4050.072265625, Val_Loss: 4184.767578125\n",
      "Epoch 9701, Train_Loss: 4050.03662109375, Val_Loss: 4184.703125\n",
      "Epoch 9702, Train_Loss: 4049.983642578125, Val_Loss: 4184.6337890625\n",
      "Epoch 9703, Train_Loss: 4049.94189453125, Val_Loss: 4184.56689453125\n",
      "Epoch 9704, Train_Loss: 4049.89306640625, Val_Loss: 4184.494140625\n",
      "Epoch 9705, Train_Loss: 4049.841796875, Val_Loss: 4184.43408203125\n",
      "Epoch 9706, Train_Loss: 4049.77880859375, Val_Loss: 4184.376953125\n",
      "Epoch 9707, Train_Loss: 4049.74267578125, Val_Loss: 4184.31982421875\n",
      "Epoch 9708, Train_Loss: 4049.686767578125, Val_Loss: 4184.2509765625\n",
      "Epoch 9709, Train_Loss: 4049.623291015625, Val_Loss: 4184.146484375\n",
      "Epoch 9710, Train_Loss: 4049.56201171875, Val_Loss: 4184.08740234375\n",
      "Epoch 9711, Train_Loss: 4049.480712890625, Val_Loss: 4184.0205078125\n",
      "Epoch 9712, Train_Loss: 4049.43212890625, Val_Loss: 4183.958984375\n",
      "Epoch 9713, Train_Loss: 4049.39111328125, Val_Loss: 4183.88818359375\n",
      "Epoch 9714, Train_Loss: 4049.3125, Val_Loss: 4183.83056640625\n",
      "Epoch 9715, Train_Loss: 4049.255859375, Val_Loss: 4183.76318359375\n",
      "Epoch 9716, Train_Loss: 4049.1953125, Val_Loss: 4183.69970703125\n",
      "Epoch 9717, Train_Loss: 4049.18310546875, Val_Loss: 4183.626953125\n",
      "Epoch 9718, Train_Loss: 4049.130126953125, Val_Loss: 4183.556640625\n",
      "Epoch 9719, Train_Loss: 4049.092041015625, Val_Loss: 4183.50537109375\n",
      "Epoch 9720, Train_Loss: 4049.042236328125, Val_Loss: 4183.43994140625\n",
      "Epoch 9721, Train_Loss: 4048.991943359375, Val_Loss: 4183.38720703125\n",
      "Epoch 9722, Train_Loss: 4048.9521484375, Val_Loss: 4183.32177734375\n",
      "Epoch 9723, Train_Loss: 4048.907470703125, Val_Loss: 4183.26220703125\n",
      "Epoch 9724, Train_Loss: 4048.857177734375, Val_Loss: 4183.19384765625\n",
      "Epoch 9725, Train_Loss: 4048.813720703125, Val_Loss: 4183.126953125\n",
      "Epoch 9726, Train_Loss: 4048.763427734375, Val_Loss: 4183.0546875\n",
      "Epoch 9727, Train_Loss: 4048.7158203125, Val_Loss: 4182.9970703125\n",
      "Epoch 9728, Train_Loss: 4048.611572265625, Val_Loss: 4182.93701171875\n",
      "Epoch 9729, Train_Loss: 4048.552734375, Val_Loss: 4182.8720703125\n",
      "Epoch 9730, Train_Loss: 4048.4521484375, Val_Loss: 4182.791015625\n",
      "Epoch 9731, Train_Loss: 4048.398681640625, Val_Loss: 4182.7353515625\n",
      "Epoch 9732, Train_Loss: 4048.35791015625, Val_Loss: 4182.66455078125\n",
      "Epoch 9733, Train_Loss: 4048.310302734375, Val_Loss: 4182.60546875\n",
      "Epoch 9734, Train_Loss: 4048.2353515625, Val_Loss: 4182.54248046875\n",
      "Epoch 9735, Train_Loss: 4048.1796875, Val_Loss: 4182.4755859375\n",
      "Epoch 9736, Train_Loss: 4048.12939453125, Val_Loss: 4182.421875\n",
      "Epoch 9737, Train_Loss: 4048.0791015625, Val_Loss: 4182.35791015625\n",
      "Epoch 9738, Train_Loss: 4048.02099609375, Val_Loss: 4182.291015625\n",
      "Epoch 9739, Train_Loss: 4047.973876953125, Val_Loss: 4182.2275390625\n",
      "Epoch 9740, Train_Loss: 4047.91943359375, Val_Loss: 4182.14794921875\n",
      "Epoch 9741, Train_Loss: 4047.87939453125, Val_Loss: 4182.08203125\n",
      "Epoch 9742, Train_Loss: 4047.837646484375, Val_Loss: 4182.0185546875\n",
      "Epoch 9743, Train_Loss: 4047.752685546875, Val_Loss: 4181.9619140625\n",
      "Epoch 9744, Train_Loss: 4047.695556640625, Val_Loss: 4181.89697265625\n",
      "Epoch 9745, Train_Loss: 4047.637939453125, Val_Loss: 4181.84228515625\n",
      "Epoch 9746, Train_Loss: 4047.583984375, Val_Loss: 4181.77001953125\n",
      "Epoch 9747, Train_Loss: 4047.538330078125, Val_Loss: 4181.6982421875\n",
      "Epoch 9748, Train_Loss: 4047.488525390625, Val_Loss: 4181.642578125\n",
      "Epoch 9749, Train_Loss: 4047.438720703125, Val_Loss: 4181.57421875\n",
      "Epoch 9750, Train_Loss: 4047.393310546875, Val_Loss: 4181.50927734375\n",
      "Epoch 9751, Train_Loss: 4047.3046875, Val_Loss: 4181.41943359375\n",
      "Epoch 9752, Train_Loss: 4047.244873046875, Val_Loss: 4181.35498046875\n",
      "Epoch 9753, Train_Loss: 4047.1962890625, Val_Loss: 4181.30126953125\n",
      "Epoch 9754, Train_Loss: 4047.148681640625, Val_Loss: 4181.2333984375\n",
      "Epoch 9755, Train_Loss: 4047.10302734375, Val_Loss: 4181.16015625\n",
      "Epoch 9756, Train_Loss: 4047.03369140625, Val_Loss: 4181.1005859375\n",
      "Epoch 9757, Train_Loss: 4046.987060546875, Val_Loss: 4181.03662109375\n",
      "Epoch 9758, Train_Loss: 4046.904296875, Val_Loss: 4180.9873046875\n",
      "Epoch 9759, Train_Loss: 4046.84765625, Val_Loss: 4180.92236328125\n",
      "Epoch 9760, Train_Loss: 4046.8046875, Val_Loss: 4180.85546875\n",
      "Epoch 9761, Train_Loss: 4046.75146484375, Val_Loss: 4180.80029296875\n",
      "Epoch 9762, Train_Loss: 4046.697509765625, Val_Loss: 4180.7314453125\n",
      "Epoch 9763, Train_Loss: 4046.6591796875, Val_Loss: 4180.66162109375\n",
      "Epoch 9764, Train_Loss: 4046.60302734375, Val_Loss: 4180.6005859375\n",
      "Epoch 9765, Train_Loss: 4046.55810546875, Val_Loss: 4180.541015625\n",
      "Epoch 9766, Train_Loss: 4046.509765625, Val_Loss: 4180.49072265625\n",
      "Epoch 9767, Train_Loss: 4046.463134765625, Val_Loss: 4180.4326171875\n",
      "Epoch 9768, Train_Loss: 4046.406494140625, Val_Loss: 4180.36279296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9769, Train_Loss: 4046.349609375, Val_Loss: 4180.2919921875\n",
      "Epoch 9770, Train_Loss: 4046.30859375, Val_Loss: 4180.23095703125\n",
      "Epoch 9771, Train_Loss: 4046.25244140625, Val_Loss: 4180.1669921875\n",
      "Epoch 9772, Train_Loss: 4046.19482421875, Val_Loss: 4180.095703125\n",
      "Epoch 9773, Train_Loss: 4046.126708984375, Val_Loss: 4180.0439453125\n",
      "Epoch 9774, Train_Loss: 4046.069091796875, Val_Loss: 4179.98681640625\n",
      "Epoch 9775, Train_Loss: 4046.010986328125, Val_Loss: 4179.91796875\n",
      "Epoch 9776, Train_Loss: 4045.96240234375, Val_Loss: 4179.86181640625\n",
      "Epoch 9777, Train_Loss: 4045.968505859375, Val_Loss: 4179.77978515625\n",
      "Epoch 9778, Train_Loss: 4045.924072265625, Val_Loss: 4179.7265625\n",
      "Epoch 9779, Train_Loss: 4045.859130859375, Val_Loss: 4179.65625\n",
      "Epoch 9780, Train_Loss: 4045.806640625, Val_Loss: 4179.5927734375\n",
      "Epoch 9781, Train_Loss: 4045.76171875, Val_Loss: 4179.5380859375\n",
      "Epoch 9782, Train_Loss: 4045.708251953125, Val_Loss: 4179.484375\n",
      "Epoch 9783, Train_Loss: 4045.66455078125, Val_Loss: 4179.4140625\n",
      "Epoch 9784, Train_Loss: 4045.610595703125, Val_Loss: 4179.341796875\n",
      "Epoch 9785, Train_Loss: 4045.5537109375, Val_Loss: 4179.28857421875\n",
      "Epoch 9786, Train_Loss: 4045.5048828125, Val_Loss: 4179.216796875\n",
      "Epoch 9787, Train_Loss: 4045.452880859375, Val_Loss: 4179.14990234375\n",
      "Epoch 9788, Train_Loss: 4045.358642578125, Val_Loss: 4179.10498046875\n",
      "Epoch 9789, Train_Loss: 4045.310546875, Val_Loss: 4179.04296875\n",
      "Epoch 9790, Train_Loss: 4045.242919921875, Val_Loss: 4178.9794921875\n",
      "Epoch 9791, Train_Loss: 4045.207763671875, Val_Loss: 4178.916015625\n",
      "Epoch 9792, Train_Loss: 4045.165771484375, Val_Loss: 4178.8427734375\n",
      "Epoch 9793, Train_Loss: 4045.1103515625, Val_Loss: 4178.74755859375\n",
      "Epoch 9794, Train_Loss: 4045.0556640625, Val_Loss: 4178.6845703125\n",
      "Epoch 9795, Train_Loss: 4045.017578125, Val_Loss: 4178.62744140625\n",
      "Epoch 9796, Train_Loss: 4044.955322265625, Val_Loss: 4178.56884765625\n",
      "Epoch 9797, Train_Loss: 4044.898193359375, Val_Loss: 4178.509765625\n",
      "Epoch 9798, Train_Loss: 4044.843994140625, Val_Loss: 4178.439453125\n",
      "Epoch 9799, Train_Loss: 4044.799560546875, Val_Loss: 4178.36767578125\n",
      "Epoch 9800, Train_Loss: 4044.755615234375, Val_Loss: 4178.31201171875\n",
      "Epoch 9801, Train_Loss: 4044.692138671875, Val_Loss: 4178.24462890625\n",
      "Epoch 9802, Train_Loss: 4044.643310546875, Val_Loss: 4178.18701171875\n",
      "Epoch 9803, Train_Loss: 4044.55908203125, Val_Loss: 4178.13134765625\n",
      "Epoch 9804, Train_Loss: 4044.485595703125, Val_Loss: 4178.0703125\n",
      "Epoch 9805, Train_Loss: 4044.426513671875, Val_Loss: 4178.0087890625\n",
      "Epoch 9806, Train_Loss: 4044.383544921875, Val_Loss: 4177.953125\n",
      "Epoch 9807, Train_Loss: 4044.345947265625, Val_Loss: 4177.87890625\n",
      "Epoch 9808, Train_Loss: 4044.298583984375, Val_Loss: 4177.8125\n",
      "Epoch 9809, Train_Loss: 4044.252685546875, Val_Loss: 4177.75341796875\n",
      "Epoch 9810, Train_Loss: 4044.2021484375, Val_Loss: 4177.70849609375\n",
      "Epoch 9811, Train_Loss: 4044.150390625, Val_Loss: 4177.64697265625\n",
      "Epoch 9812, Train_Loss: 4044.104736328125, Val_Loss: 4177.58251953125\n",
      "Epoch 9813, Train_Loss: 4044.053466796875, Val_Loss: 4177.5107421875\n",
      "Epoch 9814, Train_Loss: 4043.953369140625, Val_Loss: 4177.4345703125\n",
      "Epoch 9815, Train_Loss: 4043.9052734375, Val_Loss: 4177.3740234375\n",
      "Epoch 9816, Train_Loss: 4043.8486328125, Val_Loss: 4177.3154296875\n",
      "Epoch 9817, Train_Loss: 4043.795166015625, Val_Loss: 4177.25\n",
      "Epoch 9818, Train_Loss: 4043.738037109375, Val_Loss: 4177.197265625\n",
      "Epoch 9819, Train_Loss: 4043.69287109375, Val_Loss: 4177.134765625\n",
      "Epoch 9820, Train_Loss: 4043.65625, Val_Loss: 4177.0693359375\n",
      "Epoch 9821, Train_Loss: 4043.601806640625, Val_Loss: 4177.00634765625\n",
      "Epoch 9822, Train_Loss: 4043.551025390625, Val_Loss: 4176.93359375\n",
      "Epoch 9823, Train_Loss: 4043.500732421875, Val_Loss: 4176.87451171875\n",
      "Epoch 9824, Train_Loss: 4043.435791015625, Val_Loss: 4176.81298828125\n",
      "Epoch 9825, Train_Loss: 4043.450439453125, Val_Loss: 4176.75927734375\n",
      "Epoch 9826, Train_Loss: 4043.41796875, Val_Loss: 4176.70263671875\n",
      "Epoch 9827, Train_Loss: 4043.364990234375, Val_Loss: 4176.6513671875\n",
      "Epoch 9828, Train_Loss: 4043.321044921875, Val_Loss: 4176.580078125\n",
      "Epoch 9829, Train_Loss: 4043.278564453125, Val_Loss: 4176.5087890625\n",
      "Epoch 9830, Train_Loss: 4043.22021484375, Val_Loss: 4176.447265625\n",
      "Epoch 9831, Train_Loss: 4043.178466796875, Val_Loss: 4176.38232421875\n",
      "Epoch 9832, Train_Loss: 4043.135009765625, Val_Loss: 4176.3173828125\n",
      "Epoch 9833, Train_Loss: 4043.058349609375, Val_Loss: 4176.2666015625\n",
      "Epoch 9834, Train_Loss: 4043.0166015625, Val_Loss: 4176.21435546875\n",
      "Epoch 9835, Train_Loss: 4042.939453125, Val_Loss: 4176.1171875\n",
      "Epoch 9836, Train_Loss: 4042.8828125, Val_Loss: 4176.0556640625\n",
      "Epoch 9837, Train_Loss: 4042.869384765625, Val_Loss: 4175.98046875\n",
      "Epoch 9838, Train_Loss: 4042.820556640625, Val_Loss: 4175.9208984375\n",
      "Epoch 9839, Train_Loss: 4042.79638671875, Val_Loss: 4175.85546875\n",
      "Epoch 9840, Train_Loss: 4042.743408203125, Val_Loss: 4175.802734375\n",
      "Epoch 9841, Train_Loss: 4042.692626953125, Val_Loss: 4175.7451171875\n",
      "Epoch 9842, Train_Loss: 4042.6376953125, Val_Loss: 4175.68359375\n",
      "Epoch 9843, Train_Loss: 4042.598388671875, Val_Loss: 4175.62890625\n",
      "Epoch 9844, Train_Loss: 4042.552734375, Val_Loss: 4175.54736328125\n",
      "Epoch 9845, Train_Loss: 4042.501953125, Val_Loss: 4175.49462890625\n",
      "Epoch 9846, Train_Loss: 4042.448486328125, Val_Loss: 4175.4345703125\n",
      "Epoch 9847, Train_Loss: 4042.37451171875, Val_Loss: 4175.3720703125\n",
      "Epoch 9848, Train_Loss: 4042.2724609375, Val_Loss: 4175.3232421875\n",
      "Epoch 9849, Train_Loss: 4042.22802734375, Val_Loss: 4175.26171875\n",
      "Epoch 9850, Train_Loss: 4042.182373046875, Val_Loss: 4175.193359375\n",
      "Epoch 9851, Train_Loss: 4042.140380859375, Val_Loss: 4175.142578125\n",
      "Epoch 9852, Train_Loss: 4042.08544921875, Val_Loss: 4175.0712890625\n",
      "Epoch 9853, Train_Loss: 4042.045166015625, Val_Loss: 4175.0029296875\n",
      "Epoch 9854, Train_Loss: 4041.995849609375, Val_Loss: 4174.94384765625\n",
      "Epoch 9855, Train_Loss: 4041.9423828125, Val_Loss: 4174.89501953125\n",
      "Epoch 9856, Train_Loss: 4041.876953125, Val_Loss: 4174.8173828125\n",
      "Epoch 9857, Train_Loss: 4041.819091796875, Val_Loss: 4174.751953125\n",
      "Epoch 9858, Train_Loss: 4041.760986328125, Val_Loss: 4174.703125\n",
      "Epoch 9859, Train_Loss: 4041.718017578125, Val_Loss: 4174.62939453125\n",
      "Epoch 9860, Train_Loss: 4041.674560546875, Val_Loss: 4174.58056640625\n",
      "Epoch 9861, Train_Loss: 4041.626953125, Val_Loss: 4174.5126953125\n",
      "Epoch 9862, Train_Loss: 4041.5849609375, Val_Loss: 4174.44921875\n",
      "Epoch 9863, Train_Loss: 4041.501220703125, Val_Loss: 4174.39501953125\n",
      "Epoch 9864, Train_Loss: 4041.452880859375, Val_Loss: 4174.333984375\n",
      "Epoch 9865, Train_Loss: 4041.408447265625, Val_Loss: 4174.2666015625\n",
      "Epoch 9866, Train_Loss: 4041.373291015625, Val_Loss: 4174.21875\n",
      "Epoch 9867, Train_Loss: 4041.32568359375, Val_Loss: 4174.142578125\n",
      "Epoch 9868, Train_Loss: 4041.272705078125, Val_Loss: 4174.078125\n",
      "Epoch 9869, Train_Loss: 4041.2109375, Val_Loss: 4174.0224609375\n",
      "Epoch 9870, Train_Loss: 4041.16259765625, Val_Loss: 4173.9658203125\n",
      "Epoch 9871, Train_Loss: 4041.114501953125, Val_Loss: 4173.90966796875\n",
      "Epoch 9872, Train_Loss: 4041.055908203125, Val_Loss: 4173.84375\n",
      "Epoch 9873, Train_Loss: 4041.00341796875, Val_Loss: 4173.78662109375\n",
      "Epoch 9874, Train_Loss: 4040.948974609375, Val_Loss: 4173.70703125\n",
      "Epoch 9875, Train_Loss: 4040.898193359375, Val_Loss: 4173.65185546875\n",
      "Epoch 9876, Train_Loss: 4040.848876953125, Val_Loss: 4173.58984375\n",
      "Epoch 9877, Train_Loss: 4040.793212890625, Val_Loss: 4173.52783203125\n",
      "Epoch 9878, Train_Loss: 4040.718994140625, Val_Loss: 4173.43896484375\n",
      "Epoch 9879, Train_Loss: 4040.66552734375, Val_Loss: 4173.3857421875\n",
      "Epoch 9880, Train_Loss: 4040.626220703125, Val_Loss: 4173.32275390625\n",
      "Epoch 9881, Train_Loss: 4040.56787109375, Val_Loss: 4173.25341796875\n",
      "Epoch 9882, Train_Loss: 4040.517822265625, Val_Loss: 4173.1943359375\n",
      "Epoch 9883, Train_Loss: 4040.475830078125, Val_Loss: 4173.1357421875\n",
      "Epoch 9884, Train_Loss: 4040.440185546875, Val_Loss: 4173.076171875\n",
      "Epoch 9885, Train_Loss: 4040.38671875, Val_Loss: 4173.0244140625\n",
      "Epoch 9886, Train_Loss: 4040.340087890625, Val_Loss: 4172.96240234375\n",
      "Epoch 9887, Train_Loss: 4040.28857421875, Val_Loss: 4172.89306640625\n",
      "Epoch 9888, Train_Loss: 4040.241943359375, Val_Loss: 4172.8505859375\n",
      "Epoch 9889, Train_Loss: 4040.1953125, Val_Loss: 4172.78173828125\n",
      "Epoch 9890, Train_Loss: 4040.153076171875, Val_Loss: 4172.712890625\n",
      "Epoch 9891, Train_Loss: 4040.1083984375, Val_Loss: 4172.65185546875\n",
      "Epoch 9892, Train_Loss: 4040.05078125, Val_Loss: 4172.6015625\n",
      "Epoch 9893, Train_Loss: 4039.978515625, Val_Loss: 4172.54638671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9894, Train_Loss: 4039.926025390625, Val_Loss: 4172.48388671875\n",
      "Epoch 9895, Train_Loss: 4039.87939453125, Val_Loss: 4172.42578125\n",
      "Epoch 9896, Train_Loss: 4039.874755859375, Val_Loss: 4172.359375\n",
      "Epoch 9897, Train_Loss: 4039.8291015625, Val_Loss: 4172.29345703125\n",
      "Epoch 9898, Train_Loss: 4039.767822265625, Val_Loss: 4172.23876953125\n",
      "Epoch 9899, Train_Loss: 4039.589111328125, Val_Loss: 4172.16943359375\n",
      "Epoch 9900, Train_Loss: 4039.537841796875, Val_Loss: 4172.12939453125\n",
      "Epoch 9901, Train_Loss: 4039.496337890625, Val_Loss: 4172.07275390625\n",
      "Epoch 9902, Train_Loss: 4039.445068359375, Val_Loss: 4172.009765625\n",
      "Epoch 9903, Train_Loss: 4039.396484375, Val_Loss: 4171.939453125\n",
      "Epoch 9904, Train_Loss: 4039.353515625, Val_Loss: 4171.87060546875\n",
      "Epoch 9905, Train_Loss: 4039.32080078125, Val_Loss: 4171.81103515625\n",
      "Epoch 9906, Train_Loss: 4039.28173828125, Val_Loss: 4171.751953125\n",
      "Epoch 9907, Train_Loss: 4039.2314453125, Val_Loss: 4171.6982421875\n",
      "Epoch 9908, Train_Loss: 4039.12353515625, Val_Loss: 4171.640625\n",
      "Epoch 9909, Train_Loss: 4039.092041015625, Val_Loss: 4171.58056640625\n",
      "Epoch 9910, Train_Loss: 4039.048583984375, Val_Loss: 4171.515625\n",
      "Epoch 9911, Train_Loss: 4038.998046875, Val_Loss: 4171.4482421875\n",
      "Epoch 9912, Train_Loss: 4038.9521484375, Val_Loss: 4171.38623046875\n",
      "Epoch 9913, Train_Loss: 4038.89697265625, Val_Loss: 4171.32861328125\n",
      "Epoch 9914, Train_Loss: 4038.849853515625, Val_Loss: 4171.2666015625\n",
      "Epoch 9915, Train_Loss: 4038.7685546875, Val_Loss: 4171.21142578125\n",
      "Epoch 9916, Train_Loss: 4038.73046875, Val_Loss: 4171.154296875\n",
      "Epoch 9917, Train_Loss: 4038.68701171875, Val_Loss: 4171.1025390625\n",
      "Epoch 9918, Train_Loss: 4038.628662109375, Val_Loss: 4171.04150390625\n",
      "Epoch 9919, Train_Loss: 4038.5751953125, Val_Loss: 4170.97216796875\n",
      "Epoch 9920, Train_Loss: 4038.48095703125, Val_Loss: 4170.875\n",
      "Epoch 9921, Train_Loss: 4038.4306640625, Val_Loss: 4170.80517578125\n",
      "Epoch 9922, Train_Loss: 4038.39404296875, Val_Loss: 4170.74951171875\n",
      "Epoch 9923, Train_Loss: 4038.318603515625, Val_Loss: 4170.70166015625\n",
      "Epoch 9924, Train_Loss: 4038.27392578125, Val_Loss: 4170.6337890625\n",
      "Epoch 9925, Train_Loss: 4038.2353515625, Val_Loss: 4170.5791015625\n",
      "Epoch 9926, Train_Loss: 4038.190185546875, Val_Loss: 4170.5126953125\n",
      "Epoch 9927, Train_Loss: 4038.145751953125, Val_Loss: 4170.43896484375\n",
      "Epoch 9928, Train_Loss: 4038.102294921875, Val_Loss: 4170.38916015625\n",
      "Epoch 9929, Train_Loss: 4038.048828125, Val_Loss: 4170.32666015625\n",
      "Epoch 9930, Train_Loss: 4038.001708984375, Val_Loss: 4170.28857421875\n",
      "Epoch 9931, Train_Loss: 4037.952392578125, Val_Loss: 4170.232421875\n",
      "Epoch 9932, Train_Loss: 4037.895751953125, Val_Loss: 4170.17529296875\n",
      "Epoch 9933, Train_Loss: 4037.854736328125, Val_Loss: 4170.11083984375\n",
      "Epoch 9934, Train_Loss: 4037.819091796875, Val_Loss: 4170.04052734375\n",
      "Epoch 9935, Train_Loss: 4037.7724609375, Val_Loss: 4169.9833984375\n",
      "Epoch 9936, Train_Loss: 4037.720703125, Val_Loss: 4169.91357421875\n",
      "Epoch 9937, Train_Loss: 4037.665283203125, Val_Loss: 4169.86181640625\n",
      "Epoch 9938, Train_Loss: 4037.60009765625, Val_Loss: 4169.80517578125\n",
      "Epoch 9939, Train_Loss: 4037.560791015625, Val_Loss: 4169.7490234375\n",
      "Epoch 9940, Train_Loss: 4037.5283203125, Val_Loss: 4169.69580078125\n",
      "Epoch 9941, Train_Loss: 4037.4755859375, Val_Loss: 4169.61572265625\n",
      "Epoch 9942, Train_Loss: 4037.4248046875, Val_Loss: 4169.564453125\n",
      "Epoch 9943, Train_Loss: 4037.369140625, Val_Loss: 4169.49462890625\n",
      "Epoch 9944, Train_Loss: 4037.322998046875, Val_Loss: 4169.42822265625\n",
      "Epoch 9945, Train_Loss: 4037.26904296875, Val_Loss: 4169.38037109375\n",
      "Epoch 9946, Train_Loss: 4037.22509765625, Val_Loss: 4169.3291015625\n",
      "Epoch 9947, Train_Loss: 4037.179931640625, Val_Loss: 4169.2646484375\n",
      "Epoch 9948, Train_Loss: 4037.127197265625, Val_Loss: 4169.208984375\n",
      "Epoch 9949, Train_Loss: 4037.078369140625, Val_Loss: 4169.1318359375\n",
      "Epoch 9950, Train_Loss: 4037.043212890625, Val_Loss: 4169.07275390625\n",
      "Epoch 9951, Train_Loss: 4036.999267578125, Val_Loss: 4169.0224609375\n",
      "Epoch 9952, Train_Loss: 4036.955078125, Val_Loss: 4168.95458984375\n",
      "Epoch 9953, Train_Loss: 4036.877197265625, Val_Loss: 4168.90869140625\n",
      "Epoch 9954, Train_Loss: 4036.8310546875, Val_Loss: 4168.84619140625\n",
      "Epoch 9955, Train_Loss: 4036.8037109375, Val_Loss: 4168.7861328125\n",
      "Epoch 9956, Train_Loss: 4036.755615234375, Val_Loss: 4168.72265625\n",
      "Epoch 9957, Train_Loss: 4036.75146484375, Val_Loss: 4168.65478515625\n",
      "Epoch 9958, Train_Loss: 4036.7158203125, Val_Loss: 4168.59521484375\n",
      "Epoch 9959, Train_Loss: 4036.669677734375, Val_Loss: 4168.54345703125\n",
      "Epoch 9960, Train_Loss: 4036.59375, Val_Loss: 4168.4921875\n",
      "Epoch 9961, Train_Loss: 4036.542236328125, Val_Loss: 4168.43896484375\n",
      "Epoch 9962, Train_Loss: 4036.475830078125, Val_Loss: 4168.33251953125\n",
      "Epoch 9963, Train_Loss: 4036.43603515625, Val_Loss: 4168.28271484375\n",
      "Epoch 9964, Train_Loss: 4036.39599609375, Val_Loss: 4168.21435546875\n",
      "Epoch 9965, Train_Loss: 4036.35986328125, Val_Loss: 4168.146484375\n",
      "Epoch 9966, Train_Loss: 4036.312744140625, Val_Loss: 4168.0947265625\n",
      "Epoch 9967, Train_Loss: 4036.26611328125, Val_Loss: 4168.02392578125\n",
      "Epoch 9968, Train_Loss: 4036.16357421875, Val_Loss: 4167.9814453125\n",
      "Epoch 9969, Train_Loss: 4036.1279296875, Val_Loss: 4167.92919921875\n",
      "Epoch 9970, Train_Loss: 4036.096435546875, Val_Loss: 4167.859375\n",
      "Epoch 9971, Train_Loss: 4036.033447265625, Val_Loss: 4167.79638671875\n",
      "Epoch 9972, Train_Loss: 4035.994140625, Val_Loss: 4167.73779296875\n",
      "Epoch 9973, Train_Loss: 4035.947998046875, Val_Loss: 4167.685546875\n",
      "Epoch 9974, Train_Loss: 4035.897705078125, Val_Loss: 4167.63525390625\n",
      "Epoch 9975, Train_Loss: 4035.85498046875, Val_Loss: 4167.58203125\n",
      "Epoch 9976, Train_Loss: 4035.817626953125, Val_Loss: 4167.52099609375\n",
      "Epoch 9977, Train_Loss: 4035.76220703125, Val_Loss: 4167.45458984375\n",
      "Epoch 9978, Train_Loss: 4035.71240234375, Val_Loss: 4167.4033203125\n",
      "Epoch 9979, Train_Loss: 4035.66845703125, Val_Loss: 4167.34130859375\n",
      "Epoch 9980, Train_Loss: 4035.6181640625, Val_Loss: 4167.27685546875\n",
      "Epoch 9981, Train_Loss: 4035.5693359375, Val_Loss: 4167.21337890625\n",
      "Epoch 9982, Train_Loss: 4035.521484375, Val_Loss: 4167.16748046875\n",
      "Epoch 9983, Train_Loss: 4035.364990234375, Val_Loss: 4167.1083984375\n",
      "Epoch 9984, Train_Loss: 4035.31982421875, Val_Loss: 4167.04150390625\n",
      "Epoch 9985, Train_Loss: 4035.27001953125, Val_Loss: 4166.98779296875\n",
      "Epoch 9986, Train_Loss: 4035.216064453125, Val_Loss: 4166.9208984375\n",
      "Epoch 9987, Train_Loss: 4035.1630859375, Val_Loss: 4166.85498046875\n",
      "Epoch 9988, Train_Loss: 4035.127197265625, Val_Loss: 4166.7978515625\n",
      "Epoch 9989, Train_Loss: 4035.0751953125, Val_Loss: 4166.73828125\n",
      "Epoch 9990, Train_Loss: 4035.03125, Val_Loss: 4166.68017578125\n",
      "Epoch 9991, Train_Loss: 4034.977783203125, Val_Loss: 4166.63232421875\n",
      "Epoch 9992, Train_Loss: 4034.937255859375, Val_Loss: 4166.57470703125\n",
      "Epoch 9993, Train_Loss: 4034.89599609375, Val_Loss: 4166.5087890625\n",
      "Epoch 9994, Train_Loss: 4034.8388671875, Val_Loss: 4166.44384765625\n",
      "Epoch 9995, Train_Loss: 4034.7939453125, Val_Loss: 4166.3857421875\n",
      "Epoch 9996, Train_Loss: 4034.749755859375, Val_Loss: 4166.3330078125\n",
      "Epoch 9997, Train_Loss: 4034.704833984375, Val_Loss: 4166.27001953125\n",
      "Epoch 9998, Train_Loss: 4034.627685546875, Val_Loss: 4166.2255859375\n",
      "Epoch 9999, Train_Loss: 4034.57958984375, Val_Loss: 4166.154296875\n",
      "Test Loss: 3936.1962890625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3936.1963)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train nn module -- ONLY APPROVED CLAIMS\n",
    "assessor_config =  config['assessor']\n",
    "assessor_config['simple_nn']['epochs'] = 10000\n",
    "assessor_config['model'] = 'simple_nn'\n",
    "assessor = HomeAssessor(assessor_config)\n",
    "\n",
    "train_calculate_test_loss(assessor, \n",
    "                          train_test_val_dataset_nz['train_x'], \n",
    "                          train_test_val_dataset_nz['train_y'],\n",
    "                          train_test_val_dataset_nz['test_x'],\n",
    "                          train_test_val_dataset_nz['test_y'],\n",
    "                          train_test_val_dataset_nz['val_x'],\n",
    "                          train_test_val_dataset_nz['val_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
